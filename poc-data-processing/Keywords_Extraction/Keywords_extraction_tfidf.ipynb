{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d30f0f53-a5eb-496b-9e16-64d53e3a136b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PartitionTextFile\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the large text file\n",
    "input_path = \"g_brf_sum_text_2024.tsv\"\n",
    "output_path = \"./output_summary_partitioned\"\n",
    "\n",
    "# Read the large text file\n",
    "text_rdd = spark.sparkContext.textFile(input_path)\n",
    "\n",
    "# Optional: Maintain order using zipWithIndex and repartition based on keys\n",
    "text_rdd_with_index = text_rdd.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Number of output files (partitions)\n",
    "num_partitions = 10\n",
    "\n",
    "# Repartition the RDD to control the number of output files\n",
    "partitioned_rdd = text_rdd_with_index.repartition(num_partitions).sortByKey().values()\n",
    "\n",
    "# Save partitioned files to the output directory\n",
    "partitioned_rdd.saveAsTextFile(output_path)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6edf41c1-0dea-48bf-a031-925ac7ac0263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 18:28:59 WARN TaskSetManager: Stage 1 contains a task of very large size (49659 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:29:05 WARN TaskSetManager: Stage 3 contains a task of very large size (49659 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:29:09 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 3 (TID 18): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|patent_id|            keywords|\n",
      "+---------+--------------------+\n",
      "| 11990388|[cooling, supplie...|\n",
      "| 11990389|[device, interpos...|\n",
      "| 11990390|[\"technical, fiel...|\n",
      "| 11990391|[field, \"technica...|\n",
      "| 11990392|[inductance, pate...|\n",
      "| 11990393|[first, \"technica...|\n",
      "| 11990394|[mentioned, reduc...|\n",
      "| 11990395|[example,, soluti...|\n",
      "| 11990396|[assembly., metal...|\n",
      "| 11990397|[issue, conductiv...|\n",
      "+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 18:29:09 WARN TaskSetManager: Stage 4 contains a task of very large size (49659 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame output_summary_partitioned/part-00005 saved to output_keywords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 18:29:25 WARN TaskSetManager: Stage 1 contains a task of very large size (59063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:29:32 WARN TaskSetManager: Stage 3 contains a task of very large size (59063 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:29:36 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 3 (TID 18): Attempting to kill Python Worker\n",
      "24/11/21 18:29:36 WARN TaskSetManager: Stage 4 contains a task of very large size (59063 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|patent_id|            keywords|\n",
      "+---------+--------------------+\n",
      "| 11906438|[\"background, , h...|\n",
      "| 11906439|[\"cross-reference...|\n",
      "| 11906440|[\"cross-reference...|\n",
      "| 11906441|[execute, geometr...|\n",
      "| 11906442|[difficult, displ...|\n",
      "| 11906443|[accurately, refl...|\n",
      "| 11906444|[result., necessa...|\n",
      "| 11906445|[data, location, ...|\n",
      "| 11906446|[view, inspection...|\n",
      "| 11906447|[imaging, least, ...|\n",
      "+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame output_summary_partitioned/part-00002 saved to output_keywords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 18:29:55 WARN TaskSetManager: Stage 1 contains a task of very large size (78920 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:30:01 WARN TaskSetManager: Stage 3 contains a task of very large size (78920 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:30:05 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 3 (TID 18): Attempting to kill Python Worker\n",
      "24/11/21 18:30:06 WARN TaskSetManager: Stage 4 contains a task of very large size (78920 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|patent_id|            keywords|\n",
      "+---------+--------------------+\n",
      "| 11937697|[\"background, , 1...|\n",
      "| 11937698|[vibrations, matt...|\n",
      "| 11937699|[unit, frame., ma...|\n",
      "| 11937700|[body., painful.,...|\n",
      "| 11937701|[(e.g.,, pressure...|\n",
      "| 11937702|[escaping, mattre...|\n",
      "| 11937703|[wall, pressure.,...|\n",
      "| 11937704|[air, mattresses,...|\n",
      "| 11937705|[fast, signal, in...|\n",
      "| 11937706|[mattress, patien...|\n",
      "+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame output_summary_partitioned/part-00003 saved to output_keywords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 18:30:23 WARN TaskSetManager: Stage 1 contains a task of very large size (54333 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:30:27 WARN TaskSetManager: Stage 3 contains a task of very large size (54333 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:30:31 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 3 (TID 18): Attempting to kill Python Worker\n",
      "24/11/21 18:30:31 WARN TaskSetManager: Stage 4 contains a task of very large size (54333 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|patent_id|            keywords|\n",
      "+---------+--------------------+\n",
      "| 11968511|[openings, circui...|\n",
      "| 11968512|[\"background, , i...|\n",
      "| 11968513|[mobile, facing, ...|\n",
      "| 11968514|[mobile, two, mag...|\n",
      "| 11968515|[hole;, deformati...|\n",
      "| 11968516|[sound, , sound, ...|\n",
      "| 11968517|[first, method, ,...|\n",
      "| 11968518|[monopole, depend...|\n",
      "| 11968519|[executed, device...|\n",
      "| 11968520|[element, indicat...|\n",
      "+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame output_summary_partitioned/part-00004 saved to output_keywords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 18:30:45 WARN TaskSetManager: Stage 1 contains a task of very large size (43004 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:30:49 WARN TaskSetManager: Stage 3 contains a task of very large size (43004 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:30:53 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 3 (TID 18): Attempting to kill Python Worker\n",
      "24/11/21 18:30:53 WARN TaskSetManager: Stage 4 contains a task of very large size (43004 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|patent_id|            keywords|\n",
      "+---------+--------------------+\n",
      "| 12053941|[locking, plurali...|\n",
      "| 12053942|[manufacturing, l...|\n",
      "| 12053943|[create, area, th...|\n",
      "| 12053944|[development, lig...|\n",
      "| 12053945|[weight, thermofo...|\n",
      "| 12053946|[mm., paraffin,, ...|\n",
      "| 12053947|[“weld”., second,...|\n",
      "| 12053948|[sockliners,, thr...|\n",
      "| 12053949|[installation,, i...|\n",
      "| 12053950|[items, digital, ...|\n",
      "+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame output_summary_partitioned/part-00008 saved to output_keywords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 18:31:06 WARN TaskSetManager: Stage 1 contains a task of very large size (41666 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:31:09 WARN TaskSetManager: Stage 3 contains a task of very large size (41666 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:31:13 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 3 (TID 18): Attempting to kill Python Worker\n",
      "24/11/21 18:31:13 WARN TaskSetManager: Stage 4 contains a task of very large size (41666 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|patent_id|            keywords|\n",
      "+---------+--------------------+\n",
      "| 11881523|[subcollector,, d...|\n",
      "| 11881524|[electrode, chip,...|\n",
      "| 11881525|[semiconductor, b...|\n",
      "| 11881526|[surface, type, \"...|\n",
      "| 11881527|[region, guard, r...|\n",
      "| 11881528|[in,, apparent, d...|\n",
      "| 11881529|[semiconductor, s...|\n",
      "| 11881530|[\"background, , a...|\n",
      "| 11881531|[array, , dispose...|\n",
      "| 11881532|[terminal, solar,...|\n",
      "+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame output_summary_partitioned/part-00001 saved to output_keywords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 18:31:25 WARN TaskSetManager: Stage 1 contains a task of very large size (48957 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:31:29 WARN TaskSetManager: Stage 3 contains a task of very large size (48957 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:31:33 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 3 (TID 18): Attempting to kill Python Worker\n",
      "24/11/21 18:31:33 WARN TaskSetManager: Stage 4 contains a task of very large size (48957 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|patent_id|            keywords|\n",
      "+---------+--------------------+\n",
      "| 12010587|[walls,, signal, ...|\n",
      "| 12010588|[aspect,, state,,...|\n",
      "| 12010589|[\"field, disclosu...|\n",
      "| 12010590|[station, referen...|\n",
      "| 12010591|[(mbms), setup”,,...|\n",
      "| 12010592|[module, system, ...|\n",
      "| 12010593|[interest, switch...|\n",
      "| 12010594|[aspect, receivin...|\n",
      "| 12010595|[associated, inst...|\n",
      "| 12010596|[safety-related, ...|\n",
      "+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame output_summary_partitioned/part-00006 saved to output_keywords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 18:31:46 WARN TaskSetManager: Stage 1 contains a task of very large size (65497 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:31:50 WARN TaskSetManager: Stage 3 contains a task of very large size (65497 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:31:54 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 3 (TID 18): Attempting to kill Python Worker\n",
      "24/11/21 18:31:55 WARN TaskSetManager: Stage 4 contains a task of very large size (65497 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|patent_id|            keywords|\n",
      "+---------+--------------------+\n",
      "| 12029594|[selected, probe,...|\n",
      "| 12029595|[directed, substr...|\n",
      "| 12029596|[axis, e.g., two,...|\n",
      "| 12029597|[mri, receiver, c...|\n",
      "| 12029599|[distribution, as...|\n",
      "| 12029600|[comprise, values...|\n",
      "| 12029601|[sentinel, second...|\n",
      "| 12029602|[breast, image;, ...|\n",
      "| 12029603|[radiographic, re...|\n",
      "| 12029604|[target, detectio...|\n",
      "+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame output_summary_partitioned/part-00007 saved to output_keywords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 18:32:10 WARN TaskSetManager: Stage 1 contains a task of very large size (81563 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:32:16 WARN TaskSetManager: Stage 3 contains a task of very large size (81563 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:32:20 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 3 (TID 18): Attempting to kill Python Worker\n",
      "24/11/21 18:32:20 WARN TaskSetManager: Stage 4 contains a task of very large size (81563 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|patent_id|            keywords|\n",
      "+---------+--------------------+\n",
      "|  PP35566|[consisting, plan...|\n",
      "|  PP35567|[, \"genus, specie...|\n",
      "|  PP35568|[variety, , genus...|\n",
      "|  RE49771|[thread, , screw,...|\n",
      "|  RE49772|[locomotion, pola...|\n",
      "|  RE49773|[two, hence, engi...|\n",
      "|  RE49774|[hapten-antibody,...|\n",
      "|  RE49775|[engines,, repeat...|\n",
      "|  RE49776|[axis, lamp, pate...|\n",
      "|  RE49777|[state, case, cal...|\n",
      "+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame output_summary_partitioned/part-00000 saved to output_keywords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 18:32:40 WARN TaskSetManager: Stage 1 contains a task of very large size (65076 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:32:46 WARN TaskSetManager: Stage 3 contains a task of very large size (65076 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/11/21 18:32:50 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 3 (TID 18): Attempting to kill Python Worker\n",
      "24/11/21 18:32:50 WARN TaskSetManager: Stage 4 contains a task of very large size (65076 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|patent_id|            keywords|\n",
      "+---------+--------------------+\n",
      "| 12068339|[development, cir...|\n",
      "| 12068340|[front, image, ar...|\n",
      "| 12068341|[silicon, present...|\n",
      "| 12068342|[however,, predet...|\n",
      "| 12068343|[isolation, recen...|\n",
      "| 12068344|[shielding, proce...|\n",
      "| 12068345|[surface, various...|\n",
      "| 12068346|[moisture, coveri...|\n",
      "| 12068347|[light,, light, w...|\n",
      "| 12068348|[mold, includes, ...|\n",
      "+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame output_summary_partitioned/part-00009 saved to output_keywords\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import udf, array_sort, slice, col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "import os\n",
    "import gc\n",
    "\n",
    "def list_files_in_directory(directory_path, extension):\n",
    "    try:\n",
    "        files = [\n",
    "            f for f in os.listdir(directory_path)\n",
    "            if os.path.isfile(os.path.join(directory_path, f))  # Ensure it's a file\n",
    "            and (extension == '' or f.endswith(extension))  # Filter by extension if provided\n",
    "            and not f.startswith('.')  # Exclude hidden files\n",
    "            and not f.endswith('.crc')  # Exclude .crc files\n",
    "            and os.path.getsize(os.path.join(directory_path, f)) > 0  # Exclude empty files\n",
    "        ]\n",
    "        return files\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The directory {directory_path} was not found.\")\n",
    "        return []\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied to access the directory {directory_path}.\")\n",
    "        return []\n",
    "\n",
    "def extract_keywords_save_to_file(input_file):\n",
    "    # Initialize a Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"Key words extraction\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    lines_df = spark.read.text(input_file)\n",
    "    \n",
    "    # Step 2: Skip the header and process the lines\n",
    "    lines = [row[\"value\"] for row in lines_df.collect()][1:]  # Skip the header line\n",
    "    \n",
    "    # Step 3: Extract patent_id and summary_text\n",
    "    data = []\n",
    "    current_id = None\n",
    "    current_summary = []\n",
    "    \n",
    "    for line in lines:\n",
    "        try:\n",
    "            if line.startswith('\"'):\n",
    "                if current_id is not None and current_summary:\n",
    "                    # Save the current record\n",
    "                    data.append((current_id, \" \".join(current_summary).strip()))\n",
    "                # Extract the new patent_id\n",
    "                current_id = line.split('\"')[1]  # First text block inside quotes\n",
    "                # Extract the start of the summary text\n",
    "                current_summary = [line.split('\"', 2)[2].strip()] if '\"' in line else []\n",
    "            else:\n",
    "                # Add subsequent lines to the summary\n",
    "                current_summary.append(line.strip())\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Append the last record\n",
    "    if current_id is not None and current_summary:\n",
    "        data.append((current_id, \" \".join(current_summary).strip()))\n",
    "    \n",
    "    # Step 4: Create DataFrame\n",
    "    df = spark.createDataFrame(data, [\"patent_id\", \"summary_text\"])\n",
    "    \n",
    "    df = df.filter(col(\"patent_id\") != '')\n",
    "    \n",
    "    tokenizer = Tokenizer(inputCol=\"summary_text\", outputCol=\"words\")\n",
    "    df_tokens = tokenizer.transform(df)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "    df_filtered = stopwords_remover.transform(df_tokens)\n",
    "    \n",
    "    # Compute TF (Term Frequency)\n",
    "    hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=10000)\n",
    "    df_tf = hashing_tf.transform(df_filtered)\n",
    "    \n",
    "    # Compute IDF (Inverse Document Frequency)\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "    idf_model = idf.fit(df_tf)\n",
    "    df_tfidf = idf_model.transform(df_tf)\n",
    "    \n",
    "    # Extract Keywords\n",
    "    def extract_top_keywords(features, words, n=20):\n",
    "        # Map TF-IDF scores to words\n",
    "        tfidf_scores = list(zip(words, features.toArray()))\n",
    "        # Sort by score in descending order and take top n\n",
    "        sorted_keywords = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "        return [keyword for keyword, score in sorted_keywords[:n]]\n",
    "    \n",
    "    # Register UDF for extracting top keywords\n",
    "    extract_keywords_udf = udf(lambda features, words: extract_top_keywords(features, words), ArrayType(StringType()))\n",
    "    \n",
    "    # Apply the UDF to extract top keywords\n",
    "    df_keywords = df_tfidf.withColumn(\n",
    "        \"keywords\",\n",
    "        extract_keywords_udf(col(\"features\"), col(\"filtered_words\"))\n",
    "    )\n",
    "    \n",
    "    # Select relevant columns and show results\n",
    "    df_keywords.select(\"patent_id\", \"keywords\").show(10)\n",
    "    \n",
    "    from pyspark.sql.functions import concat_ws\n",
    "    \n",
    "    # Convert the 'keywords' column (array) into a single string\n",
    "    df_keywords_csv = df_keywords.withColumn(\"keywords\", concat_ws(\", \", \"keywords\"))\n",
    "    \n",
    "    # Write the DataFrame to a CSV file\n",
    "    output_path = \"output_keywords\"\n",
    "    df_keywords_csv.select(\"patent_id\", \"keywords\").write.csv(output_path, header=True, mode=\"append\")\n",
    "    \n",
    "    print(f\"DataFrame {input_file} saved to {output_path}\")\n",
    "\n",
    "    del df\n",
    "    del df_tokens\n",
    "    del df_filtered\n",
    "    del df_tfidf\n",
    "    del df_tf\n",
    "    del df_keywords\n",
    "    del df_keywords_csv\n",
    "    spark.catalog.clearCache()\n",
    "    spark.stop()\n",
    "    gc.collect()\n",
    "\n",
    "directory_path = 'output_summary_partitioned'\n",
    "extension = ''\n",
    "files = list_files_in_directory(directory_path, extension)\n",
    "for file in files:\n",
    "    extract_keywords_save_to_file(os.path.join(directory_path, file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c13598e8-3f30-4565-9ac2-214c962eacf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of concatenated data:\n",
      "root\n",
      " |-- patent_id: string (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Concatenate CSV Files\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Load all CSV files from the directory\n",
    "input_directory = \"./output_keywords\"\n",
    "all_csv_files = spark.read.csv(input_directory, header=True, inferSchema=True)\n",
    "\n",
    "# Optional: Preview the data\n",
    "print(\"Schema of concatenated data:\")\n",
    "all_csv_files.printSchema()\n",
    "\n",
    "# Step 3: Write the combined data into smaller files\n",
    "output_directory = \"./output_keywords_smaller_files\"\n",
    "number_of_partitions = 8  # Adjust the number of partitions as needed\n",
    "all_csv_files.repartition(number_of_partitions).write.csv(output_directory, header=True)\n",
    "\n",
    "# Stop the Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb6221c-63f7-4e93-8dbb-44f3891399e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/21 18:52:09 WARN Utils: Your hostname, MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.33 instead (on interface en0)\n",
      "24/11/21 18:52:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/21 18:52:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/21 18:52:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema and cleaned data:\n",
      "root\n",
      " |-- patent_id: string (nullable = true)\n",
      " |-- keywords: string (nullable = true)\n",
      "\n",
      "+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|patent_id|keywords                                                                                                                                                                                             |\n",
      "+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|11919419 |cell, hev, abnormal, cell, distinct, chiller, battery, module, , includes, battery, groups, battery, includes, cell, battery, abnormal, distinct, abnormal, embodiments                              |\n",
      "|11887314 |features, device, device, frame-level, similarity, smart, comprising, background, , devices, cameras, network, connectivity, eg, camera, devices, common, devices, include, security                 |\n",
      "|11929824 |change, units, size, downlink, satellite, “signal, high, fig1shows, entitled, well, two, polarized, signal, filters9, avoid, driven, personal, band, two, art                                        |\n",
      "|11885590 |clothing, channeled, holster, height, used, attached, sitting, pocket, field, invention, , invention, generally, relates, holsters, carrying, objects, handguns, knives, tools                       |\n",
      "|11944014 |summary, enhancement, entirety, device, magnetic, cross-reference, related, application, , application, based, claims, priority, 35, usc, §, 119, korean, patent, application                        |\n",
      "|11921449 |slide, fixing, features, image, configured, fixing, background, invention, , field, invention, , present, invention, relates, apparatus, preferably, used, forming, apparatus                        |\n",
      "|11912852 |conductor, making, 60, ref, covered, , reactor, preferably, metal, insulated, composition, high, zone, wire, tubular, increases, less, sheaths, peroxide-crosslinked, making                         |\n",
      "|11861154 |user, drawing, mobile, engine, another, edit, environments, publication, products, data, object, policies, second, experience, systems, field, invention, , field, invention                         |\n",
      "|11923379 |film, intersects, , embodiments, second, one-time, line, connecting, disclosure, gate, lines, includes:, bridges, plurality, technical, field, , present, disclosure, relates                        |\n",
      "|11931424 |antibody, example, 5-10, containing, heteroatom, field, , application, generally, relates, combination, therapies, mcl-1, inhibitors, antibody-drug, conjugates, adcs, treatment, cancers, particular|\n",
      "+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, trim\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Clean Keywords in CSV Files\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Load all CSV files from the directory\n",
    "input_directory = \"./output_keywords_smaller_files\"\n",
    "df = spark.read.csv(input_directory, header=True, inferSchema=True)\n",
    "\n",
    "# Step 3: Clean the 'keywords' column\n",
    "# Remove unwanted characters like quotes and ensure comma-separated keywords\n",
    "df_cleaned = df.withColumn(\n",
    "    \"keywords\",\n",
    "    # Remove quotes, brackets, and dots; fix double commas\n",
    "    regexp_replace(\n",
    "        col(\"keywords\"),\n",
    "        r\"[\\\"\\'\\[\\]\\{\\}\\(\\)\\.]\", \"\"  # Remove unwanted characters\n",
    "    )\n",
    ").withColumn(\n",
    "    \"keywords\",\n",
    "    regexp_replace(col(\"keywords\"), r\",,+\", \",\")  # Replace double commas with a single comma\n",
    ").withColumn(\n",
    "    \"keywords\",\n",
    "    trim(regexp_replace(col(\"keywords\"), r\"(^,|,$)\", \"\"))  # Remove leading/trailing commas\n",
    ")\n",
    "\n",
    "# Optional: Preview the cleaned data\n",
    "print(\"Schema and cleaned data:\")\n",
    "df_cleaned.printSchema()\n",
    "df_cleaned.show(10, truncate=False)\n",
    "\n",
    "# Step 4: Write the cleaned data into smaller files\n",
    "output_directory = \"./output_keywords_cleaned_files\"\n",
    "number_of_partitions = 8  # Adjust the number of partitions as needed\n",
    "df_cleaned.repartition(number_of_partitions).write.csv(output_directory, header=True)\n",
    "\n",
    "# Stop the Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4e45a0-3f18-4359-abc6-5b0183c36ded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
