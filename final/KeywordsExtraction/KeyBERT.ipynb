{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59bd5f6b-261b-44df-8bb0-d1765391b019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/27 19:37:37 WARN Utils: Your hostname, MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.33 instead (on interface en0)\n",
      "24/11/27 19:37:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/27 19:37:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KeyBERT\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a7c585-b2a9-4183-93b9-cd3091fa30bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/g_brf_sum_text_2024.tsv\"\n",
    "output_path = \"data/summary_partitioned\"\n",
    "\n",
    "# Read the large text file\n",
    "text_rdd = spark.sparkContext.textFile(input_path)\n",
    "\n",
    "# Optional: Maintain order using zipWithIndex and repartition based on keys\n",
    "text_rdd_with_index = text_rdd.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "\n",
    "# Number of output files (partitions)\n",
    "num_partitions = 10\n",
    "\n",
    "# Repartition the RDD to control the number of output files\n",
    "partitioned_rdd = text_rdd_with_index.repartition(num_partitions).sortByKey().values()\n",
    "\n",
    "# Save partitioned files to the output directory\n",
    "partitioned_rdd.saveAsTextFile(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2a84140-957d-411c-9697-42309bed71b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|patent_id|\n",
      "+---------+\n",
      "| 11966422|\n",
      "| 11966824|\n",
      "| 11880729|\n",
      "| 12061966|\n",
      "| 11928212|\n",
      "| 11928737|\n",
      "| 11886956|\n",
      "| 11887494|\n",
      "| 11887599|\n",
      "| 12067651|\n",
      "+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Total number of patent IDs: 42530\n"
     ]
    }
   ],
   "source": [
    "filtered_patents_id_path = \"data/patent_ids\"\n",
    "\n",
    "patent_ids_df = spark.read.parquet(input_parquet_path)\n",
    "\n",
    "patent_ids_df.show(10)\n",
    "patent_ids_count = patent_ids_df.count()\n",
    "\n",
    "print(f\"Total number of patent IDs: {patent_ids_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "446a32d4-68e2-4040-aa37-2a4698043cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_ids = patent_ids_df.select(\"patent_id\").rdd.flatMap(lambda row: row).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44410889-84e6-41d1-b226-790ea213f980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhland/miniforge3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from keybert import KeyBERT\n",
    "\n",
    "\n",
    "\n",
    "def extract_keywords_from_dataframe(spark_df: DataFrame, \n",
    "                                    text_col: str, \n",
    "                                    id_col: str, \n",
    "                                    num_keywords: int = 5, \n",
    "                                    model_name: str = 'all-MiniLM-L6-v2') -> DataFrame:\n",
    "\n",
    "    print(\"here\")\n",
    "    # Initialize KeyBERT model\n",
    "    kw_model = KeyBERT(model=model_name)\n",
    "\n",
    "    # Collect data from the Spark DataFrame\n",
    "    data_collected = spark_df.select(id_col, text_col).collect()\n",
    "\n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for row in data_collected:\n",
    "        patent_id = row[id_col]\n",
    "        text = row[text_col]\n",
    "    \n",
    "        # Extract keywords\n",
    "        keywords = kw_model.extract_keywords(\n",
    "            text,\n",
    "            keyphrase_ngram_range=(1, 1),\n",
    "            top_n=num_keywords\n",
    "        )\n",
    "\n",
    "        # Append results\n",
    "        results.append((patent_id, [(kw[0], kw[1]) for kw in keywords]))\n",
    "\n",
    "    # Convert results back to Spark DataFrame\n",
    "    result_df = spark.createDataFrame(results, [id_col, \"keywords\"])\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "778049bd-d453-4ce6-b5d9-2a2509bab09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_summary_partitioned/part-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/27 19:38:09 WARN TaskSetManager: Stage 7 contains a task of very large size (41666 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2944\n",
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/27 19:38:12 WARN TaskSetManager: Stage 10 contains a task of very large size (41666 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|patent_id|keywords                                                                                                         |\n",
      "+---------+-----------------------------------------------------------------------------------------------------------------+\n",
      "|11881627 |[{satellites, 0.5401}, {satellite, 0.4685}, {antennas, 0.4384}, {antenna, 0.4092}, {radiated, 0.3852}]           |\n",
      "|11881629 |[{antenna, 0.4533}, {transmitting, 0.4325}, {wireless, 0.4203}, {polarization, 0.3885}, {transmits, 0.3674}]     |\n",
      "|11881714 |[{lte, 0.5121}, {telecommunication, 0.5106}, {communications, 0.4669}, {wireless, 0.4654}, {transmit, 0.4562}]   |\n",
      "|11881727 |[{wireless, 0.4695}, {charging, 0.4662}, {devices, 0.4488}, {telephone, 0.4463}, {telephones, 0.4413}]           |\n",
      "|11881758 |[{transformer, 0.5703}, {display, 0.3971}, {converter, 0.3824}, {circuits, 0.3052}, {voltages, 0.302}]           |\n",
      "|11881818 |[{amplifiers, 0.5236}, {wireless, 0.4904}, {rf, 0.4867}, {amplifier, 0.4797}, {antennas, 0.4724}]                |\n",
      "|11881885 |[{antenna, 0.5902}, {transmitting, 0.4388}, {signal, 0.3978}, {transmit, 0.3968}, {compensator, 0.3964}]         |\n",
      "|11881886 |[{ghz, 0.5524}, {frequency, 0.4669}, {wireless, 0.36}, {millimeter, 0.3563}, {radio, 0.3255}]                    |\n",
      "|11881905 |[{transmitting, 0.5004}, {antenna, 0.473}, {antennas, 0.4714}, {transmit, 0.4565}, {wireless, 0.4527}]           |\n",
      "|11881909 |[{antennas, 0.5558}, {antenna, 0.5271}, {5g, 0.4881}, {interference, 0.4771}, {communications, 0.4171}]          |\n",
      "|11881910 |[{protocol, 0.4527}, {transferjet, 0.4271}, {priority, 0.356}, {transmission, 0.3405}, {technologies, 0.321}]    |\n",
      "|11881913 |[{beamforming, 0.5765}, {transmitting, 0.5309}, {antenna, 0.4901}, {wireless, 0.4687}, {transmit, 0.4662}]       |\n",
      "|11881914 |[{satellite, 0.5571}, {satellites, 0.5462}, {communications, 0.5321}, {wireless, 0.4495}, {communication, 0.425}]|\n",
      "|11881915 |[{transmit, 0.4168}, {transmitting, 0.4091}, {wireless, 0.3954}, {delay, 0.3571}, {modulation, 0.3527}]          |\n",
      "|11881916 |[{antennas, 0.4052}, {antenna, 0.385}, {transmit, 0.3772}, {precoding, 0.342}, {normalization, 0.3056}]          |\n",
      "|11881917 |[{transmit, 0.5209}, {antennas, 0.5023}, {communications, 0.4932}, {transceiver, 0.4772}, {antenna, 0.4694}]     |\n",
      "|11881919 |[{communications, 0.4484}, {transmit, 0.4373}, {wireless, 0.4196}, {transmits, 0.3975}, {radio, 0.3947}]         |\n",
      "|11881920 |[{multiplexing, 0.4843}, {communications, 0.4597}, {wireless, 0.453}, {transmitting, 0.4334}, {transmit, 0.4273}]|\n",
      "|11881921 |[{mmwave, 0.5659}, {ghz, 0.514}, {802, 0.4436}, {mhz, 0.3846}, {wireless, 0.3515}]                               |\n",
      "|11881922 |[{telecommunication, 0.511}, {lte, 0.509}, {wireless, 0.4632}, {communications, 0.4609}, {transmit, 0.4536}]     |\n",
      "+---------+-----------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "DataFrame output_summary_partitioned/part-00001 saved to output_keywords\n",
      "output_summary_partitioned/part-00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/27 19:46:20 WARN TaskSetManager: Stage 14 contains a task of very large size (81563 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5247\n",
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/27 19:46:24 WARN TaskSetManager: Stage 17 contains a task of very large size (81563 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|patent_id|keywords                                                                                                                    |\n",
      "+---------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|11856881 |[{patent, 0.434}, {detection, 0.4196}, {recognizing, 0.4027}, {classifying, 0.4011}, {photos, 0.3493}]                      |\n",
      "|11856883 |[{probes, 0.467}, {watering, 0.3555}, {vegetation, 0.3425}, {moisture, 0.3305}, {soil, 0.3174}]                             |\n",
      "|11856937 |[{herbicides, 0.3918}, {weeds, 0.3794}, {herbicide, 0.3749}, {applications, 0.3725}, {weed, 0.3632}]                        |\n",
      "|11857063 |[{wireless, 0.5168}, {wirelessly, 0.5131}, {gps, 0.4116}, {devices, 0.3897}, {antenna, 0.3857}]                             |\n",
      "|11857151 |[{endoscope, 0.5687}, {optics, 0.4215}, {objective, 0.3689}, {objectives, 0.3672}, {imaging, 0.3568}]                       |\n",
      "|11857153 |[{depth, 0.4878}, {sensing, 0.4311}, {determining, 0.4119}, {imaging, 0.3617}, {coordinate, 0.3527}]                        |\n",
      "|11857164 |[{fluorescence, 0.5855}, {fluorophores, 0.5254}, {imaging, 0.5027}, {microscope, 0.4805}, {fluorescent, 0.4583}]            |\n",
      "|11857165 |[{endoscope, 0.4547}, {imaging, 0.4444}, {endoscopes, 0.4415}, {rdi, 0.383}, {infrared, 0.3797}]                            |\n",
      "|11857255 |[{ophthalmic, 0.4379}, {anterior, 0.3621}, {eye, 0.347}, {angle, 0.3197}, {eyelid, 0.3143}]                                 |\n",
      "|11857257 |[{imaging, 0.4449}, {retina, 0.4347}, {optical, 0.3709}, {tomography, 0.3349}, {processing, 0.3267}]                        |\n",
      "|11857264 |[{spinal, 0.4467}, {spine, 0.4448}, {vertebral, 0.442}, {vertebra, 0.4091}, {surgeries, 0.4079}]                            |\n",
      "|11857271 |[{tracking, 0.4338}, {robotic, 0.3877}, {surgical, 0.3642}, {aiming, 0.323}, {triangulation, 0.3191}]                       |\n",
      "|11857273 |[{tracking, 0.4844}, {sensors, 0.4419}, {infrared, 0.4344}, {robotic, 0.4161}, {objects, 0.4065}]                           |\n",
      "|11857274 |[{3d, 0.3797}, {interactive, 0.3796}, {instrument, 0.3698}, {surgical, 0.3596}, {instruments, 0.358}]                       |\n",
      "|11857276 |[{visualizing, 0.4406}, {visualization, 0.4364}, {workstation, 0.3886}, {computing, 0.3696}, {3d, 0.3597}]                  |\n",
      "|11857288 |[{mri, 0.3622}, {cardiac, 0.3342}, {imaging, 0.3107}, {ventricular, 0.3029}, {myocardial, 0.2935}]                          |\n",
      "|11857306 |[{mri, 0.4242}, {imaging, 0.3397}, {spectroscopic, 0.2405}, {water, 0.2396}, {fmri, 0.2289}]                                |\n",
      "|11857307 |[{ablation, 0.3951}, {arrhythmia, 0.3803}, {cardiac, 0.3734}, {procedures, 0.3628}, {electrode, 0.3602}]                    |\n",
      "|11857323 |[{electrocardiograph, 0.5248}, {ecg, 0.4818}, {stress, 0.4752}, {electrocardiogram, 0.4605}, {physiological, 0.4293}]       |\n",
      "|11857335 |[{rehabilitation, 0.5913}, {neurorehabilitation, 0.5484}, {stroke, 0.5315}, {neurological, 0.418}, {rehabilitative, 0.3958}]|\n",
      "+---------+----------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "DataFrame output_summary_partitioned/part-00000 saved to output_keywords\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.functions import udf, array_sort, slice, col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "import os\n",
    "import gc\n",
    "\n",
    "def list_files_in_directory(directory_path, extension):\n",
    "    try:\n",
    "        files = [\n",
    "            f for f in os.listdir(directory_path)\n",
    "            if os.path.isfile(os.path.join(directory_path, f))  \n",
    "            and (extension == '' or f.endswith(extension))  \n",
    "            and not f.startswith('.')  # Exclude hidden files\n",
    "            and not f.endswith('.crc')  # Exclude .crc files\n",
    "            and os.path.getsize(os.path.join(directory_path, f)) > 0  # Exclude empty files\n",
    "        ]\n",
    "        return files\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The directory {directory_path} was not found.\")\n",
    "        return []\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied to access the directory {directory_path}.\")\n",
    "        return []\n",
    "\n",
    "def extract_keywords_save_to_file(input_file):\n",
    "\n",
    "    print(input_file)\n",
    "    \n",
    "    lines_df = spark.read.text(input_file)\n",
    "    \n",
    "    # Skip the header and process the lines\n",
    "    lines = [row[\"value\"] for row in lines_df.collect()][1:]  \n",
    "    \n",
    "    # Extract patent_id and summary_text\n",
    "    data = []\n",
    "    current_id = None\n",
    "    current_summary = []\n",
    "    \n",
    "    for line in lines:\n",
    "        try:\n",
    "            if line.startswith('\"'):\n",
    "                if current_id is not None and current_summary:\n",
    "                    # Save the current record\n",
    "                    data.append((current_id, \" \".join(current_summary).strip()))\n",
    "                # Extract the new patent_id\n",
    "                current_id = line.split('\"')[1]  \n",
    "                # Extract the start of the summary text\n",
    "                current_summary = [line.split('\"', 2)[2].strip()] if '\"' in line else []\n",
    "            else:\n",
    "                # Add subsequent lines to the summary\n",
    "                current_summary.append(line.strip())\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Append the last record\n",
    "    if current_id is not None and current_summary:\n",
    "        data.append((current_id, \" \".join(current_summary).strip()))\n",
    "\n",
    "    df = spark.createDataFrame(data, [\"patent_id\", \"summary_text\"])\n",
    "    \n",
    "    df = df.filter(col(\"patent_id\") != '')\n",
    "    \n",
    "    filtered_df = df.filter(df[\"patent_id\"].isin(patent_ids))\n",
    "    print(filtered_df.count())\n",
    "\n",
    "    keywords_df = extract_keywords_from_dataframe(\n",
    "    spark_df=filtered_df,\n",
    "    text_col=\"summary_text\",\n",
    "    id_col=\"patent_id\",\n",
    "    num_keywords=5\n",
    "    )\n",
    "\n",
    "    keywords_df.show(truncate=False)\n",
    "   \n",
    "    output_path = \"data/keywords\"\n",
    "    keywords_df.write.mode(\"append\").parquet(output_path)\n",
    "    \n",
    "    print(f\"DataFrame {input_file} saved to {output_path}\")\n",
    "\n",
    "\n",
    "directory_path = 'data/summary_partitioned'\n",
    "extension = ''\n",
    "files = list_files_in_directory(directory_path, extension)\n",
    "for file in files:\n",
    "    extract_keywords_save_to_file(os.path.join(directory_path, file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70ff986-00b3-444b-8579-9a1a8931bba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
