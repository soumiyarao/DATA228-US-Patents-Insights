lens_id,jurisdiction,doc_number,kind,date_published,doc_key,lang,biblio.publication_reference.jurisdiction,biblio.publication_reference.doc_number,biblio.publication_reference.kind,biblio.publication_reference.date,biblio.application_reference.jurisdiction,biblio.application_reference.doc_number,biblio.application_reference.date,biblio.invention_title[0].text,biblio.invention_title[0].lang,biblio.parties.applicants[0].residence,biblio.parties.applicants[0].extracted_name.value,biblio.parties.applicants[0].extracted_address,biblio.parties.applicants[1].residence,biblio.parties.applicants[1].extracted_name.value,biblio.parties.applicants[1].extracted_address,biblio.parties.inventors[0].residence,biblio.parties.inventors[0].extracted_name.value,biblio.parties.inventors[0].extracted_address,biblio.parties.inventors[1].residence,biblio.parties.inventors[1].sequence,biblio.parties.inventors[1].extracted_name.value,biblio.parties.inventors[1].extracted_address,biblio.parties.inventors[2].residence,biblio.parties.inventors[2].sequence,biblio.parties.inventors[2].extracted_name.value,biblio.parties.inventors[2].extracted_address,biblio.classifications_ipcr.classifications[0].symbol,biblio.classifications_ipcr.classifications[0].classification_value,biblio.classifications_ipcr.classifications[0].classification_symbol_position,biblio.classifications_cpc.classifications[0].symbol,biblio.classifications_cpc.classifications[0].classification_value,biblio.classifications_cpc.classifications[0].classification_symbol_position,biblio.classifications_cpc.classifications[1].symbol,biblio.classifications_cpc.classifications[1].classification_value,biblio.classifications_cpc.classifications[1].classification_symbol_position,biblio.classifications_cpc.classifications[2].symbol,biblio.classifications_cpc.classifications[2].classification_value,biblio.classifications_cpc.classifications[2].classification_symbol_position,biblio.classifications_cpc.classifications[3].symbol,biblio.classifications_cpc.classifications[3].classification_value,biblio.classifications_cpc.classifications[3].classification_symbol_position,biblio.classifications_cpc.classifications[4].symbol,biblio.classifications_cpc.classifications[4].classification_value,biblio.classifications_cpc.classifications[4].classification_symbol_position,families.simple_family.members[0].document_id.jurisdiction,families.simple_family.members[0].document_id.doc_number,families.simple_family.members[0].document_id.kind,families.simple_family.members[0].document_id.date,families.simple_family.members[0].lens_id,families.simple_family.size,families.extended_family.members[0].document_id.jurisdiction,families.extended_family.members[0].document_id.doc_number,families.extended_family.members[0].document_id.kind,families.extended_family.members[0].document_id.date,families.extended_family.members[0].lens_id,families.extended_family.size,legal_status.patent_status,abstract[0].text,abstract[0].lang,claims[0].claims[0].claim_text[0],claims[0].claims[1].claim_text[0],claims[0].claims[2].claim_text[0],claims[0].claims[3].claim_text[0],claims[0].claims[4].claim_text[0],claims[0].claims[5].claim_text[0],claims[0].claims[6].claim_text[0],claims[0].claims[7].claim_text[0],claims[0].claims[8].claim_text[0],claims[0].claims[9].claim_text[0],claims[0].claims[10].claim_text[0],claims[0].claims[11].claim_text[0],claims[0].claims[12].claim_text[0],claims[0].claims[13].claim_text[0],claims[0].claims[14].claim_text[0],claims[0].claims[15].claim_text[0],claims[0].claims[16].claim_text[0],claims[0].claims[17].claim_text[0],claims[0].claims[18].claim_text[0],claims[0].lang,description.text,description.lang,publication_type
004-633-620-227-341,US,20240386837,A1,2024-11-21,US_20240386837_A1_20241121,en,US,20240386837,A1,2024-11-21,US,18273054,2022-01-28,DISPLAY DEVICE AND VOLTAGE DROP COMPENSATION CIRCUIT,en,CN,"Chengdu BOE Optoelectronics Technology Co., Ltd.","Chengdu, Sichuan",CN,"BOE Technology Group Co., Ltd.",Beijing,CN,Qiang LI,Beijing,CN,1,Chengte LAI,Beijing,CN,2,Chengjie ZHAO,Beijing,G09G3/20,I,F,G09G3/2096,I,F,G09G2320/0233,A,L,G09G2320/0242,A,L,G09G2320/0247,A,L,G09G2330/028,A,L,US,20240386837,A1,2024-11-21,004-633-620-227-341,1,US,20240386837,A1,2024-11-21,004-633-620-227-341,1,UNKNOWN,"The present disclosure provides a display device and voltage drop compensation circuit. The display device includes a signal source part, a display part and a transmission part, and the signal source part is connected to the display part through the transmission part. The signal source part includes a power supply module including a voltage output end for outputting a power supply signal and a voltage feedback end for obtaining a feedback voltage signal, and the power supply module is used to adjust the power supply signal based on the feedback voltage signal. The transmission part includes: a first transmission line with a first end connected to the voltage output end and a second end outputting the power supply signal; and a second transmission line, including a first end connected to the voltage feedback end and a second end connected to the second end of the first transmission line.",en,"1 . A display device, comprising a signal source part, a display part and a transmission part, and the signal source part being connected to the display part through the transmission part, wherein the signal source part comprises: a power supply module, comprising a voltage output end and a voltage feedback end, the voltage output end being configured to output a power supply signal, the voltage feedback end being configured to obtain a feedback voltage signal, and the power supply module being configured to adjust the power supply signal based on the feedback voltage signal, the transmission part comprises: a first transmission line, comprising a first end connected to the voltage output end and a second end outputting the power supply signal; and a second transmission line, provided separately from the first transmission line, and comprising a first end connected to the voltage feedback end and a second end connected to the second end of the first transmission line.","2 . The display device according to claim 1 , wherein the transmission part comprises: a first connector, connected to the signal source part and comprising a first pin and a second pin, the first pin being connected to the first transmission line, the second pin being connected to the second transmission line, and the first connector being connected to the voltage output end through the first pin and to the voltage feedback end through the second pin; and a second connector, connected to the display part, the first transmission line being connected to the second transmission line at an end of the second connector.","3 . The display device according to claim 2 , wherein the display device comprises a plurality of the transmission parts, wherein the signal source part comprises a plurality of the power supply modules, and the plurality of the power supply modules are configured to output a plurality of different power supply signals, and wherein the plurality of the transmission parts are provided in one-to-one correspondence with the plurality of the power supply modules.","4 . The display device according to claim 3 , wherein the plurality of the power supply modules comprise a first power supply module, a second power supply module and a third power supply module, the first power supply module is configured to output a first power supply signal, the second power supply module is configured to output a second power supply signal and the third power supply module is configured to output a third power supply signal, and voltage magnitudes of the first power supply signal, the second power supply signal, and the third power supply signal are different from each other, wherein the plurality of the transmission parts comprise a first transmission part, a second transmission part and a third transmission part, and wherein the first transmission part and the second transmission part are provided on a first circuit board, and the third transmission part is provided on a second circuit board.","5 . The display device according to claim 4 , wherein the transmission part comprises a first conductive layer and a second conductive layer, and the first conductive layer is insulated from the second conductive layer, and the second connector comprises a third pin, and the third pin is connected to the first transmission line, and wherein the first pin, the second pin, the first transmission line and the second transmission line are all located in the first conductive layer, and the third pin is located in the second conductive layer; or the first pin, the second pin, the first transmission line and the second transmission line are all located in the second conductive layer, and the third pin is located in the first conductive layer; or the first pin and the second pin are located in the first conductive layer, and the first transmission line, the second transmission line and the third pin all are located in the second conductive layer; or the first pin and the second pin are located in the second conductive layer, and the first transmission line, the second transmission line and the third pin all are located in the first conductive layer; or the first pin, the second pin, the first transmission line, the second transmission line and the third pin are located in the first conductive layer or the second conductive layer.","6 . The display device according to claim 5 , wherein the first connector comprises a plurality of the first pins, and the plurality of the first pins are provided in parallel, and the second connector comprises a plurality of the third pins, and the plurality of the third pins are provided in parallel, and wherein the plurality of the first pins are each connected to a first end of the first transmission line, and the plurality of the third pins are each connected to the second end of the first transmission line.","7 . The display device according to claim 6 , wherein the first pin, the second pin, the first transmission line and the second transmission line are all located in the first conductive layer, and the third pin is located in the second conductive layer, the first transmission part and the second transmission part each comprises: a first conductive part, provided at the end of the second connector, located in the first conductive layer, and connected to the second end of the first transmission line; and a second conductive part, located in the second conductive layer, provided to be opposite to the first conductive part, and connected to the first conductive part through a via hole, and wherein in each of the first transmission part and the second transmission part, the second conductive part is further connected to the second transmission line and to each of the plurality of the third pins.","8 . The display device according to claim 7 , wherein a width of the second conductive part is greater than a width of the first conductive part.","9 . The display device according to claim 1 , wherein a line width of the first transmission line is greater than a line width of the second transmission line.","10 . The display device according to claim 4 , wherein in the first transmission part, a line width of the first transmission line is d 1 , a line width of the second transmission line is d 2 , and d 1 /d 2 is greater than or equal to 8 and less than or equal to 10; in the second transmission part, the line width of the first transmission line is d 3 , the line width of the second transmission line is d 4 , and d 3 /d 4 is greater than or equal to 8 and less than or equal to 10; and in the third transmission part, the line width of the first transmission line is d 5 , the line width of the second transmission line is d 6 , and d 5 /d 6 is greater than or equal to 2 and less than or equal to 4.","11 . The display device according to claim 10 , wherein d 1 /d 5 is greater than or equal to 2 and less than or equal to 4, and d 3 /d 5 is greater than or equal to 2 and less than or equal to 4.","12 . The display device according to claim 6 , wherein in the first transmission part, a number of the third pins is greater than a number of the first pins, in the second transmission part, the number of the third pins is greater than the number of the first pins, and in the third transmission part, the number of the third pins is the same as the number of the first pins.","13 . The display device according to claim 4 , wherein the display device further comprises an adapter part, the adapter part comprises an end connected to the signal source part and another end connected to a first end of the first transmission part, a first end of the second transmission part and a first end of the third transmission part, and a second end of the first transmission part, a second end of the second transmission part and a second end of the third transmission part are connected to the display part.","14 . A voltage drop compensation circuit, applied to the display device according to claim 1 , wherein the voltage drop compensation circuit comprises: the power supply module, provided in the signal source part and comprising: the voltage output end configured to output a power supply signal, and the feedback voltage end configured to obtain a feedback voltage signal; and a voltage division module, comprising an input end connected to the voltage output end and an output end connected to the feedback voltage end, and configured to determine, according to a predetermined voltage division ratio, the feedback voltage signal based on an output voltage signal of the voltage output end, wherein the power supply module is configured to adjust the power supply signal output from the voltage output end based on the feedback voltage signal.","15 . The voltage drop compensation circuit according to claim 14 , wherein the voltage division module comprises: a first resistor, comprising a first end as the input end of the voltage division module; and a second resistor, comprising a first end, as the output end of the voltage division module, connected to a second end of the first resistor, and a second end being grounded.","16 . The voltage drop compensation circuit according to claim 14 , further comprising: a filter capacitor, comprising an end connected to the input end of the voltage division module and another end connected to the output end of the voltage division module.","17 . The display device according to claim 1 , further comprising a voltage drop compensation circuit, wherein the voltage drop compensation circuit comprises: a voltage division module, comprising an input end connected to the voltage output end and an output end connected to the feedback voltage end, and configured to determine, according to a predetermined voltage division ratio, the feedback voltage signal based on an output voltage signal of the voltage output end.","18 . The display device according to claim 17 , wherein the voltage division module comprises: a first resistor, comprising a first end as the input end of the voltage division module; and a second resistor, comprising a first end, as the output end of the voltage division module, connected to a second end of the first resistor, and a second end being grounded.","19 . The display device according to claim 17 , wherein he voltage drop compensation circuit further comprises: a filter capacitor, comprising an end connected to the input end of the voltage division module and another end connected to the output end of the voltage division module.",en,"CROSS-REFERENCE TO RELATED APPLICATIONS The present application is a U.S. National Stage of International Application No. PCT/CN2022/074551 filed on Jan. 28, 2022, the entire contents thereof are incorporated herein by reference. TECHNICAL FIELD The present disclosure relates to the field of display technology, in particular, to a display device and a voltage drop compensation circuit. BACKGROUND An organic light emitting diode (OLED) is an active light-emitting display device with the advantages of self-illumination, wide viewing angle, high contrast ratio, low power consumption, very high response speed, and being thin and light, and bendable and so on. With the continuous development of display technology, the display device by using the OLED as the light-emitting device is becoming more and more widely used. In the related art, as for the vehicle-mounted OLED applications of medium and large sizes, there are abnormalities in the screen display due to voltage loss. It is to be noted that the above information disclosed in the Background section is only for enhancement of understanding of the background of the present disclosure and therefore it may contain information that does not form the prior art that is already known to a person skilled in the art. SUMMARY The present disclosure is to provide a display device and a voltage drop compensation circuit. An aspect of the present disclosure provides a display device, including a signal source part, a display part and a transmission part, and the signal source part being connected to the display part through the transmission part, wherein the signal source part includes: a power supply module, including a voltage output end and a voltage feedback end, the voltage output end being configured to output a power supply signal, the voltage feedback end being configured to obtain a feedback voltage signal, and the power supply module being configured to adjust the power supply signal based on the feedback voltage signal, the transmission part includes: a first transmission line, including a first end connected to the voltage output end and a second end outputting the power supply signal; and a second transmission line, provided separately from the first transmission line, and including a first end connected to the voltage feedback end and a second end connected to the second end of the first transmission line. In an exemplary embodiment of the present disclosure, the transmission part includes: a first connector, connected to the signal source part and including a first pin and a second pin, the first pin being connected to the first transmission line, the second pin being connected to the second transmission line, and the first connector being connected to the voltage output end through the first pin and to the voltage feedback end through the second pin; and a second connector, connected to the display part, the first transmission line being connected to the second transmission line at an end of the second connector. In an exemplary embodiment of the present disclosure, the display device includes a plurality of the transmission parts, wherein the signal source part includes a plurality of the power supply modules, and the plurality of the power supply modules are configured to output a plurality of different power supply signals, and wherein the plurality of the transmission parts are provided in one-to-one correspondence with the plurality of the power supply modules. In an exemplary embodiment of the present disclosure, the plurality of the power supply modules include a first power supply module, a second power supply module and a third power supply module, the first power supply module is configured to output a first power supply signal, the second power supply module is configured to output a second power supply signal and the third power supply module is configured to output a third power supply signal, and voltage magnitudes of the first power supply signal, the second power supply signal, and the third power supply signal are different from each other, wherein the plurality of the transmission parts include a first transmission part, a second transmission part and a third transmission part, and wherein the first transmission part and the second transmission part are provided on a first circuit board, and the third transmission part is provided on a second circuit hoard. In an exemplary embodiment of the present disclosure, the transmission part includes a first conductive layer and a second conductive layer, and the first conductive layer is insulated from the second conductive layer, and the second connector includes a third pin, and the third pin is connected to the first transmission line, and wherein the first pin, the second pin, the first transmission line and the second transmission line are all located in the first conductive layer, and the third pin is located in the second conductive layer; or the first pin, the second pin, the first transmission line and the second transmission line are all located in the second conductive layer, and the third pin is located in the first conductive layer; or the first pin and the second pin are located in the first conductive layer, and the first transmission line, the second transmission line and the third pin all are located in the second conductive layer; or the first pin and the second pin are located in the second conductive layer, and the first transmission line, the second transmission line and the third pin all are located in the first conductive layer; or the first pin, the second pin, the first transmission line, the second transmission line and the third pin are located in the first conductive layer or the second conductive layer. In an exemplary embodiment of the present disclosure, the first connector includes a plurality of the first pins, and the plurality of the first pins are provided in parallel, and the second connector includes a plurality of the third pins, and the plurality of the third pins are provided in parallel, and wherein the plurality of the first pins are each connected to a first end of the first transmission line, and the plurality of the third pins are each connected to the second end of the first transmission line. in an exemplary embodiment of the present disclosure, the first pin, the second pin, the first transmission line and the second transmission line are all located in the first conductive layer, and the third pin is located in the second conductive layer, the first transmission part and the second transmission part each includes: a first conductive part, provided at the end of the second connector, located in the first conductive layer, and connected to the second end of the first transmission line; and a second conductive part, located in the second conductive layer, provided to be opposite to the first conductive part, and connected to the first conductive part through a via hole, and wherein in a same transmission part, the second conductive part is further connected to the second transmission line and to each of the plurality of the third pins. In an exemplary embodiment of the present disclosure, a width of the second conductive part is greater than a width of the first conductive part. In an exemplary embodiment of the present disclosure, a line width of the first transmission line is greater than a line width of the second transmission line. In an exemplary embodiment of the present disclosure, in the first transmission part, a line width of the first transmission line is d 1 , a line width of the second transmission line is d 2 , and d 1 /d 2 is greater than or equal to 8 and less than or equal to 10; in the second transmission part, the line width of the first transmission line is d 3 , the line width of the second transmission line is d 4 , and d 3 /d 4 is greater than or equal to 8 and less than or equal to 10; and in the third transmission part, the line width of the first transmission line is d 5 , the line width of the second transmission line is d 6 , and d 5 /d 6 is greater than or equal to 2 and less than or equal to 4. In an exemplary embodiment of the present disclosure, d 1 /d 5 is greater than or equal to 2 and less than or equal to 4, and d 3 /d 5 is greater than or equal to 2 and less than or equal to 4. In an exemplary embodiment of the present disclosure, in the first transmission part, a number of the third pins is greater than a number of the first pins, in the second transmission part, the number of the third pins is greater than the number of the first pins, and in the third transmission part, the number of the third pins is the same as the number of the first pins. In an exemplary embodiment of the present disclosure, the second connector in the first transmission part and that in the second transmission part each includes 9 third pins, and the second connector in the third transmission part includes 3 third pins. In an exemplary embodiment of the present disclosure, the display device further includes an adapter part, the adapter part includes an end connected to the signal source part and another end connected to a first end of the first transmission part, a first end of the second transmission part and a first end of the third transmission part, and a second end of the first transmission part, a second end of the second transmission part and a second end of the third transmission part are connected to the display part. Another aspect of the present disclosure provides a voltage drop compensation circuit, applied to the display device as described in any embodiment of the present disclosure, the voltage drop compensation circuit includes: a power supply module, provided in a signal source part and including a voltage output end configured to output a power supply signal, and a feedback voltage end configured to obtain a feedback voltage signal; and a voltage division module, including an input end connected to the voltage output end and an output end connected to the feedback voltage end, and configured to determine, according to a predetermined voltage division ratio, the feedback voltage signal based on an output voltage signal of the voltage output end, wherein the power supply module is configured to adjust the power supply signal output from the voltage output end based on the feedback voltage signal. In an exemplary embodiment of the present disclosure, the voltage division module includes: a first resistor, including a first end as the input end of the voltage division module; and a second resistor, including a first end, as the output end of the voltage division module, connected to a second end of the first resistor, and a second end being grounded. In an exemplary embodiment of the present disclosure, the voltage drop compensation circuit further includes: a filter capacitor, including an end connected to the input end of the voltage division module and another end connected to the output end of the voltage division module. It should be understood that the above general description and the detailed descriptions that follow are only exemplary and explanatory and do not limit the present disclosure. BRIEF DESCRIPTION OF THE DRAWINGS The accompanying drawings herein are incorporated into and form part of the specification, illustrate embodiments consistent with the present disclosure, and are used in conjunction with the specification to explain the principles of the present disclosure. It will he apparent that the accompanying drawings in the following description are only some embodiments of the present disclosure, and that according to these accompanying drawings, a person skilled in the art may obtain other accompanying drawings without creative effort. FIG. 1 is a schematic structure diagram of a display device according to an embodiment of the present disclosure; FIG. 2 shows a schematic structure diagram of a display device according to another embodiment of the present disclosure; FIG. 3 is a schematic structure diagram of a transmission part in FIG. 1 ; FIG. 4 shows a cross-sectional view of the transmission part in FIG. 3 taken along the AA direction; FIG. 5 is a schematic structure diagram of a connection state of a transmission part with a signal source part and a display part according to an embodiment of the present disclosure; FIG. 6 is a schematic structure view of a connection state of a transmission part with a signal source part and a display part according to another embodiment of the present disclosure; FIG. 7 is a schematic structure diagram of a display device according to another embodiment of the present disclosure; FIG. 8 is a schematic diagram of pins of a first transmission part and a second transmission part according to an embodiment of the present disclosure; FIG. 9 is a schematic diagram of a pin of a third transmission part according to an embodiment of the present disclosure; FIG. 10 is a top view of a stricture of a first transmission part according to an embodiment of the present disclosure; FIG. 11 is a top view of a structure of a first transmission part according to an embodiment of the present disclosure; FIG. 12 is a schematic structure diagram of a display device according to another embodiment of the present disclosure; and FIG. 13 is a schematic diagram of a voltage drop compensation circuit according to an embodiment of the present disclosure. DETAILED DESCRIPTION Example embodiments will now be described more fully with reference to the accompanying drawings. Example embodiments, however, may be embodied in various forms and should not be construed as limited to the examples set forth herein; rather, these embodiments are provided so that the present disclosure will be thorough and complete, and the concept of example embodiments would be fully conveyed to those skilled in the art, The same reference numerals in the drawings denote the same or similar structures, and thus their detailed descriptions will be omitted. In addition, the accompanying drawings are only schematic illustrations of the present disclosure and are not necessarily to scale. Although relative terms such as “up” and “down” are used in this specification to describe the relative relationship of one component with another component shown, these terms are used in this specification only for convenience of the description, for example according to the example orientation described in the accompanying drawings. It is to be understood that if a device shown is turned upside down, the component described as being “up” will become the component described as being “down”. When a structure is “on” another structure, it may mean that the structure is integrally formed on said another structure, or that the structure is disposed “directly” on said another structure, or that the structure is disposed “indirectly” on said another structure via an additional structure. The terms “a”, “an”, “the”, “said” and “at least one of ” are used to indicate the presence of one or more elements/components/etc., the terms “including” and “having” are used to indicate an open-ended inclusive meaning and that additional elements/components/etc. may be present in addition to the listed elements/components/etc. The terms “first”, “second”, “third” and the like are used only as labels and are not intended to limit the number of the objects thereof. FIG. 1 shows a schematic structure diagram of a display device according to an embodiment of the present disclosure. As shown in FIG. 1 , the display device may include a signal source part 10 , a display part 30 , and a transmission part 20 , and the signal source part 10 may be connected to the display part 30 through the transmission part 20 . The signal source part 10 may include a power supply module 10 , and the power supply module 10 may include a voltage output end Vout and a voltage feedback end FB. The voltage output end Vout is configured to output a power supply signal, and the voltage feedback end FB is configured to obtain a feedback voltage signal. The power supply module 10 is configured to adjust the power supply signal based on the feedback voltage signal. The transmission part 20 may include: a first transmission line L 1 , including a first end connected to the voltage output end Vout and a second end outputting, the power supply signal; and a second transmission line L 2 , provided separately from the first transmission line L 1 , and including a first end connected to the voltage feedback end FB and a second end connected to the second end of the first transmission line L 1 . In the display device provided in the present disclosure, the transmission part 20 is provided with the first transmission line L 1 and the second transmission line L 2 , the second transmission line L 2 is provided separately from the first transmission line L 1 , and the second end of the second transmission line L 2 is connected to the first transmission line L 1 at the second end of the first transmission line L 1 , therefore a voltage signal collected by the second transmission line L 2 is an actual power supply signal after being subjected to line loss. The second transmission line L 2 transmits this actual power supply signal to the feedback voltage end of the power supply module 11 , so that the power supply module 11 can adjust the output voltage based on this actual power supply signal, and thus the end of the transmission part 20 connected to the display part 30 can output a stable target voltage, which can solve not only the problems of display horizontal stripe and display flicking caused by logic voltage drop, but also the problems of brightness drop, Gamma drift and CIE overspecification caused by EL voltage drop. As shown in FIG. 1 , in an exemplary embodiment, the first transmission line L 1 is a voltage output line for outputting the power supply signal from the power supply module 11 to the display part, and the second transmission line L 2 is a voltage feedback line for collecting the actual power supply signal to the display part as the feedback voltage signal and transmitting the same to the power supply module 11 . As shown in FIG. 1 , in an exemplary embodiment, the transmission part 20 may include a first connector J 1 and a second connector J 2 . The first connector J 1 may be connected to the signal source part 10 , and the second connector J 2 may be connected to the display part 30 . The second transmission line L 2 may be connected to the first transmission line L 1 at an end of the second connector J 2 to collect the actual power supply signal output by the transmission part 20 . That is, in an exemplary embodiment, the second end of the first transmission line L 1 refers to an end of the first transmission line L 1 connected to the second connector J 2 . In other words, the second transmission line L 2 is connected to the first transmission line L 1 at the output end of the first transmission line L 1 . Since there is a certain impedance in the line, when the current of the power supply signal is high, a voltage drop loss is inevitably generated on the line. In the exemplary embodiment, the second end of the second transmission line L 2 is connected to the output end of the first transmission line L 1 , therefore the second transmission line L 2 may collect an actual power supply signal after being subjected to line loss, and thus the power supply module 11 adjusts the power supply signal at the voltage output end Vout based on this actual power supply signal, so that the power supply signal at the voltage output end Vout can output a stable target voltage at the remote end after being subjected to line loss to match the demand voltage of the display part 30 . It is to be understood that structures of the first connector J 1 and the second connector J 2 may be set specifically according to connector structure of the signal source part 10 and the display part 30 , which is not limited in the present disclosure. In an exemplary embodiment, the power supply module 11 may be an integrated chip, such as a power management chip, and the power supply module 11 may adjust the output power supply signal based on the feedback voltage signal by means of a built-in algorithm, The specific method for adjusting the voltage by the power supply module 11 is not limited in the present disclosure. In an exemplary embodiment, the display device may be, for example, a vehicle-mounted terminal, a tablet computer, and the like. FIG. 2 shows a schematic structure diagram of a display device according to another embodiment of the present disclosure. As shown in FIG. 2 , in an exemplary embodiment, the signal source part 10 may correspond to an SOC part M 3 in FIG. 2 , and the SOC part M 3 may include a system control circuit board. The display part 30 may correspond to a panel part M 1 in FIG. 2 , and the panel part M 1 may include a PCB drive circuit board M 2 connected to a display panel. The PCB drive circuit board M 2 is integrated with various devices such as a POWER IC chip providing the power supply signal to the display panel, a TCON chip providing a display signal, and a GAM chip. In an exemplary embodiment, the power supply module 11 is disposed in the SOC part, which may effectively reduce the volume of the PCB drive circuit board M 2 , and may avoid temperature increasing in the PCB drive circuit board M 2 , The PCB drive circuit board M 2 may be connected to the system control circuit board M 3 via the transmission part 20 provided in the exemplary embodiment to compensate for line loss voltage drop. FIG. 3 shows a schematic structure diagram of the transmission part in FIG. 1 . As shown in FIG. 3 , in an exemplary embodiment, the first connector J 1 may include a first pin 1 and a second pin 2 . The first pin 1 may be connected to the first transmission line L 1 and the second pin 2 may be connected to the second transmission line L 2 . After the first connector J 1 is inserted into the signal source part 10 , the first transmission line L 1 in the transmission part 20 is connected to the voltage output end Vout of the power supply module 11 via the first pin 1 , and the second transmission line L 2 in the transmission part 20 is connected to the voltage feedback end FB of the power supply module 11 via the second pin 2 . The second connector J 2 may include a third pin 3 which is connected to the first transmission line L 1 . After the second connector J 2 is inserted into the display part 30 , the first transmission line L 1 in the transmission part 20 outputs the power supply signal to the corresponding chip of the display part 30 via the third pin 3 . As shown in FIG. 3 , in an exemplary embodiment, a line width of the first transmission line L 1 may be set to be greater than a line width of the second transmission line L 2 . Both the first transmission line L 1 and the second transmission line L 2 may be copper lines. Because of the high current of the power supply signal, the line width of the first transmission line L 1 may be set to be relatively large so that the first transmission line L 1 may transmit a higher current supply signal. For example, the line width of the first transmission line L 1 is d 1 , the line width of the second transmission line L 2 is d 2 , and d 1 /d 2 may be set to greater than or equal to 3 and less than or equal to 10, for example d 1 /d 2 may be 3, 4, 5, 6, 7, 8, 9, 10, etc., depending on the current of the power supply signal to be transmitted by the first transmission line L 1 . In addition, the first transmission line L 1 and the second transmission line L 2 may be set to have the same characteristics, except that the line widths are different. As shown in FIG. 3 , in an exemplary embodiment, a first conductive part 21 may be provided at an end of the second connector J 2 , and the second transmission line L 2 and the first transmission line L 1 may be connected to the first conductive part 21 , so that the second transmission line L 2 is connected to the first transmission line L 1 at the output end of the transmission part 20 by the first conductive part 21 to capture the actual power supply signal after being subjected to transmission loss. By providing the first conductive part 21 , the wiring length of the first transmission line L 1 may be reduced, i.e., at the end of the second connector J 2 , the first transmission line L 1 only needs to be extended to the first conductive part 21 , which is then connected to the third pin 3 at end of the second connector J 2 . In an exemplary embodiment, the advantage of providing the first conductive part 21 is that when the number of the third pins 3 that are needed to be connected to the end of the second connector J 2 is relatively large or larger than the number of the first pins 1 at the end of the first connector J 1 , the line width of the first transmission line L 1 may only need to withstand the current strength of the power supply signal, there is no need to widen the first transmission line L 1 to cover all the third pins 3 , and it only needs that the first conductive part 21 covers all of the third pins 3 , which reduces the difficulty of wiring in the circuit board. Of course, in other exemplary embodiments, the first conductive part 21 may not be provided at the end of the first conductive part 21 , and the second transmission line L 2 may be connected to the first transmission line L 1 directly at the end of the second connector J 2 . FIG. 4 shows a cross-sectional view of the transmission part in FIG. 3 in the direction of AA. As shown in FIG. 4 , in an exemplary embodiment, the transmission part 20 may include a first conductive layer 211 and a second conductive layer 212 with an insulating layer 213 provided between the first conductive layer 211 and the second conductive layer 212 . The first transmission line L 1 and the second transmission line L 2 may be both provided in the first conductive layer 211 or the second conductive layer 212 , i.e., in the same conductive layer, which may simplify the wiring and reduces the wiring difficulty. Of course, in other exemplary embodiments, the first transmission line L 1 and the second transmission line L 2 may also be provided in different conductive layers, for example, the first transmission line L 1 is provided in the first conductive layer 211 and the second transmission line L 2 is provided in the second conductive layer 212 , and the like, which all belong to the protection scope of the present disclosure. In an exemplary embodiment, a single pin is difficult to withstand the large current of the power supply signal output by the power supply module 11 of the signal source part 10 , so the first connector J 1 may include a plurality of first pins 1 provided in parallel, and the second connector J 2 may include a plurality of third pins 3 provided in parallel. By providing multiple pins for current shunting, it may avoid burning down of the pin by the current. It is to be understood that the first transmission line L 1 connects a plurality of first pins 1 at the end of the first connector J 1 and connects a plurality of third pins 3 at the end of the second connector J 2 to output the power supply signal. In an exemplary embodiment, the pins in the first connector J 1 and the pins in the second connector J 2 may be provided in different conductive layers or in the same conductive layer according to the connection manner of the system control circuit board M 3 in the SOC part and the PCB drive circuit M 2 in the panel part. For example, FIG. 5 shows a schematic structure diagram of the connection state of the transmission part with the signal source part and the display part according to an embodiment of the present disclosure. As shown in FIG. 5 , the transmission part 20 is bent in a U-shape to connect the first connector J 1 to the system control circuit board M 3 and connect the second connector J 2 to the PCB drive circuit M 2 . If the connection of the first connector J 1 to the system control circuit board M 3 and the connection of the second connector J 2 to the PCB drive circuit M 2 are both on the same side of the circuit board where the transmission part 20 is provided, e.g., both on the upper side of the transmission part 20 , the first pin 1 and the second pin 2 may be provided in a different conductive layer from the third pin 3 , e.g., the first pin 1 and the second pin 2 are provided in the first conductive layer 211 and the third pin 3 is provided in the second conductive layer 212 . Alternatively, FIG. 6 shows a schematic structure diagram of the connection state of the transmission part with the signal source part and the display part according to another embodiment of the present disclosure. As shown in FIG. 6 , when the transmission part 20 is bent to be connected to the system control circuit board M 3 and the PCB drive circuit M 2 respectively at the two sides of the circuit board where the transmission part 20 is provided, the first pin 1 and the second pin 2 may be provided in the same conductive layer as the third pin 3 , for example, all the first pin 1 , second pin 2 and third pin 3 are provided in the first conductive layer 211 or in the second conductive layer 212 . In addition, in an exemplary embodiment, the pins in the first connector J 1 and the pins in the second connector J 2 may be provided in the same conductive layer as or in a different conductive layer from the first transmission line L 1 and the second transmission line L 2 , For example, the first pin 1 , the second pin 2 , the first transmission line L 1 and the second transmission line L 2 may all be provided in the first conductive layer 211 , and the third pin 3 is provided in the second conductive layer 212 ; or, the first pin 1 , the second pin 2 , the first transmission line L 1 and the second transmission line L 2 are all provided in the second conductive layer 212 , and the third pin 3 is provided in the first conductive layer 211 ; or, the first pin 1 and the second pin 2 are provided in the first conductive layer 211 , and the first transmission line L 1 , the second transmission line L 2 and the third pin 3 are all provided in the second conductive layer 212 ; or, the first pin 1 and the second pin 2 are provided in the second conductive layer 212 , and the first transmission line L 1 the second transmission line L 2 and the third pin 3 are all provided in the first conductive layer 211 ; or, the first pin 1 , the second pin 2 , the first transmission line L 1 , the second transmission line L 2 and the third pin 3 are all provided in the first conductive layer 211 or the second conductive layer 212 , which all fall within the protection scope of the present disclosure. As shown in FIG. 2 , in an exemplary embodiment, the panel part typically includes different chips such as a power management chip, a TCON chip, etc., to control the panel for light-emitting display. It may he appreciated that the different chips may require different power supply signals from each other, i.e., in an exemplary embodiment, the signal source part 10 may provide several different types of power supply signals to the display part 30 . For example, FIG. 7 shows a schematic structure diagram of a display device according to another embodiment of the present disclosure. As shown in FIG. 7 , the signal source part 10 may include a first power supply module 101 , a second power supply module 102 and a third power supply module 103 . The first power supply module 101 may output a first power supply signal, and the first power supply signal may be for example an ELVDD signal, which is used as a first voltage signal for a pixel drive circuit in the display panel. The second power supply module 102 may output a second power supply signal, and the second power supply signal may be, for example, an ELVSS signal, which is used as a second voltage signal for the pixel drive circuit. The pixel drive circuit controls a light-emitting unit to emit light during a light-emitting phase according to a driving current generated by the voltage difference between the ELVDD signal and the ELVSS signal. The third power supply module 103 may output a third supply signal, which may be, for example, a VDD signal for powering a TCON chip. As shown in FIG. 7 , in order to cooperate with the three power supply signals, the display device provided in an exemplary embodiment may include a first transmission part 201 , a second transmission part 202 and a third transmission part 203 . The first transmission part 201 may be connected to the first power supply module 101 for outputting a first power supply signal and outputting a first feedback voltage signal to the first power supply module 101 ; the second transmission part 202 may be connected to the second power supply part 102 for outputting a second power supply signal and outputting a second feedback voltage signal to the second power supply module 102 ; and the third transmission part 203 may be connected to the third power supply module 103 for outputting a third power supply signal and outputting a third feedback voltage signal to the third power supply module 103 , It is to be understood that the first transmission part 201 , the second transmission part 202 , and the third transmission part 203 each includes the first transmission line L 1 and the second transmission line L 2 . FIG. 8 shows a schematic diagram of the pins of the first transmission part and the second transmission part according to an embodiment of the present disclosure, and FIG. 9 shows a schematic diagram of the pins of the third transmission part according to an embodiment of the present disclosure. As shown in FIGS. 8 and 9 , in an exemplary embodiment, the first transmission part 201 for transmitting the first power supply signal and the second transmission part 202 for transmitting the second power supply signal are provided on the same circuit board, and the third transmission part 203 for transmitting the third power supply signal may be provided on another circuit board. The circuit board may be, for example, a flexible circuit board FPC, and the display device may include two circuit boards, i.e., a first FPC board and a second FPC board. As shown in FIG. 8 , the first transmission part 201 and the second transmission part 202 are both provided on the first FPC board, and the first connector J 1 and the second connector J 2 in the first FPC board each has 40 pins. As shown in FIG. 9 , the third transmission part 203 is provided on the second FPC board, and the first connector J 1 and the second connector J 2 in the second FPC board each has 50 pins, Furthermore, the second FPC board may further transmit other signals such as display control signal. In addition, as shown in FIG. 8 , in the first transmission part 201 , a plurality of pins for transmitting the ELVDD signal are arranged in order, the pin at the first place for transmitting the ELVDD signal may be used as the second pin 2 , and the other pins for transmitting the ELVDD signal may be used as the first pin 1 . The second transmission part 202 and the third transmission part 203 may have a similar structure. Of course, in other exemplary embodiments, the first transmission part 201 , the second transmission part 202 and the third transmission part 203 may also have other arrangements. The structure of the transmission part 20 is further described below in relation to the three different power supply modules. FIG. 10 shows a top view of a structure of the first transmission part according to an embodiment of the present disclosure. As shown in FIG. 10 , in an exemplary embodiment, in the first transmission part 201 , the first pin 1 , the second pin 2 , the first transmission line L 1 and the second transmission line L 2 are all provided in the first conductive layer 211 , and the third pin 3 is provided in the second conductive layer 212 . The first transmission part 201 may further include a first conductive part 21 and a second conductive part 22 provided at the end of the second connector J 2 , the first conductive part 21 may be provided in the first conductive layer 211 , and the second conductive part 22 may be provided in the second conductive layer 212 opposite to the first conductive part 21 and be connected to the first conductive part 21 through a via hole. The first conductive part 21 and the second conductive part 22 are provided opposite to each other in such a way that the orthographic projection of the first conductive part 21 on the insulating layer 213 and the orthographic projection of the second conductive part 22 on the insulating layer 213 are overlapped. The arrangement of the first conductive part 21 may reduce the wiring length of the first transmission line L 1 , i.e., the first transmission line L 1 only needs to be extended to the first conductive part 21 at the end of the second connector J 2 . The first conductive part 21 is connected to the first transmission line L 1 at the end of the second connector J 2 and is connected to the second conductive part 22 provided in the second conductive layer 212 through a via hole H 1 . At the same time, the second transmission line L 2 may be connected to the second conductive part 22 through a via hole H 2 , and thus may be connected to the first transmission line L 1 at the end of the second connector J 2 to capture the output voltage of the first transmission part 201 . As can be seen from FIG. 10 , the second transmission line L 2 may not be connected to the first conductive part 21 , but directly connected to the second conductive part 22 through the via, hole H 2 . Furthermore, it is to be understood that a plurality of via holes H 1 may be provided to connect the first conductive part 21 and the second conductive part 22 to ensure that the first conductive part 21 and the second conductive part 22 are sufficiently electrically connected. In addition, in other exemplary embodiments, the second transmission line L 2 may also be directly connected to the first conductive part 21 . As shown in FIG. 10 , in an exemplary embodiment, in the first transmission part 201 , the number of the third pins 3 at the end of the second connector J 2 may be larger than the number of the first pins 1 in the first connector J 1 . For example, the current of the first power supply signal is about 2 A, 9 first pins 1 may be provided at the end of the first connector J 1 , and the first supply signal is shunted by the 9 first pins 1 without burning the first pin 1 . Further, 10 third pins 3 may be provided at the end of the second connector J 2 to match the number of pins by which the ELVDD Power IC in the display part 30 obtains the first power supply signal. On this basis, the width of the second conductive part 22 may be provided to be greater than the width of the first conductive part 21 . For example, the width of the second conductive part 22 may be substantially equal to the width of the area where the 10 third pins 3 are located, so that the second conductive part 22 may be connected to each of the third pins 3 , and the first conductive part 21 does not need to cover all of the third pins 3 . Furthermore, the width of the first conductive part 21 may be provided to be substantially equal to or slightly greater than the width of the first transmission line L 1 . It is to be understood that in other exemplary embodiments, the number of first pins 1 at the end of the first connector J 1 may also be the same as the number of third pins 3 at the end of the second connector J 2 , for example, both the numbers of the first pins 1 and the second pins 2 may be set to be 10 and the like, which are within the protection scope of the present disclosure. In an exemplary embodiment, the width of the conductive part may be understood as the distance of the conductive part in the arrangement direction of the pins. As shown in FIG. 10 , in an exemplary embodiment, in the first transmission part 201 , the line width of the first transmission line L 1 is d 1 , the line width of the second transmission line L 2 is d 2 , and d 1 /d 2 may be set to be greater than or equal to 8 and less than or equal to 10, for example, it may be 8, 8,5, 9, 9.5, 10, and the like. In an exemplary embodiment, the line width of the transmission part may be understood as the distance in a direction perpendicular to the extending direction of the transmission part in the plane in which the transmission part is located. In addition, in an exemplary embodiment, the second transmission part 202 may have the same structure as the first transmission part 201 , which will not be described in the embodiment. FIG. 11 shows a top view of the structure of the first transmission part according to an embodiment of the present disclosure. As shown in FIG. 11 , in an exemplary embodiment, in the third transmission part 203 , the number of the first pins 1 in the first connector J 1 may be the same as the number of the third pins 3 in the second connector J 2 , the first transmission line L 1 may be directly connected to the first pin 1 and the third pin 3 , and the second transmission line L 2 may be directly connected to the second pin 2 and may be connected to the first transmission line L 1 at the end of the second connector J 2 . In other exemplary embodiments, for example, each of the first connector J 1 and the second connector 32 may include a first conductive part 21 . At an end of the first connector J 1 , the first conductive part 21 may be connected between the first transmission line L 1 and the first pin 1 , and at an end of the second connector J 2 , the first conductive part 21 is connected between the first transmission line L 1 and the third pin 3 . The second transmission line L 2 may be connected to the first conductive part 21 so as to be connected to the second end of the first transmission line L 1 . As shown in FIG. 11 , in an exemplary embodiment, in the third transmission part 203 , the line width of the first transmission line L 1 is d 5 , the line width of the second transmission line L 2 is d 6 , and d 5 /d 6 may be set to be greater than or equal to 2 and less than or equal to 4, for example, it may be 2, 2.53, 3.2, 3.4, 3.5, 3.6, 3.8, 4, and the like. Furthermore, in an exemplary embodiment, the line width of the first transmission line L 1 in the first transmission part 201 may be greater than the line width of the first transmission line L 1 in the third transmission part 203 , for example, the line width of the first transmission line L 1 in the first transmission part 201 is d 1 , the line width of the first transmission line L 1 in the third transmission part 203 is d 5 , and d 1 /d 5 may be set to be greater than or equal to 2 and less than or equal to 4, for example, it may be 2, 2.5, 3, 3.5. 4, and the like. Similarly, the line width of the first transmission line L 1 in the second transmission part 202 is d 3 , and d 3 /d 5 may be set to be greater than or equal to 2 and less than or equal to 4, for example, it may be 2, 2.5, 3, 3.5, 4, and the like. FIG. 12 shows a schematic structure diagram of a display device according to another embodiment of the present disclosure. As shown in FIG. 12 , in an exemplary embodiment, the display device may further include an adapter part with an end that may be connected to the signal source part 10 and another end which is respectively connected to the first end of the first transmission part 201 , the first end of the second transmission part 202 and the first end of the third transmission part 203 . The second end of the first transmission part 201 , the second end of the second transmission part 202 and the second end of the third transmission part 203 are connected to the display part 30 . For example, the adapter part may be a connector having 90 pins which connects a first FPC having 40 pins and a second FPC having 50 pins as a single unit. In an exemplary embodiment, the first transmission part 201 , the second transmission part 202 and the third transmission part 203 are connected in one unit by means of the adapter part, so that at the signal source part, the system control boards may be connected by this adapter part with only one plugging/unplugging operation, making the operation more convenient. This structure may be used in a test phase, for example in a factory test phase where frequent plugging/unplugging operations are required, which simplifies the operation with fewer errors. Since the first transmission part 201 , the second transmission part 202 , and the third transmission part 203 have the structure of the above-described embodiment of the present disclosure, they can compensation for the voltage drop at the remote end during the test phase, making the test results more accurate. In addition, the present disclosure also provides a voltage drop compensation circuit which can be applied to the display device described in any embodiment of the present disclosure. FIG. 13 shows a schematic diagram of a voltage drop compensation circuit according to an embodiment of the present disclosure. As shown in FIG. 13 , a voltage division module 12 may be connected between the voltage output end Vout and the voltage feedback end FB of the power supply module 11 . By adjusting a voltage division ratio of the voltage division module 12 , the collected feedback voltage signal may be set within a specified voltage range of the voltage feedback end FB Then the power supply module 11 may adjust the amplitude of the power supply signal output from the voltage output end Vout through a built-in algorithm, so that the power supply signal output from the voltage output end Vout matches the voltage required by the display part, i.e., the power supply module 11 outputs a stable target voltage. In an exemplary embodiment, there is a certain line resistance between nodes AB, and thus the line between the nodes AB may be equivalent to an equivalent resistance, therefore there is a certain voltage drop between the nodes A and B, and the voltages at the nodes A and B are not the same. The voltage division module 12 may be a resistive voltage division module, for example, the voltage division module 12 may include a first resistor R 1 and a second resistor R 2 , a an end of the first resistor R 1 is used as an input end of the voltage division module 12 , another end of the first resistor R 1 is connected to an end of the second resistor R 2 , another end of the second resistor R 2 is grounded, and the common connection end of the first resistor R 1 and the second resistor R 2 is used as the output end of the voltage division module 12 . Furthermore, as shown in FIG. 13 , in an exemplary embodiment, the voltage drop compensation circuit may also include a filter capacitor CF with an end which is connected to the input end of the voltage division module 12 and another end which is connected to the output end of the voltage division module 12 . The filter capacitor CF filters out spurious signals so that the voltage division module 12 may output a stable feedback power supply signal to the voltage feedback terminal FB. A person skilled in the art may easily conceive other embodiments of the present disclosure upon consideration of the specification and practice of the invention disclosed herein. The present application is intended to cover any variation, use or adaptation of the present disclosure that follows the general principle of the present disclosure and includes a common knowledge or conventional technical means in the art that is not disclosed herein. The specification and embodiments are to be considered exemplary only and the true scope and spirit of the present disclosure is indicated by the appended claims.",en,PATENT_APPLICATION
009-181-701-950-889,US,20240385975,A1,2024-11-21,US_20240385975_A1_20241121,en,US,20240385975,A1,2024-11-21,US,18671095,2024-05-22,EFFICIENT DATA SHARING FOR GRAPHICS DATA PROCESSING OPERATIONS,en,US,Intel Corporation,"Santa Clara, CA",US,Joydeep Ray,"Folsom, CA",US,1,Altug Koker,"El Dorado Hills, CA",US,2,Elmoustapha Ould-Ahmed-Vall,"Chandler, AZ",US,3,Michael Macpherson,"Portland, OR",US,4,Aravindh V. Anantaraman,"Folsom, CA",US,5,Vasanth Ranganathan,"El Dorado Hills, CA",US,6,Lakshminarayanan Striramassarma,"Folsom, CA",US,7,Varghese George,"Folsom, CA",US,8,Abhishek Appu,"El Dorado Hills, CA",US,9,Prasoonkumar Surti,"Folsom, CA",G06F13/16,I,F,G06F9/30,I,L,G06F9/38,I,L,G06F9/50,I,L,G06T1/20,I,L,G06T1/60,I,L,G06F13/1605,I,F,G06F9/3004,I,L,G06F9/3887,I,L,G06F9/5016,I,L,G06T1/20,I,L,G06T1/60,I,L,US,20240385975,A1,2024-11-21,009-181-701-950-889,1,US,20240385975,A1,2024-11-21,009-181-701-950-889,1,UNKNOWN,"An apparatus to facilitate efficient data sharing for graphics data processing operations is disclosed. The apparatus includes a processing resource to generate a stream of instructions, an L1 cache communicably coupled to the processing resource and comprising an on-page detector circuit to determine that a set of memory requests in the stream of instructions access a same memory page; and set a marker in a first request of the set of memory requests; and arbitration circuitry communicably coupled to the L1 cache, the arbitration circuitry to route the set of memory requests to memory comprising the memory page and to, in response to receiving the first request with the marker set, remain with the processing resource to process the set of memory requests.",en,1 .- 20 . (canceled),"21 . An apparatus comprising: a processing resource of a chiplet; an L1 cache communicably coupled to the processing resource and comprising synchronization hardware circuit to: allocate a first thread group as a member of a super thread group (SGT) comprising a collection of thread groups running on the chiplet; receive from a second thread group within the SGT, a request to communicate with the first thread group identified by a thread group identifier (ID) within the SGT; access a routing table to determine a location of the first thread group based on the thread group ID; and route the request to the determined location of the first thread group using communication links between L1 caches of the chiplet.","22 . The apparatus of claim 21 , wherein the processing resource is an execution unit in a graphics processing unit (GPU).","23 . The apparatus of claim 21 , wherein the synchronization hardware circuit comprises one or more of a router, a receiver/transmitter (Rx/Tx), or the routing table.","24 . The apparatus of claim 23 , wherein the router is to implement an application programming interface (API) defined to allow communication between sub-slices of the chiplet.","25 . The apparatus of claim 23 , wherein router is to communication with the Rx/Tx to cause communications from the L1 cache to be sent or received over buses of a point-to-point communication network.","26 . The apparatus of claim 21 , wherein a kernel is to utilize query functions to inquire an SGT ID number, to inquire a number of thread groups in the STF, or to send and receive data from/to thread in the same STG as the kernel using a logical thread number.","27 . The apparatus of claim 21 , wherein the chiplet further comprises a chiplet-level cache for sub-slice communication within the chiplet and to enable communication between threads on different sub-slices of the chiplet that are within the SGT of the chiplet.","28 . The apparatus of claim 27 , wherein the chiplet-level cache comprises an L2 cache that is in communication with a global cache of a base die of the chiplet.","29 . The apparatus of claim 21 , wherein the apparatus is at least one of a single instruction multiple data (SIMD) machine or a single instruction multiple thread (SIMT) machine.","30 . A method comprising: allocating, by a synchronization hardware circuit of an L1 cache of a graphics processor, a first thread group as a member of a super thread group (SGT) comprising a collection of thread groups running on the chiplet; receiving, by the synchronization hardware circuit, from a second thread group within the SGT, a request to communicate with the first thread group identified by a thread group identifier (ID) within the SGT; accessing, by the synchronization hardware circuit, a routing table to determine a location of the first thread group based on the thread group ID; and routing, by the synchronization hardware circuit, the request to the determined location of the first thread group using communication links between L1 caches of the chiplet.","31 . The method of claim 30 , wherein the synchronization hardware circuit comprises one or more of a router, a receiver/transmitter (Rx/Tx), or the routing table.","32 . The method of claim 31 , wherein the router is to implement an application programming interface (API) defined to allow communication between sub-slices of the chiplet.","33 . The method of claim 31 , wherein router is to communication with the Rx/Tx to cause communications from the L1 cache to be sent or received over buses of a point-to-point communication network.","34 . The method of claim 30 , wherein a kernel is to utilize query functions to inquire an SGT ID number, to inquire a number of thread groups in the STF, or to send and receive data from/to thread in the same STG as the kernel using a logical thread number.","35 . The method of claim 30 , wherein the chiplet further comprises a chiplet-level cache for sub-slice communication within the chiplet and to enable communication between threads on different sub-slices of the chiplet that are within the SGT of the chiplet.","36 . A non-transitory computer-readable medium having instructions stored thereon, which when executed by one or more processors, cause the one or more processors to: allocate, by a synchronization hardware circuit of an L1 cache of the one or more processors, a first thread group as a member of a super thread group (SGT) comprising a collection of thread groups running on the chiplet; receive, by the synchronization hardware circuit, from a second thread group within the SGT, a request to communicate with the first thread group identified by a thread group identifier (ID) within the SGT; access, by the synchronization hardware circuit, a routing table to determine a location of the first thread group based on the thread group ID; and route, by the synchronization hardware circuit, the request to the determined location of the first thread group using communication links between L1 caches of the chiplet.","37 . The non-transitory computer-readable medium of claim 36 , wherein the synchronization hardware circuit comprises one or more of a router, a receiver/transmitter (Rx/Tx), or the routing table.","38 . The non-transitory computer-readable medium of claim 37 , wherein the router is to implement an application programming interface (API) defined to allow communication between sub-slices of the chiplet.","39 . The non-transitory computer-readable medium of claim 36 , wherein a kernel is to utilize query functions to inquire an SGT ID number, to inquire a number of thread groups in the STF, or to send and receive data from/to thread in the same STG as the kernel using a logical thread number.","40 . The non-transitory computer-readable medium of claim 36 , wherein the chiplet further comprises a chiplet-level cache for sub-slice communication within the chiplet and to enable communication between threads on different sub-slices of the chiplet that are within the SGT of the chiplet.",en,"CROSS-REFERENCE The present application is a continuation of U.S. Non-Provisional application Ser. No. 18/358,550, filed Jul. 25, 2023 (Attorney Docket No. AC8752-US-C1), now U.S. Patent Publication No. 2024/0012767, published Jan. 11, 2024, now allowed, which is a continuation of U.S. Non-Provisional application Ser. No. 17/212,503, filed Mar. 25, 2021 (Attorney Docket No. AC8752-US), now U.S. Pat. No. 11,755,501, issued Sep. 12, 2023, which is related to and, under 35 U.S.C. 119 (e), claims the benefit of and priority to U.S. Provisional Application 63/000,784, filed Mar. 27, 2020 (Attorney Docket No. AC8752-Z), now expired, the contents of which are incorporated herein by reference. FIELD This document relates generally to data processing and more particularly to efficient data sharing for graphics data processing operations. BACKGROUND Current parallel graphics data processing includes systems and methods developed to perform specific operations on graphics data such as, for example, linear interpolation, tessellation, rasterization, texture mapping, depth testing, etc. Traditionally, graphics processors used fixed function computational units to process graphics data. However, more recently, portions of graphics processors have been made programmable, enabling such processors to support a wider variety of operations for processing vertex and fragment data. To further increase performance, graphics processors typically implement processing techniques such as pipelining that attempt to process, in parallel, as much graphics data as possible throughout the different parts of the graphics pipeline. Parallel graphics processors with single instruction, multiple thread (SIMT) architectures are designed to maximize the amount of parallel processing in the graphics pipeline. In a SIMT architecture, groups of parallel threads attempt to execute program instructions synchronously together as often as possible to increase processing efficiency. A general overview of software and hardware for SIMT architectures can be found in Shane Cook, CUDA Programming Chapter 3, pages 37-51 (2013). A technical problem encountered with graphics processors is a lack of low-latency, high-bandwidth data sharing when performing graphics operations. Conventionally, data sharing is performed via a global cache, such as the L2 or L3 cache. This is inefficient in terms of both power consumed and latency experienced by the GPU. This conventional approach to data sharing can create issues in security and effectiveness of operation when applied in GPUs. Furthermore, conventional approaches are inefficient with respect to system resources and lead to latency. BRIEF DESCRIPTION OF THE DRAWINGS So that the manner in which the above recited features of the present embodiments can be understood in detail, a more particular description of the embodiments, briefly summarized above, may be had by reference to embodiments, some of which are illustrated in the appended drawings. It is to be noted, however, that the appended drawings illustrate only typical embodiments and are therefore not to be considered limiting of its scope. FIG. 1 is a block diagram illustrating a computer system configured to implement one or more aspects of the embodiments described herein; FIG. 2A-2D illustrate parallel processor components; FIG. 3A-3C are block diagrams of graphics multiprocessors and multiprocessor-based GPUs; FIG. 4A-4F illustrate an example architecture in which a plurality of GPUs is communicatively coupled to a plurality of multi-core processors; FIG. 5 illustrates a graphics processing pipeline; FIG. 6 illustrates a machine learning software stack; FIG. 7 illustrates a general-purpose graphics processing unit; FIG. 8 illustrates a multi-GPU computing system; FIG. 9A-9B illustrate layers of example deep neural networks; FIG. 10 illustrates an example recurrent neural network; FIG. 11 illustrates training and deployment of a deep neural network; FIG. 12A is a block diagram illustrating distributed learning; FIG. 12B is a block diagram illustrating a programmable network interface and data processing unit; FIG. 13 illustrates an example inferencing system on a chip (SOC) suitable for performing inferencing using a trained model; FIG. 14 is a block diagram of a processing system; FIG. 15A-15C illustrate computing systems and graphics processors; FIG. 16A-16C illustrate block diagrams of additional graphics processor and compute accelerator architectures; FIG. 17 is a block diagram of a graphics processing engine of a graphics processor; FIG. 18A-18B illustrate thread execution logic including an array of processing elements employed in a graphics processor core; FIG. 19 illustrates an additional execution unit; FIG. 20 is a block diagram illustrating a graphics processor instruction formats; FIG. 21 is a block diagram of an additional graphics processor architecture; FIG. 22A-22B illustrate a graphics processor command format and command sequence; FIG. 23 illustrates example graphics software architecture for a data processing system; FIG. 24A is a block diagram illustrating an IP core development system; FIG. 24B illustrates a cross-section side view of an integrated circuit package assembly; FIG. 24C illustrates a package assembly that includes multiple units of hardware logic chiplets connected to a substrate (e.g., base die); FIG. 24D illustrates a package assembly including interchangeable chiplets; FIG. 25 is a block diagram illustrating an example system on a chip integrated circuit; FIG. 26A-26B are block diagrams illustrating example graphics processors for use within an SoC; FIG. 27A is a block diagram illustrating a GPU vector register, in accordance with embodiments; FIG. 27B is a block diagram illustrating a bank of scalar registers, according to embodiments; FIG. 28 illustrates a further example execution unit of a graphic processor in accordance with embodiments; FIG. 29 is a flow diagram illustrating an embodiment of a method for provide scalar registers for GPU threads; FIG. 30 is a block diagram illustrating a cross slice direct data communication system implemented between L1 caches of multiple SS's of GPUs, according to embodiments; FIG. 31 is a block diagram illustrating an example point-to-point communication network between L1 caches in accordance with embodiments; FIG. 32 is a block diagram depicting a synchronization circuit to provide cross slice direct data communications using L1 to L1 communication, in accordance with embodiments; FIGS. 33A and 33B are flow diagrams illustrating embodiments of methods for cross slice direct data communications using L1 to L1 communication; FIG. 34 is a flow diagram illustrating an embodiment of a method for utilizing a hierarchy of thread groups for cross slice direct data communications using L1 to L1 communication; FIG. 35 is a block diagram illustrating a GPU including a set of SRAM banks implementing energy aware SRAM address mapping, according to embodiments; FIG. 36 illustrates a system implementing energy aware SRAM address mapping, according to embodiments; FIG. 37 is a flow diagram illustrating an embodiment of a method for providing energy aware SRAM address mapping in graphics processors; FIG. 38 is a block diagram illustrating a computing system for fabric block creation for memory paging, in accordance with embodiments; FIG. 39 is a block diagram illustrating a fabric block creation system for memory paging, according to embodiments; and FIG. 40 is a flow diagram illustrating an embodiment of a method for fabric block creation for DRAM paging. DETAILED DESCRIPTION A graphics processing unit (GPU) is communicatively coupled to host/processor cores to accelerate, for example, graphics operations, machine-learning operations, pattern analysis operations, and/or various general-purpose GPU (GPGPU) functions. The GPU may be communicatively coupled to the host processor/cores over a bus or another interconnect (e.g., a high-speed interconnect such as PCIe or NVLink). Alternatively, the GPU may be integrated on the same package or chip as the cores and communicatively coupled to the cores over an internal processor bus/interconnect (i.e., internal to the package or chip). Regardless of the manner in which the GPU is connected, the processor cores may allocate work to the GPU in the form of sequences of commands/instructions contained in a work descriptor. The GPU then uses dedicated circuitry/logic for efficiently processing these commands/instructions. In the following description, numerous specific details are set forth to provide a more thorough understanding. However, it will be apparent to one of skill in the art that the embodiments described herein may be practiced without one or more of these specific details. In other instances, well-known features have not been described to avoid obscuring the details of the present embodiments. System Overview FIG. 1 is a block diagram illustrating a computing system 100 configured to implement one or more aspects of the embodiments described herein. The computing system 100 includes a processing subsystem 101 having one or more processor(s) 102 and a system memory 104 communicating via an interconnection path that may include a memory hub 105 . In embodiments herein, a processor can refer to dedicated hardware circuitry for efficiently processing commands/instructions, and may be referred to as processor circuitry. The memory hub 105 may be a separate component within a chipset component or may be integrated within the one or more processor(s) 102 . The memory hub 105 couples with an I/O subsystem 111 via a communication link 106 . The I/O subsystem 111 includes an I/O hub 107 that can enable the computing system 100 to receive input from one or more input device(s) 108 . Additionally, the I/O hub 107 can enable a display controller, which may be included in the one or more processor(s) 102 , to provide outputs to one or more display device(s) 110 A. In one embodiment the one or more display device(s) 110 A coupled with the I/O hub 107 can include a local, internal, or embedded display device. The processing subsystem 101 , for example, includes one or more parallel processor(s) 112 coupled to memory hub 105 via a bus or other communication link 113 . The communication link 113 may be one of any number of standards-based communication link technologies or protocols, such as, but not limited to PCI Express, or may be a vendor specific communications interface or communications fabric. The one or more parallel processor(s) 112 may form a computationally focused parallel or vector processing system that can include a large number of processing cores and/or processing clusters, such as a many integrated core (MIC) processor. For example, the one or more parallel processor(s) 112 form a graphics processing subsystem that can output pixels to one of the one or more display device(s) 110 A coupled via the I/O Hub 107 . The one or more parallel processor(s) 112 can also include a display controller and display interface (not shown) to enable a direct connection to one or more display device(s) 110 B. Within the I/O subsystem 111 , a system storage unit 114 can connect to the I/O hub 107 to provide a storage mechanism for the computing system 100 . An I/O switch 116 can be used to provide an interface mechanism to enable connections between the I/O hub 107 and other components, such as a network adapter 118 and/or wireless network adapter 119 that may be integrated into the platform, and various other devices that can be added via one or more add-in device(s) 120 . The add-in device(s) 120 may also include, for example, one or more external graphics processor devices, graphics cards, and/or compute accelerators. The network adapter 118 can be an Ethernet adapter or another wired network adapter. The wireless network adapter 119 can include one or more of a Wi-Fi, Bluetooth, near field communication (NFC), or other network device that includes one or more wireless radios. The computing system 100 can include other components not explicitly shown, including USB or other port connections, optical storage drives, video capture devices, and the like, which may also be connected to the I/O hub 107 . Communication paths interconnecting the various components in FIG. 1 may be implemented using any suitable protocols, such as PCI (Peripheral Component Interconnect) based protocols (e.g., PCI-Express), or any other bus or point-to-point communication interfaces and/or protocol(s), such as the NVLink high-speed interconnect, Compute Express Link™ (CXL™) (e.g., CXL.mem), Infinity Fabric (IF), Ethernet (IEEE 802.3), remote direct memory access (RDMA), InfiniBand, Internet Wide Area RDMA Protocol (iWARP), Transmission Control Protocol (TCP), User Datagram Protocol (UDP), quick UDP Internet Connections (QUIC), RDMA over Converged Ethernet (ROCE), Intel QuickPath Interconnect (QPI), Intel Ultra Path Interconnect (UPI), Intel On-Chip System Fabric (IOSF), Omnipath, HyperTransport, Advanced Microcontroller Bus Architecture (AMBA) interconnect, OpenCAPI, Gen-Z, Cache Coherent Interconnect for Accelerators (CCIX), 3GPP Long Term Evolution (LTE) (4G), 3GPP 5G, and variations thereof, or wired or wireless interconnect protocols known in the art. In some examples, data can be copied or stored to virtualized storage nodes using a protocol such as non-volatile memory express (NVMe) over Fabrics (NVMe-oF) or NVMe. The one or more parallel processor(s) 112 may incorporate circuitry optimized for graphics and video processing, including, for example, video output circuitry, and constitutes a graphics processing unit (GPU). Alternatively or additionally, the one or more parallel processor(s) 112 can incorporate circuitry optimized for general purpose processing, while preserving the underlying computational architecture, described in greater detail herein. Components of the computing system 100 may be integrated with one or more other system elements on a single integrated circuit. For example, the one or more parallel processor(s) 112 , memory hub 105 , processor(s) 102 , and I/O hub 107 can be integrated into a system on chip (SoC) integrated circuit. Alternatively, the components of the computing system 100 can be integrated into a single package to form a system in package (SIP) configuration. In one embodiment at least a portion of the components of the computing system 100 can be integrated into a multi-chip module (MCM), which can be interconnected with other multi-chip modules into a modular computing system. It will be appreciated that the computing system 100 shown herein is illustrative and that variations and modifications are possible. The connection topology, including the number and arrangement of bridges, the number of processor(s) 102 , and the number of parallel processor(s) 112 , may be modified as desired. For instance, system memory 104 can be connected to the processor(s) 102 directly rather than through a bridge, while other devices communicate with system memory 104 via the memory hub 105 and the processor(s) 102 . In other alternative topologies, the parallel processor(s) 112 are connected to the I/O hub 107 or directly to one of the one or more processor(s) 102 , rather than to the memory hub 105 . In other embodiments, the I/O hub 107 and memory hub 105 may be integrated into a single chip. It is also possible that two or more sets of processor(s) 102 are attached via multiple sockets, which can couple with two or more instances of the parallel processor(s) 112 . Some of the particular components shown herein are optional and may not be included in all implementations of the computing system 100 . For example, any number of add-in cards or peripherals may be supported, or some components may be eliminated. Furthermore, some architectures may use different terminology for components similar to those illustrated in FIG. 1 . For example, the memory hub 105 may be referred to as a Northbridge in some architectures, while the I/O hub 107 may be referred to as a Southbridge. FIG. 2A illustrates a parallel processor 200 . The parallel processor 200 may be a GPU, GPGPU or the like as described herein. The various components of the parallel processor 200 may be implemented using one or more integrated circuit devices, such as programmable processors, application specific integrated circuits (ASICs), or field programmable gate arrays (FPGA). The illustrated parallel processor 200 may be one or more of the parallel processor(s) 112 shown in FIG. 1 . The parallel processor 200 includes a parallel processing unit 202 . The parallel processing unit includes an I/O unit 204 that enables communication with other devices, including other instances of the parallel processing unit 202 . The I/O unit 204 may be directly connected to other devices. For instance, the I/O unit 204 connects with other devices via the use of a hub or switch interface, such as memory hub 105 . The connections between the memory hub 105 and the I/O unit 204 form a communication link 113 . Within the parallel processing unit 202 , the I/O unit 204 connects with a host interface 206 and a memory crossbar 216 , where the host interface 206 receives commands directed to performing processing operations and the memory crossbar 216 receives commands directed to performing memory operations. When the host interface 206 receives a command buffer via the I/O unit 204 , the host interface 206 can direct work operations to perform those commands to a front end 208 . In one embodiment the front end 208 couples with a scheduler 210 , which is configured to distribute commands or other work items to a processing cluster array 212 . The scheduler 210 ensures that the processing cluster array 212 is properly configured and in a valid state before tasks are distributed to the processing clusters of the processing cluster array 212 . The scheduler 210 may be implemented via firmware logic executing on a microcontroller. The microcontroller implemented scheduler 210 is configurable to perform complex scheduling and work distribution operations at coarse and fine granularity, enabling rapid preemption and context switching of threads executing on the processing cluster array 212 . Preferably, the host software can prove workloads for scheduling on the processing cluster array 212 via one of multiple graphics processing doorbells. In other examples, polling for new workloads or interrupts can be used to identify or indicate availability of work to perform. The workloads can then be automatically distributed across the processing cluster array 212 by the scheduler 210 logic within the scheduler microcontroller. The processing cluster array 212 can include up to “N” processing clusters (e.g., cluster 214 A, cluster 214 B, through cluster 214 N). Each cluster 214 A- 214 N of the processing cluster array 212 can execute a large number of concurrent threads. The scheduler 210 can allocate work to the clusters 214 A- 214 N of the processing cluster array 212 using various scheduling and/or work distribution algorithms, which may vary depending on the workload arising for each type of program or computation. The scheduling can be handled dynamically by the scheduler 210 , or can be assisted in part by compiler logic during compilation of program logic configured for execution by the processing cluster array 212 . Optionally, different clusters 214 A- 214 N of the processing cluster array 212 can be allocated for processing different types of programs or for performing different types of computations. The processing cluster array 212 can be configured to perform various types of parallel processing operations. For example, the processing cluster array 212 is configured to perform general-purpose parallel compute operations. For example, the processing cluster array 212 can include logic to execute processing tasks including filtering of video and/or audio data, performing modeling operations, including physics operations, and performing data transformations. The processing cluster array 212 is configured to perform parallel graphics processing operations. In such embodiments in which the parallel processor 200 is configured to perform graphics processing operations, the processing cluster array 212 can include additional logic to support the execution of such graphics processing operations, including, but not limited to texture sampling logic to perform texture operations, as well as tessellation logic and other vertex processing logic. Additionally, the processing cluster array 212 can be configured to execute graphics processing related shader programs such as, but not limited to vertex shaders, tessellation shaders, geometry shaders, and pixel shaders. The parallel processing unit 202 can transfer data from system memory via the I/O unit 204 for processing. During processing the transferred data can be stored to on-chip memory (e.g., parallel processor memory 222 ) during processing, then written back to system memory. In embodiments in which the parallel processing unit 202 is used to perform graphics processing, the scheduler 210 may be configured to divide the processing workload into approximately equal sized tasks, to better enable distribution of the graphics processing operations to multiple clusters 214 A- 214 N of the processing cluster array 212 . In some of these embodiments, portions of the processing cluster array 212 can be configured to perform different types of processing. For example a first portion may be configured to perform vertex shading and topology generation, a second portion may be configured to perform tessellation and geometry shading, and a third portion may be configured to perform pixel shading or other screen space operations, to produce a rendered image for display. Intermediate data produced by one or more of the clusters 214 A- 214 N may be stored in buffers to allow the intermediate data to be transmitted between clusters 214 A- 214 N for further processing. During operation, the processing cluster array 212 can receive processing tasks to be executed via the scheduler 210 , which receives commands defining processing tasks from front end 208 . For graphics processing operations, processing tasks can include indices of data to be processed, e.g., surface (patch) data, primitive data, vertex data, and/or pixel data, as well as state parameters and commands defining how the data is to be processed (e.g., what program is to be executed). The scheduler 210 may be configured to fetch the indices corresponding to the tasks or may receive the indices from the front end 208 . The front end 208 can be configured to ensure the processing cluster array 212 is configured to a valid state before the workload specified by incoming command buffers (e.g., batch-buffers, push buffers, etc.) is initiated. Each of the one or more instances of the parallel processing unit 202 can couple with parallel processor memory 222 . The parallel processor memory 222 can be accessed via the memory crossbar 216 , which can receive memory requests from the processing cluster array 212 as well as the I/O unit 204 . The memory crossbar 216 can access the parallel processor memory 222 via a memory interface 218 . The memory interface 218 can include multiple partition units (e.g., partition unit 220 A, partition unit 220 B, through partition unit 220 N) that can each couple to a portion (e.g., memory unit) of parallel processor memory 222 . The number of partition units 220 A- 220 N may be configured to be equal to the number of memory units, such that a first partition unit 220 A has a corresponding first memory unit 224 A, a second partition unit 220 B has a corresponding second memory unit 224 B, and an Nth partition unit 220 N has a corresponding Nth memory unit 224 N. In other embodiments, the number of partition units 220 A- 220 N may not be equal to the number of memory devices. The memory units 224 A- 224 N can include various types of memory devices, including dynamic random-access memory (DRAM) or graphics random access memory, such as synchronous graphics random access memory (SGRAM), including graphics double data rate (GDDR) memory. Optionally, the memory units 224 A- 224 N may also include 3D stacked memory, including but not limited to high bandwidth memory (HBM). Persons skilled in the art will appreciate that the specific implementation of the memory units 224 A- 224 N can vary, and can be selected from one of various conventional designs. Render targets, such as frame buffers or texture maps may be stored across the memory units 224 A- 224 N, allowing partition units 220 A- 220 N to write portions of each render target in parallel to efficiently use the available bandwidth of parallel processor memory 222 . In some embodiments, a local instance of the parallel processor memory 222 may be excluded in favor of a unified memory design that utilizes system memory in conjunction with local cache memory. Optionally, any one of the clusters 214 A- 214 N of the processing cluster array 212 has the ability to process data that will be written to any of the memory units 224 A- 224 N within parallel processor memory 222 . The memory crossbar 216 can be configured to transfer the output of each cluster 214 A- 214 N to any partition unit 220 A- 220 N or to another cluster 214 A- 214 N, which can perform additional processing operations on the output. Each cluster 214 A- 214 N can communicate with the memory interface 218 through the memory crossbar 216 to read from or write to various external memory devices. In one of the embodiments with the memory crossbar 216 the memory crossbar 216 has a connection to the memory interface 218 to communicate with the I/O unit 204 , as well as a connection to a local instance of the parallel processor memory 222 , enabling the processing units within the different processing clusters 214 A- 214 N to communicate with system memory or other memory that is not local to the parallel processing unit 202 . Generally, the memory crossbar 216 may, for example, be able to use virtual channels to separate traffic streams between the clusters 214 A- 214 N and the partition units 220 A- 220 N. While a single instance of the parallel processing unit 202 is illustrated within the parallel processor 200 , any number of instances of the parallel processing unit 202 can be included. For example, multiple instances of the parallel processing unit 202 can be provided on a single add-in card, or multiple add-in cards can be interconnected. For example, the parallel processor 200 can be an add-in device, such as add-in device 120 of FIG. 1 , which may be a graphics card such as a discrete graphics card that includes one or more GPUs, one or more memory devices, and device-to-device or network or fabric interfaces. The different instances of the parallel processing unit 202 can be configured to inter-operate even if the different instances have different numbers of processing cores, different amounts of local parallel processor memory, and/or other configuration differences. Optionally, some instances of the parallel processing unit 202 can include higher precision floating point units relative to other instances. Systems incorporating one or more instances of the parallel processing unit 202 or the parallel processor 200 can be implemented in a variety of configurations and form factors, including but not limited to desktop, laptop, or handheld personal computers, servers, workstations, game consoles, and/or embedded systems. An orchestrator can form composite nodes for workload performance using one or more of: disaggregated processor resources, cache resources, memory resources, storage resources, and networking resources. FIG. 2B is a block diagram of a partition unit 220 . The partition unit 220 may be an instance of one of the partition units 220 A- 220 N of FIG. 2A . As illustrated, the partition unit 220 includes an L2 cache 221 , a frame buffer interface 225 , and a ROP 226 (raster operations unit). The L2 cache 221 is a read/write cache that is configured to perform load and store operations received from the memory crossbar 216 and ROP 226 . Read misses and write-back requests are output by L2 cache 221 to frame buffer interface 225 for processing. Updates can also be sent to the frame buffer via the frame buffer interface 225 for processing. In one embodiment the frame buffer interface 225 interfaces with one of the memory units in parallel processor memory, such as the memory units 224 A- 224 N of FIG. 2A (e.g., within parallel processor memory 222 ). The partition unit 220 may additionally or alternatively also interface with one of the memory units in parallel processor memory via a memory controller (not shown). In graphics applications, the ROP 226 is a processing unit that performs raster operations such as stencil, z test, blending, and the like. The ROP 226 then outputs processed graphics data that is stored in graphics memory. In some embodiments the ROP 226 includes or couples with a CODEC 227 that includes compression logic to compress depth or color data that is written to memory or the L2 cache 221 and decompress depth or color data that is read from memory or the L2 cache 221 . The compression logic can be lossless compression logic that makes use of one or more of multiple compression algorithms. The type of compression that is performed by the CODEC 227 can vary based on the statistical characteristics of the data to be compressed. For example, in one embodiment, delta color compression is performed on depth and color data on a per-tile basis. In one embodiment the CODEC 227 includes compression and decompression logic that can compress and decompress compute data associated with machine learning operations. The CODEC 227 can, for example, compress sparse matrix data for sparse machine learning operations. The CODEC 227 can also compress sparse matrix data that is encoded in a sparse matrix format (e.g., coordinate list encoding (COO), compressed sparse row (CSR), compress sparse column (CSC), etc.) to generate compressed and encoded sparse matrix data. The compressed and encoded sparse matrix data can be decompressed and/or decoded before being processed by processing elements or the processing elements can be configured to consume compressed, encoded, or compressed and encoded data for processing. The ROP 226 may be included within each processing cluster (e.g., cluster 214 A- 214 N of FIG. 2A ) instead of within the partition unit 220 . In such embodiment, read and write requests for pixel data are transmitted over the memory crossbar 216 instead of pixel fragment data. The processed graphics data may be displayed on a display device, such as one of the one or more display device(s) 110 of FIG. 1 , routed for further processing by the processor(s) 102 , or routed for further processing by one of the processing entities within the parallel processor 200 of FIG. 2A . FIG. 2C is a block diagram of a processing cluster 214 within a parallel processing unit. For example, the processing cluster is an instance of one of the processing clusters 214 A- 214 N of FIG. 2A . The processing cluster 214 can be configured to execute many threads in parallel, where the term “thread” refers to an instance of a particular program executing on a particular set of input data. Optionally, single-instruction, multiple-data (SIMD) instruction issue techniques may be used to support parallel execution of a large number of threads without providing multiple independent instruction units. Alternatively, single-instruction, multiple-thread (SIMT) techniques may be used to support parallel execution of a large number of generally synchronized threads, using a common instruction unit configured to issue instructions to a set of processing engines within each one of the processing clusters. Unlike a SIMD execution regime, where all processing engines typically execute identical instructions, SIMT execution allows different threads to more readily follow divergent execution paths through a given thread program. Persons skilled in the art will understand that a SIMD processing regime represents a functional subset of a SIMT processing regime. Operation of the processing cluster 214 can be controlled via a pipeline manager 232 that distributes processing tasks to SIMT parallel processors. The pipeline manager 232 receives instructions from the scheduler 210 of FIG. 2A and manages execution of those instructions via a graphics multiprocessor 234 and/or a texture unit 236 . The illustrated graphics multiprocessor 234 is an example instance of a SIMT parallel processor. However, various types of SIMT parallel processors of differing architectures may be included within the processing cluster 214 . One or more instances of the graphics multiprocessor 234 can be included within a processing cluster 214 . The graphics multiprocessor 234 can process data and a data crossbar 240 can be used to distribute the processed data to one of multiple possible destinations, including other shader units. The pipeline manager 232 can facilitate the distribution of processed data by specifying destinations for processed data to be distributed via the data crossbar 240 . Each graphics multiprocessor 234 within the processing cluster 214 can include an identical set of functional execution logic (e.g., arithmetic logic units, load-store units, etc.). The functional execution logic can be configured in a pipelined manner in which new instructions can be issued before previous instructions are complete. The functional execution logic supports a variety of operations including integer and floating-point arithmetic, comparison operations, Boolean operations, bit-shifting, and computation of various algebraic functions. The same functional-unit hardware could be leveraged to perform different operations and any combination of functional units may be present. The instructions transmitted to the processing cluster 214 constitute a thread. A set of threads executing across the set of parallel processing engines is a thread group. A thread group executes the same program on different input data. Each thread within a thread group can be assigned to a different processing engine within a graphics multiprocessor 234 . A thread group may include fewer threads than the number of processing engines within the graphics multiprocessor 234 . When a thread group includes fewer threads than the number of processing engines, one or more of the processing engines may be idle during cycles in which that thread group is being processed. A thread group may also include more threads than the number of processing engines within the graphics multiprocessor 234 . When the thread group includes more threads than the number of processing engines within the graphics multiprocessor 234 , processing can be performed over consecutive clock cycles. Optionally, multiple thread groups can be executed concurrently on the graphics multiprocessor 234 . The graphics multiprocessor 234 may include an internal cache memory to perform load and store operations. Optionally, the graphics multiprocessor 234 can forego an internal cache and use a cache memory (e.g., level 1 (L1) cache 248 ) within the processing cluster 214 . Each graphics multiprocessor 234 also has access to level 2 (L2) caches within the partition units (e.g., partition units 220 A- 220 N of FIG. 2A ) that are shared among all processing clusters 214 and may be used to transfer data between threads. The graphics multiprocessor 234 may also access off-chip global memory, which can include one or more of local parallel processor memory and/or system memory. Any memory external to the parallel processing unit 202 may be used as global memory. Embodiments in which the processing cluster 214 includes multiple instances of the graphics multiprocessor 234 can share common instructions and data, which may be stored in the L1 cache 248 . Each processing cluster 214 may include an MMU 245 (memory management unit) that is configured to map virtual addresses into physical addresses. In other embodiments, one or more instances of the MMU 245 may reside within the memory interface 218 of FIG. 2A . The MMU 245 includes a set of page table entries (PTEs) used to map a virtual address to a physical address of a tile and optionally a cache line index. The MMU 245 may include address translation lookaside buffers (TLB) or caches that may reside within the graphics multiprocessor 234 or the L1 cache or processing cluster 214 . The physical address is processed to distribute surface data access locality to allow efficient request interleaving among partition units. The cache line index may be used to determine whether a request for a cache line is a hit or miss. In graphics and computing applications, a processing cluster 214 may be configured such that each graphics multiprocessor 234 is coupled to a texture unit 236 for performing texture mapping operations, e.g., determining texture sample positions, reading texture data, and filtering the texture data. Texture data is read from an internal texture L1 cache (not shown) or in some embodiments from the L1 cache within graphics multiprocessor 234 and is fetched from an L2 cache, local parallel processor memory, or system memory. Each graphics multiprocessor 234 outputs processed tasks to the data crossbar 240 to provide the processed task to another processing cluster 214 for further processing or to store the processed task in an L2 cache, local parallel processor memory, or system memory via the memory crossbar 216 . A preROP 242 (pre-raster operations unit) is configured to receive data from graphics multiprocessor 234 , direct data to ROP units, which may be located with partition units as described herein (e.g., partition units 220 A- 220 N of FIG. 2A ). The preROP 242 unit can perform optimizations for color blending, organize pixel color data, and perform address translations. It will be appreciated that the core architecture described herein is illustrative and that variations and modifications are possible. Any number of processing units, e.g., graphics multiprocessor 234 , texture units 236 , preROPs 242 , etc., may be included within a processing cluster 214 . Further, while only one processing cluster 214 is shown, a parallel processing unit as described herein may include any number of instances of the processing cluster 214 . Optionally, each processing cluster 214 can be configured to operate independently of other processing clusters 214 using separate and distinct processing units, L1 caches, L2 caches, etc. FIG. 2D shows an example of the graphics multiprocessor 234 in which the graphics multiprocessor 234 couples with the pipeline manager 232 of the processing cluster 214 . The graphics multiprocessor 234 has an execution pipeline including but not limited to an instruction cache 252 , an instruction unit 254 , an address mapping unit 256 , a register file 258 , one or more general purpose graphics processing unit (GPGPU) cores 262 , and one or more load/store units 266 . The GPGPU cores 262 and load/store units 266 are coupled with cache memory 272 and shared memory 270 via a memory and cache interconnect 268 . The graphics multiprocessor 234 may additionally include tensor and/or ray-tracing cores 263 that include hardware logic to accelerate matrix and/or ray-tracing operations. The instruction cache 252 may receive a stream of instructions to execute from the pipeline manager 232 . The instructions are cached in the instruction cache 252 and dispatched for execution by the instruction unit 254 . The instruction unit 254 can dispatch instructions as thread groups (e.g., warps), with each thread of the thread group assigned to a different execution unit within GPGPU core 262 . An instruction can access any of a local, shared, or global address space by specifying an address within a unified address space. The address mapping unit 256 can be used to translate addresses in the unified address space into a distinct memory address that can be accessed by the load/store units 266 . The register file 258 provides a set of registers for the functional units of the graphics multiprocessor 234 . The register file 258 provides temporary storage for operands connected to the data paths of the functional units (e.g., GPGPU cores 262 , load/store units 266 ) of the graphics multiprocessor 234 . The register file 258 may be divided between each of the functional units such that each functional unit is allocated a dedicated portion of the register file 258 . For example, the register file 258 may be divided between the different warps being executed by the graphics multiprocessor 234 . The GPGPU cores 262 can each include floating point units (FPUs) and/or integer arithmetic logic units (ALUs) that are used to execute instructions of the graphics multiprocessor 234 . In some implementations, the GPGPU cores 262 can include hardware logic that may otherwise reside within the tensor and/or ray-tracing cores 263 . The GPGPU cores 262 can be similar in architecture or can differ in architecture. For example and in one embodiment, a first portion of the GPGPU cores 262 include a single precision FPU and an integer ALU while a second portion of the GPGPU cores include a double precision FPU. Optionally, the FPUs can implement the IEEE 754-2008 standard for floating point arithmetic or enable variable precision floating point arithmetic. The graphics multiprocessor 234 can additionally include one or more fixed function or special function units to perform specific functions such as copy rectangle or pixel blending operations. One or more of the GPGPU cores can also include fixed or special function logic. The GPGPU cores 262 may include SIMD logic capable of performing a single instruction on multiple sets of data. Optionally, GPGPU cores 262 can physically execute SIMD4, SIMD8, and SIMD16 instructions and logically execute SIMD1, SIMD2, and SIMD32 instructions. The SIMD instructions for the GPGPU cores can be generated at compile time by a shader compiler or automatically generated when executing programs written and compiled for single program multiple data (SPMD) or SIMT architectures. Multiple threads of a program configured for the SIMT execution model can be executed via a single SIMD instruction. For example and in one embodiment, eight SIMT threads that perform the same or similar operations can be executed in parallel via a single SIMD8 logic unit. The memory and cache interconnect 268 is an interconnect network that connects each of the functional units of the graphics multiprocessor 234 to the register file 258 and to the shared memory 270 . For example, the memory and cache interconnect 268 is a crossbar interconnect that allows the load/store unit 266 to implement load and store operations between the shared memory 270 and the register file 258 . The register file 258 can operate at the same frequency as the GPGPU cores 262 , thus data transfer between the GPGPU cores 262 and the register file 258 is low latency. The shared memory 270 can be used to enable communication between threads that execute on the functional units within the graphics multiprocessor 234 . The cache memory 272 can be used as a data cache for example, to cache texture data communicated between the functional units and the texture unit 236 . The shared memory 270 can also be used as a program managed cached. The shared memory 270 and the cache memory 272 can couple with the data crossbar 240 to enable communication with other components of the processing cluster. Threads executing on the GPGPU cores 262 can programmatically store data within the shared memory in addition to the automatically cached data that is stored within the cache memory 272 . FIG. 3A-3C illustrate additional graphics multiprocessors, according to embodiments. FIG. 3A-3B illustrate graphics multiprocessors 325 , 350 , which are related to the graphics multiprocessor 234 of FIG. 2C and may be used in place of one of those. Therefore, the disclosure of any features in combination with the graphics multiprocessor 234 herein also discloses a corresponding combination with the graphics multiprocessor(s) 325 , 350 , but is not limited to such. FIG. 3C illustrates a graphics processing unit (GPU) 380 which includes dedicated sets of graphics processing resources arranged into multi-core groups 365 A- 365 N, which correspond to the graphics multiprocessors 325 , 350 . The illustrated graphics multiprocessors 325 , 350 and the multi-core groups 365 A- 365 N can be streaming multiprocessors (SM) capable of simultaneous execution of a large number of execution threads. The graphics multiprocessor 325 of FIG. 3A includes multiple additional instances of execution resource units relative to the graphics multiprocessor 234 of FIG. 2D . For example, the graphics multiprocessor 325 can include multiple instances of the instruction unit 332 A- 332 B, register file 334 A- 334 B, and texture unit(s) 344 A- 344 B. The graphics multiprocessor 325 also includes multiple sets of graphics or compute execution units (e.g., GPGPU core 336 A- 336 B, tensor core 337 A- 337 B, ray-tracing core 338 A- 338 B) and multiple sets of load/store units 340 A- 340 B. The execution resource units have a common instruction cache 330 , texture and/or data cache memory 342 , and shared memory 346 . The various components can communicate via an interconnect fabric 327 . The interconnect fabric 327 may include one or more crossbar switches to enable communication between the various components of the graphics multiprocessor 325 . The interconnect fabric 327 may be a separate, high-speed network fabric layer upon which each component of the graphics multiprocessor 325 is stacked. The components of the graphics multiprocessor 325 communicate with remote components via the interconnect fabric 327 . For example, the cores 336 A- 336 B, 337 A- 337 B, and 338 A- 338 B can each communicate with shared memory 346 via the interconnect fabric 327 . The interconnect fabric 327 can arbitrate communication within the graphics multiprocessor 325 to ensure a fair bandwidth allocation between components. The graphics multiprocessor 350 of FIG. 3B includes multiple sets of execution resources 356 A- 356 D, where each set of execution resource includes multiple instruction units, register files, GPGPU cores, and load store units, as illustrated in FIG. 2D and FIG. 3A . The execution resources 356 A- 356 D can work in concert with texture unit(s) 360 A- 360 D for texture operations, while sharing an instruction cache 354 , and shared memory 353 . For example, the execution resources 356 A- 356 D can share an instruction cache 354 and shared memory 353 , as well as multiple instances of a texture and/or data cache memory 358 A- 358 B. The various components can communicate via an interconnect fabric 352 similar to the interconnect fabric 327 of FIG. 3A . Persons skilled in the art will understand that the architecture described in FIGS. 1, 2A-2D, and 3A-3B are descriptive and not limiting as to the scope of the present embodiments. Thus, the techniques described herein may be implemented on any properly configured processing unit, including, without limitation, one or more mobile application processors, one or more desktop or server central processing units (CPUs) including multi-core CPUs, one or more parallel processing units, such as the parallel processing unit 202 of FIG. 2A , as well as one or more graphics processors or special purpose processing units, without departure from the scope of the embodiments described herein. The parallel processor or GPGPU as described herein may be communicatively coupled to host/processor cores to accelerate graphics operations, machine-learning operations, pattern analysis operations, and various general-purpose GPU (GPGPU) functions. The GPU may be communicatively coupled to the host processor/cores over a bus or other interconnect (e.g., a high-speed interconnect such as PCIe, NVLink, or other known protocols, standardized protocols, or proprietary protocols). In other embodiments, the GPU may be integrated on the same package or chip as the cores and communicatively coupled to the cores over an internal processor bus/interconnect (i.e., internal to the package or chip). Regardless of the manner in which the GPU is connected, the processor cores may allocate work to the GPU in the form of sequences of commands/instructions contained in a work descriptor. The GPU then uses dedicated circuitry/logic for efficiently processing these commands/instructions. FIG. 3C illustrates a graphics processing unit (GPU) 380 which includes dedicated sets of graphics processing resources arranged into multi-core groups 365 A- 365 N. While the details of only a single multi-core group 365 A are provided, it will be appreciated that the other multi-core groups 365 B- 365 N may be equipped with the same or similar sets of graphics processing resources. Details described with respect to the multi-core groups 365 A- 365 N may also apply to any graphics multiprocessor 234 , 325 , 350 described herein. As illustrated, a multi-core group 365 A may include a set of graphics cores 370 , a set of tensor cores 371 , and a set of ray tracing cores 372 . A scheduler/dispatcher 368 schedules and dispatches the graphics threads for execution on the various cores 370 , 371 , 372 . A set of register files 369 store operand values used by the cores 370 , 371 , 372 when executing the graphics threads. These may include, for example, integer registers for storing integer values, floating point registers for storing floating point values, vector registers for storing packed data elements (integer and/or floating-point data elements) and tile registers for storing tensor/matrix values. The tile registers may be implemented as combined sets of vector registers. One or more combined level 1 (L1) caches and shared memory units 373 store graphics data such as texture data, vertex data, pixel data, ray data, bounding volume data, etc., locally within each multi-core group 365 A. One or more texture units 374 can also be used to perform texturing operations, such as texture mapping and sampling. A Level 2 (L2) cache 375 shared by all or a subset of the multi-core groups 365 A- 365 N stores graphics data and/or instructions for multiple concurrent graphics threads. As illustrated, the L2 cache 375 may be shared across a plurality of multi-core groups 365 A- 365 N. One or more memory controllers 367 couple the GPU 380 to a memory 366 which may be a system memory (e.g., DRAM) and/or a dedicated graphics memory (e.g., GDDR6 memory). Input/output (I/O) circuitry 363 couples the GPU 380 to one or more I/O devices 362 such as digital signal processors (DSPs), network controllers, or user input devices. An on-chip interconnect may be used to couple the I/O devices 362 to the GPU 380 and memory 366 . One or more I/O memory management units (IOMMUs) 364 of the I/O circuitry 363 couple the I/O devices 362 directly to the system memory 366 . Optionally, the IOMMU 364 manages multiple sets of page tables to map virtual addresses to physical addresses in system memory 366 . The I/O devices 362 , CPU(s) 361 , and GPU(s) 380 may then share the same virtual address space. In one implementation of the IOMMU 364 , the IOMMU 364 supports virtualization. In this case, it may manage a first set of page tables to map guest/graphics virtual addresses to guest/graphics physical addresses and a second set of page tables to map the guest/graphics physical addresses to system/host physical addresses (e.g., within system memory 366 ). The base addresses of each of the first and second sets of page tables may be stored in control registers and swapped out on a context switch (e.g., so that the new context is provided with access to the relevant set of page tables). While not illustrated in FIG. 3C , each of the cores 370 , 371 , 372 and/or multi-core groups 365 A- 365 N may include translation lookaside buffers (TLBs) to cache guest virtual to guest physical translations, guest physical to host physical translations, and guest virtual to host physical translations. The CPUs 361 , GPUs 380 , and I/O devices 362 may be integrated on a single semiconductor chip and/or chip package. The illustrated memory 366 may be integrated on the same chip or may be coupled to the memory controllers 367 via an off-chip interface. In one implementation, the memory 366 comprises GDDR6 memory which shares the same virtual address space as other physical system-level memories, although the underlying principles described herein are not limited to this specific implementation. The tensor cores 371 may include a plurality of execution units specifically designed to perform matrix operations, which are the basic compute operation used to perform deep learning operations. For example, simultaneous matrix multiplication operations may be used for neural network training and inferencing. The tensor cores 371 may perform matrix processing using a variety of operand precisions including single precision floating-point (e.g., 32 bits), half-precision floating point (e.g., 16 bits), integer words (16 bits), bytes (8 bits), and half-bytes (4 bits). For example, a neural network implementation extracts features of each rendered scene, potentially combining details from multiple frames, to construct a high-quality final image. In deep learning implementations, parallel matrix multiplication work may be scheduled for execution on the tensor cores 371 . The training of neural networks, in particular, utilizes a significant number matrix dot product operations. In order to process an inner-product formulation of an N×N×N matrix multiply, the tensor cores 371 may include at least N dot-product processing elements. Before the matrix multiply begins, one entire matrix is loaded into tile registers and at least one column of a second matrix is loaded each cycle for N cycles. Each cycle, there are N dot products that are processed. Matrix elements may be stored at different precisions depending on the particular implementation, including 16-bit words, 8-bit bytes (e.g., INT8) and 4-bit half-bytes (e.g., INT4). Different precision modes may be specified for the tensor cores 371 to ensure that the most efficient precision is used for different workloads (e.g., such as inferencing workloads which can tolerate quantization to bytes and half-bytes). Supported formats additionally include 64-bit floating point (FP64) and non-IEEE floating point formats such as the bfloat16 format (e.g., Brain floating point), a 16-bit floating point format with one sign bit, eight exponents bits, and eight significand bits, of which seven are explicitly stored. One embodiment includes support for a reduced precision tensor-float format (TF32), which has the range of FP32 (8-bits) with the precision of FP16 (10-bits). Reduced precision TF32 operations can be performed on FP32 inputs and produce FP32 outputs at higher performance relative to FP32 and increased precision relative to FP16. In one embodiment the tensor cores 371 support a sparse mode of operation for matrices in which the vast majority of values are zero. The tensor cores 371 include support for sparse input matrices that are encoded in a sparse matrix representation (e.g., coordinate list encoding (COO), compressed sparse row (CSR), compress sparse column (CSC), etc.). The tensor cores 371 also include support for compressed sparse matrix representations in the event that the sparse matrix representation may be further compressed. Compressed, encoded, and/or compressed and encoded matrix data, along with associated compression and/or encoding metadata, can be ready by the tensor cores 371 and the non-zero values can be extracted. For example, for a given input matrix A, a non-zero value can be loaded from the compressed and/or encoded representation of at least a portion of matrix A. Based on the location in matrix A for the non-zero value, which may be determined from index or coordinate metadata associated with the non-zero value, a corresponding value in input matrix B may be loaded. Depending on the operation to be performed (e.g., multiply), the load of the value from input matrix B may be bypassed if the corresponding value is a zero value. In one embodiment, the pairings of values for certain operations, such as multiply operations, may be pre-scanned by scheduler logic and only operations between non-zero inputs are scheduled. Depending on the dimensions of matrix A and matrix B and the operation to be performed, output matrix C may be dense or sparse. Where output matrix C is sparse, and depending on the configuration of the tensor cores 371 , output matrix C may be output in a compressed format, a sparse encoding, or a compressed sparse encoding. The ray tracing cores 372 may accelerate ray tracing operations for both real-time ray tracing and non-real-time ray tracing implementations. In particular, the ray tracing cores 372 may include ray traversal/intersection circuitry for performing ray traversal using bounding volume hierarchies (BVHs) and identifying intersections between rays and primitives enclosed within the BVH volumes. The ray tracing cores 372 may also include circuitry for performing depth testing and culling (e.g., using a Z buffer or similar arrangement). In one implementation, the ray tracing cores 372 perform traversal and intersection operations in concert with the image denoising techniques described herein, at least a portion of which may be executed on the tensor cores 371 . For example, the tensor cores 371 may implement a deep learning neural network to perform denoising of frames generated by the ray tracing cores 372 . However, the CPU(s) 361 , graphics cores 370 , and/or ray tracing cores 372 may also implement all or a portion of the denoising and/or deep learning algorithms. In addition, as described above, a distributed approach to denoising may be employed in which the GPU 380 is in a computing device coupled to other computing devices over a network or high-speed interconnect. In this distributed approach, the interconnected computing devices may share neural network learning/training data to improve the speed with which the overall system learns to perform denoising for different types of image frames and/or different graphics applications. The ray tracing cores 372 may process all BVH traversal and/or ray-primitive intersections, saving the graphics cores 370 from being overloaded with thousands of instructions per ray. For example, each ray tracing core 372 includes a first set of specialized circuitry for performing bounding box tests (e.g., for traversal operations) and/or a second set of specialized circuitry for performing the ray-triangle intersection tests (e.g., intersecting rays which have been traversed). Thus, for example, the multi-core group 365 A can simply launch a ray probe, and the ray tracing cores 372 independently perform ray traversal and intersection and return hit data (e.g., a hit, no hit, multiple hits, etc.) to the thread context. The other cores 370 , 371 are freed to perform other graphics or compute work while the ray tracing cores 372 perform the traversal and intersection operations. Optionally, each ray tracing core 372 may include a traversal unit to perform BVH testing operations and/or an intersection unit which performs ray-primitive intersection tests. The intersection unit generates a “hit”, “no hit”, or “multiple hit” response, which it provides to the appropriate thread. During the traversal and intersection operations, the execution resources of the other cores (e.g., graphics cores 370 and tensor cores 371 ) are freed to perform other forms of graphics work. In one optional embodiment described below, a hybrid rasterization/ray tracing approach is used in which work is distributed between the graphics cores 370 and ray tracing cores 372 . The ray tracing cores 372 (and/or other cores 370 , 371 ) may include hardware support for a ray tracing instruction set such as Microsoft's DirectX Ray Tracing (DXR) which includes a DispatchRays command, as well as ray-generation, closest-hit, any-hit, and miss shaders, which enable the assignment of sets of shaders and textures for each object. Another ray tracing platform which may be supported by the ray tracing cores 372 , graphics cores 370 and tensor cores 371 is Vulkan 1.1.85. Note, however, that the underlying principles described herein are not limited to any particular ray tracing ISA. In general, the various cores 372 , 371 , 370 may support a ray tracing instruction set that includes instructions/functions for one or more of ray generation, closest hit, any hit, ray-primitive intersection, per-primitive and hierarchical bounding box construction, miss, visit, and exceptions. More specifically, an embodiment includes ray tracing instructions to perform one or more of the following functions: Ray Generation—Ray generation instructions may be executed for each pixel, sample, or other user-defined work assignment. Closest Hit—A closest hit instruction may be executed to locate the closest intersection point of a ray with primitives within a scene. Any Hit—An any hit instruction identifies multiple intersections between a ray and primitives within a scene, potentially to identify a new closest intersection point. Intersection—An intersection instruction performs a ray-primitive intersection test and outputs a result. Per-primitive Bounding box Construction—This instruction builds a bounding box around a given primitive or group of primitives (e.g., when building a new BVH or other acceleration data structure). Miss—Indicates that a ray misses all geometry within a scene, or specified region of a scene. Visit—Indicates the children volumes a ray will traverse. Exceptions—Includes various types of exception handlers (e.g., invoked for various error conditions). In one embodiment the ray tracing cores 372 may be adapted to accelerate general-purpose compute operations that can be accelerated using computational techniques that are analogous to ray intersection tests. A compute framework can be provided that enables shader programs to be compiled into low level instructions and/or primitives that perform general-purpose compute operations via the ray tracing cores. Example computational problems that can benefit from compute operations performed on the ray tracing cores 372 include computations involving beam, wave, ray, or particle propagation within a coordinate space. Interactions associated with that propagation can be computed relative to a geometry or mesh within the coordinate space. For example, computations associated with electromagnetic signal propagation through an environment can be accelerated via the use of instructions or primitives that are executed via the ray tracing cores. Diffraction and reflection of the signals by objects in the environment can be computed as direct ray-tracing analogies. Ray tracing cores 372 can also be used to perform computations that are not directly analogous to ray tracing. For example, mesh projection, mesh refinement, and volume sampling computations can be accelerated using the ray tracing cores 372 . Generic coordinate space calculations, such as nearest neighbor calculations can also be performed. For example, the set of points near a given point can be discovered by defining a bounding box in the coordinate space around the point. BVH and ray probe logic within the ray tracing cores 372 can then be used to determine the set of point intersections within the bounding box. The intersections constitute the origin point and the nearest neighbors to that origin point. Computations that are performed using the ray tracing cores 372 can be performed in parallel with computations performed on the graphics cores 372 and tensor cores 371 . A shader compiler can be configured to compile a compute shader or other general-purpose graphics processing program into low level primitives that can be parallelized across the graphics cores 370 , tensor cores 371 , and ray tracing cores 372 . Techniques for GPU to Host Processor Interconnection FIG. 4A illustrates an example architecture in which a plurality of GPUs 410 - 413 , e.g. such as the parallel processors 200 shown in FIG. 2A , are communicatively coupled to a plurality of multi-core processors 405 - 406 over high-speed links 440 A- 440 D (e.g., buses, point-to-point interconnects, etc.). The high-speed links 440 A- 440 D may support a communication throughput of 4 GB/s, 30 GB/s, 80 GB/s or higher, depending on the implementation. Various interconnect protocols may be used including, but not limited to, PCIe 4.0 or 5.0 and NVLink 2.0. However, the underlying principles described herein are not limited to any particular communication protocol or throughput. Two or more of the GPUs 410 - 413 may be interconnected over high-speed links 442 A- 442 B, which may be implemented using the same or different protocols/links than those used for high-speed links 440 A- 440 D. Similarly, two or more of the multi-core processors 405 - 406 may be connected over high speed link 443 which may be symmetric multi-processor (SMP) buses operating at 20 GB/s, 30 GB/s, 120 GB/s or lower or higher speeds. Alternatively, all communication between the various system components shown in FIG. 4A may be accomplished using the same protocols/links (e.g., over a common interconnection fabric). As mentioned, however, the underlying principles described herein are not limited to any particular type of interconnect technology. Each multi-core processor 405 - 406 may be communicatively coupled to a processor memory 401 - 402 , via memory interconnects 430 A- 430 B, respectively, and each GPU 410 - 413 is communicatively coupled to GPU memory 420 - 423 over GPU memory interconnects 450 A- 450 D, respectively. The memory interconnects 430 A- 430 B and 450 A- 450 D may utilize the same or different memory access technologies. By way of example, and not limitation, the processor memories 401 - 402 and GPU memories 420 - 423 may be volatile memories such as dynamic random-access memories (DRAMs) (including stacked DRAMs), Graphics DDR SDRAM (GDDR) (e.g., GDDR5, GDDR6), or High Bandwidth Memory (HBM) and/or may be non-volatile memories such as 3D XPoint/Optane or Nano-Ram. For example, some portion of the memories may be volatile memory and another portion may be non-volatile memory (e.g., using a two-level memory (2LM) hierarchy). A memory subsystem as described herein may be compatible with a number of memory technologies, such as Double Data Rate versions released by JEDEC (Joint Electronic Device Engineering Council). As described below, although the various processors 405 - 406 and GPUs 410 - 413 may be physically coupled to a particular memory 401 - 402 , 420 - 423 , respectively, a unified memory architecture may be implemented in which the same virtual system address space (also referred to as the “effective address” space) is distributed among all of the various physical memories. For example, processor memories 401 - 402 may each comprise 64 GB of the system memory address space and GPU memories 420 - 423 may each comprise 32 GB of the system memory address space (resulting in a total of 256 GB addressable memory in this example). FIG. 4B illustrates additional optional details for an interconnection between a multi-core processor 407 and a graphics acceleration module 446 . The graphics acceleration module 446 may include one or more GPU chips integrated on a line card which is coupled to the processor 407 via the high-speed link 440 . Alternatively, the graphics acceleration module 446 may be integrated on the same package or chip as the processor 407 . The illustrated processor 407 includes a plurality of cores 460 A- 460 D, each with a translation lookaside buffer 461 A- 461 D and one or more caches 462 A- 462 D. The cores may include various other components for executing instructions and processing data which are not illustrated to avoid obscuring the underlying principles of the components described herein (e.g., instruction fetch units, branch prediction units, decoders, execution units, reorder buffers, etc.). The caches 462 A- 462 D may comprise level 1 (L1) and level 2 (L2) caches. In addition, one or more shared caches 456 may be included in the caching hierarchy and shared by sets of the cores 460 A- 460 D. For example, one embodiment of the processor 407 includes 24 cores, each with its own L1 cache, twelve shared L2 caches, and twelve shared L3 caches. In this embodiment, one of the L2 and L3 caches are shared by two adjacent cores. The processor 407 and the graphics accelerator integration module 446 connect with system memory 441 , which may include processor memories 401 - 402 . Coherency is maintained for data and instructions stored in the various caches 462 A- 462 D, 456 and system memory 441 via inter-core communication over a coherence bus 464 . For example, each cache may have cache coherency logic/circuitry associated therewith to communicate to over the coherence bus 464 in response to detected reads or writes to particular cache lines. In one implementation, a cache snooping protocol is implemented over the coherence bus 464 to snoop cache accesses. Cache snooping/coherency techniques are well understood by those of skill in the art and will not be described in detail here to avoid obscuring the underlying principles described herein. A proxy circuit 425 may be provided that communicatively couples the graphics acceleration module 446 to the coherence bus 464 , allowing the graphics acceleration module 446 to participate in the cache coherence protocol as a peer of the cores. In particular, an interface 435 provides connectivity to the proxy circuit 425 over high-speed link 440 (e.g., a PCIe bus, NVLink, etc.) and an interface 437 connects the graphics acceleration module 446 to the high-speed link 440 . In one implementation, an accelerator integration circuit 436 provides cache management, memory access, context management, and interrupt management services on behalf of a plurality of graphics processing engines 431 , 432 , N of the graphics acceleration module 446 . The graphics processing engines 431 , 432 , N may each comprise a separate graphics processing unit (GPU). Alternatively, the graphics processing engines 431 , 432 , N may comprise different types of graphics processing engines within a GPU such as graphics execution units, media processing engines (e.g., video encoders/decoders), samplers, and blit engines. In other words, the graphics acceleration module may be a GPU with a plurality of graphics processing engines 431 - 432 , N or the graphics processing engines 431 - 432 , N may be individual GPUs integrated on a common package, line card, or chip. The accelerator integration circuit 436 may include a memory management unit (MMU) 439 for performing various memory management functions such as virtual-to-physical memory translations (also referred to as effective-to-real memory translations) and memory access protocols for accessing system memory 441 . The MMU 439 may also include a translation lookaside buffer (TLB) (not shown) for caching the virtual/effective to physical/real address translations. In one implementation, a cache 438 stores commands and data for efficient access by the graphics processing engines 431 , 432 , N. The data stored in cache 438 and graphics memories 433 - 434 , M may be kept coherent with the core caches 462 A- 462 D, 456 and system memory 441 . As mentioned, this may be accomplished via proxy circuit 425 which takes part in the cache coherency mechanism on behalf of cache 438 and memories 433 - 434 , M (e.g., sending updates to the cache 438 related to modifications/accesses of cache lines on processor caches 462 A- 462 D, 456 and receiving updates from the cache 438 ). A set of registers 445 store context data for threads executed by the graphics processing engines 431 - 432 , N and a context management circuit 448 manages the thread contexts. For example, the context management circuit 448 may perform save and restore operations to save and restore contexts of the various threads during contexts switches (e.g., where a first thread is saved and a second thread is restored so that the second thread can be execute by a graphics processing engine). For example, on a context switch, the context management circuit 448 may store current register values to a designated region in memory (e.g., identified by a context pointer). It may then restore the register values when returning to the context. An interrupt management circuit 447 , for example, may receive and processes interrupts received from system devices. In one implementation, virtual/effective addresses from a graphics processing engine 431 are translated to real/physical addresses in system memory 441 by the MMU 439 . Optionally, the accelerator integration circuit 436 supports multiple (e.g., 4, 8, 16) graphics accelerator modules 446 and/or other accelerator devices. The graphics accelerator module 446 may be dedicated to a single application executed on the processor 407 or may be shared between multiple applications. Optionally, a virtualized graphics execution environment is provided in which the resources of the graphics processing engines 431 - 432 , N are shared with multiple applications, virtual machines (VMs), or containers. The resources may be subdivided into “slices” which are allocated to different VMs and/or applications based on the processing requirements and priorities associated with the VMs and/or applications. VMs and containers can be used interchangeably herein. A virtual machine (VM) can be software that runs an operating system and one or more applications. A VM can be defined by specification, configuration files, virtual disk file, non-volatile random access memory (NVRAM) setting file, and the log file and is backed by the physical resources of a host computing platform. A VM can include an operating system (OS) or application environment that is installed on software, which imitates dedicated hardware. The end user has the same experience on a virtual machine as they would have on dedicated hardware. Specialized software, called a hypervisor, emulates the PC client or server's CPU, memory, hard disk, network and other hardware resources completely, enabling virtual machines to share the resources. The hypervisor can emulate multiple virtual hardware platforms that are isolated from each other, allowing virtual machines to run Linux®, Windows® Server, VMware ESXi, and other operating systems on the same underlying physical host. A container can be a software package of applications, configurations and dependencies so the applications run reliably on one computing environment to another. Containers can share an operating system installed on the server platform and run as isolated processes. A container can be a software package that contains everything the software utilizes to run such as system tools, libraries, and settings. Containers are not installed like traditional software programs, which allows them to be isolated from the other software and the operating system itself. The isolated nature of containers provides several benefits. First, the software in a container will run the same in different environments. For example, a container that includes PHP and MySQL can run identically on both a Linux® computer and a Windows® machine. Second, containers provide added security since the software will not affect the host operating system. While an installed application may alter system settings and modify resources, such as the Windows registry, a container can only modify settings within the container. Thus, the accelerator integration circuit 436 acts as a bridge to the system for the graphics acceleration module 446 and provides address translation and system memory cache services. In one embodiment, to facilitate the bridging functionality, the accelerator integration circuit 436 may also include shared I/O 497 (e.g., PCIe, USB, or others) and hardware to enable system control of voltage, clocking, performance, thermals, and security. The shared I/O 497 may utilize separate physical connections or may traverse the high-speed link 440 . In addition, the accelerator integration circuit 436 may provide virtualization facilities for the host processor to manage virtualization of the graphics processing engines, interrupts, and memory management. Because hardware resources of the graphics processing engines 431 - 432 , N are mapped explicitly to the real address space seen by the host processor 407 , any host processor can address these resources directly using an effective address value. One optional function of the accelerator integration circuit 436 is the physical separation of the graphics processing engines 431 - 432 , N so that they appear to the system as independent units. One or more graphics memories 433 - 434 , M may be coupled to each of the graphics processing engines 431 - 432 , N, respectively. The graphics memories 433 - 434 , M store instructions and data being processed by each of the graphics processing engines 431 - 432 , N. The graphics memories 433 - 434 , M may be volatile memories such as DRAMs (including stacked DRAMs), GDDR memory (e.g., GDDR5, GDDR6), or HBM, and/or may be non-volatile memories such as 3D XPoint/Optane, Samsung Z-NAND, or Nano-Ram. To reduce data traffic over the high-speed link 440 , biasing techniques may be used to ensure that the data stored in graphics memories 433 - 434 , M is data which will be used most frequently by the graphics processing engines 431 - 432 , N and preferably not used by the cores 460 A- 460 D (at least not frequently). Similarly, the biasing mechanism attempts to keep data utilized by the cores (and preferably not the graphics processing engines 431 - 432 , N) within the caches 462 A- 462 D, 456 of the cores and system memory 441 . According to a variant shown in FIG. 4C the accelerator integration circuit 436 is integrated within the processor 407 . The graphics processing engines 431 - 432 , N communicate directly over the high-speed link 440 to the accelerator integration circuit 436 via interface 437 and interface 435 (which, again, may be utilize any form of bus or interface protocol). The accelerator integration circuit 436 may perform the same operations as those described with respect to FIG. 4B , but potentially at a higher throughput given its close proximity to the coherence bus 464 and caches 462 A- 462 D, 456 . The embodiments described may support different programming models including a dedicated-process programming model (no graphics acceleration module virtualization) and shared programming models (with virtualization). The latter may include programming models which are controlled by the accelerator integration circuit 436 and programming models which are controlled by the graphics acceleration module 446 . In the embodiments of the dedicated process model, graphics processing engines 431 , 432 , . . . . N may be dedicated to a single application or process under a single operating system. The single application can funnel other application requests to the graphics engines 431 , 432 , . . . . N, providing virtualization within a VM/partition. In the dedicated-process programming models, the graphics processing engines 431 , 432 , N, may be shared by multiple VM/application partitions. The shared models utilize a system hypervisor to virtualize the graphics processing engines 431 - 432 , N to allow access by each operating system. For single-partition systems without a hypervisor, the graphics processing engines 431 - 432 , N are owned by the operating system. In both cases, the operating system can virtualize the graphics processing engines 431 - 432 , N to provide access to each process or application. For the shared programming model, the graphics acceleration module 446 or an individual graphics processing engine 431 - 432 , N selects a process element using a process handle. The process elements may be stored in system memory 441 and be addressable using the effective address to real address translation techniques described herein. The process handle may be an implementation-specific value provided to the host process when registering its context with the graphics processing engine 431 - 432 , N (that is, calling system software to add the process element to the process element linked list). The lower 16-bits of the process handle may be the offset of the process element within the process element linked list. FIG. 4D illustrates an example accelerator integration slice 490 . As used herein, a “slice” comprises a specified portion of the processing resources of the accelerator integration circuit 436 . Application effective address space 482 within system memory 441 stores process elements 483 . The process elements 483 may be stored in response to GPU invocations 481 from applications 480 executed on the processor 407 . A process element 483 contains the process state for the corresponding application 480 . A work descriptor (WD) 484 contained in the process element 483 can be a single job requested by an application or may contain a pointer to a queue of jobs. In the latter case, the WD 484 is a pointer to the job request queue in the application's address space 482 . The graphics acceleration module 446 and/or the individual graphics processing engines 431 - 432 , N can be shared by all or a subset of the processes in the system. For example, the technologies described herein may include an infrastructure for setting up the process state and sending a WD 484 to a graphics acceleration module 446 to start a job in a virtualized environment. In one implementation, the dedicated-process programming model is implementation-specific. In this model, a single process owns the graphics acceleration module 446 or an individual graphics processing engine 431 . Because the graphics acceleration module 446 is owned by a single process, the hypervisor initializes the accelerator integration circuit 436 for the owning partition and the operating system initializes the accelerator integration circuit 436 for the owning process at the time when the graphics acceleration module 446 is assigned. In operation, a WD fetch unit 491 in the accelerator integration slice 490 fetches the next WD 484 which includes an indication of the work to be done by one of the graphics processing engines of the graphics acceleration module 446 . Data from the WD 484 may be stored in registers 445 and used by the MMU 439 , interrupt management circuit 447 and/or context management circuit 448 as illustrated. For example, the MMU 439 may include segment/page walk circuitry for accessing segment/page tables 486 within the OS virtual address space 485 . The interrupt management circuit 447 may process interrupt events 492 received from the graphics acceleration module 446 . When performing graphics operations, an effective address 493 generated by a graphics processing engine 431 - 432 , N is translated to a real address by the MMU 439 . The same set of registers 445 may be duplicated for each graphics processing engine 431 - 432 , N and/or graphics acceleration module 446 and may be initialized by the hypervisor or operating system. Each of these duplicated registers may be included in an accelerator integration slice 490 . In one embodiment, each graphics processing engine 431 - 432 , N may be presented to the hypervisor 496 as a distinct graphics processor device. QoS settings can be configured for clients of a specific graphics processing engine 431 - 432 , N and data isolation between the clients of each engine can be enabled. Example registers that may be initialized by the hypervisor are shown in Table 1. TABLE 1Hypervisor Initialized Registers1Slice Control Register2Real Address (RA) Scheduled Processes Area Pointer3Authority Mask Override Register4Interrupt Vector Table Entry Offset5Interrupt Vector Table Entry Limit6State Register7Logical Partition ID8Real address (RA) Hypervisor Accelerator Utilization Record Pointer9Storage Description Register Example registers that may be initialized by the operating system are shown in Table 2. TABLE 2Operating System Initialized Registers1Process and Thread Identification2Effective Address (EA) Context Save/Restore Pointer3Virtual Address (VA) Accelerator Utilization Record Pointer4Virtual Address (VA) Storage Segment Table Pointer5Authority Mask6Work descriptor Each WD 484 may be specific to a particular graphics acceleration module 446 and/or graphics processing engine 431 - 432 , N. It contains all the information a graphics processing engine 431 - 432 , N utilizes to do its work or it can be a pointer to a memory location where the application has set up a command queue of work to be completed. FIG. 4E illustrates additional optional details of a shared model. It includes a hypervisor real address space 498 in which a process element list 499 is stored. The hypervisor real address space 498 is accessible via a hypervisor 496 which virtualizes the graphics acceleration module engines for the operating system 495 . The shared programming models allow for all or a subset of processes from all or a subset of partitions in the system to use a graphics acceleration module 446 . There are two programming models where the graphics acceleration module 446 is shared by multiple processes and partitions: time-sliced shared and graphics directed shared. In this model, the system hypervisor 496 owns the graphics acceleration module 446 and makes its function available to all operating systems 495 . For a graphics acceleration module 446 to support virtualization by the system hypervisor 496 , the graphics acceleration module 446 may adhere to the following requirements: 1) An application's job request should be autonomous (that is, the state does not have to be maintained between jobs), or the graphics acceleration module 446 should provide a context save and restore mechanism. 2) An application's job request is guaranteed by the graphics acceleration module 446 to complete in a specified amount of time, including any translation faults, or the graphics acceleration module 446 provides the ability to preempt the processing of the job. 3) The graphics acceleration module 446 should be guaranteed fairness between processes when operating in the directed shared programming model. For the shared model, the application 480 may be utilized to make an operating system 495 system call with a graphics acceleration module 446 type, a work descriptor (WD), an authority mask register (AMR) value, and a context save/restore area pointer (CSRP). The graphics acceleration module 446 type describes the targeted acceleration function for the system call. The graphics acceleration module 446 type may be a system-specific value. The WD is formatted specifically for the graphics acceleration module 446 and can be in the form of a graphics acceleration module 446 command, an effective address pointer to a user-defined structure, an effective address pointer to a queue of commands, or any other data structure to describe the work to be done by the graphics acceleration module 446 . In one embodiment, the AMR value is the AMR state to use for the current process. The value passed to the operating system is similar to an application setting the AMR. If the accelerator integration circuit 436 and graphics acceleration module 446 implementations do not support a User Authority Mask Override Register (UAMOR), the operating system may apply the current UAMOR value to the AMR value before passing the AMR in the hypervisor call. The hypervisor 496 may optionally apply the current Authority Mask Override Register (AMOR) value before placing the AMR into the process element 483 . The CSRP may be one of the registers 445 containing the effective address of an area in the application's address space 482 for the graphics acceleration module 446 to save and restore the context state. This pointer is optional if no state is to be saved between jobs or when a job is preempted. The context save/restore area may be pinned system memory. Upon receiving the system call, the operating system 495 may verify that the application 480 has registered and been given the authority to use the graphics acceleration module 446 . The operating system 495 then calls the hypervisor 496 with the information shown in Table 3. TABLE 3OS to Hypervisor Call Parameters1A work descriptor (WD)2An Authority Mask Register (AMR) value (potentially masked).3An effective address (EA) Context Save/Restore Area Pointer (CSRP)4A process ID (PID) and optional thread ID (TID)5A virtual address (VA) accelerator utilization record pointer (AURP)6Virtual address of storage segment table pointer (SSTP)7A logical interrupt service number (LISN) Upon receiving the hypervisor call, the hypervisor 496 verifies that the operating system 495 has registered and been given the authority to use the graphics acceleration module 446 . The hypervisor 496 then puts the process element 483 into the process element linked list for the corresponding graphics acceleration module 446 type. The process element may include the information shown in Table 4. TABLE 4Process Element Information1A work descriptor (WD)2An Authority Mask Register (AMR) value (potentially masked).3An effective address (EA) Context Save/Restore Area Pointer (CSRP)4A process ID (PID) and optional thread ID (TID)5A virtual address (VA) accelerator utilization record pointer (AURP)6Virtual address of storage segment table pointer (SSTP)7A logical interrupt service number (LISN)8Interrupt vector table, derived from the hypervisor call parameters.9A state register (SR) value10A logical partition ID (LPID)11A real address (RA) hypervisor accelerator utilization record pointer12The Storage Descriptor Register (SDR) The hypervisor may initialize a plurality of accelerator integration slice 490 registers 445 . As illustrated in FIG. 4F , in one optional implementation a unified memory addressable via a common virtual memory address space used to access the physical processor memories 401 - 402 and GPU memories 420 - 423 is employed. In this implementation, operations executed on the GPUs 410 - 413 utilize the same virtual/effective memory address space to access the processors memories 401 - 402 and vice versa, thereby simplifying programmability. A first portion of the virtual/effective address space may be allocated to the processor memory 401 , a second portion to the second processor memory 402 , a third portion to the GPU memory 420 , and so on. The entire virtual/effective memory space (sometimes referred to as the effective address space) may thereby be distributed across each of the processor memories 401 - 402 and GPU memories 420 - 423 , allowing any processor or GPU to access any physical memory with a virtual address mapped to that memory. Bias/coherence management circuitry 494 A- 494 E within one or more of the MMUs 439 A- 439 E may be provided that ensures cache coherence between the caches of the host processors (e.g., 405 ) and the GPUs 410 - 413 and implements biasing techniques indicating the physical memories in which certain types of data should be stored. While multiple instances of bias/coherence management circuitry 494 A- 494 E are illustrated in FIG. 4F , the bias/coherence circuitry may be implemented within the MMU of one or more host processors 405 and/or within the accelerator integration circuit 436 . The GPU-attached memory 420 - 423 may be mapped as part of system memory, and accessed using shared virtual memory (SVM) technology, but without suffering the typical performance drawbacks associated with full system cache coherence. The ability to GPU-attached memory 420 - 423 to be accessed as system memory without onerous cache coherence overhead provides a beneficial operating environment for GPU offload. This arrangement allows the host processor 405 software to setup operands and access computation results, without the overhead of tradition I/O DMA data copies. Such traditional copies involve driver calls, interrupts and memory mapped I/O (MMIO) accesses that are all inefficient relative to simple memory accesses. At the same time, the ability to access GPU attached memory 420 - 423 without cache coherence overheads can be relevant to the execution time of an offloaded computation. In cases with substantial streaming write memory traffic, for example, cache coherence overhead can significantly reduce the effective write bandwidth seen by a GPU 410 - 413 . The efficiency of operand setup, the efficiency of results access, and the efficiency of GPU computation all play a role in determining the effectiveness of GPU offload. A selection between GPU bias and host processor bias may be driven by a bias tracker data structure. A bias table may be used, for example, which may be a page-granular structure (i.e., controlled at the granularity of a memory page) that includes 1 or 2 bits per GPU-attached memory page. The bias table may be implemented in a stolen memory range of one or more GPU-attached memories 420 - 423 , with or without a bias cache in the GPU 410 - 413 (e.g., to cache frequently/recently used entries of the bias table). Alternatively, the entire bias table may be maintained within the GPU. In one implementation, the bias table entry associated with each access to the GPU-attached memory 420 - 423 is accessed prior the actual access to the GPU memory, causing the following operations. First, local requests from the GPU 410 - 413 that find their page in GPU bias are forwarded directly to a corresponding GPU memory 420 - 423 . Local requests from the GPU that find their page in host bias are forwarded to the processor 405 (e.g., over a high-speed link as discussed above). Optionally, requests from the processor 405 that find the requested page in host processor bias complete the request like a normal memory read. Alternatively, requests directed to a GPU-biased page may be forwarded to the GPU 410 - 413 . The GPU may then transition the page to a host processor bias if it is not currently using the page. The bias state of a page can be changed either by a software-based mechanism, a hardware-assisted software-based mechanism, or, for a limited set of cases, a purely hardware-based mechanism. One mechanism for changing the bias state employs an API call (e.g. OpenCL), which, in turn, calls the GPU's device driver which, in turn, sends a message (or enqueues a command descriptor) to the GPU directing it to change the bias state and, for some transitions, perform a cache flushing operation in the host. The cache flushing operation is utilized for a transition from host processor 405 bias to GPU bias, but is not utilized for the opposite transition. Cache coherency may be maintained by temporarily rendering GPU-biased pages uncacheable by the host processor 405 . To access these pages, the processor 405 may request access from the GPU 410 which may or may not grant access right away, depending on the implementation. Thus, to reduce communication between the host processor 405 and GPU 410 it is beneficial to ensure that GPU-biased pages are those which are utilized by the GPU but not the host processor 405 and vice versa. Graphics Processing Pipeline FIG. 5 illustrates a graphics processing pipeline 500 . A graphics multiprocessor, such as graphics multiprocessor 234 as in FIG. 2D , graphics multiprocessor 325 of FIG. 3A , graphics multiprocessor 350 of FIG. 3B can implement the illustrated graphics processing pipeline 500 . The graphics multiprocessor can be included within the parallel processing subsystems as described herein, such as the parallel processor 200 of FIG. 2A , which may be related to the parallel processor(s) 112 of FIG. 1 and may be used in place of one of those. The various parallel processing systems can implement the graphics processing pipeline 500 via one or more instances of the parallel processing unit (e.g., parallel processing unit 202 of FIG. 2A ) as described herein. For example, a shader unit (e.g., graphics multiprocessor 234 of FIG. 2C ) may be configured to perform the functions of one or more of a vertex processing unit 504 , a tessellation control processing unit 508 , a tessellation evaluation processing unit 512 , a geometry processing unit 516 , and a fragment/pixel processing unit 524 . The functions of data assembler 502 , primitive assemblers 506 , 514 , 518 , tessellation unit 510 , rasterizer 522 , and raster operations unit 526 may also be performed by other processing engines within a processing cluster (e.g., processing cluster 214 of FIG. 2A ) and a corresponding partition unit (e.g., partition unit 220 A- 220 N of FIG. 2A ). The graphics processing pipeline 500 may also be implemented using dedicated processing units for one or more functions. It is also possible that one or more portions of the graphics processing pipeline 500 are performed by parallel processing logic within a general-purpose processor (e.g., CPU). Optionally, one or more portions of the graphics processing pipeline 500 can access on-chip memory (e.g., parallel processor memory 222 as in FIG. 2A ) via a memory interface 528 , which may be an instance of the memory interface 218 of FIG. 2A . The graphics processor pipeline 500 may also be implemented via a multi-core group 365 A as in FIG. 3C . The data assembler 502 is a processing unit that may collect vertex data for surfaces and primitives. The data assembler 502 then outputs the vertex data, including the vertex attributes, to the vertex processing unit 504 . The vertex processing unit 504 is a programmable execution unit that executes vertex shader programs, lighting and transforming vertex data as specified by the vertex shader programs. The vertex processing unit 504 reads data that is stored in cache, local or system memory for use in processing the vertex data and may be programmed to transform the vertex data from an object-based coordinate representation to a world space coordinate space or a normalized device coordinate space. A first instance of a primitive assembler 506 receives vertex attributes from the vertex processing unit 504 . The primitive assembler 506 readings stored vertex attributes and constructs graphics primitives for processing by tessellation control processing unit 508 . The graphics primitives include triangles, line segments, points, patches, and so forth, as supported by various graphics processing application programming interfaces (APIs). The tessellation control processing unit 508 treats the input vertices as control points for a geometric patch. The control points are transformed from an input representation from the patch (e.g., the patch's bases) to a representation that is suitable for use in surface evaluation by the tessellation evaluation processing unit 512 . The tessellation control processing unit 508 can also compute tessellation factors for edges of geometric patches. A tessellation factor applies to a single edge and quantifies a view-dependent level of detail associated with the edge. A tessellation unit 510 is configured to receive the tessellation factors for edges of a patch and to tessellate the patch into multiple geometric primitives such as line, triangle, or quadrilateral primitives, which are transmitted to a tessellation evaluation processing unit 512 . The tessellation evaluation processing unit 512 operates on parameterized coordinates of the subdivided patch to generate a surface representation and vertex attributes for each vertex associated with the geometric primitives. A second instance of a primitive assembler 514 receives vertex attributes from the tessellation evaluation processing unit 512 , reading stored vertex attributes, and constructs graphics primitives for processing by the geometry processing unit 516 . The geometry processing unit 516 is a programmable execution unit that executes geometry shader programs to transform graphics primitives received from primitive assembler 514 as specified by the geometry shader programs. The geometry processing unit 516 may be programmed to subdivide the graphics primitives into one or more new graphics primitives and calculate parameters used to rasterize the new graphics primitives. The geometry processing unit 516 may be able to add or delete elements in the geometry stream. The geometry processing unit 516 outputs the parameters and vertices specifying new graphics primitives to primitive assembler 518 . The primitive assembler 518 receives the parameters and vertices from the geometry processing unit 516 and constructs graphics primitives for processing by a viewport scale, cull, and clip unit 520 . The geometry processing unit 516 reads data that is stored in parallel processor memory or system memory for use in processing the geometry data. The viewport scale, cull, and clip unit 520 performs clipping, culling, and viewport scaling and outputs processed graphics primitives to a rasterizer 522 . The rasterizer 522 can perform depth culling and other depth-based optimizations. The rasterizer 522 also performs scan conversion on the new graphics primitives to generate fragments and output those fragments and associated coverage data to the fragment/pixel processing unit 524 . The fragment/pixel processing unit 524 is a programmable execution unit that is configured to execute fragment shader programs or pixel shader programs. The fragment/pixel processing unit 524 transforming fragments or pixels received from rasterizer 522 , as specified by the fragment or pixel shader programs. For example, the fragment/pixel processing unit 524 may be programmed to perform operations included but not limited to texture mapping, shading, blending, texture correction and perspective correction to produce shaded fragments or pixels that are output to a raster operations unit 526 . The fragment/pixel processing unit 524 can read data that is stored in either the parallel processor memory or the system memory for use when processing the fragment data. Fragment or pixel shader programs may be configured to shade at sample, pixel, tile, or other granularities depending on the sampling rate configured for the processing units. The raster operations unit 526 is a processing unit that performs raster operations including, but not limited to stencil, z-test, blending, and the like, and outputs pixel data as processed graphics data to be stored in graphics memory (e.g., parallel processor memory 222 as in FIG. 2A , and/or system memory 104 as in FIG. 1 ), to be displayed on the one or more display device(s) 110 or for further processing by one of the one or more processor(s) 102 or parallel processor(s) 112 . The raster operations unit 526 may be configured to compress z or color data that is written to memory and decompress z or color data that is read from memory. Machine Learning Overview The architecture described above can be applied to perform training and inference operations using machine learning models. Machine learning has been successful at solving many kinds of tasks. The computations that arise when training and using machine learning algorithms (e.g., neural networks) lend themselves naturally to efficient parallel implementations. Accordingly, parallel processors such as general-purpose graphic processing units (GPGPUs) have played a significant role in the practical implementation of deep neural networks. Parallel graphics processors with single instruction, multiple thread (SIMT) architectures are designed to maximize the amount of parallel processing in the graphics pipeline. In an SIMT architecture, groups of parallel threads attempt to execute program instructions synchronously together as often as possible to increase processing efficiency. The efficiency provided by parallel machine learning algorithm implementations allows the use of high capacity networks and enables those networks to be trained on larger datasets. A machine learning algorithm is an algorithm that can learn based on a set of data. For example, machine learning algorithms can be designed to model high-level abstractions within a data set. For example, image recognition algorithms can be used to determine which of several categories to which a given input belong; regression algorithms can output a numerical value given an input; and pattern recognition algorithms can be used to generate translated text or perform text to speech and/or speech recognition. An example type of machine learning algorithm is a neural network. There are many types of neural networks; a simple type of neural network is a feedforward network. A feedforward network may be implemented as an acyclic graph in which the nodes are arranged in layers. Typically, a feedforward network topology includes an input layer and an output layer that are separated by at least one hidden layer. The hidden layer transforms input received by the input layer into a representation that is useful for generating output in the output layer. The network nodes are fully connected via edges to the nodes in adjacent layers, but there are no edges between nodes within each layer. Data received at the nodes of an input layer of a feedforward network are propagated (i.e., “fed forward”) to the nodes of the output layer via an activation function that calculates the states of the nodes of each successive layer in the network based on coefficients (“weights”) respectively associated with each of the edges connecting the layers. Depending on the specific model being represented by the algorithm being executed, the output from the neural network algorithm can take various forms. Before a machine learning algorithm can be used to model a particular problem, the algorithm is trained using a training data set. Training a neural network involves selecting a network topology, using a set of training data representing a problem being modeled by the network, and adjusting the weights until the network model performs with a minimal error for all instances of the training data set. For example, during a supervised learning training process for a neural network, the output produced by the network in response to the input representing an instance in a training data set is compared to the “correct” labeled output for that instance, an error signal representing the difference between the output and the labeled output is calculated, and the weights associated with the connections are adjusted to minimize that error as the error signal is backward propagated through the layers of the network. The network is considered “trained” when the errors for each of the outputs generated from the instances of the training data set are minimized. The accuracy of a machine learning algorithm can be affected significantly by the quality of the data set used to train the algorithm. The training process can be computationally intensive and may utilize a significant amount of time on a conventional general-purpose processor. Accordingly, parallel processing hardware is used to train many types of machine learning algorithms. This is particularly useful for optimizing the training of neural networks, as the computations performed in adjusting the coefficients in neural networks lend themselves naturally to parallel implementations. Specifically, many machine learning algorithms and software applications have been adapted to make use of the parallel processing hardware within general-purpose graphics processing devices. FIG. 6 is a generalized diagram of a machine learning software stack 600 . A machine learning application 602 is any logic that can be configured to train a neural network using a training dataset or to use a trained deep neural network to implement machine intelligence. The machine learning application 602 can include training and inference functionality for a neural network and/or specialized software that can be used to train a neural network before deployment. The machine learning application 602 can implement any type of machine intelligence including but not limited to image recognition, mapping and localization, autonomous navigation, speech synthesis, medical imaging, or language translation. Example machine learning applications 602 include, but are not limited to, voice-based virtual assistants, image or facial recognition algorithms, autonomous navigation, and the software tools that are used to train the machine learning models used by the machine learning applications 602 . Hardware acceleration for the machine learning application 602 can be enabled via a machine learning framework 604 . The machine learning framework 604 can provide a library of machine learning primitives. Machine learning primitives are basic operations that are commonly performed by machine learning algorithms. Without the machine learning framework 604 , developers of machine learning algorithms would be utilized to create and optimize the main computational logic associated with the machine learning algorithm, then re-optimize the computational logic as new parallel processors are developed. Instead, the machine learning application can be configured to perform the computations using the primitives provided by the machine learning framework 604 . Example primitives include tensor convolutions, activation functions, and pooling, which are computational operations that are performed while training a convolutional neural network (CNN). The machine learning framework 604 can also provide primitives to implement basic linear algebra subprograms performed by many machine-learning algorithms, such as matrix and vector operations. Examples of a machine learning framework 604 include, but are not limited to, TensorFlow, TensorRT, PyTorch, MXNet, Caffee, and other high-level machine learning frameworks. The machine learning framework 604 can process input data received from the machine learning application 602 and generate the appropriate input to a compute framework 606 . The compute framework 606 can abstract the underlying instructions provided to the GPGPU driver 608 to enable the machine learning framework 604 to take advantage of hardware acceleration via the GPGPU hardware 610 without requiring the machine learning framework 604 to have intimate knowledge of the architecture of the GPGPU hardware 610 . Additionally, the compute framework 606 can enable hardware acceleration for the machine learning framework 604 across a variety of types and generations of the GPGPU hardware 610 . Example compute frameworks 606 include the CUDA compute framework and associated machine learning libraries, such as the CUDA Deep Neural Network (cuDNN) library. The machine learning software stack 600 can also include communication libraries or frameworks to facilitate multi-GPU and multi-node compute. GPGPU Machine Learning Acceleration FIG. 7 illustrates a general-purpose graphics processing unit 700 , which may be the parallel processor 200 of FIG. 2A or the parallel processor(s) 112 of FIG. 1 . The general-purpose processing unit (GPGPU) 700 may be configured to provide support for hardware acceleration of primitives provided by a machine learning framework to accelerate the processing the type of computational workloads associated with training deep neural networks. Additionally, the GPGPU 700 can be linked directly to other instances of the GPGPU to create a multi-GPU cluster to improve training speed for particularly deep neural networks. Primitives are also supported to accelerate inference operations for deployed neural networks. The GPGPU 700 includes a host interface 702 to enable a connection with a host processor. The host interface 702 may be a PCI Express interface. However, the host interface can also be a vendor specific communications interface or communications fabric. The GPGPU 700 receives commands from the host processor and uses a global scheduler 704 to distribute execution threads associated with those commands to a set of processing clusters 706 A- 706 H. The processing clusters 706 A- 706 H share a cache memory 708 . The cache memory 708 can serve as a higher-level cache for cache memories within the processing clusters 706 A- 706 H. The illustrated processing clusters 706 A- 706 H may correspond with processing clusters 214 A- 214 N as in FIG. 2A . The GPGPU 700 includes memory 714 A- 714 B coupled with the processing clusters 706 A- 706 H via a set of memory controllers 712 A- 712 B. The memory 714 A- 714 B can include various types of memory devices including dynamic random-access memory (DRAM) or graphics random access memory, such as synchronous graphics random access memory (SGRAM), including graphics double data rate (GDDR) memory. The memory 714 A- 714 B may also include 3D stacked memory, including but not limited to high bandwidth memory (HBM). Each of the processing clusters 706 A- 706 H may include a set of graphics multiprocessors, such as the graphics multiprocessor 234 of FIG. 2D , graphics multiprocessor 325 of FIG. 3A , graphics multiprocessor 350 of FIG. 3B , or may include a multi-core group 365 A- 365 N as in FIG. 3C . The graphics multiprocessors of the compute cluster include multiple types of integer and floating-point logic units that can perform computational operations at a range of precisions including suited for machine learning computations. For example, at least a subset of the floating-point units in each of the processing clusters 706 A- 706 H can be configured to perform 16-bit or 32-bit floating point operations, while a different subset of the floating-point units can be configured to perform 64-bit floating point operations. Multiple instances of the GPGPU 700 can be configured to operate as a compute cluster. The communication mechanism used by the compute cluster for synchronization and data exchange varies across embodiments. For example, the multiple instances of the GPGPU 700 communicate over the host interface 702 . In one embodiment the GPGPU 700 includes an I/O hub 709 that couples the GPGPU 700 with a GPU link 710 that enables a direct connection to other instances of the GPGPU. The GPU link 710 may be coupled to a dedicated GPU-to-GPU bridge that enables communication and synchronization between multiple instances of the GPGPU 700 . Optionally, the GPU link 710 couples with a high-speed interconnect to transmit and receive data to other GPGPUs or parallel processors. The multiple instances of the GPGPU 700 may be located in separate data processing systems and communicate via a network device that is accessible via the host interface 702 . The GPU link 710 may be configured to enable a connection to a host processor in addition to or as an alternative to the host interface 702 . While the illustrated configuration of the GPGPU 700 can be configured to train neural networks, an alternate configuration of the GPGPU 700 can be configured for deployment within a high performance or low power inferencing platform. In an inferencing configuration, the GPGPU 700 includes fewer of the processing clusters 706 A- 706 H relative to the training configuration. Additionally, memory technology associated with the memory 714 A- 714 B may differ between inferencing and training configurations. In one embodiment, the inferencing configuration of the GPGPU 700 can support inferencing specific instructions. For example, an inferencing configuration can provide support for one or more 8-bit integer dot product instructions, which are commonly used during inferencing operations for deployed neural networks. FIG. 8 illustrates a multi-GPU computing system 800 . The multi-GPU computing system 800 can include a processor 802 coupled to multiple GPGPUs 806 A- 806 D via a host interface switch 804 . The host interface switch 804 may be a PCI express switch device that couples the processor 802 to a PCI express bus over which the processor 802 can communicate with the set of GPGPUs 806 A- 806 D. Each of the multiple GPGPUs 806 A- 806 D can be an instance of the GPGPU 700 of FIG. 7 . The GPGPUs 806 A- 806 D can interconnect via a set of high-speed point to point GPU to GPU links 816 . The high-speed GPU to GPU links can connect to each of the GPGPUs 806 A- 806 D via a dedicated GPU link, such as the GPU link 710 as in FIG. 7 . The P2P GPU links 816 enable direct communication between each of the GPGPUs 806 A- 806 D without requiring communication over the host interface bus to which the processor 802 is connected. With GPU-to-GPU traffic directed to the P2P GPU links, the host interface bus remains available for system memory access or to communicate with other instances of the multi-GPU computing system 800 , for example, via one or more network devices. While in FIG. 8 the GPGPUs 806 A- 806 D connect to the processor 802 via the host interface switch 804 , the processor 802 may alternatively include direct support for the P2P GPU links 816 and connect directly to the GPGPUs 806 A- 806 D. In one embodiment the P2P GPU link 816 enable the multi-GPU computing system 800 to operate as a single logical GPU. Machine Learning Neural Network Implementations The computing architecture described herein can be configured to perform the types of parallel processing that is particularly suited for training and deploying neural networks for machine learning. A neural network can be generalized as a network of functions having a graph relationship. As is well-known in the art, there are a variety of types of neural network implementations used in machine learning. One example type of neural network is the feedforward network, as previously described. A second example type of neural network is the Convolutional Neural Network (CNN). A CNN is a specialized feedforward neural network for processing data having a known, grid-like topology, such as image data. Accordingly, CNNs are commonly used for compute vision and image recognition applications, but they also may be used for other types of pattern recognition such as speech and language processing. The nodes in the CNN input layer are organized into a set of “filters” (feature detectors inspired by the receptive fields found in the retina), and the output of each set of filters is propagated to nodes in successive layers of the network. The computations for a CNN include applying the convolution mathematical operation to each filter to produce the output of that filter. Convolution is a specialized kind of mathematical operation performed by two functions to produce a third function that is a modified version of one of the two original functions. In convolutional network terminology, the first function to the convolution can be referred to as the input, while the second function can be referred to as the convolution kernel. The output may be referred to as the feature map. For example, the input to a convolution layer can be a multidimensional array of data that defines the various color components of an input image. The convolution kernel can be a multidimensional array of parameters, where the parameters are adapted by the training process for the neural network. Recurrent neural networks (RNNs) are a family of feedforward neural networks that include feedback connections between layers. RNNs enable modeling of sequential data by sharing parameter data across different parts of the neural network. The architecture for an RNN includes cycles. The cycles represent the influence of a present value of a variable on its own value at a future time, as at least a portion of the output data from the RNN is used as feedback for processing subsequent input in a sequence. This feature makes RNNs particularly useful for language processing due to the variable nature in which language data can be composed. The figures described below present example feedforward, CNN, and RNN networks, as well as describe a general process for respectively training and deploying each of those types of networks. It will be understood that these descriptions are example and non-limiting as to any specific embodiment described herein and the concepts illustrated can be applied generally to deep neural networks and machine learning techniques in general. The example neural networks described above can be used to perform deep learning. Deep learning is machine learning using deep neural networks. The deep neural networks used in deep learning are artificial neural networks composed of multiple hidden layers, as opposed to shallow neural networks that include only a single hidden layer. Deeper neural networks are generally more computationally intensive to train. However, the additional hidden layers of the network enable multistep pattern recognition that results in reduced output error relative to shallow machine learning techniques. Deep neural networks used in deep learning typically include a front-end network to perform feature recognition coupled to a back-end network which represents a mathematical model that can perform operations (e.g., object classification, speech recognition, etc.) based on the feature representation provided to the model. Deep learning enables machine learning to be performed without requiring hand crafted feature engineering to be performed for the model. Instead, deep neural networks can learn features based on statistical structure or correlation within the input data. The learned features can be provided to a mathematical model that can map detected features to an output. The mathematical model used by the network is generally specialized for the specific task to be performed, and different models will be used to perform different task. Once the neural network is structured, a learning model can be applied to the network to train the network to perform specific tasks. The learning model describes how to adjust the weights within the model to reduce the output error of the network. Backpropagation of errors is a common method used to train neural networks. An input vector is presented to the network for processing. The output of the network is compared to the desired output using a loss function and an error value is calculated for each of the neurons in the output layer. The error values are then propagated backwards until each neuron has an associated error value which roughly represents its contribution to the original output. The network can then learn from those errors using an algorithm, such as the stochastic gradient descent algorithm, to update the weights of the of the neural network. FIG. 9A-9B illustrate an example convolutional neural network. FIG. 9A illustrates various layers within a CNN. As shown in FIG. 9A , an example CNN used to model image processing can receive input 902 describing the red, green, and blue (RGB) components of an input image. The input 902 can be processed by multiple convolutional layers (e.g., convolutional layer 904 , convolutional layer 906 ). The output from the multiple convolutional layers may optionally be processed by a set of fully connected layers 908 . Neurons in a fully connected layer have full connections to all activations in the previous layer, as previously described for a feedforward network. The output from the fully connected layers 908 can be used to generate an output result from the network. The activations within the fully connected layers 908 can be computed using matrix multiplication instead of convolution. Not all CNN implementations make use of fully connected layers 908 . For example, in some implementations the convolutional layer 906 can generate output for the CNN. The convolutional layers are sparsely connected, which differs from traditional neural network configuration found in the fully connected layers 908 . Traditional neural network layers are fully connected, such that every output unit interacts with every input unit. However, the convolutional layers are sparsely connected because the output of the convolution of a field is input (instead of the respective state value of each of the nodes in the field) to the nodes of the subsequent layer, as illustrated. The kernels associated with the convolutional layers perform convolution operations, the output of which is sent to the next layer. The dimensionality reduction performed within the convolutional layers is one aspect that enables the CNN to scale to process large images. FIG. 9B illustrates example computation stages within a convolutional layer of a CNN. Input to a convolutional layer 912 of a CNN can be processed in three stages of a convolutional layer 914 . The three stages can include a convolution stage 916 , a detector stage 918 , and a pooling stage 920 . The convolutional layer 914 can then output data to a successive convolutional layer. The final convolutional layer of the network can generate output feature map data or provide input to a fully connected layer, for example, to generate a classification value for the input to the CNN. In the convolution stage 916 performs several convolutions in parallel to produce a set of linear activations. The convolution stage 916 can include an affine transformation, which is any transformation that can be specified as a linear transformation plus a translation. Affine transformations include rotations, translations, scaling, and combinations of these transformations. The convolution stage computes the output of functions (e.g., neurons) that are connected to specific regions in the input, which can be determined as the local region associated with the neuron. The neurons compute a dot product between the weights of the neurons and the region in the local input to which the neurons are connected. The output from the convolution stage 916 defines a set of linear activations that are processed by successive stages of the convolutional layer 914 . The linear activations can be processed by a detector stage 918 . In the detector stage 918 , each linear activation is processed by a non-linear activation function. The non-linear activation function increases the nonlinear properties of the overall network without affecting the receptive fields of the convolution layer. Several types of non-linear activation functions may be used. One particular type is the rectified linear unit (ReLU), which uses an activation function defined as ƒ(x)=max (0, x), such that the activation is thresholded at zero. The pooling stage 920 uses a pooling function that replaces the output of the convolutional layer 906 with a summary statistic of the nearby outputs. The pooling function can be used to introduce translation invariance into the neural network, such that small translations to the input do not change the pooled outputs. Invariance to local translation can be useful in scenarios where the presence of a feature in the input data is more important than the precise location of the feature. Various types of pooling functions can be used during the pooling stage 920 , including max pooling, average pooling, and l2-norm pooling. Additionally, some CNN implementations do not include a pooling stage. Instead, such implementations substitute and additional convolution stage having an increased stride relative to previous convolution stages. The output from the convolutional layer 914 can then be processed by the next layer 922 . The next layer 922 can be an additional convolutional layer or one of the fully connected layers 908 . For example, the first convolutional layer 904 of FIG. 9A can output to the second convolutional layer 906 , while the second convolutional layer can output to a first layer of the fully connected layers 908 . FIG. 10 illustrates an example recurrent neural network 1000 . In a recurrent neural network (RNN), the previous state of the network influences the output of the current state of the network. RNNs can be built in a variety of ways using a variety of functions. The use of RNNs generally revolves around using mathematical models to predict the future based on a prior sequence of inputs. For example, an RNN may be used to perform statistical language modeling to predict an upcoming word given a previous sequence of words. The illustrated RNN 1000 can be described has having an input layer 1002 that receives an input vector, hidden layers 1004 to implement a recurrent function, a feedback mechanism 1005 to enable a ‘memory’ of previous states, and an output layer 1006 to output a result. The RNN 1000 operates based on time-steps. The state of the RNN at a given time step is influenced based on the previous time step via the feedback mechanism 1005 . For a given time step, the state of the hidden layers 1004 is defined by the previous state and the input at the current time step. An initial input (x 1 ) at a first time step can be processed by the hidden layer 1004 . A second input (x 2 ) can be processed by the hidden layer 1004 using state information that is determined during the processing of the initial input (x 1 ). A given state can be computed as s t =ƒ(Ux t +Ws t-1 ), where U and W are parameter matrices. The function ƒ is generally a nonlinearity, such as the hyperbolic tangent function (Tanh) or a variant of the rectifier function ƒ(x)=max(0,x). However, the specific mathematical function used in the hidden layers 1004 can vary depending on the specific implementation details of the RNN 1000 . In addition to the basic CNN and RNN networks described, acceleration for variations on those networks may be enabled. One example RNN variant is the long short term memory (LSTM) RNN. LSTM RNNs are capable of learning long-term dependencies that may be used for processing longer sequences of language. A variant on the CNN is a convolutional deep belief network, which has a structure similar to a CNN and is trained in a manner similar to a deep belief network. A deep belief network (DBN) is a generative neural network that is composed of multiple layers of stochastic (random) variables. DBNs can be trained layer-by-layer using greedy unsupervised learning. The learned weights of the DBN can then be used to provide pre-train neural networks by determining an initial set of weights for the neural network. In further embodiments, acceleration for reinforcement learning is enabled. In reinforcement learning, an artificial agent learn by interacting with its environment. The agent is configured to optimize certain objectives to maximize cumulative rewards. FIG. 11 illustrates training and deployment of a deep neural network. Once a given network has been structured for a task the neural network is trained using a training dataset 1102 . Various training frameworks 1104 have been developed to enable hardware acceleration of the training process. For example, the machine learning framework 604 of FIG. 6 may be configured as a training framework 604 . The training framework 604 can hook into an untrained neural network 1106 and enable the untrained neural net to be trained using the parallel processing resources described herein to generate a trained neural network 1108 . To start the training process the initial weights may be chosen randomly or by pre-training using a deep belief network. The training cycle then be performed in either a supervised or unsupervised manner. Supervised learning is a learning method in which training is performed as a mediated operation, such as when the training dataset 1102 includes input paired with the desired output for the input, or where the training dataset includes input having known output and the output of the neural network is manually graded. The network processes the inputs and compares the resulting outputs against a set of expected or desired outputs. Errors are then propagated back through the system. The training framework 1104 can adjust to adjust the weights that control the untrained neural network 1106 . The training framework 1104 can provide tools to monitor how well the untrained neural network 1106 is converging towards a model suitable to generating correct answers based on known input data. The training process occurs repeatedly as the weights of the network are adjusted to refine the output generated by the neural network. The training process can continue until the neural network reaches a statistically desired accuracy associated with a trained neural net 1108 . The trained neural network 1108 can then be deployed to implement any number of machine learning operations to generate an inference result 1114 based on input of new data 1112 . Unsupervised learning is a learning method in which the network attempts to train itself using unlabeled data. Thus, for unsupervised learning the training dataset 1102 will include input data without any associated output data. The untrained neural network 1106 can learn groupings within the unlabeled input and can determine how individual inputs are related to the overall dataset. Unsupervised training can be used to generate a self-organizing map, which is a type of trained neural network 1108 capable of performing operations useful in reducing the dimensionality of data. Unsupervised training can also be used to perform anomaly detection, which allows the identification of data points in an input dataset that deviate from the normal patterns of the data. Variations on supervised and unsupervised training may also be employed. Semi-supervised learning is a technique in which in the training dataset 1102 includes a mix of labeled and unlabeled data of the same distribution. Incremental learning is a variant of supervised learning in which input data is continuously used to further train the model. Incremental learning enables the trained neural network 1108 to adapt to the new data 1112 without forgetting the knowledge instilled within the network during initial training. Whether supervised or unsupervised, the training process for particularly deep neural networks may be too computationally intensive for a single compute node. Instead of using a single compute node, a distributed network of computational nodes can be used to accelerate the training process. FIG. 12A is a block diagram illustrating distributed learning. Distributed learning is a training model that uses multiple distributed computing nodes to perform supervised or unsupervised training of a neural network. The distributed computational nodes can each include one or more host processors and one or more of the general-purpose processing nodes, such as the highly parallel general-purpose graphics processing unit 700 as in FIG. 7 . As illustrated, distributed learning can be performed with model parallelism 1202 , data parallelism 1204 , or a combination of model and data parallelism 1206 . In model parallelism 1202 , different computational nodes in a distributed system can perform training computations for different parts of a single network. For example, each layer of a neural network can be trained by a different processing node of the distributed system. The benefits of model parallelism include the ability to scale to particularly large models. Splitting the computations associated with different layers of the neural network enables the training of large neural networks in which the weights of all layers would not fit into the memory of a single computational node. In some instances, model parallelism can be particularly useful in performing unsupervised training of large neural networks. In data parallelism 1204 , the different nodes of the distributed network have a complete instance of the model and each node receives a different portion of the data. The results from the different nodes are then combined. While different approaches to data parallelism are possible, data parallel training approaches all utilize a technique of combining results and synchronizing the model parameters between each node. Example approaches to combining data include parameter averaging and update based data parallelism. Parameter averaging trains each node on a subset of the training data and sets the global parameters (e.g., weights, biases) to the average of the parameters from each node. Parameter averaging uses a central parameter server that maintains the parameter data. Update based data parallelism is similar to parameter averaging except that instead of transferring parameters from the nodes to the parameter server, the updates to the model are transferred. Additionally, update based data parallelism can be performed in a decentralized manner, where the updates are compressed and transferred between nodes. Combined model and data parallelism 1206 can be implemented, for example, in a distributed system in which each computational node includes multiple GPUs. Each node can have a complete instance of the model with separate GPUs within each node are used to train different portions of the model. Distributed training has increased overhead relative to training on a single machine. However, the parallel processors and GPGPUs described herein can each implement various techniques to reduce the overhead of distributed training, including techniques to enable high bandwidth GPU-to-GPU data transfer and accelerated remote data synchronization. FIG. 12B is a block diagram illustrating a programmable network interface 1210 and data processing unit. The programmable network interface 1210 is a programmable network engine that can be used to accelerate network-based compute tasks within a distributed environment. The programmable network interface 1210 can couple with a host system via host interface 1270 . The programmable network interface 1210 can be used to accelerate network or storage operations for CPUs or GPUs of the host system. The host system can be, for example, a node of a distributed learning system used to perform distributed training, for example, as shown in FIG. 12A . The host system can also be a data center node within a data center. In one embodiment, access to remote storage containing model data can be accelerated by the programmable network interface 1210 . For example, the programmable network interface 1210 can be configured to present remote storage devices as local storage devices to the host system. The programmable network interface 1210 can also accelerate remote direct memory access (RDMA) operations performed between GPUs of the host system with GPUs of remote systems. In one embodiment, the programmable network interface 1210 can enable storage functionality such as, but not limited to NVME-OF. The programmable network interface 1210 can also accelerate encryption, data integrity, compression, and other operations for remote storage on behalf of the host system, allowing remote storage to approach the latencies of storage devices that are directly attached to the host system. The programmable network interface 1210 can also perform resource allocation and management on behalf of the host system. Storage security operations can be offloaded to the programmable network interface 1210 and performed in concert with the allocation and management of remote storage resources. Network-based operations to manage access to the remote storage that would otherwise by performed by a processor of the host system can instead be performed by the programmable network interface 1210 . In one embodiment, network and/or data security operations can be offloaded from the host system to the programmable network interface 1210 . Data center security policies for a data center node can be handled by the programmable network interface 1210 instead of the processors of the host system. For example, the programmable network interface 1210 can detect and mitigate against an attempted network-based attack (e.g., DDOS) on the host system, preventing the attack from compromising the availability of the host system. The programmable network interface 1210 can include a system on a chip (SoC 1220 ) that executes an operating system via multiple processor cores 1222 . The processor cores 1222 can include general-purpose processor (e.g., CPU) cores. In one embodiment the processor cores 1222 can also include one or more GPU cores. The SoC 1220 can execute instructions stored in a memory device 1240 . A storage device 1250 can store local operating system data. The storage device 1250 and memory device 1240 can also be used to cache remote data for the host system. Network ports 1260 A- 1260 B enable a connection to a network or fabric and facilitate network access for the SoC 1220 and, via the host interface 1270 , for the host system. The programmable network interface 1210 can also include an I/O interface 1275 , such as a USB interface. The I/O interface 1275 can be used to couple external devices to the programmable network interface 1210 or as a debug interface. The programmable network interface 1210 also includes a management interface 1230 that enables software on the host device to manage and configure the programmable network interface 1210 and/or SoC 1220 . In one embodiment the programmable network interface 1210 may also include one or more accelerators or GPUs 1245 to accept offload of parallel compute tasks from the SoC 1220 , host system, or remote systems coupled via the network ports 1260 A- 1260 B. Example Machine Learning Applications Machine learning can be applied to solve a variety of technological problems, including but not limited to computer vision, autonomous driving and navigation, speech recognition, and language processing. Computer vision has traditionally been one of the most active research areas for machine learning applications. Applications of computer vision range from reproducing human visual abilities, such as recognizing faces, to creating new categories of visual abilities. For example, computer vision applications can be configured to recognize sound waves from the vibrations induced in objects visible in a video. Parallel processor accelerated machine learning enables computer vision applications to be trained using significantly larger training dataset than previously feasible and enables inferencing systems to be deployed using low power parallel processors. Parallel processor accelerated machine learning has autonomous driving applications including lane and road sign recognition, obstacle avoidance, navigation, and driving control. Accelerated machine learning techniques can be used to train driving models based on datasets that define the appropriate responses to specific training input. The parallel processors described herein can enable rapid training of the increasingly complex neural networks used for autonomous driving solutions and enables the deployment of low power inferencing processors in a mobile platform suitable for integration into autonomous vehicles. Parallel processor accelerated deep neural networks have enabled machine learning approaches to automatic speech recognition (ASR). ASR includes the creation of a function that computes the most probable linguistic sequence given an input acoustic sequence. Accelerated machine learning using deep neural networks have enabled the replacement of the hidden Markov models (HMMs) and Gaussian mixture models (GMMs) previously used for ASR. Parallel processor accelerated machine learning can also be used to accelerate natural language processing. Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to erroneous or unfamiliar input. Example natural language processor applications include automatic machine translation between human languages. The parallel processing platforms used for machine learning can be divided into training platforms and deployment platforms. Training platforms are generally highly parallel and include optimizations to accelerate multi-GPU single node training and multi-node, multi-GPU training. Example parallel processors suited for training include the general-purpose graphics processing unit 700 of FIG. 7 and the multi-GPU computing system 800 of FIG. 8 . On the contrary, deployed machine learning platforms generally include lower power parallel processors suitable for use in products such as cameras, autonomous robots, and autonomous vehicles. Additionally, machine learning techniques can be applied to accelerate or enhance graphics processing activities. For example, a machine learning model can be trained to recognize output generated by a GPU accelerated application and generate an upscaled version of that output. Such techniques can be applied to accelerate the generation of high resolution images for a gaming application. Various other graphics pipeline activities can benefit from the use of machine learning. For example, machine learning models can be trained to perform tessellation operations on geometry data to increase the complexity of geometric models, allowing fine-detailed geometry to be automatically generated from geometry of relatively lower detail. FIG. 13 illustrates an example inferencing system on a chip (SOC) 1300 suitable for performing inferencing using a trained model. The SOC 1300 can integrate processing components including a media processor 1302 , a vision processor 1304 , a GPGPU 1306 and a multi-core processor 1308 . The GPGPU 1306 may be a GPGPU as described herein, such as the GPGPU 700 , and the multi-core processor 1308 may be a multi-core processor described herein, such as the multi-core processors 405 - 406 . The SOC 1300 can additionally include on-chip memory 1305 that can enable a shared on-chip data pool that is accessible by each of the processing components. The processing components can be optimized for low power operation to enable deployment to a variety of machine learning platforms, including autonomous vehicles and autonomous robots. For example, one implementation of the SOC 1300 can be used as a portion of the main control system for an autonomous vehicle. Where the SOC 1300 is configured for use in autonomous vehicles the SOC is designed and configured for compliance with the relevant functional safety standards of the deployment jurisdiction. During operation, the media processor 1302 and vision processor 1304 can work in concert to accelerate computer vision operations. The media processor 1302 can enable low latency decode of multiple high-resolution (e.g., 4 K, 8 K) video streams. The decoded video streams can be written to a buffer in the on-chip memory 1305 . The vision processor 1304 can then parse the decoded video and perform preliminary processing operations on the frames of the decoded video in preparation of processing the frames using a trained image recognition model. For example, the vision processor 1304 can accelerate convolution operations for a CNN that is used to perform image recognition on the high-resolution video data, while back end model computations are performed by the GPGPU 1306 . The multi-core processor 1308 can include control logic to assist with sequencing and synchronization of data transfers and shared memory operations performed by the media processor 1302 and the vision processor 1304 . The multi-core processor 1308 can also function as an application processor to execute software applications that can make use of the inferencing compute capability of the GPGPU 1306 . For example, at least a portion of the navigation and driving logic can be implemented in software executing on the multi-core processor 1308 . Such software can directly issue computational workloads to the GPGPU 1306 or the computational workloads can be issued to the multi-core processor 1308 , which can offload at least a portion of those operations to the GPGPU 1306 . The GPGPU 1306 can include compute clusters such as a low power configuration of the processing clusters 706 A- 706 H within general-purpose graphics processing unit 700 . The compute clusters within the GPGPU 1306 can support instruction that are specifically optimized to perform inferencing computations on a trained neural network. For example, the GPGPU 1306 can support instructions to perform low precision computations such as 8-bit and 4-bit integer vector operations. Additional System Overview FIG. 14 is a block diagram of a processing system 1400 . The elements of FIG. 14 having the same or similar names as the elements of any other figure herein describe the same elements as in the other figures, can operate or function in a manner similar to that, can comprise the same components, and can be linked to other entities, as those described elsewhere herein, but are not limited to such. System 1400 may be used in a single processor desktop system, a multiprocessor workstation system, or a server system having a large number of processors 1402 or processor cores 1407 . The system 1400 may be a processing platform incorporated within a system-on-a-chip (SoC) integrated circuit for use in mobile, handheld, or embedded devices such as within Internet-of-things (IoT) devices with wired or wireless connectivity to a local or wide area network. The system 1400 may be a processing system having components that correspond with those of FIG. 1 . For example, in different configurations, processor(s) 1402 or processor core(s) 1407 may correspond with processor(s) 102 of FIG. 1 . Graphics processor(s) 1408 may correspond with parallel processor(s) 112 of FIG. 1 . External graphics processor 1418 may be one of the add-in device(s) 120 of FIG. 1 . The system 1400 can include, couple with, or be integrated within: a server-based gaming platform; a game console, including a game and media console; a mobile gaming console, a handheld game console, or an online game console. The system 1400 may be part of a mobile phone, smart phone, tablet computing device or mobile Internet-connected device such as a laptop with low internal storage capacity. Processing system 1400 can also include, couple with, or be integrated within: a wearable device, such as a smart watch wearable device; smart eyewear or clothing enhanced with augmented reality (AR) or virtual reality (VR) features to provide visual, audio or tactile outputs to supplement real world visual, audio or tactile experiences or otherwise provide text, audio, graphics, video, holographic images or video, or tactile feedback; other augmented reality (AR) device; or other virtual reality (VR) device. The processing system 1400 may include or be part of a television or set top box device. The system 1400 can include, couple with, or be integrated within a self-driving vehicle such as a bus, tractor trailer, car, motor or electric power cycle, plane or glider (or any combination thereof). The self-driving vehicle may use system 1400 to process the environment sensed around the vehicle. The one or more processors 1402 may include one or more processor cores 1407 to process instructions which, when executed, perform operations for system or user software. The least one of the one or more processor cores 1407 may be configured to process a specific instruction set 1409 . The instruction set 1409 may facilitate Complex Instruction Set Computing (CISC), Reduced Instruction Set Computing (RISC), or computing via a Very Long Instruction Word (VLIW). One or more processor cores 1407 may process a different instruction set 1409 , which may include instructions to facilitate the emulation of other instruction sets. Processor core 1407 may also include other processing devices, such as a Digital Signal Processor (DSP). The processor 1402 may include cache memory 1404 . Depending on the architecture, the processor 1402 can have a single internal cache or multiple levels of internal cache. In some embodiments, the cache memory is shared among various components of the processor 1402 . In some embodiments, the processor 1402 also uses an external cache (e.g., a Level-3 (L3) cache or Last Level Cache (LLC)) (not shown), which may be shared among processor cores 1407 using known cache coherency techniques. A register file 1406 can be additionally included in processor 1402 and may include different types of registers for storing different types of data (e.g., integer registers, floating point registers, status registers, and an instruction pointer register). Some registers may be general-purpose registers, while other registers may be specific to the design of the processor 1402 . The one or more processor(s) 1402 may be coupled with one or more interface bus(es) 1410 to transmit communication signals such as address, data, or control signals between processor 1402 and other components in the system 1400 . The interface bus 1410 , in one of these embodiments, can be a processor bus, such as a version of the Direct Media Interface (DMI) bus. However, processor busses are not limited to the DMI bus, and may include one or more Peripheral Component Interconnect buses (e.g., PCI, PCI express), memory busses, or other types of interface busses. For example, the processor(s) 1402 may include an integrated memory controller 1416 and a platform controller hub 1430 . The memory controller 1416 facilitates communication between a memory device and other components of the system 1400 , while the platform controller hub (PCH) 1430 provides connections to I/O devices via a local I/O bus. The memory device 1420 can be a dynamic random-access memory (DRAM) device, a static random-access memory (SRAM) device, flash memory device, phase-change memory device, or some other memory device having suitable performance to serve as process memory. The memory device 1420 can, for example, operate as system memory for the system 1400 , to store data 1422 and instructions 1421 for use when the one or more processors 1402 executes an application or process. Memory controller 1416 also couples with an optional external graphics processor 1418 , which may communicate with the one or more graphics processors 1408 in processors 1402 to perform graphics and media operations. In some embodiments, graphics, media, and or compute operations may be assisted by an accelerator 1412 which is a coprocessor that can be configured to perform a specialized set of graphics, media, or compute operations. For example, the accelerator 1412 may be a matrix multiplication accelerator used to optimize machine learning or compute operations. The accelerator 1412 can be a ray-tracing accelerator that can be used to perform ray-tracing operations in concert with the graphics processor 1408 . In one embodiment, an external accelerator 1419 may be used in place of or in concert with the accelerator 1412 . A display device 1411 may be provided that can connect to the processor(s) 1402 . The display device 1411 can be one or more of an internal display device, as in a mobile electronic device or a laptop device or an external display device attached via a display interface (e.g., DisplayPort, etc.). The display device 1411 can be a head mounted display (HMD) such as a stereoscopic display device for use in virtual reality (VR) applications or augmented reality (AR) applications. The platform controller hub 1430 may enable peripherals to connect to memory device 1420 and processor 1402 via a high-speed I/O bus. The I/O peripherals include, but are not limited to, an audio controller 1446 , a network controller 1434 , a firmware interface 1428 , a wireless transceiver 1426 , touch sensors 1425 , a data storage device 1424 (e.g., non-volatile memory, volatile memory, hard disk drive, flash memory, NAND, 3D NAND, 3D XPoint/Optane, etc.). The data storage device 1424 can connect via a storage interface (e.g., SATA) or via a peripheral bus, such as a Peripheral Component Interconnect bus (e.g., PCI, PCI express). The touch sensors 1425 can include touch screen sensors, pressure sensors, or fingerprint sensors. The wireless transceiver 1426 can be a Wi-Fi transceiver, a Bluetooth transceiver, or a mobile network transceiver such as a 3G, 4G, 5G, or Long-Term Evolution (LTE) transceiver. The firmware interface 1428 enables communication with system firmware, and can be, for example, a unified extensible firmware interface (UEFI). The network controller 1434 can enable a network connection to a wired network. In some embodiments, a high-performance network controller (not shown) couples with the interface bus 1410 . The audio controller 1446 may be a multi-channel high definition audio controller. In some of these embodiments the system 1400 includes an optional legacy I/O controller 1440 for coupling legacy (e.g., Personal System 2 (PS/2)) devices to the system. The platform controller hub 1430 can also connect to one or more Universal Serial Bus (USB) controllers 1442 connect input devices, such as keyboard and mouse 1443 combinations, a camera 1444 , or other USB input devices. It will be appreciated that the system 1400 shown is example and not limiting, as other types of data processing systems that are differently configured may also be used. For example, an instance of the memory controller 1416 and platform controller hub 1430 may be integrated into a discrete external graphics processor, such as the external graphics processor 1418 . The platform controller hub 1430 and/or memory controller 1416 may be external to the one or more processor(s) 1402 . For example, the system 1400 can include an external memory controller 1416 and platform controller hub 1430 , which may be configured as a memory controller hub and peripheral controller hub within a system chipset that is in communication with the processor(s) 1402 . For example, circuit boards (“sleds”) can be used on which components such as CPUs, memory, and other components are placed are designed for increased thermal performance. Processing components such as the processors may be located on a top side of a sled while near memory, such as DIMMs, are located on a bottom side of the sled. As a result of the enhanced airflow provided by this design, the components may operate at higher frequencies and power levels than in typical systems, thereby increasing performance. Furthermore, the sleds are configured to blindly mate with power and data communication cables in a rack, thereby enhancing their ability to be quickly removed, upgraded, reinstalled, and/or replaced. Similarly, individual components located on the sleds, such as processors, accelerators, memory, and data storage drives, are configured to be easily upgraded due to their increased spacing from each other. In the illustrative embodiment, the components additionally include hardware attestation features to prove their authenticity. A data center can utilize a single network architecture (“fabric”) that supports multiple other network architectures including Ethernet and Omni-Path. The sleds can be coupled to switches via optical fibers, which provide higher bandwidth and lower latency than typical twisted pair cabling (e.g., Category 5, Category 5e, Category 6, etc.). Due to the high bandwidth, low latency interconnections and network architecture, the data center may, in use, pool resources, such as memory, accelerators (e.g., GPUs, graphics accelerators, FPGAs, ASICs, neural network and/or artificial intelligence accelerators, etc.), and data storage drives that are physically disaggregated, and provide them to compute resources (e.g., processors) on an as-needed basis, enabling the compute resources to access the pooled resources as if they were local. A power supply or source can provide voltage and/or current to system 1400 or any component or system described herein. In one example, the power supply includes an AC to DC (alternating current to direct current) adapter to plug into a wall outlet. Such AC power can be renewable energy (e.g., solar power) power source. In one example, the power source includes a DC power source, such as an external AC to DC converter. A power source or power supply may also include wireless charging hardware to charge via proximity to a charging field. The power source can include an internal battery, alternating current supply, motion-based power supply, solar power supply, or fuel cell source. FIG. 15A-15C illustrate computing systems and graphics processors. The elements of FIG. 15A-15C having the same or similar names as the elements of any other figure herein describe the same elements as in the other figures, can operate or function in a manner similar to that, can comprise the same components, and can be linked to other entities, as those described elsewhere herein, but are not limited to such. FIG. 15A is a block diagram of a processor 1500 , which may be a variant of one of the processors 1402 and may be used in place of one of those. Therefore, the disclosure of any features in combination with the processor 1500 herein also discloses a corresponding combination with the processor(s) 1402 , but is not limited to such. The processor 1500 may have one or more processor cores 1502 A- 1502 N, an integrated memory controller 1514 , and an integrated graphics processor 1508 . Where an integrated graphics processor 1508 is excluded, the system that includes the processor will include a graphics processor device within a system chipset or coupled via a system bus. Processor 1500 can include additional cores up to and including additional core 1502 N represented by the dashed lined boxes. Each of processor cores 1502 A- 1502 N includes one or more internal cache units 1504 A- 1504 N. In some embodiments each processor core 1502 A- 1502 N also has access to one or more shared cache units 1506 . The internal cache units 1504 A- 1504 N and shared cache units 1506 represent a cache memory hierarchy within the processor 1500 . The cache memory hierarchy may include at least one level of instruction and data cache within each processor core and one or more levels of shared mid-level cache, such as a Level 2 (L2), Level 3 (L3), Level 4 (L4), or other levels of cache, where the highest level of cache before external memory is classified as the LLC. In some embodiments, cache coherency logic maintains coherency between the various cache units 1506 and 1504 A- 1504 N. The processor 1500 may also include a set of one or more bus controller units 1516 and a system agent core 1510 . The one or more bus controller units 1516 manage a set of peripheral buses, such as one or more PCI or PCI express busses. System agent core 1510 provides management functionality for the various processor components. The system agent core 1510 may include one or more integrated memory controllers 1514 to manage access to various external memory devices (not shown). For example, one or more of the processor cores 1502 A- 1502 N may include support for simultaneous multi-threading. The system agent core 1510 includes components for coordinating and operating cores 1502 A- 1502 N during multi-threaded processing. System agent core 1510 may additionally include a power control unit (PCU), which includes logic and components to regulate the power state of processor cores 1502 A- 1502 N and graphics processor 1508 . The processor 1500 may additionally include graphics processor 1508 to execute graphics processing operations. In some of these embodiments, the graphics processor 1508 couples with the set of shared cache units 1506 , and the system agent core 1510 , including the one or more integrated memory controllers 1514 . The system agent core 1510 may also include a display controller 1511 to drive graphics processor output to one or more coupled displays. The display controller 1511 may also be a separate module coupled with the graphics processor via at least one interconnect, or may be integrated within the graphics processor 1508 . A ring-based interconnect unit 1512 may be used to couple the internal components of the processor 1500 . However, an alternative interconnect unit may be used, such as a point-to-point interconnect, a switched interconnect, or other techniques, including techniques well known in the art. In some of these embodiments with a ring-based interconnect 1512 , the graphics processor 1508 couples with the ring-based interconnect 1512 via an I/O link 1513 . The example I/O link 1513 represents at least one of multiple varieties of I/O interconnects, including an on package I/O interconnect which facilitates communication between various processor components and a high-performance embedded memory module 1518 , such as an eDRAM module. Optionally, each of the processor cores 1502 A- 1502 N and graphics processor 1508 can use embedded memory modules 1518 as a shared Last Level Cache. The processor cores 1502 A- 1502 N may, for example, be homogenous cores executing the same instruction set architecture. Alternatively, the processor cores 1502 A- 1502 N are heterogeneous in terms of instruction set architecture (ISA), where one or more of processor cores 1502 A- 1502 N execute a first instruction set, while at least one of the other cores executes a subset of the first instruction set or a different instruction set. The processor cores 1502 A- 1502 N may be heterogeneous in terms of microarchitecture, where one or more cores having a relatively higher power consumption couple with one or more power cores having a lower power consumption. As another example, the processor cores 1502 A- 1502 N are heterogeneous in terms of computational capability. Additionally, processor 1500 can be implemented on one or more chips or as an SoC integrated circuit having the illustrated components, in addition to other components. FIG. 15B is a block diagram of hardware logic of a graphics processor core 1519 , according to some embodiments described herein. The graphics processor core 1519 , sometimes referred to as a core slice, can be one or multiple graphics cores within a modular graphics processor. The graphics processor core 1519 is example of one graphics core slice, and a graphics processor as described herein may include multiple graphics core slices based on target power and performance envelopes. Each graphics processor core 1519 can include a fixed function block 1530 coupled with multiple sub-cores 1521 A- 1521 F, also referred to as sub-slices, that include modular blocks of general-purpose and fixed function logic. The fixed function block 1530 may include a geometry/fixed function pipeline 1531 that can be shared by all sub-cores in the graphics processor core 1519 , for example, in lower performance and/or lower power graphics processor implementations. The geometry/fixed function pipeline 1531 may include a 3D fixed function pipeline (e.g., 3D pipeline 1612 as in FIG. 16A described below) a video front-end unit, a thread spawner and thread dispatcher, and a unified return buffer manager, which manages unified return buffers (e.g., unified return buffer 1718 in FIG. 17 , as described below). The fixed function block 1530 may also include a graphics SoC interface 1532 , a graphics microcontroller 1533 , and a media pipeline 1534 . The graphics SoC interface 1532 provides an interface between the graphics processor core 1519 and other processor cores within a system on a chip integrated circuit. The graphics microcontroller 1533 is a programmable sub-processor that is configurable to manage various functions of the graphics processor core 1519 , including thread dispatch, scheduling, and pre-emption. The media pipeline 1534 (e.g., media pipeline 1616 of FIG. 16A and FIG. 17 ) includes logic to facilitate the decoding, encoding, pre-processing, and/or post-processing of multimedia data, including image and video data. The media pipeline 1534 implement media operations via requests to compute or sampling logic within the sub-cores 1521 - 1521 F. The SoC interface 1532 may enable the graphics processor core 1519 to communicate with general-purpose application processor cores (e.g., CPUs) and/or other components within an SoC, including memory hierarchy elements such as a shared last level cache memory, the system RAM, and/or embedded on-chip or on-package DRAM. The SoC interface 1532 can also enable communication with fixed function devices within the SoC, such as camera imaging pipelines, and enables the use of and/or implements global memory atomics that may be shared between the graphics processor core 1519 and CPUs within the SoC. The SoC interface 1532 can also implement power management controls for the graphics processor core 1519 and enable an interface between a clock domain of the graphics processor core 1519 and other clock domains within the SoC. Optionally, the SoC interface 1532 enables receipt of command buffers from a command streamer and global thread dispatcher that are configured to provide commands and instructions to each of one or more graphics cores within a graphics processor. The commands and instructions can be dispatched to the media pipeline 1534 , when media operations are to be performed, or a geometry and fixed function pipeline (e.g., geometry and fixed function pipeline 1531 , geometry and fixed function pipeline 1537 ) when graphics processing operations are to be performed. The graphics microcontroller 1533 can be configured to perform various scheduling and management tasks for the graphics processor core 1519 . In one configuration the graphics microcontroller 1533 can, for example, perform graphics and/or compute workload scheduling on the various graphics parallel engines within execution unit (EU) arrays 1522 A- 1522 F, 1524 A- 1524 F within the sub-cores 1521 A- 1521 F. In this workload scheduling, host software executing on a CPU core of an SoC including the graphics processor core 1519 can submit workloads to one of multiple graphic processor doorbells, which invokes a scheduling operation on the appropriate graphics engine. Scheduling operations include determining which workload to run next, submitting a workload to a command streamer, pre-empting existing workloads running on an engine, monitoring progress of a workload, and notifying host software when a workload is complete. Optionally, the graphics microcontroller 1533 can also facilitate low-power or idle states for the graphics processor core 1519 , providing the graphics processor core 1519 with the ability to save and restore registers within the graphics processor core 1519 across low-power state transitions independently from the operating system and/or graphics driver software on the system. The graphics processor core 1519 may have more than or fewer than the illustrated sub-cores 1521 A- 1521 F, up to N modular sub-cores. For each set of N sub-cores, the graphics processor core 1519 can also include shared function logic 1535 , shared and/or cache memory 1536 , a geometry/fixed function pipeline 1537 , as well as additional fixed function logic 1538 to accelerate various graphics and compute processing operations. The shared function logic 1535 can include logic units associated with the shared function logic 1720 of FIG. 17 (e.g., sampler, math, and/or inter-thread communication logic) that can be shared by each N sub-cores within the graphics processor core 1519 . The shared and/or cache memory 1536 can be a last-level cache for the set of N sub-cores 1521 A- 1521 F within the graphics processor core 1519 , and can also serve as shared memory that is accessible by multiple sub-cores. The geometry/fixed function pipeline 1537 can be included instead of the geometry/fixed function pipeline 1531 within the fixed function block 1530 and can include the same or similar logic units. The graphics processor core 1519 may include additional fixed function logic 1538 that can include various fixed function acceleration logic for use by the graphics processor core 1519 . Optionally, the additional fixed function logic 1538 includes an additional geometry pipeline for use in position only shading. In position-only shading, two geometry pipelines exist, the full geometry pipeline within the geometry/fixed function pipeline 1538 , 1531 , and a cull pipeline, which is an additional geometry pipeline which may be included within the additional fixed function logic 1538 . For example, the cull pipeline may be a trimmed down version of the full geometry pipeline. The full pipeline and the cull pipeline can execute different instances of the same application, each instance having a separate context. Position only shading can hide long cull runs of discarded triangles, enabling shading to be completed earlier in some instances. For example, the cull pipeline logic within the additional fixed function logic 1538 can execute position shaders in parallel with the main application and generally generates results faster than the full pipeline, as the cull pipeline fetches and shades only the position attribute of the vertices, without performing rasterization and rendering of the pixels to the frame buffer. The cull pipeline can use the generated results to compute visibility information for all the triangles without regard to whether those triangles are culled. The full pipeline (which in this instance may be referred to as a replay pipeline) can consume the visibility information to skip the culled triangles to shade only the visible triangles that are finally passed to the rasterization phase. Optionally, the additional fixed function logic 1538 can also include machine-learning acceleration logic, such as fixed function matrix multiplication logic, for implementations including optimizations for machine learning training or inferencing. Within each graphics sub-core 1521 A- 1521 F a set of execution resources is included that may be used to perform graphics, media, and compute operations in response to requests by graphics pipeline, media pipeline, or shader programs. The graphics sub-cores 1521 A- 1521 F include multiple EU arrays 1522 A- 1522 F, 1524 A- 1524 F, thread dispatch and inter-thread communication (TD/IC) logic 1523 A- 1523 F, a 3D (e.g., texture) sampler 1525 A- 1525 F, a media sampler 1526 A- 1526 F, a shader processor 1527 A- 1527 F, and shared local memory (SLM) 1528 A- 1528 F. The EU arrays 1522 A- 1522 F, 1524 A- 1524 F each include multiple execution units, which are general-purpose graphics processing units capable of performing floating-point and integer/fixed-point logic operations in service of a graphics, media, or compute operation, including graphics, media, or compute shader programs. The TD/IC logic 1523 A- 1523 F performs local thread dispatch and thread control operations for the execution units within a sub-core and facilitate communication between threads executing on the execution units of the sub-core. The 3D sampler 1525 A- 1525 F can read texture or other 3D graphics related data into memory. The 3D sampler can read texture data differently based on a configured sample state and the texture format associated with a given texture. The media sampler 1526 A- 1526 F can perform similar read operations based on the type and format associated with media data. For example, each graphics sub-core 1521 A- 1521 F can alternately include a unified 3D and media sampler. Threads executing on the execution units within each of the sub-cores 1521 A- 1521 F can make use of shared local memory 1528 A- 1528 F within each sub-core, to enable threads executing within a thread group to execute using a common pool of on-chip memory. FIG. 15C is a block diagram of general-purpose graphics processing unit (GPGPU) 1570 that can be configured as a graphics processor, e.g. the graphics processor 1508 , and/or compute accelerator, according to embodiments described herein. The GPGPU 1570 can interconnect with host processors (e.g., one or more CPU(s) 1546 ) and memory 1571 , 1572 via one or more system and/or memory busses. Memory 1571 may be system memory that can be shared with the one or more CPU(s) 1546 , while memory 1572 is device memory that is dedicated to the GPGPU 1570 . For example, components within the GPGPU 1570 and memory 1572 may be mapped into memory addresses that are accessible to the one or more CPU(s) 1546 . Access to memory 1571 and 1572 may be facilitated via a memory controller 1568 . The memory controller 1568 may include an internal direct memory access (DMA) controller 1569 or can include logic to perform operations that would otherwise be performed by a DMA controller. The GPGPU 1570 includes multiple cache memories, including an L2 cache 1553 , L1 cache 1554 , an instruction cache 1555 , and shared memory 1556 , at least a portion of which may also be partitioned as a cache memory. The GPGPU 1570 also includes multiple compute units 1560 A- 1560 N. Each compute unit 1560 A- 1560 N includes a set of vector registers 1561 , scalar registers 1562 , vector logic units 1563 , and scalar logic units 1564 . The compute units 1560 A- 1560 N can also include local shared memory 1565 and a program counter 1566 . The compute units 1560 A- 1560 N can couple with a constant cache 1567 , which can be used to store constant data, which is data that will not change during the run of kernel or shader program that executes on the GPGPU 1570 . The constant cache 1567 may be a scalar data cache and cached data can be fetched directly into the scalar registers 1562 . During operation, the one or more CPU(s) 1546 can write commands into registers or memory in the GPGPU 1570 that has been mapped into an accessible address space. The command processors 1557 can read the commands from registers or memory and determine how those commands will be processed within the GPGPU 1570 . A thread dispatcher 1558 can then be used to dispatch threads to the compute units 1560 A- 1560 N to perform those commands. Each compute unit 1560 A- 1560 N can execute threads independently of the other compute units. Additionally, each compute unit 1560 A- 1560 N can be independently configured for conditional computation and can conditionally output the results of computation to memory. The command processors 1557 can interrupt the one or more CPU(s) 1546 when the submitted commands are complete. FIG. 16A-16C illustrate block diagrams of additional graphics processor and compute accelerator architectures provided by embodiments described herein, e.g. in accordance with FIG. 15A-15C . The elements of FIG. 16A-16C having the same or similar names as the elements of any other figure herein describe the same elements as in the other figures, can operate or function in a manner similar to that, can comprise the same components, and can be linked to other entities, as those described elsewhere herein, but are not limited to such. FIG. 16A is a block diagram of a graphics processor 1600 , which may be a discrete graphics processing unit, or may be a graphics processor integrated with a plurality of processing cores, or other semiconductor devices such as, but not limited to, memory devices or network interfaces. The graphics processor 1600 may be a variant of the graphics processor 1508 and may be used in place of the graphics processor 1508 . Therefore, the disclosure of any features in combination with the graphics processor 1508 herein also discloses a corresponding combination with the graphics processor 1600 , but is not limited to such. The graphics processor may communicate via a memory mapped I/O interface to registers on the graphics processor and with commands placed into the processor memory. Graphics processor 1600 may include a memory interface 1614 to access memory. Memory interface 1614 can be an interface to local memory, one or more internal caches, one or more shared external caches, and/or to system memory. Optionally, graphics processor 1600 also includes a display controller 1602 to drive display output data to a display device 1618 . Display controller 1602 includes hardware for one or more overlay planes for the display and composition of multiple layers of video or user interface elements. The display device 1618 can be an internal or external display device. In one embodiment the display device 1618 is a head mounted display device, such as a virtual reality (VR) display device or an augmented reality (AR) display device. Graphics processor 1600 may include a video codec engine 1606 to encode, decode, or transcode media to, from, or between one or more media encoding formats, including, but not limited to Moving Picture Experts Group (MPEG) formats such as MPEG-2, Advanced Video Coding (AVC) formats such as H.264/MPEG-4 AVC, H.265/HEVC, Alliance for Open Media (AOMedia) VP8, VP9, as well as the Society of Motion Picture & Television Engineers (SMPTE) 421M/VC-1, and Joint Photographic Experts Group (JPEG) formats such as JPEG, and Motion JPEG (MJPEG) formats. Graphics processor 1600 may include a block image transfer (BLIT) engine 1603 to perform two-dimensional (2D) rasterizer operations including, for example, bit-boundary block transfers. However, alternatively, 2D graphics operations may be performed using one or more components of graphics processing engine (GPE) 1610 . In some embodiments, GPE 1610 is a compute engine for performing graphics operations, including three-dimensional (3D) graphics operations and media operations. GPE 1610 may include a 3D pipeline 1612 for performing 3D operations, such as rendering three-dimensional images and scenes using processing functions that act upon 3D primitive shapes (e.g., rectangle, triangle, etc.). The 3D pipeline 1612 includes programmable and fixed function elements that perform various tasks within the element and/or spawn execution threads to a 3D/Media subsystem 1615 . While 3D pipeline 1612 can be used to perform media operations, an embodiment of GPE 1610 also includes a media pipeline 1616 that is specifically used to perform media operations, such as video post-processing and image enhancement. Media pipeline 1616 may include fixed function or programmable logic units to perform one or more specialized media operations, such as video decode acceleration, video de-interlacing, and video encode acceleration in place of, or on behalf of video codec engine 1606 . Media pipeline 1616 may additionally include a thread spawning unit to spawn threads for execution on 3D/Media subsystem 1615 . The spawned threads perform computations for the media operations on one or more graphics execution units included in 3D/Media subsystem 1615 . The 3D/Media subsystem 1615 may include logic for executing threads spawned by 3D pipeline 1612 and media pipeline 1616 . The pipelines may send thread execution requests to 3D/Media subsystem 1615 , which includes thread dispatch logic for arbitrating and dispatching the various requests to available thread execution resources. The execution resources include an array of graphics execution units to process the 3D and media threads. The 3D/Media subsystem 1615 may include one or more internal caches for thread instructions and data. Additionally, the 3D/Media subsystem 1615 may also include shared memory, including registers and addressable memory, to share data between threads and to store output data. FIG. 16B illustrates a graphics processor 1620 , being a variant of the graphics processor 1600 and may be used in place of the graphics processor 1600 and vice versa. Therefore, the disclosure of any features in combination with the graphics processor 1600 herein also discloses a corresponding combination with the graphics processor 1620 , but is not limited to such. The graphics processor 1620 has a tiled architecture, according to embodiments described herein. The graphics processor 1620 may include a graphics processing engine cluster 1622 having multiple instances of the graphics processing engine 1610 of FIG. 16A within a graphics engine tile 1610 A- 1610 D. Each graphics engine tile 1610 A- 1610 D can be interconnected via a set of tile interconnects 1623 A- 1623 F. Each graphics engine tile 1610 A- 1610 D can also be connected to a memory module or memory device 1626 A- 1626 D via memory interconnects 1625 A- 1625 D. The memory devices 1626 A- 1626 D can use any graphics memory technology. For example, the memory devices 1626 A- 1626 D may be graphics double data rate (GDDR) memory. The memory devices 1626 A- 1626 D may be high-bandwidth memory (HBM) modules that can be on-die with their respective graphics engine tile 1610 A- 1610 D. The memory devices 1626 A- 1626 D may be stacked memory devices that can be stacked on top of their respective graphics engine tile 1610 A- 1610 D. Each graphics engine tile 1610 A- 1610 D and associated memory 1626 A- 1626 D may reside on separate chiplets, which are bonded to a base die or base substrate, as described in further detail in FIG. 24B-24D . The graphics processor 1620 may be configured with a non-uniform memory access (NUMA) system in which memory devices 1626 A- 1626 D are coupled with associated graphics engine tiles 1610 A- 1610 D. A given memory device may be accessed by graphics engine tiles other than the tile to which it is directly connected. However, access latency to the memory devices 1626 A- 1626 D may be lowest when accessing a local tile. In one embodiment, a cache coherent NUMA (ccNUMA) system is enabled that uses the tile interconnects 1623 A- 1623 F to enable communication between cache controllers within the graphics engine tiles 1610 A- 1610 D to keep a consistent memory image when more than one cache stores the same memory location. The graphics processing engine cluster 1622 can connect with an on-chip or on-package fabric interconnect 1624 . In one embodiment the fabric interconnect 1624 includes a network processor, network on a chip (NoC), or another switching processor to enable the fabric interconnect 1624 to act as a packet switched fabric interconnect that switches data packets between components of the graphics processor 1620 . The fabric interconnect 1624 can enable communication between graphics engine tiles 1610 A- 1610 D and components such as the video codec engine 1606 and one or more copy engines 1604 . The copy engines 1604 can be used to move data out of, into, and between the memory devices 1626 A- 1626 D and memory that is external to the graphics processor 1620 (e.g., system memory). The fabric interconnect 1624 can also be used to interconnect the graphics engine tiles 1610 A- 1610 D. The graphics processor 1620 may optionally include a display controller 1602 to enable a connection with an external display device 1618 . The graphics processor may also be configured as a graphics or compute accelerator. In the accelerator configuration, the display controller 1602 and display device 1618 may be omitted. The graphics processor 1620 can connect to a host system via a host interface 1628 . The host interface 1628 can enable communication between the graphics processor 1620 , system memory, and/or other system components. The host interface 1628 can be, for example, a PCI express bus or another type of host system interface. For example, the host interface 1628 may be an NVLink or NVSwitch interface. The host interface 1628 and fabric interconnect 1624 can cooperate to enable multiple instances of the graphics processor 1620 to act as single logical device. Cooperation between the host interface 1628 and fabric interconnect 1624 can also enable the individual graphics engine tiles 1610 A- 1610 D to be presented to the host system as distinct logical graphics devices. FIG. 16C illustrates a compute accelerator 1630 , according to embodiments described herein. The compute accelerator 1630 can include architectural similarities with the graphics processor 1620 of FIG. 16B and is optimized for compute acceleration. A compute engine cluster 1632 can include a set of compute engine tiles 1640 A- 1640 D that include execution logic that is optimized for parallel or vector-based general-purpose compute operations. The compute engine tiles 1640 A- 1640 D may not include fixed function graphics processing logic, although in some embodiments one or more of the compute engine tiles 1640 A- 1640 D can include logic to perform media acceleration. The compute engine tiles 1640 A- 1640 D can connect to memory 1626 A- 1626 D via memory interconnects 1625 A- 1625 D. The memory 1626 A- 1626 D and memory interconnects 1625 A- 1625 D may be similar technology as in graphics processor 1620 , or can be different. The graphics compute engine tiles 1640 A- 1640 D can also be interconnected via a set of tile interconnects 1623 A- 1623 F and may be connected with and/or interconnected by a fabric interconnect 1624 . In one embodiment the compute accelerator 1630 includes a large L3 cache 1636 that can be configured as a device-wide cache. The compute accelerator 1630 can also connect to a host processor and memory via a host interface 1628 in a similar manner as the graphics processor 1620 of FIG. 16B . The compute accelerator 1630 can also include an integrated network interface 1642 . In one embodiment the integrated network interface 1642 includes a network processor and controller logic that enables the compute engine cluster 1632 to communicate over a physical layer interconnect 1644 without requiring data to traverse memory of a host system. In one embodiment, one of the compute engine tiles 1640 A- 1640 D is replaced by network processor logic and data to be transmitted or received via the physical layer interconnect 1644 may be transmitted directly to or from memory 1626 A- 1626 D. Multiple instances of the compute accelerator 1630 may be joined via the physical layer interconnect 1644 into a single logical device. Alternatively, the various compute engine tiles 1640 A- 1640 D may be presented as distinct network accessible compute accelerator devices. Graphics Processing Engine FIG. 17 is a block diagram of a graphics processing engine 1710 of a graphics processor in accordance with some embodiments. The graphics processing engine (GPE) 1710 may be a version of the GPE 1610 shown in FIG. 16A , and may also represent a graphics engine tile 1610 A- 1610 D of FIG. 16B . The elements of FIG. 17 having the same or similar names as the elements of any other figure herein describe the same elements as in the other figures, can operate or function in a manner similar to that, can comprise the same components, and can be linked to other entities, as those described elsewhere herein, but are not limited to such. For example, the 3D pipeline 1612 and media pipeline 1616 of FIG. 16A are also illustrated in FIG. 17 . The media pipeline 1616 is optional in some embodiments of the GPE 1710 and may not be explicitly included within the GPE 1710 . For example and in at least one embodiment, a separate media and/or image processor is coupled to the GPE 1710 . GPE 1710 may couple with or include a command streamer 1703 , which provides a command stream to the 3D pipeline 1612 and/or media pipelines 1616 . Alternatively or additionally, the command streamer 1703 may be directly coupled to a unified return buffer 1718 . The unified return buffer 1718 may be communicatively coupled to a graphics core array 1714 . Optionally, the command streamer 1703 is coupled with memory, which can be system memory, or one or more of internal cache memory and shared cache memory. The command streamer 1703 may receive commands from the memory and sends the commands to 3D pipeline 1612 and/or media pipeline 1616 . The commands are directives fetched from a ring buffer, which stores commands for the 3D pipeline 1612 and media pipeline 1616 . The ring buffer can additionally include batch command buffers storing batches of multiple commands. The commands for the 3D pipeline 1612 can also include references to data stored in memory, such as but not limited to vertex and geometry data for the 3D pipeline 1612 and/or image data and memory objects for the media pipeline 1616 . The 3D pipeline 1612 and media pipeline 1616 process the commands and data by performing operations via logic within the respective pipelines or by dispatching one or more execution threads to the graphics core array 1714 . The graphics core array 1714 may include one or more blocks of graphics cores (e.g., graphics core(s) 1715 A, graphics core(s) 1715 B), each block including one or more graphics cores. Each graphics core includes a set of graphics execution resources that includes general-purpose and graphics specific execution logic to perform graphics and compute operations, as well as fixed function texture processing and/or machine learning and artificial intelligence acceleration logic. In various embodiments the 3D pipeline 1612 can include fixed function and programmable logic to process one or more shader programs, such as vertex shaders, geometry shaders, pixel shaders, fragment shaders, compute shaders, or other shader programs, by processing the instructions and dispatching execution threads to the graphics core array 1714 . The graphics core array 1714 provides a unified block of execution resources for use in processing these shader programs. Multi-purpose execution logic (e.g., execution units) within the graphics core(s) 1715 A- 1715 B of the graphics core array 1714 includes support for various 3D API shader languages and can execute multiple simultaneous execution threads associated with multiple shaders. The graphics core array 1714 may include execution logic to perform media functions, such as video and/or image processing. The execution units may include general-purpose logic that is programmable to perform parallel general-purpose computational operations, in addition to graphics processing operations. The general-purpose logic can perform processing operations in parallel or in conjunction with general-purpose logic within the processor core(s) 1407 of FIG. 14 or core 1502 A- 1502 N as in FIG. 15A . Output data generated by threads executing on the graphics core array 1714 can output data to memory in a unified return buffer (URB) 1718 . The URB 1718 can store data for multiple threads. The URB 1718 may be used to send data between different threads executing on the graphics core array 1714 . The URB 1718 may additionally be used for synchronization between threads on the graphics core array 1714 and fixed function logic within the shared function logic 1720 . Optionally, the graphics core array 1714 may be scalable, such that the array includes a variable number of graphics cores, each having a variable number of execution units based on the target power and performance level of GPE 1710 . The execution resources may be dynamically scalable, such that execution resources may be enabled or disabled. The graphics core array 1714 couples with shared function logic 1720 that includes multiple resources that are shared between the graphics cores in the graphics core array. The shared functions within the shared function logic 1720 are hardware logic units that provide specialized supplemental functionality to the graphics core array 1714 . In various embodiments, shared function logic 1720 includes but is not limited to sampler 1721 , math 1722 , and inter-thread communication (ITC) 1723 logic. Additionally, one or more cache(s) 1725 within the shared function logic 1720 may be implemented. A shared function is implemented at least in a case where the demand for a given specialized function is insufficient for inclusion within the graphics core array 1714 . Instead a single instantiation of that specialized function is implemented as a stand-alone entity in the shared function logic 1720 and shared among the execution resources within the graphics core array 1714 . The precise set of functions that are shared between the graphics core array 1714 and included within the graphics core array 1714 varies across embodiments. Specific shared functions within the shared function logic 1720 that are used extensively by the graphics core array 1714 may be included within shared function logic 1716 within the graphics core array 1714 . Optionally, the shared function logic 1716 within the graphics core array 1714 can include some or all logic within the shared function logic 1720 . All logic elements within the shared function logic 1720 may be duplicated within the shared function logic 1716 of the graphics core array 1714 . Alternatively, the shared function logic 1720 is excluded in favor of the shared function logic 1716 within the graphics core array 1714 . Execution Units FIG. 18A-18B illustrate thread execution logic 1800 including an array of processing elements employed in a graphics processor core according to embodiments described herein. The elements of FIG. 18A-18B having the same or similar names as the elements of any other figure herein describe the same elements as in the other figures, can operate or function in a manner similar to that, can comprise the same components, and can be linked to other entities, as those described elsewhere herein, but are not limited to such. FIG. 18A-18B illustrates an overview of thread execution logic 1800 , which may be representative of hardware logic illustrated with each sub-core 1521 A- 1521 F of FIG. 15B . FIG. 18A is representative of an execution unit within a general-purpose graphics processor, while FIG. 18B is representative of an execution unit that may be used within a compute accelerator. As illustrated in FIG. 18A , thread execution logic 1800 may include a shader processor 1802 , a thread dispatcher 1804 , instruction cache 1806 , a scalable execution unit array including a plurality of graphics execution units 1808 A- 1808 N, a sampler 1810 , shared local memory 1811 , a data cache 1812 , and a data port 1814 . Optionally, the scalable execution unit array can dynamically scale by enabling or disabling one or more execution units (e.g., any of graphics execution units 1808 A, 1808 B, 1808 C, 1808 D, through 1808 N- 1 and 1808 N) based on the computational requirements of a workload. The included components may be interconnected via an interconnect fabric that links to each of the components. Thread execution logic 1800 may include one or more connections to memory, such as system memory or cache memory, through one or more of instruction cache 1806 , data port 1814 , sampler 1810 , and graphics execution units 1808 A- 1808 N. Each execution unit (e.g. 1808 A) may be a stand-alone programmable general-purpose computational unit that is capable of executing multiple simultaneous hardware threads while processing multiple data elements in parallel for each thread. In various embodiments, the array of execution units 1808 A- 1808 N is scalable to include any number individual execution units. In some embodiments the graphics execution units 1808 A- 1808 N may be primarily used to execute shader programs. A shader processor 1802 can process the various shader programs and dispatch execution threads associated with the shader programs via a thread dispatcher 1804 . The thread dispatcher may include logic to arbitrate thread initiation requests from the graphics and media pipelines and instantiate the requested threads on one or more execution units in the graphics execution units 1808 A- 1808 N. For example, a geometry pipeline can dispatch vertex, tessellation, or geometry shaders to the thread execution logic for processing. Optionally, the thread dispatcher 1804 can also process runtime thread spawning requests from the executing shader programs. In some embodiments, the graphics execution units 1808 A- 1808 N may support an instruction set that includes native support for many standard 3D graphics shader instructions, such that shader programs from graphics libraries (e.g., Direct 3D and OpenGL) are executed with a minimal translation. The execution units support vertex and geometry processing (e.g., vertex programs, geometry programs, vertex shaders), pixel processing (e.g., pixel shaders, fragment shaders) and general-purpose processing (e.g., compute and media shaders). Each of the graphics execution units 1808 A- 1808 N is capable of multi-issue single instruction multiple data (SIMD) execution and multi-threaded operation enables an efficient execution environment in the face of higher latency memory accesses. Each hardware thread within each execution unit has a dedicated high-bandwidth register file and associated independent thread-state. Execution is multi-issue per clock to pipelines capable of integer, single and double precision floating point operations, SIMD branch capability, logical operations, transcendental operations, and other miscellaneous operations. While waiting for data from memory or one of the shared functions, dependency logic within the execution units 1808 A- 1808 N causes a waiting thread to sleep until the requested data has been returned. While the waiting thread is sleeping, hardware resources may be devoted to processing other threads. For example, during a delay associated with a vertex shader operation, an execution unit can perform operations for a pixel shader, fragment shader, or another type of shader program, including a different vertex shader, such as vertex shader 2107 illustrated in FIG. 21 . Various embodiments can apply to use execution by use of Single Instruction Multiple Thread (SIMT) as an alternate to use of SIMD or in addition to use of SIMD. Reference to a SIMD core or operation can apply also to SIMT or apply to SIMD in combination with SIMT. Each execution unit in graphics execution units 1808 A- 1808 N operates on arrays of data elements. The number of data elements is the “execution size,” or the number of channels for the instruction. An execution channel is a logical unit of execution for data element access, masking, and flow control within instructions. The number of channels may be independent of the number of physical Arithmetic Logic Units (ALUs), Floating-Point Units (FPUs), or other logic units (e.g., tensor cores, ray tracing cores, etc.) for a particular graphics processor. Additionally, the graphics execution units 1808 A- 1808 N may support integer and floating-point data types. The execution unit instruction set includes SIMD instructions. The various data elements can be stored as a packed data type in a register and the execution unit will process the various elements based on the data size of the elements. For example, when operating on a 256-bit wide vector, the 256 bits of the vector are stored in a register and the execution unit operates on the vector as four separate 184-bit packed data elements (Quad-Word (QW) size data elements), eight separate 32-bit packed data elements (Double Word (DW) size data elements), sixteen separate 16-bit packed data elements (Word (W) size data elements), or thirty-two separate 8-bit data elements (byte (B) size data elements). However, different vector widths and register sizes are possible. Optionally, one or more execution units can be combined into a fused graphics execution unit 1809 A- 1809 N having thread control logic ( 1807 A- 1807 N) that is common to the fused EUs. Multiple EUs can be fused into an EU group. Each EU in the fused EU group can be configured to execute a separate SIMD hardware thread. The number of EUs in a fused EU group can vary according to embodiments. Additionally, various SIMD widths can be performed per-EU, including but not limited to SIMD8, SIMD16, and SIMD32. Each fused graphics execution unit 1809 A- 1809 N includes at least two execution units. For example, fused execution unit 1809 A includes a first EU 1808 A, second EU 1808 B, and thread control logic 1807 A that is common to the first EU 1808 A and the second EU 1808 B. The thread control logic 1807 A controls threads executed on the fused graphics execution unit 1809 A, allowing each EU within the fused execution units 1809 A- 1809 N to execute using a common instruction pointer register. One or more internal instruction caches (e.g., 1806 ) are included in the thread execution logic 1800 to cache thread instructions for the execution units. One or more data caches (e.g., 1812 ) may be included in the thread execution logic 1800 to cache thread data during thread execution. Threads executing on the execution logic 1800 can also store explicitly managed data in the shared local memory 1811 . A sampler 1810 may be included to provide texture sampling for 3D operations and media sampling for media operations. Sampler 1810 may include specialized texture or media sampling functionality to process texture or media data during the sampling process before providing the sampled data to an execution unit. During execution, the graphics and media pipelines send thread initiation requests to thread execution logic 1800 via thread spawning and dispatch logic. Once a group of geometric objects has been processed and rasterized into pixel data, pixel processor logic (e.g., pixel shader logic, fragment shader logic, etc.) within the shader processor 1802 is invoked to further compute output information and cause results to be written to output surfaces (e.g., color buffers, depth buffers, stencil buffers, etc.). A pixel shader or fragment shader may calculate the values of the various vertex attributes that are to be interpolated across the rasterized object. The pixel processor logic within the shader processor 1802 may then execute an application programming interface (API)-supplied pixel or fragment shader program. To execute the shader program, the shader processor 1802 dispatches threads to an execution unit (e.g., 1808 A) via thread dispatcher 1804 . Shader processor 1802 may use texture sampling logic in the sampler 1810 to access texture data in texture maps stored in memory. Arithmetic operations on the texture data and the input geometry data compute pixel color data for each geometric fragment, or discards one or more pixels from further processing. In addition, the data port 1814 may provide a memory access mechanism for the thread execution logic 1800 to output processed data to memory for further processing on a graphics processor output pipeline. The data port 1814 may include or couple to one or more cache memories (e.g., data cache 1812 ) to cache data for memory access via the data port 1814 . Optionally, the execution logic 1800 can also include a ray tracer 1805 that can provide ray tracing acceleration functionality. The ray tracer 1805 can support a ray tracing instruction set that includes instructions/functions for ray generation. The ray tracing instruction set can be similar to or different from the ray-tracing instruction set supported by the ray tracing cores 372 in FIG. 3C . FIG. 18B illustrates example internal details of an execution unit 1808 . A graphics execution unit 1808 can include an instruction fetch unit 1837 , a general register file array (GRF) 1824 , an architectural register file array (ARF) 1826 , a thread arbiter 1822 , a send unit 1830 , a branch unit 1832 , a set of SIMD floating point units (FPUs) 1834 , and optionally a set of dedicated integer SIMD ALUs 1835 . The GRF 1824 and ARF 1826 includes the set of general register files and architecture register files associated with each simultaneous hardware thread that may be active in the graphics execution unit 1808 . Per thread architectural state may be maintained in the ARF 1826 , while data used during thread execution is stored in the GRF 1824 . The execution state of each thread, including the instruction pointers for each thread, can be held in thread-specific registers in the ARF 1826 . The graphics execution unit 1808 may have an architecture that is a combination of Simultaneous Multi-Threading (SMT) and fine-grained Interleaved Multi-Threading (IMT). The architecture may have a modular configuration that can be fine-tuned at design time based on a target number of simultaneous threads and number of registers per execution unit, where execution unit resources are divided across logic used to execute multiple simultaneous threads. The number of logical threads that may be executed by the graphics execution unit 1808 is not limited to the number of hardware threads, and multiple logical threads can be assigned to each hardware thread. Optionally, the graphics execution unit 1808 can co-issue multiple instructions, which may each be different instructions. The thread arbiter 1822 of the graphics execution unit thread 1808 can dispatch the instructions to one of the send unit 1830 , branch unit 1832 , or SIMD FPU(s) 1834 for execution. Each execution thread can access 128 general-purpose registers within the GRF 1824 , where each register can store 32 bytes, accessible as a SIMD 8-element vector of 32-bit data elements. Each execution unit thread may have access to 4 Kbytes within the GRF 1824 , although embodiments are not so limited, and greater or fewer register resources may be provided in other embodiments. The graphics execution unit 1808 may be partitioned into seven hardware threads that can independently perform computational operations, although the number of threads per execution unit can also vary according to embodiments, for example, up to 16 hardware threads may be supported. In an example embodiment, in which seven threads may access 4 Kbytes, the GRF 1824 can store a total of 28 Kbytes. In another example embodiment, where 16 threads may access 4 Kbytes, the GRF 1824 can store a total of 64 Kbytes. The number of threads per execution unit are, however, not limited to those examples and may be more or less than the given numbers. Flexible addressing modes can permit registers to be addressed together to build effectively wider registers or to represent strided rectangular block data structures. Additionally or alternatively, memory operations, sampler operations, and other longer-latency system communications may be dispatched via “send” instructions that are executed by the message passing send unit 1830 . Branch instructions may be dispatched to a dedicated branch unit 1832 to facilitate SIMD divergence and eventual convergence. The graphics execution unit 1808 may include one or more SIMD floating point units (FPU(s)) 1834 to perform floating-point operations. The FPU(s) 1834 may also support integer computation. In some instances, the FPU(s) 1834 can SIMD execute up to M number of 32-bit floating-point (or integer) operations, or SIMD execute up to 2M 16-bit integer or 16-bit floating-point operations. Optionally, at least one of the FPU(s) provides extended math capability to support high-throughput transcendental math functions and double precision 184-bit floating-point. A set of 8-bit integer SIMD ALUs 1835 may also be present, and may be specifically optimized to perform operations associated with machine learning computations. Optionally, arrays of multiple instances of the graphics execution unit 1808 can be instantiated in a graphics sub-core grouping (e.g., a sub-slice). For scalability, product architects can choose the exact number of execution units per sub-core grouping. The execution unit 1808 may execute instructions across a plurality of execution channels. In addition, each thread executed on the graphics execution unit 1808 may be executed on a different channel. FIG. 19 illustrates a further example execution unit 1900 . The elements of FIG. 19 having the same or similar names as the elements of any other figure herein describe the same elements as in the other figures, can operate or function in a manner similar to that, can comprise the same components, and can be linked to other entities, as those described elsewhere herein, but are not limited to such. The execution unit 1900 may be a compute-optimized execution unit for use in, for example, a compute engine tile 1640 A- 1640 D as in FIG. 16C , but is not limited as such. The execution unit 1900 may also be used in a graphics engine tile 1610 A- 1610 D as in FIG. 16B . The execution unit 1900 may include a thread control unit 1901 , a thread state unit 1902 , an instruction fetch/prefetch unit 1903 , and an instruction decode unit 1904 . The execution unit 1900 may additionally include a register file 1906 that stores registers that can be assigned to hardware threads within the execution unit. The execution unit 1900 may additionally include a send unit 1907 and a branch unit 1908 . The send unit 1907 and branch unit 1908 may operate similarly as the send unit 1830 and a branch unit 1832 of the graphics execution unit 1808 of FIG. 18B . The execution unit 1900 can also include a compute unit 1910 that includes multiple different types of functional units. The compute unit 1910 may also include an ALU 1911 , a systolic array 1912 , and a math unit 1913 . The ALU 1911 includes an array of arithmetic logic units. The ALU 1911 can be configured to perform 64-bit, 32-bit, and 16-bit integer and floating-point operations across multiple processing lanes and data channels and for multiple hardware and/or software threads. The ALU 1911 can perform integer and floating-point operations simultaneously (e.g., within the same clock cycle). The systolic array 1912 includes a W wide and D deep network of data processing units that can be used to perform vector or other data-parallel operations in a systolic manner. The systolic array 1912 can be configured to perform various matrix operations, including as dot product, outer product, and general matrix-matrix multiplication (GEMM) operations. The systolic array 1912 may support 16-bit floating point operations, as well as 8-bit, 4-bit, 2-bit, and binary integer operations. The systolic array 1912 may be configured to accelerate machine learning operations. The systolic array 1912 can be configured with support for bfloat16, (brain floating point) 16-bit floating point format or a tensor float 32-bit floating point format (TF32) that have different numbers of mantissa and exponent bits relative to Institute of Electrical and Electronics Engineers (IEEE) 754 formats. FP64 formats can also be supported. In one embodiment, the systolic array 1912 includes hardware to accelerate sparse matrix operations. Multiplication operations for sparse regions of input data can be bypassed without sacrificing throughput. Block sparsity within input matrices can be detected and operations having known output values can be bypassed. In one embodiment, the systolic array 1912 includes hardware to enable operations on sparse data having a compressed representation. A compressed representation of a sparse matrix stores non-zero values and metadata that defines the position of the non-zero values within the matrix. Example compressed representations include but are not limited to compressed tensor representations such as compressed sparse row (CSR), compressed sparse column (CSC), compressed sparse fiber (CSF) representations. Support for compressed representations enable operations to be performed on input in a compressed tensor format without requiring the compressed representation to be decompressed or decoded. In such embodiment, operations can be performed only on non-zero input values and the resulting non-zero output values can be mapped into an output matrix. In some embodiments, hardware support is also provided for machine-specific lossless data compression formats that are used when transmitting data within hardware or across system busses. Such data may be retained in a compressed format for sparse input data and the systolic array 1912 can used the compression metadata for the compressed data to enable operations to be performed on only non-zero values, or to enable blocks of zero data input to be bypassed for multiply operations. The math unit 1913 can be configured to perform a specific subset of mathematical operations in an efficient and lower-power manner than then ALU unit 1911 . The math unit 1913 can include math logic found in shared function logic of a graphics processing engine provided by other embodiments described, e.g., the math logic 1722 of the shared function logic 1720 of FIG. 17 . The math unit 1913 can be configured to perform 32-bit and 64-bit floating point operations. The thread control unit 1901 includes logic to control the execution of threads within the execution unit. The thread control unit 1901 can include thread arbitration logic to start, stop, and preempt execution of threads within the execution unit 1900 . The thread state unit 1902 can be used to store thread state for threads assigned to execute on the execution unit 1900 . Storing the thread state within the execution unit 1900 enables the rapid pre-emption of threads when those threads become blocked or idle. The instruction fetch/prefetch unit 1903 can fetch instructions from an instruction cache of higher-level execution logic (e.g., instruction cache 1806 as in FIG. 18A ). The instruction fetch/prefetch unit 1903 can also issue prefetch requests for instructions to be loaded into the instruction cache based on an analysis of currently executing threads. The instruction decode unit 1904 can be used to decode instructions to be executed by the compute units. The instruction decode unit 1904 can be used as a secondary decoder to decode complex instructions into constituent micro-operations. The execution unit 1900 additionally includes a register file 1906 that can be used by hardware threads executing on the execution unit 1900 . Registers in the register file 1906 can be divided across the logic used to execute multiple simultaneous threads within the compute unit 1910 of the execution unit 1900 . The number of logical threads that may be executed by the graphics execution unit 1900 is not limited to the number of hardware threads, and multiple logical threads can be assigned to each hardware thread. The size of the register file 1906 can vary across embodiments based on the number of supported hardware threads. Register renaming may be used to dynamically allocate registers to hardware threads. FIG. 20 is a block diagram illustrating graphics processor instruction formats 2000 . The graphics processor execution units support an instruction set having instructions in multiple formats. The solid lined boxes illustrate the components that are generally included in an execution unit instruction, while the dashed lines include components that are optional or that are only included in a sub-set of the instructions. In some embodiments the graphics processor instruction formats 2000 described and illustrated are macro-instructions, in that they are instructions supplied to the execution unit, as opposed to micro-operations resulting from instruction decode once the instruction is processed. Thus, a single instructions may cause hardware to perform multiple micro-operations The graphics processor execution units as described herein may natively support instructions in a 128-bit instruction format 2010 . A 64-bit compacted instruction format 2030 is available for some instructions based on the selected instruction, instruction options, and number of operands. The native 128-bit instruction format 2010 provides access to all instruction options, while some options and operations are restricted in the 64-bit format 2030 . The native instructions available in the 64-bit format 2030 vary by embodiment. The instruction is compacted in part using a set of index values in an index field 2013 . The execution unit hardware references a set of compaction tables based on the index values and uses the compaction table outputs to reconstruct a native instruction in the 128-bit instruction format 2010 . Other sizes and formats of instruction can be used. For each format, instruction opcode 2012 defines the operation that the execution unit is to perform. The execution units execute each instruction in parallel across the multiple data elements of each operand. For example, in response to an add instruction the execution unit performs a simultaneous add operation across each color channel representing a texture element or picture element. By default, the execution unit performs each instruction across all data channels of the operands. Instruction control field 2014 may enable control over certain execution options, such as channels selection (e.g., predication) and data channel order (e.g., swizzle). For instructions in the 128-bit instruction format 2010 an exec-size field 2016 limits the number of data channels that will be executed in parallel. An exec-size field 2016 may not be available for use in the 64-bit compact instruction format 2030 . Some execution unit instructions have up to three operands including two source operands, src0 2020 , src1 2022 , and one destination 2018 . The execution units may support dual destination instructions, where one of the destinations is implied. Data manipulation instructions can have a third source operand (e.g., SRC2 2024 ), where the instruction opcode 2012 determines the number of source operands. An instruction's last source operand can be an immediate (e.g., hard-coded) value passed with the instruction. The 128-bit instruction format 2010 may include an access/address mode field 2026 specifying, for example, whether direct register addressing mode or indirect register addressing mode is used. When direct register addressing mode is used, the register address of one or more operands is directly provided by bits in the instruction. The 128-bit instruction format 2010 may also include an access/address mode field 2026 , which specifies an address mode and/or an access mode for the instruction. The access mode may be used to define a data access alignment for the instruction. Access modes including a 16-byte aligned access mode and a 1-byte aligned access mode may be supported, where the byte alignment of the access mode determines the access alignment of the instruction operands. For example, when in a first mode, the instruction may use byte-aligned addressing for source and destination operands and when in a second mode, the instruction may use 16-byte-aligned addressing for all source and destination operands. The address mode portion of the access/address mode field 2026 may determine whether the instruction is to use direct or indirect addressing. When direct register addressing mode is used bits in the instruction directly provide the register address of one or more operands. When indirect register addressing mode is used, the register address of one or more operands may be computed based on an address register value and an address immediate field in the instruction. Instructions may be grouped based on opcode 2012 bit-fields to simplify Opcode decode 2040 . For an 8-bit opcode, bits 4, 5, and 6 allow the execution unit to determine the type of opcode. The precise opcode grouping shown is merely an example. A move and logic opcode group 2042 may include data movement and logic instructions (e.g., move (mov), compare (cmp)). Move and logic group 2042 may share the five least significant bits (LSB), where move (mov) instructions are in the form of 0000xxxxb and logic instructions are in the form of 0001xxxxb. A flow control instruction group 2044 (e.g., call, jump (jmp)) includes instructions in the form of 0010xxxxb (e.g., 0x20). A miscellaneous instruction group 2046 includes a mix of instructions, including synchronization instructions (e.g., wait, send) in the form of 0011xxxxb (e.g., 0x30). A parallel math instruction group 2048 includes component-wise arithmetic instructions (e.g., add, multiply (mul)) in the form of 0100xxxxb (e.g., 0x40). The parallel math instruction group 2048 performs the arithmetic operations in parallel across data channels. The vector math group 2050 includes arithmetic instructions (e.g., dp4) in the form of 0101xxxxb (e.g., 0x50). The vector math group performs arithmetic such as dot product calculations on vector operands. The illustrated opcode decode 2040 , in one embodiment, can be used to determine which portion of an execution unit will be used to execute a decoded instruction. For example, some instructions may be designated as systolic instructions that will be performed by a systolic array. Other instructions, such as ray-tracing instructions (not shown) can be routed to a ray-tracing core or ray-tracing logic within a slice or partition of execution logic. Graphics Pipeline FIG. 21 is a block diagram of graphics processor 2100 , according to another embodiment. The elements of FIG. 21 having the same or similar names as the elements of any other figure herein describe the same elements as in the other figures, can operate or function in a manner similar to that, can comprise the same components, and can be linked to other entities, as those described elsewhere herein, but are not limited to such. The graphics processor 2100 may include different types of graphics processing pipelines, such as a geometry pipeline 2120 , a media pipeline 2130 , a display engine 2140 , thread execution logic 2150 , and a render output pipeline 2170 . Graphics processor 2100 may be a graphics processor within a multi-core processing system that includes one or more general-purpose processing cores. The graphics processor may be controlled by register writes to one or more control registers (not shown) or via commands issued to graphics processor 2100 via a ring interconnect 2102 . Ring interconnect 2102 may couple graphics processor 2100 to other processing components, such as other graphics processors or general-purpose processors. Commands from ring interconnect 2102 are interpreted by a command streamer 2103 , which supplies instructions to individual components of the geometry pipeline 2120 or the media pipeline 2130 . Command streamer 2103 may direct the operation of a vertex fetcher 2105 that reads vertex data from memory and executes vertex-processing commands provided by command streamer 2103 . The vertex fetcher 2105 may provide vertex data to a vertex shader 2107 , which performs coordinate space transformation and lighting operations to each vertex. Vertex fetcher 2105 and vertex shader 2107 may execute vertex-processing instructions by dispatching execution threads to execution units 2152 A- 2152 B via a thread dispatcher 2131 . The execution units 2152 A- 2152 B may be an array of vector processors having an instruction set for performing graphics and media operations. The execution units 2152 A- 2152 B may have an attached L1 cache 2151 that is specific for each array or shared between the arrays. The cache can be configured as a data cache, an instruction cache, or a single cache that is partitioned to contain data and instructions in different partitions. A geometry pipeline 2120 may include tessellation components to perform hardware-accelerated tessellation of 3D objects. A programmable hull shader 2111 may configure the tessellation operations. A programmable domain shader 2117 may provide back-end evaluation of tessellation output. A tessellator 2113 may operate at the direction of hull shader 2111 and contain special purpose logic to generate a set of detailed geometric objects based on a coarse geometric model that is provided as input to geometry pipeline 2120 . In addition, if tessellation is not used, tessellation components (e.g., hull shader 2111 , tessellator 2113 , and domain shader 2117 ) can be bypassed. The tessellation components can operate based on data received from the vertex shader 2107 . Complete geometric objects may be processed by a geometry shader 2119 via one or more threads dispatched to execution units 2152 A- 2152 B, or can proceed directly to the clipper 2129 . The geometry shader may operate on entire geometric objects, rather than vertices or patches of vertices as in previous stages of the graphics pipeline. If the tessellation is disabled the geometry shader 2119 receives input from the vertex shader 2107 . The geometry shader 2119 may be programmable by a geometry shader program to perform geometry tessellation if the tessellation units are disabled. Before rasterization, a clipper 2129 processes vertex data. The clipper 2129 may be a fixed function clipper or a programmable clipper having clipping and geometry shader functions. A rasterizer and depth test component 2173 in the render output pipeline 2170 may dispatch pixel shaders to convert the geometric objects into per pixel representations. The pixel shader logic may be included in thread execution logic 2150 . Optionally, an application can bypass the rasterizer and depth test component 2173 and access un-rasterized vertex data via a stream out unit 2123 . The graphics processor 2100 has an interconnect bus, interconnect fabric, or some other interconnect mechanism that allows data and message passing amongst the major components of the processor. In some embodiments, execution units 2152 A- 2152 B and associated logic units (e.g., L1 cache 2151 , sampler 2154 , texture cache 2158 , etc.) interconnect via a data port 2156 to perform memory access and communicate with render output pipeline components of the processor. A sampler 2154 , caches 2151 , 2158 and execution units 2152 A- 2152 B each may have separate memory access paths. Optionally, the texture cache 2158 can also be configured as a sampler cache. The render output pipeline 2170 may contain a rasterizer and depth test component 2173 that converts vertex-based objects into an associated pixel-based representation. The rasterizer logic may include a windower/masker unit to perform fixed function triangle and line rasterization. An associated render cache 2178 and depth cache 2179 are also available in some embodiments. A pixel operations component 2177 performs pixel-based operations on the data, though in some instances, pixel operations associated with 2D operations (e.g. bit block image transfers with blending) are performed by the 2D engine 2141 , or substituted at display time by the display controller 2143 using overlay display planes. A shared L3 cache 2175 may be available to all graphics components, allowing the sharing of data without the use of main system memory. The media pipeline 2130 may include a media engine 2137 and a video front-end 2134 . Video front-end 2134 may receive pipeline commands from the command streamer 2103 . The media pipeline 2130 may include a separate command streamer. Video front-end 2134 may process media commands before sending the command to the media engine 2137 . Media engine 2137 may include thread spawning functionality to spawn threads for dispatch to thread execution logic 2150 via thread dispatcher 2131 . The graphics processor 2100 may include a display engine 2140 . This display engine 2140 may be external to processor 2100 and may couple with the graphics processor via the ring interconnect 2102 , or some other interconnect bus or fabric. Display engine 2140 may include a 2D engine 2141 and a display controller 2143 . Display engine 2140 may contain special purpose logic capable of operating independently of the 3D pipeline. Display controller 2143 may couple with a display device (not shown), which may be a system integrated display device, as in a laptop computer, or an external display device attached via a display device connector. The geometry pipeline 2120 and media pipeline 2130 maybe configurable to perform operations based on multiple graphics and media programming interfaces and are not specific to any one application programming interface (API). A driver software for the graphics processor may translate API calls that are specific to a particular graphics or media library into commands that can be processed by the graphics processor. Support may be provided for the Open Graphics Library (OpenGL), Open Computing Language (OpenCL), and/or Vulkan graphics and compute API, all from the Khronos Group. Support may also be provided for the Direct3D library from the Microsoft Corporation. A combination of these libraries may be supported. Support may also be provided for the Open Source Computer Vision Library (OpenCV). A future API with a compatible 3D pipeline would also be supported if a mapping can be made from the pipeline of the future API to the pipeline of the graphics processor. Graphics Pipeline Programming FIG. 22A is a block diagram illustrating a graphics processor command format 2200 used for programming graphics processing pipelines, such as, for example, the pipelines described herein in conjunction with FIG. 16A, 17, 21 . FIG. 22B is a block diagram illustrating a graphics processor command sequence 2210 according to an embodiment. The solid lined boxes in FIG. 22A illustrate the components that are generally included in a graphics command while the dashed lines include components that are optional or that are only included in a sub-set of the graphics commands. The example graphics processor command format 2200 of FIG. 22A includes data fields to identify a client 2202 , a command operation code (opcode) 2204 , and data 2206 for the command. A sub-opcode 2205 and a command size 2208 are also included in some commands. Client 2202 may specify the client unit of the graphics device that processes the command data. A graphics processor command parser may examine the client field of each command to condition the further processing of the command and route the command data to the appropriate client unit. The graphics processor client units may include a memory interface unit, a render unit, a 2D unit, a 3D unit, and a media unit. Each client unit may have a corresponding processing pipeline that processes the commands. Once the command is received by the client unit, the client unit reads the opcode 2204 and, if present, sub-opcode 2205 to determine the operation to perform. The client unit performs the command using information in data field 2206 . For some commands an explicit command size 2208 is expected to specify the size of the command. The command parser may automatically determine the size of at least some of the commands based on the command opcode. Commands may be aligned via multiples of a double word. Other command formats can also be used. The flow diagram in FIG. 22B illustrates an example graphics processor command sequence 2210 . Software or firmware of a data processing system that features an example graphics processor may use a version of the command sequence shown to set up, execute, and terminate a set of graphics operations. A sample command sequence is shown and described for purposes of example only and is not limited to these specific commands or to this command sequence. Moreover, the commands may be issued as batch of commands in a command sequence, such that the graphics processor will process the sequence of commands in at least partially concurrence. The graphics processor command sequence 2210 may begin with a pipeline flush command 2212 to cause any active graphics pipeline to complete the currently pending commands for the pipeline. Optionally, the 3D pipeline 2222 and the media pipeline 2224 may not operate concurrently. The pipeline flush is performed to cause the active graphics pipeline to complete any pending commands. In response to a pipeline flush, the command parser for the graphics processor will pause command processing until the active drawing engines complete pending operations and the relevant read caches are invalidated. Optionally, any data in the render cache that is marked ‘dirty’ can be flushed to memory. Pipeline flush command 2212 can be used for pipeline synchronization or before placing the graphics processor into a low power state. A pipeline select command 2213 may be used when a command sequence utilizes the graphics processor to explicitly switch between pipelines. A pipeline select command 2213 may be utilized only once within an execution context before issuing pipeline commands unless the context is to issue commands for both pipelines. A pipeline flush command 2212 may be utilized immediately before a pipeline switch via the pipeline select command 2213 . A pipeline control command 2214 may configure a graphics pipeline for operation and may be used to program the 3D pipeline 2222 and the media pipeline 2224 . The pipeline control command 2214 may configure the pipeline state for the active pipeline. The pipeline control command 2214 may be used for pipeline synchronization and to clear data from one or more cache memories within the active pipeline before processing a batch of commands. Commands related to the return buffer state 2216 may be used to configure a set of return buffers for the respective pipelines to write data. Some pipeline operations utilize the allocation, selection, or configuration of one or more return buffers into which the operations write intermediate data during processing. The graphics processor may also use one or more return buffers to store output data and to perform cross thread communication. The return buffer state 2216 may include selecting the size and number of return buffers to use for a set of pipeline operations. The remaining commands in the command sequence differ based on the active pipeline for operations. Based on a pipeline determination 2220 , the command sequence is tailored to the 3D pipeline 2222 beginning with the 3D pipeline state 2230 or the media pipeline 2224 beginning at the media pipeline state 2240 . The commands to configure the 3D pipeline state 2230 include 3D state setting commands for vertex buffer state, vertex element state, constant color state, depth buffer state, and other state variables that are to be configured before 3D primitive commands are processed. The values of these commands are determined at least in part based on the particular 3D API in use. The 3D pipeline state 2230 commands may also be able to selectively disable or bypass certain pipeline elements if those elements will not be used. A 3D primitive 2232 command may be used to submit 3D primitives to be processed by the 3D pipeline. Commands and associated parameters that are passed to the graphics processor via the 3D primitive 2232 command are forwarded to the vertex fetch function in the graphics pipeline. The vertex fetch function uses the 3D primitive 2232 command data to generate vertex data structures. The vertex data structures are stored in one or more return buffers. The 3D primitive 2232 command may be used to perform vertex operations on 3D primitives via vertex shaders. To process vertex shaders, 3D pipeline 2222 dispatches shader execution threads to graphics processor execution units. The 3D pipeline 2222 may be triggered via an execute 2234 command or event. A register may write trigger command executions. An execution may be triggered via a ‘go’ or ‘kick’ command in the command sequence. Command execution may be triggered using a pipeline synchronization command to flush the command sequence through the graphics pipeline. The 3D pipeline will perform geometry processing for the 3D primitives. Once operations are complete, the resulting geometric objects are rasterized and the pixel engine colors the resulting pixels. Additional commands to control pixel shading and pixel back end operations may also be included for those operations. The graphics processor command sequence 2210 may follow the media pipeline 2224 path when performing media operations. In general, the specific use and manner of programming for the media pipeline 2224 depends on the media or compute operations to be performed. Specific media decode operations may be offloaded to the media pipeline during media decode. The media pipeline can also be bypassed and media decode can be performed in whole or in part using resources provided by one or more general-purpose processing cores. The media pipeline may also include elements for general-purpose graphics processor unit (GPGPU) operations, where the graphics processor is used to perform SIMD vector operations using computational shader programs that are not explicitly related to the rendering of graphics primitives. Media pipeline 2224 may be configured in a similar manner as the 3D pipeline 2222 . A set of commands to configure the media pipeline state 2240 are dispatched or placed into a command queue before the media object commands 2242 . Commands for the media pipeline state 2240 may include data to configure the media pipeline elements that will be used to process the media objects. This includes data to configure the video decode and video encode logic within the media pipeline, such as encode or decode format. Commands for the media pipeline state 2240 may also support the use of one or more pointers to “indirect” state elements that contain a batch of state settings. Media object commands 2242 may supply pointers to media objects for processing by the media pipeline. The media objects include memory buffers containing video data to be processed. Optionally, all media pipeline states should be valid before issuing a media object command 2242 . Once the pipeline state is configured and media object commands 2242 are queued, the media pipeline 2224 is triggered via an execute command 2244 or an equivalent execute event (e.g., register write). Output from media pipeline 2224 may then be post processed by operations provided by the 3D pipeline 2222 or the media pipeline 2224 . GPGPU operations may be configured and executed in a similar manner as media operations. Graphics Software Architecture FIG. 23 illustrates an example graphics software architecture for a data processing system 2300 . Such a software architecture may include a 3D graphics application 2310 , an operating system 2320 , and at least one processor 2330 . Processor 2330 may include a graphics processor 2332 and one or more general-purpose processor core(s) 2334 . The processor 2330 may be a variant of the processor 1402 or any other of the processors described herein. The processor 2330 may be used in place of the processor 1402 or any other of the processors described herein. Therefore, the disclosure of any features in combination with the processor 1402 or any other of the processors described herein also discloses a corresponding combination with the graphics processor 2330 , but is not limited to such. Moreover, the elements of FIG. 23 having the same or similar names as the elements of any other figure herein describe the same elements as in the other figures, can operate or function in a manner similar to that, can comprise the same components, and can be linked to other entities, as those described elsewhere herein, but are not limited to such. The graphics application 2310 and operating system 2320 are each executed in the system memory 2350 of the data processing system. 3D graphics application 2310 may contain one or more shader programs including shader instructions 2312 . The shader language instructions may be in a high-level shader language, such as the High-Level Shader Language (HLSL) of Direct3D, the OpenGL Shader Language (GLSL), and so forth. The application may also include executable instructions 2314 in a machine language suitable for execution by the general-purpose processor core 2334 . The application may also include graphics objects 2316 defined by vertex data. The operating system 2320 may be a Microsoft® Windows® operating system from the Microsoft Corporation, a proprietary UNIX-like operating system, or an open source UNIX-like operating system using a variant of the Linux kernel. The operating system 2320 can support a graphics API 2322 such as the Direct3D API, the OpenGL API, or the Vulkan API. When the Direct3D API is in use, the operating system 2320 uses a front-end shader compiler 2324 to compile any shader instructions 2312 in HLSL into a lower-level shader language. The compilation may be a just-in-time (JIT) compilation or the application can perform shader pre-compilation. High-level shaders may be compiled into low-level shaders during the compilation of the 3D graphics application 2310 . The shader instructions 2312 may be provided in an intermediate form, such as a version of the Standard Portable Intermediate Representation (SPIR) used by the Vulkan API. User mode graphics driver 2326 may contain a back-end shader compiler 2327 to convert the shader instructions 2312 into a hardware specific representation. When the OpenGL API is in use, shader instructions 2312 in the GLSL high-level language are passed to a user mode graphics driver 2326 for compilation. The user mode graphics driver 2326 may use operating system kernel mode functions 2328 to communicate with a kernel mode graphics driver 2329 . The kernel mode graphics driver 2329 may communicate with graphics processor 2332 to dispatch commands and instructions. IP Core Implementations One or more aspects may be implemented by representative code stored on a machine-readable medium which represents and/or defines logic within an integrated circuit such as a processor. For example, the machine-readable medium may include instructions which represent various logic within the processor. In one embodiment, the machine-readable medium may also be referred to herein as a computer-readable medium and/or a non-transitory computer-readable medium. When read by a machine, the instructions may cause the machine to fabricate the logic to perform the techniques described herein. Such representations, known as “IP cores,” are reusable units of logic for an integrated circuit that may be stored on a tangible, machine-readable medium as a hardware model that describes the structure of the integrated circuit. The hardware model may be supplied to various customers or manufacturing facilities, which load the hardware model on fabrication machines that manufacture the integrated circuit. The integrated circuit may be fabricated such that the circuit performs operations described in association with any of the embodiments described herein. FIG. 24A is a block diagram illustrating an IP core development system 2400 that may be used to manufacture an integrated circuit to perform operations according to an embodiment. The IP core development system 2400 may be used to generate modular, re-usable designs that can be incorporated into a larger design or used to construct an entire integrated circuit (e.g., an SOC integrated circuit). A design facility 2430 can generate a software simulation 2410 of an IP core design in a high-level programming language (e.g., C/C++). The software simulation 2410 can be used to design, test, and verify the behavior of the IP core using a simulation model 2412 . The simulation model 2412 may include functional, behavioral, and/or timing simulations. A register transfer level (RTL) design 2415 can then be created or synthesized from the simulation model 2412 . The RTL design 2415 is an abstraction of the behavior of the integrated circuit that models the flow of digital signals between hardware registers, including the associated logic performed using the modeled digital signals. In addition to an RTL design 2415 , lower-level designs at the logic level or transistor level may also be created, designed, or synthesized. Thus, the particular details of the initial design and simulation may vary. The RTL design 2415 or equivalent may be further synthesized by the design facility into a hardware model 2420 , which may be in a hardware description language (HDL), or some other representation of physical design data. The HDL may be further simulated or tested to verify the IP core design. The IP core design can be stored for delivery to a 3 rd party fabrication facility 2465 using non-volatile memory 2440 (e.g., hard disk, flash memory, or any non-volatile storage medium). Alternatively, the IP core design may be transmitted (e.g., via the Internet) over a wired connection 2450 or wireless connection 2460 . The fabrication facility 2465 may then fabricate an integrated circuit that is based at least in part on the IP core design. The fabricated integrated circuit can be configured to perform operations in accordance with at least one embodiment described herein. FIG. 24B illustrates a cross-section side view of an integrated circuit package assembly 2470 . The integrated circuit package assembly 2470 illustrates an implementation of one or more processor or accelerator devices as described herein. The package assembly 2470 includes multiple units of hardware logic 2472 , 2474 connected to a substrate 2480 . The logic 2472 , 2474 may be implemented at least partly in configurable logic or fixed-functionality logic hardware, and can include one or more portions of any of the processor core(s), graphics processor(s), or other accelerator devices described herein. Each unit of logic 2472 , 2474 can be implemented within a semiconductor die and coupled with the substrate 2480 via an interconnect structure 2473 . The interconnect structure 2473 may be configured to route electrical signals between the logic 2472 , 2474 and the substrate 2480 , and can include interconnects such as, but not limited to bumps or pillars. The interconnect structure 2473 may be configured to route electrical signals such as, for example, input/output (I/O) signals and/or power or ground signals associated with the operation of the logic 2472 , 2474 . Optionally, the substrate 2480 may be an epoxy-based laminate substrate. The substrate 2480 may also include other suitable types of substrates. The package assembly 2470 can be connected to other electrical devices via a package interconnect 2483 . The package interconnect 2483 may be coupled to a surface of the substrate 2480 to route electrical signals to other electrical devices, such as a motherboard, other chipset, or multi-chip module. The units of logic 2472 , 2474 may be electrically coupled with a bridge 2482 that is configured to route electrical signals between the logic 2472 , 2474 . The bridge 2482 may be a dense interconnect structure that provides a route for electrical signals. The bridge 2482 may include a bridge substrate composed of glass or a suitable semiconductor material. Electrical routing features can be formed on the bridge substrate to provide a chip-to-chip connection between the logic 2472 , 2474 . Although two units of logic 2472 , 2474 and a bridge 2482 are illustrated, embodiments described herein may include more or fewer logic units on one or more dies. The one or more dies may be connected by zero or more bridges, as the bridge 2482 may be excluded when the logic is included on a single die. Alternatively, multiple dies or units of logic can be connected by one or more bridges. Additionally, multiple logic units, dies, and bridges can be connected together in other possible configurations, including three-dimensional configurations. FIG. 24C illustrates a package assembly 2490 that includes multiple units of hardware logic chiplets connected to a substrate 2480 (e.g., base die). A graphics processing unit, parallel processor, and/or compute accelerator as described herein can be composed from diverse silicon chiplets that are separately manufactured. In this context, a chiplet is an at least partially packaged integrated circuit that includes distinct units of logic that can be assembled with other chiplets into a larger package. A diverse set of chiplets with different IP core logic can be assembled into a single device. Additionally the chiplets can be integrated into a base die or base chiplet using active interposer technology. The concepts described herein enable the interconnection and communication between the different forms of IP within the GPU. IP cores can be manufactured using different process technologies and composed during manufacturing, which avoids the complexity of converging multiple IPs, especially on a large SoC with several flavors IPs, to the same manufacturing process. Enabling the use of multiple process technologies improves the time to market and provides a cost-effective way to create multiple product SKUs. Additionally, the disaggregated IPs are more amenable to being power gated independently, components that are not in use on a given workload can be powered off, reducing overall power consumption. In various embodiments a package assembly 2490 can include fewer or greater number of components and chiplets that are interconnected by a fabric 2485 or one or more bridges 2487 . The chiplets within the package assembly 2490 may have a 2.5D arrangement using Chip-on-Wafer-on-Substrate stacking in which multiple dies are stacked side-by-side on a silicon interposer that includes through-silicon vias (TSVs) to couple the chiplets with the substrate 2480 , which includes electrical connections to the package interconnect 2483 . In one embodiment, silicon interposer is an active interposer 2489 that includes embedded logic in addition to TSVs. In such embodiment, the chiplets within the package assembly 2490 are arranged using 3D face to face die stacking on top of the active interposer 2489 . The active interposer 2489 can include hardware logic for I/O 2491 , cache memory 2492 , and other hardware logic 2493 , in addition to interconnect fabric 2485 and a silicon bridge 2487 . The fabric 2485 enables communication between the various logic chiplets 2472 , 2474 and the logic 2491 , 2493 within the active interposer 2489 . The fabric 2485 may be an NoC interconnect or another form of packet switched fabric that switches data packets between components of the package assembly. For complex assemblies, the fabric 2485 may be a dedicated chiplet enables communication between the various hardware logic of the package assembly 2490 . Bridge structures 2487 within the active interposer 2489 may be used to facilitate a point to point interconnect between, for example, logic or I/O chiplets 2474 and memory chiplets 2475 . In some implementations, bridge structures 2487 may also be embedded within the substrate 2480 . The hardware logic chiplets can include special purpose hardware logic chiplets 2472 , logic or I/O chiplets 2474 , and/or memory chiplets 2475 . The hardware logic chiplets 2472 and logic or I/O chiplets 2474 may be implemented at least partly in configurable logic or fixed-functionality logic hardware and can include one or more portions of any of the processor core(s), graphics processor(s), parallel processors, or other accelerator devices described herein. The memory chiplets 2475 can be DRAM (e.g., GDDR, HBM) memory or cache (SRAM) memory. Cache memory 2492 within the active interposer 2489 (or substrate 2480 ) can act as a global cache for the package assembly 2490 , part of a distributed global cache, or as a dedicated cache for the fabric 2485 Each chiplet can be fabricated as separate semiconductor die and coupled with a base die that is embedded within or coupled with the substrate 2480 . The coupling with the substrate 2480 can be performed via an interconnect structure 2473 . The interconnect structure 2473 may be configured to route electrical signals between the various chiplets and logic within the substrate 2480 . The interconnect structure 2473 can include interconnects such as, but not limited to bumps or pillars. In some embodiments, the interconnect structure 2473 may be configured to route electrical signals such as, for example, input/output (I/O) signals and/or power or ground signals associated with the operation of the logic, I/O and memory chiplets. In one embodiment, an additional interconnect structure couples the active interposer 2489 with the substrate 2480 . The substrate 2480 may be an epoxy-based laminate substrate, however, it is not limited to that and the substrate 2480 may also include other suitable types of substrates. The package assembly 2490 can be connected to other electrical devices via a package interconnect 2483 . The package interconnect 2483 may be coupled to a surface of the substrate 2480 to route electrical signals to other electrical devices, such as a motherboard, other chipset, or multi-chip module. A logic or I/O chiplet 2474 and a memory chiplet 2475 may be electrically coupled via a bridge 2487 that is configured to route electrical signals between the logic or I/O chiplet 2474 and a memory chiplet 2475 . The bridge 2487 may be a dense interconnect structure that provides a route for electrical signals. The bridge 2487 may include a bridge substrate composed of glass or a suitable semiconductor material. Electrical routing features can be formed on the bridge substrate to provide a chip-to-chip connection between the logic or I/O chiplet 2474 and a memory chiplet 2475 . The bridge 2487 may also be referred to as a silicon bridge or an interconnect bridge. For example, the bridge 2487 is an Embedded Multi-die Interconnect Bridge (EMIB). Alternatively, the bridge 2487 may simply be a direct connection from one chiplet to another chiplet. FIG. 24D illustrates a package assembly 2494 including interchangeable chiplets 2495 , according to an embodiment. The interchangeable chiplets 2495 can be assembled into standardized slots on one or more base chiplets 2496 , 2498 . The base chiplets 2496 , 2498 can be coupled via a bridge interconnect 2497 , which can be similar to the other bridge interconnects described herein and may be, for example, an EMIB. Memory chiplets can also be connected to logic or I/O chiplets via a bridge interconnect. I/O and logic chiplets can communicate via an interconnect fabric. The base chiplets can each support one or more slots in a standardized format for one of logic or I/O or memory/cache. SRAM and power delivery circuits may be fabricated into one or more of the base chiplets 2496 , 2498 , which can be fabricated using a different process technology relative to the interchangeable chiplets 2495 that are stacked on top of the base chiplets. For example, the base chiplets 2496 , 2498 can be fabricated using a larger process technology, while the interchangeable chiplets can be manufactured using a smaller process technology. One or more of the interchangeable chiplets 2495 may be memory (e.g., DRAM) chiplets. Different memory densities can be selected for the package assembly 2494 based on the power, and/or performance targeted for the product that uses the package assembly 2494 . Additionally, logic chiplets with a different number of type of functional units can be selected at time of assembly based on the power, and/or performance targeted for the product. Additionally, chiplets containing IP logic cores of differing types can be inserted into the interchangeable chiplet slots, enabling hybrid processor designs that can mix and match different technology IP blocks. Example System on a Chip Integrated Circuit FIG. 25-26B illustrate example integrated circuits and associated graphics processors that may be fabricated using one or more IP cores. In addition to what is illustrated, other logic and circuits may be included, including additional graphics processors/cores, peripheral interface controllers, or general-purpose processor cores. The elements of FIG. 25-26B having the same or similar names as the elements of any other figure herein describe the same elements as in the other figures, can operate or function in a manner similar to that, can comprise the same components, and can be linked to other entities, as those described elsewhere herein, but are not limited to such. FIG. 25 is a block diagram illustrating an example system on a chip integrated circuit 2500 that may be fabricated using one or more IP cores. Example integrated circuit 2500 includes one or more application processor(s) 2505 (e.g., CPUs), at least one graphics processor 2510 , which may be a variant of the graphics processor 1408 , 1508 , 2510 , or of any graphics processor described herein and may be used in place of any graphics processor described. Therefore, the disclosure of any features in combination with a graphics processor herein also discloses a corresponding combination with the graphics processor 2510 , but is not limited to such. The integrated circuit 2500 may additionally include an image processor 2515 and/or a video processor 2520 , any of which may be a modular IP core from the same or multiple different design facilities. Integrated circuit 2500 may include peripheral or bus logic including a USB controller 2525 , UART controller 2530 , an SPI/SDIO controller 2535 , and an I 2 S/I 2 C controller 2540 . Additionally, the integrated circuit can include a display device 2545 coupled to one or more of a high-definition multimedia interface (HDMI) controller 2550 and a mobile industry processor interface (MIPI) display interface 2555 . Storage may be provided by a flash memory subsystem 2560 including flash memory and a flash memory controller. Memory interface may be provided via a memory controller 2565 for access to SDRAM or SRAM memory devices. Some integrated circuits additionally include an embedded security engine 2570 . FIG. 26A-26B are block diagrams illustrating example graphics processors for use within an SoC, according to embodiments described herein. The graphics processors may be variants of the graphics processor 1408 , 1508 , 2510 , or any other graphics processor described herein. The graphics processors may be used in place of the graphics processor 1408 , 1508 , 2510 , or any other of the graphics processors described herein. Therefore, the disclosure of any features in combination with the graphics processor 1408 , 1508 , 2510 , or any other of the graphics processors described herein also discloses a corresponding combination with the graphics processors of FIG. 26A-26B , but is not limited to such. FIG. 26A illustrates an example graphics processor 2610 of a system on a chip integrated circuit that may be fabricated using one or more IP cores, according to an embodiment. FIG. 26B illustrates an additional example graphics processor 2640 of a system on a chip integrated circuit that may be fabricated using one or more IP cores, according to an embodiment. Graphics processor 2610 of FIG. 26A is an example of a low power graphics processor core. Graphics processor 2640 of FIG. 26B is an example of a higher performance graphics processor core. For example, each of graphics processor 2610 and graphics processor 2640 can be a variant of the graphics processor 2510 of FIG. 25 , as mentioned at the outset of this paragraph. As shown in FIG. 26A , graphics processor 2610 includes a vertex processor 2605 and one or more fragment processor(s) 2615 A- 2615 N (e.g., 2615 A, 2615 B, 2615 C, 2615 D, through 2615 N- 1 , and 2615 N). Graphics processor 2610 can execute different shader programs via separate logic, such that the vertex processor 2605 is optimized to execute operations for vertex shader programs, while the one or more fragment processor(s) 2615 A- 2615 N execute fragment (e.g., pixel) shading operations for fragment or pixel shader programs. The vertex processor 2605 performs the vertex processing stage of the 3D graphics pipeline and generates primitives and vertex data. The fragment processor(s) 2615 A- 2615 N use the primitive and vertex data generated by the vertex processor 2605 to produce a framebuffer that is displayed on a display device. The fragment processor(s) 2615 A- 2615 N may be optimized to execute fragment shader programs as provided for in the OpenGL API, which may be used to perform similar operations as a pixel shader program as provided for in the Direct 3D API. Graphics processor 2610 additionally includes one or more memory management units (MMUs) 2620 A- 2620 B, cache(s) 2625 A- 2625 B, and circuit interconnect(s) 2630 A- 2630 B. The one or more MMU(s) 2620 A- 2620 B provide for virtual to physical address mapping for the graphics processor 2610 , including for the vertex processor 2605 and/or fragment processor(s) 2615 A- 2615 N, which may reference vertex or image/texture data stored in memory, in addition to vertex or image/texture data stored in the one or more cache(s) 2625 A- 2625 B. The one or more MMU(s) 2620 A- 2620 B may be synchronized with other MMUs within the system, including one or more MMUs associated with the one or more application processor(s) 2505 , image processor 2515 , and/or video processor 2520 of FIG. 25 , such that each processor 2505 - 2520 can participate in a shared or unified virtual memory system. Components of graphics processor 2610 may correspond with components of other graphics processors described herein. The one or more MMU(s) 2620 A- 2620 B may correspond with MMU 245 of FIG. 2C . Vertex processor 2605 and fragment processor 2615 A- 2615 N may correspond with graphics multiprocessor 234 . The one or more circuit interconnect(s) 2630 A- 2630 B enable graphics processor 2610 to interface with other IP cores within the SoC, either via an internal bus of the SoC or via a direct connection, according to embodiments. The one or more circuit interconnect(s) 2630 A- 2630 B may correspond with the data crossbar 240 of FIG. 2C . Further correspondence may be found between analogous components of the graphics processor 2610 and the various graphics processor architectures described herein. As shown FIG. 26B , graphics processor 2640 includes the one or more MMU(s) 2620 A- 2620 B, cache(s) 2625 A- 2625 B, and circuit interconnect(s) 2630 A- 2630 B of the graphics processor 2610 of FIG. 26A . Graphics processor 2640 includes one or more shader cores 2655 A- 2655 N (e.g., 2655 A, 2655 B, 2655 C, 2655 D, 2655 E, 2655 F, through 2655 N- 1 , and 2655 N), which provides for a unified shader core architecture in which a single core or type or core can execute all types of programmable shader code, including shader program code to implement vertex shaders, fragment shaders, and/or compute shaders. The exact number of shader cores present can vary among embodiments and implementations. Additionally, graphics processor 2640 includes an inter-core task manager 2645 , which acts as a thread dispatcher to dispatch execution threads to one or more shader cores 2655 A- 2655 N and a tiling unit 2658 to accelerate tiling operations for tile-based rendering, in which rendering operations for a scene are subdivided in image space, for example to exploit local spatial coherence within a scene or to optimize use of internal caches. Shader cores 2655 A- 2655 N may correspond with, for example, graphics multiprocessor 234 as in FIG. 2D , or graphics multiprocessors 325 , 350 of FIGS. 3A and 3B respectively, or multi-core group 365 A of FIG. 3C . Efficient Data Sharing for Graphics Data Processing Operations Parallel computing is a type of computation in which many calculations or the execution of processes are carried out simultaneously. Parallel computing may come in a variety of forms, including, but not limited to, SIMD or SIMT. SIMD describes computers with multiple processing elements that perform the same operation on multiple data points simultaneously. In one example, FIGS. 5A-5B discussed above refer to SIMD and its implementation in a general processor in terms of EUs, FPUs, and ALUs. In a common SIMD machine, data is packaged into registers, each containing an array of channels. Instructions operate on the data found in channel n of a register with the data found in the same channel of another register. SIMD machines are advantageous in areas where a single sequence of instructions can be simultaneously applied to high amounts of data. For example, in one embodiment, a graphics processor (e.g., GPGPU, GPU, etc.) can be used to perform SIMD vector operations using computational shader programs. Various embodiments can also apply to use execution by use of Single Instruction Multiple Thread (SIMT) as an alternate to use of SIMD or in addition to use of SIMD. Reference to a SIMD core or operation can apply also to SIMT or apply to SIMD in combination with SIMT. The following description is discussed in terms of SIMD machines. However, embodiments herein are not solely limited to application in the SIMD context and may apply in other parallel computing paradigms, such as SIMT, for example. For ease of discussion and explanation, the following description generally focuses on a SIMD implementation. However, embodiments can similarly apply to SIMT machines with no modifications to the described techniques and methodologies. With respect to SIMT machines, similar patterns as discussed below can be followed to provide instructions to the systolic array and execute the instructions on the SIMT machine. Other types of parallel computing machines may also utilize embodiments herein as well. A previously noted, a technical problem encountered with GPUs, such as the GPUs of GPGPUs described above, is the lack of low-latency, high-bandwidth data sharing when performing graphics operations. Conventionally, data sharing is performed via a global cache, such as the L2 or L3 cache. This is inefficient in terms of both power consumed and latency experienced by the GPU. This conventional approach to data sharing can create issues in security and effectiveness of operation when applied in GPUs. Furthermore, conventional approaches are inefficient with respect to system resources and lead to latency. Embodiments herein introduce a variety of techniques for efficient retrieval and communication of data in GPUs to improve the use of system resources. One technique of embodiments is utilization of scalar registers for GPU threads. Another technique of embodiments is cross-slice direct data communications using L1 to L1 communications. Another technique of embodiments is energy aware SRAM address mapping. Another technique of embodiments is fabric block creation for memory paging. The techniques of embodiments are described in further detail below. The embodiments for efficient retrieval and communication of data in GPUs described herein provide a technical advantage of improving data sharing in GPUs, which results in improved processing speed and improved communication bandwidth within and between GPUs in a computing environment. Scalar Registers for GPU Threads In some embodiments, an apparatus, system, or process is to provide scalar registers for GPU threads. Conventionally, GPUs are vector machines and registers of the GPUs are vector registers. For example, in conventional systems, GPU registers may be 32-way vectors or 16-way vectors. If there is a 32-way vector machine, then each register can have 32 elements of data (where each element could be 32 or 64 bits). However, for some operations of the GPUs, the vector registers are not fully utilized. For example, if an operation aims to push something onto a stack in memory, then a single stack pointer is used. If the stack pointer is allocated in the vector register, significant register space is wasted because the system is only calling in a single stack pointer for the whole thread. FIG. 27A is a block diagram illustrating a GPU vector register 2700 in accordance with embodiments. The GPU vector register 2700 may be part of a GPGPU and/or GPU, such as those described herein. The GPU vector register 2700 of FIG. 27A is an example 16-way vector register allocated in a GPU. The vector register 2700 may include an array of 16 double words (32 bits). The vector register 2700 can be divided across the logic used to execute multiple simultaneous threads (e.g., software thread 15 2710 through software thread 0 2720 ) within the compute unit 1910 of the execution unit 1900 . FIG. 27B is a block diagram illustrating a bank of scalar registers 2750 according to embodiments. The bank of scalar registers 2750 may be part of a GPGPU and/or GPU, such as those described herein. In one embodiment, a pool of scalar registers 2760 (e.g., each holding one data item) are created and offered in a GPU as a scalar register bank 2750 . These scalar GPU registers 2760 can be provided for all threads. In some embodiments, the scalar GPU registers 2760 may be general purpose registers (GPRs), and referred to as scalar GPRs 2760 herein. The scalar GPRs 2760 may be global 64-bit GPRs in some embodiments. The scalar GPRs 2760 can be smaller than the vector registers (such as vector register 2700 described with respect to FIG. 27A ) and can be used for particular operations, such as storing stack pointers and storing constant values, to name a few examples. In one embodiment, the scalar GPRs 2760 are associated with each thread and coupled with shared physical registers (e.g., among pools or between all threads). Further, in one embodiment, the scalar GPRs 2760 can be shared across all threads. A pool of scalar GPRs 2760 can be created and shared across threads of the GPU. In one example, if a compiler detects a same constant is being loaded/used, then the compiler can use one of the scalar registers 2760 to store this constant. In another example, threads in a GPU may utilize the same value (e.g., some constant data that is shared by all threads). In this example, instead of copying that constant data into every thread vector register, implementations can maintain the constant data in the pool of shared scalar registers 2760 and all threads can share that constant data. FIG. 28 illustrates an example execution unit 2800 of a graphic processor in accordance with embodiments. In one embodiment, the elements of FIG. 28 having the same or similar names as the elements of any other figure herein describe the same elements as in the other figures, can operate or function in a manner similar to that, can comprise the same components, and can be linked to other entities, as those described elsewhere herein, but are not limited to such. The execution unit 2800 may be the same as execution unit 1900 described with respect to FIG. 19 , having the same or similar elements that operate or function in a manner similar to that, can comprise the same components, and can be linked to other entities, as those described in FIG. 19 herein, but are not limited to such. The execution unit 2800 may be a compute-optimized execution unit for use in, for example, a compute engine tile 1640 A- 1640 D as in FIG. 16C , but is not limited as such. The execution unit 2800 may also be used in a graphics engine tile 1610 A- 1610 D as in FIG. 16B . The execution unit 2800 may include a thread control unit 1901 , a thread state unit 1902 , an instruction fetch/prefetch unit 1903 , and an instruction decode unit 1904 that operate or function in a manner similar to that, can comprise the same components, and can be linked to other entities, as those described in FIG. 19 herein, but are not limited to such. The execution unit 1900 may additionally include a send unit 1907 and a branch unit 1908 . The send unit 1907 and branch unit 1908 that operate or function in a manner similar to that, can comprise the same components, and can be linked to other entities, as those described in FIG. 19 herein, but are not limited to such. The execution unit 2800 may additionally include a register file 1906 that stores registers that can be assigned to hardware threads within the execution unit 2800 . Registers in the register file 1906 can be divided across the logic used to execute multiple simultaneous threads within the compute unit 1910 of the execution unit 1900 . The number of logical threads that may be executed by the graphics execution unit 1900 is not limited to the number of hardware threads, and multiple logical threads can be assigned to each hardware thread. The size of the register file 1906 can vary across embodiments based on the number of supported hardware threads. Register renaming may be used to dynamically allocate registers to hardware threads. Embodiments provide for the register file 1906 to include a set of vector registers in a vector register bank 2820 and a set of scalar registers in a scalar register bank 2830 . The vector register bank 2820 may include one or more vector registers, such as vector register 2700 described with respect to FIG. 27A . The scalar register bank 2830 may include one or more scalar registers 2835 A- 2835 N. In one embodiment, scalar registers 2835 A- 2835 may be the same as scalar GPRs 2760 described with respect to FIG. 27B . The execution unit 2800 can also include a compute unit 1910 that includes multiple different types of functional units. The compute unit 1910 may also include an ALUs including vector ALUs 2810 and scalar ALUs 2815 , a systolic array 1912 , and a math unit 1913 . The ALUs 2810 , 2815 includes an array of arithmetic logic units. The vector ALU 2810 can be configured to perform 64-bit, 32-bit, and 16-bit integer and floating-point operations across multiple processing lanes and data channels and for multiple hardware and/or software threads. The scalar ALU 2815 can be configured to perform 64-bit, 32-bit, and 16-bit integer and floating-point operations in association with the scalar registers of the scalar register bank 2830 . The ALUs 2810 , 2815 can perform integer and floating-point operations simultaneously (e.g., within the same clock cycle). In some embodiments, the scalar ALU 2815 can perform a set of operations including, but not limited to, address calculation, stack push, stack pop, call return, call, flag register, and/or generalized addition, subtraction multiplication, branch, etc. In one example, the address calculation may include an operation such as ‘base’+ ‘offset’+scale, where the ‘base’ and the ‘offset’ originate from different scalar registers 2835 A- 2835 N and the ‘scale’ can be a register operand or an immediate value. In one example, the stack push and stack pop may be used for a stack pointer maintained in one of the scalar registers 2835 A- 2835 N (reserved for the stack pointer). In another example, the call return and call can utilize the stack point in the scalar register 2835 A- 2835 N (reserved for the stack pointer). In one example, the flag register can be used for branching and predication. FIG. 29 is a flow diagram illustrating an embodiment of a method 2900 for provide scalar registers for GPU threads. Method 2900 may be performed by processing logic that may comprise hardware (e.g., circuitry, dedicated logic, programmable logic, etc.), software (such as instructions run on a processing device), or a combination thereof. The process of method 2900 is illustrated in linear sequences for brevity and clarity in presentation; however, it is contemplated that any number of them can be performed in parallel, asynchronously, or in different orders. Further, for brevity, clarity, and ease of understanding, many of the components and processes described with respect to FIGS. 1-28 may not be repeated or discussed hereafter. In one implementation, an execution unit of a graphics processor, such as an execution unit 2800 of FIG. 28 , may perform method 2900 . Method 2900 begins at processing block 2910 where a bank of global scalar GPRs is allocated in a GPU. In one embodiment, the scalar GPRs of the allocated bank may be associated with each thread of an application and/or are pooled together for shared access among the threads. At processing block 2920 , a compiler or other application may determine that a same constant is loaded or used by more than one thread of the application. Subsequently, at processing block 2930 , the constant is loaded into a scalar GPR of the allocated bank of scalar GPRs. Lastly, at processing block 2940 , the threads in the GPU can access the scalar GPR storing the constant in response to detecting that the constant is to be used by the thread. In one embodiment, the threads access the scalar GPRs using one or more scalar ALUs of the GPU. Cross Slice Direct Data Communications Using L-1 to L-1 Communication In some embodiments, an apparatus, system, or process is to provide cross slice direct data communications using L1 to L1 communication. In a GPU, such as the GPUs and GPGPUs described herein, a GPU core may include a set of EUs (also referred to as “shader cores” or processing units). This set of EUs can have an L1 cache that is shared between that set of EUs. The unit including the set of EUs and the shared L1 may be referred to as a sub-slice (SS) or a shader module. There may be multiple SSs in a GPU core. As noted above, the EU may be a processing unit. A processing unit may also be referred to as a processing resource. The processing resource or processing unit can represent a processing element (e.g., GPGPU core, ray-tracing core, tensor core, execution resource, execution unit (EU), stream processor, streaming multiprocessor (SM), graphics multiprocessor) associated with a graphics processor or graphics processor structure (e.g., parallel processing unit, graphics processing engine, multi-core group, compute unit, compute unit of graphics core next) in a GPU as described herein. For example, the processing resource may be one of the GPGPU cores, or tensor/ray-tracing cores of graphics multiprocessor; a ray-tracing core, tensor core or GPGPU core of graphics multiprocessor; execution resources of graphics multiprocessor; one of GFX cores, tensor cores, or ray tracing cores of a multi-core group; one of vector logic units or scalar logic units of a compute unit; execution unit with EU array or EU array; an execution unit of execution logic; and/or execution unit. The processing resource may also be an execution resource within, for example, a graphics processing engine, processing cluster, GPGPU, GPGPU, graphics processing engine, graphics processing engine cluster, and/or graphics processing engine. The processing resource may also be a processing resource within graphics processor, graphics processor, and/or graphics processor. Data sharing between SS's may be performed, for example, when a slice is activated. There are also other instances where data sharing between SS's may be utilized. Conventionally, data sharing between SS's is performed using the global cache (e.g., L2 and/or L3 cache). However, use of the global cache for data sharing between SS's can be inefficient in terms of power and latency. Embodiments, address the above-noted technical drawback by providing for a communication architecture for cross-slice direct data communication between L1 caches of graphics processors. Embodiments establish a die-to-die communication network between L1 caches of sub-slices of a GPU to enable direct data communication between the L1 caches. In some embodiments, the sub-slice (SS) is referred to by other terminology, such as a streaming multiprocessor (SM) or any other term to reference a portion of a processor, such as a GPU, utilized to execute one or more threads and including processing resources, registers, and/or memory. FIG. 30 is a block diagram illustrating a cross slice direct data communication system 3000 implemented between L1 caches of multiple SS's of GPUs, according to embodiments. System 3000 may be part of a GPGPU and/or GPU, such as those described herein. In one embodiment, system 3000 includes a chiplet 3005 communicably coupled to a global cache 3020 . In one implementation, chiplet 3005 may be the same as interchangeable chiplets 2495 described with respect to FIG. 24D . In one embodiment, global cache 302 may be an L2/L3 cache maintained on a base chiplet, such as base chiplet 2496 described with respect to FIG. 24D . Chiplet 3005 may include multiple sub-slices, including sub-slice 0 3010 A, sub0slice 1 3010 B, and sub-slice 2 3010 C (collectively referred to as sub-slice 3010 . Each subslice 3010 may include multiple EUs 3012 A- 3012 C (collectively referred to herein as EUs 3012 ) and an L1 cache 3015 A- 3105 C (collectively referred to herein as L1 cache 3015 ). More or less components than those illustrated herein may be include in chiplet 3005 and the specific illustration is not meant to be limiting, but rather provided by way of example. For example, each sub-slice 3010 may include 8 EUs 3012 or 16 EUs. In embodiments herein, an L1 cache 3015 may receive, from a corresponding EU 3012 of the sub-slice 3010 of the L1 cache 3015 , a request to send or receive data to a thread executed by an EU 3012 in a different sub-slice 3010 than the originating EU 3012 . For example, EU 3012 A in sub-slice 0 3010 A may request to send or receive data to a thread executed by EU 3012 B in sub-slice 1 3010 B. This request is received by L1 cache 3015 A. In this example, instead of navigating to a common point (e.g., global cache 3020 ) and then sharing data, the L1 cache 3015 B of a sub-slice 0 3010 A can send or receive the data to the L1 cache 3015 B of sub-slice 1 3010 B using a point-to-point communication network established between the L1 caches 3015 of chiplet 3005 . In one implementation, the point-to-point communication network may include, but is not limited to, a data bus and a signaling bus. The communication network between L1 caches 3015 of sub-slice 3010 provided by embodiments provides improved power efficiency and latency efficiency over conventional systems. FIG. 31 is a block diagram illustrating an example point-to-point communication network 3100 between L1 caches in accordance with embodiments. Point-to-point network 3100 may include a chiplet 3110 in communication with a global cache 3120 . In one embodiment, chiplet 3110 may be the same as chiplet 3005 described with respect to FIG. 30 and global cache 3120 may be the same as global cache 3020 described with respect to FIG. 30 . In one embodiment, the global cache 3120 may be a next level cache, such as L2 cache and/or L3 cache. Chiplet 3110 may include a plurality of sub-slices, SSO 3110 A-SS5 3110 F, which may be the same as sub-slices 3010 described with respect to FIG. 30 . In one embodiment, the point-to-point communication network 3100 may include communication links (e.g., buses) that establish a ring topology among the SSs 3110 A- 3110 F (and their respective L1 caches), as well as with the global cache 3120 . In other embodiments, the communication network among SSs 3110 A- 3110 F (and their respective L1 caches) is a mesh topology (mesh form) network. Other types of network topologies may also be implemented by embodiments herein. Referring back to FIG. 30 , in embodiments herein, the L1 caches 3015 can each include a synchronization circuit 3017 A- 3017 C (collectively referred to as synchronization circuits 3017 herein) to enable routing of communications between the L1 caches using the point-to-point communication network provided by embodiments. The synchronization circuit 3017 may be implemented to signal when to pull or push data between L1 caches 3015 via the point-to-point communication network. As the communication between L1 caches 3015 is a direct point-to-point communication network, embodiments provide messaging to indicate that data is being sent or received between L1 caches 3015 of sub-slices 3010 . The messaging may be in the form of an interrupt or an event, for example. The synchronization circuit 3017 can enable messages to be sent or received between sub-slices 3010 in embodiments. For example, an EU 3012 of a sub-slice 3010 may write data from its local L1 cache 3015 to another remote L1 cache 3015 of another sub-slice 3010 (e.g., during an activation process). In one implementation, the EU 3012 can send a message as an interrupt or ‘event’ to another sub-slice 3010 to indicate that the data is ready. FIG. 32 is a block diagram depicting a synchronization circuit 3200 to provide cross slice direct data communications using L1 to L1 communication, in accordance with embodiments. In one embodiment, synchronization circuit 3200 is the same as synchronization circuits 3017 described with respect to FIG. 30 . Synchronization circuit 3200 may include, but is not limited to, router 3210 , receiver/transmitter (Rx/Tx) 3220 , and mapping table 3230 . In some implementations, one or more of router 3210 , receiver/transmitter (Rx/Tx) 3220 , and mapping table 3230 are implemented via dedicated hardware such as fixed function circuitry, programmable circuitry, or the like. Fixed function circuitry may include dedicated logic or circuitry and may provide a set of fixed function entry points that may map to the dedicated logic for a fixed purpose or function. In one embodiment, the router 3210 may implement an API that is defined to allow communications between sub-slices. Router 3210 may operate to route such communications between L1 caches in accordance with the API. Router 3210 may communicate with Rx/Tx 3220 to cause the communications from an L1 cache to be sent and/or received over the buses of the point-to-point communication network. Router 3210 may further utilize a mapping table 3230 that maintains a mapping of EUs (threads) and sub-slices to determine where to route a communication. In one embodiment, router 3210 operates in accordance with a hierarchy of thread groups. Embodiments provide for the extension of the hierarchy of thread groups to include a super thread group (SGT). The SGT refers to a collection of thread groups that are running on the same chiplet. For example, with respect to FIG. 30 , the SGT may include all EUs 3012 A- 3012 C on chiplet 3005 . In embodiments herein, a thread group runs within a sub-slice and can communicate with another thread group within the SGT. Continuing with the above example, a single thread group within this SGT may include all of the EUs 3012 A of sub-slice 0 3010 A. In embodiments herein, a kernel can utilize query functions to determine information with respect to the SGT. For example, the kernel may utilize a kernel function to inquire the STG identifier (ID) number, a kernel function to inquire the number of thread groups in the STG, and/or a kernel function to send and receive data from/to threads in the same STG as the kernel using a logical thread number. In one embodiment, a chiplet-level cache may be added to the chiplet to be used for sub-slice communication purposes within the chiplet. In one embodiment, the chiplet-level cache may be an L2 cache. This chiplet-level L2 cache may be in communication with a global cache on the base die, for example. This common chiplet-level L2 cache may enable the communication between threads on different sub-slices of the chiplet, but within an SGT of the chiplet. FIGS. 33A and 33B are flow diagrams illustrating embodiments of methods 3300 , 3350 for cross slice direct data communications using L1 to L1 communication. Methods 3300 and 3350 may be performed by processing logic that may comprise hardware (e.g., circuitry, dedicated logic, programmable logic, etc.), software (such as instructions run on a processing device), or a combination thereof. The process of methods 3300 and 3350 are illustrated in linear sequences for brevity and clarity in presentation; however, it is contemplated that any number of them can be performed in parallel, asynchronously, or in different orders. Further, for brevity, clarity, and ease of understanding, many of the components and processes described with respect to FIGS. 1-32 may not be repeated or discussed hereafter. In one implementation, a synchronization circuit, such as synchronization circuit 3017 of FIG. 30 or synchronization circuit 3200 of FIG. 32 , may perform methods 3300 and/or 3350 . With respect to FIG. 33A , method 3300 illustrates a push-based messaging process for cross slice direct data communications using L1 to L1 communication. Method 3300 begins at processing block 3310 where a producer of data (e.g., an EU of an SS) can send data to another L1 cache that requests the data. At processing block 3320 , concurrently or subsequently to sending the data to the other L1 cache, the producer SS of the data can send a message, such as an interrupt or an event, that informs the receiver SS corresponding to the other L1 cache that the data was placed in the receiver L1 cache. Lastly, at processing block 3330 , in response to the message, the receiver SS can read the data now located in the local L1 cache of the receiver SS. With respect to FIG. 33B , method 3350 illustrates a pull-based messaging process for cross slice direct data communications using L1 to L1 communication. Method 3350 begins at processing block 3360 where a producer of data (e.g., an EU of an SS) stores data in an L1 cache (producer L1 cache) of the producer SS. At processing block 3370 , concurrently or subsequently to storing the data in the producer L1 cache, the producer SS of the data can send a message, such as an interrupt or an event, that informs a receiver SS that the data was stored into the producer L1 cache. Lastly, at processing block 3380 , in response to the message, the receiver SS can access the producer L1 cache in order to read the data. then come and read the data from the producer's L1 cache. FIG. 34 is a flow diagram illustrating an embodiment of a method 3400 for utilizing a hierarchy of thread groups for cross slice direct data communications using L1 to L1 communication. Method 3400 may be performed by processing logic that may comprise hardware (e.g., circuitry, dedicated logic, programmable logic, etc.), software (such as instructions run on a processing device), or a combination thereof. The process of method 3400 is illustrated in linear sequences for brevity and clarity in presentation; however, it is contemplated that any number of them can be performed in parallel, asynchronously, or in different orders. Further, for brevity, clarity, and ease of understanding, many of the components and processes described with respect to FIGS. 1-33 may not be repeated or discussed hereafter. In one implementation, a synchronization circuit, such as synchronization circuit 3017 of FIG. 30 or synchronization circuit 3200 of FIG. 32 , may perform method 3400 . Method 3400 begins at processing block 3410 where a thread group is allocated as a member of a super thread group (SGT). In one embodiment, the SGT includes a collection of thread groups running on a same chipset. At processing block 3420 , a request is received from another thread group within the SGT. In one implementation, the request is to communicate with the thread group identified by a thread group identifier (ID) within the SGT. Subsequently, at processing block 3430 , a routing table is accesses to determine location of the thread group based on the ID. Lastly, at processing block 3440 , the request is routed to the determined location of the thread group using communication links between L1 caches of the chiplet. Energy Aware SRAM Address Mapping In some embodiments, an apparatus, system, or process is to provide energy aware SRAM address mapping in graphics processors. Energy cost of SRAM access is primarily driven by wire length. As such, large SRAMs have longer average wire length and thus higher energy per access. Furthermore, some SRAM banks are physically located closer to the EUs (e.g., FPUs), while others are further away. Conventionally, address mapping in the SRAM banks is performed using uniform distribution of data across the SRAM banks. Embodiments discussed herein address this technical problem of higher energy per access in SRAMs by mapping SRAM addresses so that closer locations are mapped to lower addresses. Embodiments provide an address mapping function that places data that is commonly accessed in the closest SRAM banks to the EUs to improve latency and provide better power. As a result, embodiments can provide a technical advantage in graphics processors of reducing energy consumption per access to the SRAM by placing frequently used values in the lowest addresses. FIG. 35 is a block diagram illustrating a GPU 3500 including a set of SRAM banks implementing energy aware SRAM address mapping, according to embodiments. In one embodiment, GPU 3500 is the same as the GPUs and/or GPGPUs described herein. In some embodiments, a hardware accelerator, such as an FPGA, may implement the energy aware SRAM address mapping discussed herein. As such, embodiments are not limited to implementation in a GPU. GPU 3500 may include a set of EUs 3510 , a cache controller 3520 , and an SRAM bank 3530 . In one embodiment, SRAM bank 3530 may comprise any type of physically-addressed memory in the GPU 3500 . In the illustrated example of FIG. 35 , there are 16 banks (e.g., SRAM Bank 0 through SRAM Bank 15) of in the SRAM banks 3530 , where each individual SRAM bank (SRAM Bank 0-SRAM Bank 15) is 4 KB. In total, there is 64 KB of SRAM available in the SRAM banks 3530 . However, other configurations of SRAM banks 3530 are possible and embodiments are not limited to those illustrated and described herein. As noted above, in conventional systems, address mapping is performed using uniform distribution of data across the SRAM banks and moreover, the conventional systems split the data evenly in small granularities (e.g., 512 bytes) for this uniform distribution across the SRAM banks. In contrast, embodiments herein can split the data into larger granularities, such as 4 KB granularity. In one embodiment, an address mapping unit 3525 of the cache controller 3520 may perform the energy aware SRAM address mapping. In one embodiment, the address mapping unit 3525 is implemented via dedicated hardware such as fixed function circuitry, programmable circuitry, or the like. Fixed function circuitry may include dedicated logic or circuitry and may provide a set of fixed function entry points that may map to the dedicated logic for a fixed purpose or function. The address mapping unit 3525 may be notified of a commonly accessed address range by a compiler and/or an application, for example. In one example, the address mapping unit 3525 may be informed that address X to address X+4 KB is commonly accessed. The address mapping unit 3525 can then map the address [X: X+4 KB] to the SRAM banks 3530 closest to the EUs 3510 , resulting in improved latency and power consumption by the GPU 3500 . In some embodiments, address mapping unit 3525 can be programmed based on the commonly accessed address range hint. FIG. 36 illustrates a system 3600 implementing energy aware SRAM address mapping, according to embodiments. In one implementation, system 3600 includes SRAM banks 3610 A-D (collectively referred to as SRAM banks 3610 herein) communicably coupled to EUs 3620 . As illustrated, SRAM banks 3610 includes 4 sets of individual SRAM Banks including SRAM Bank 0 3610 A, SRAM Bank 1 3610 B, SRAM Bank 2 3610 C, and SRAM Bank 3 3610 D. SRAM banks 3610 may be the same as SRAM bank 3530 described with respect to FIG. 35 . Furthermore, EUs 3620 may be the same as EUs 3510 described with respect to FIG. 35 . Each SRAM Bank 0-3 3610 A-D may be physically configured with a set of cells (e.g., corresponding to addresses of the SRAM Banks 0-3 3610 A-D) that are considered “near” to the EUs 3620 and a set of cells that are considered “far” to the EUs 3620 . The determination of “near” and “far” for a particular cell of the SRAM Banks 0-3 3610 A-D may be based on physical distance to the EUs within the system 3600 . In one embodiment, an address mapping component, such as address mapping unit 3525 described with respect to FIG. 35 , can place data in the particular cells of the SRAM bank 0-3 3610 A-D that are closest to the EU 3620 (e.g., in the “near” cells of the SRAM banks 0-4). FIG. 37 is a flow diagram illustrating an embodiment of a method 3700 for providing energy aware SRAM address mapping in graphics processors. Method 3700 may be performed by processing logic that may comprise hardware (e.g., circuitry, dedicated logic, programmable logic, etc.), software (such as instructions run on a processing device), or a combination thereof. The process of method 3700 is illustrated in linear sequences for brevity and clarity in presentation; however, it is contemplated that any number of them can be performed in parallel, asynchronously, or in different orders. Further, for brevity, clarity, and ease of understanding, many of the components and processes described with respect to FIGS. 1-36 may not be repeated or discussed hereafter. In one implementation, an address mapping unit, such as address mapping unit 3525 of FIG. 35 , may perform method 3700 . Method 3700 begins at processing block 3710 where an address range hint is received. In one embodiment, the address range hint indicates a range of addresses that are identified as frequently-accessed addresses of a set of instructions. Frequently-accessed addresses may refer to an addresses that occurs more often than other addresses in the set of instructions. In one embodiment, a compiler or an application may generate and/or provide this address range hint to an address mapping component of a cache controller. Subsequently, at processing block 3720 , one or more SRAM banks that are physically closest to the EUs are identified. In one embodiment, the EUs may include one or more EUs that are to execute the set of instructions. Lastly, at processing block 3730 , the addresses corresponding to the address range hint are mapped to the identified one or more SRAM banks. Fabric Block Creation for Memory Paging In some embodiments, an apparatus, system, or process is to provide fabric block creation for memory paging. In embodiments herein, when the EU is making a memory request from a stream of instructions, sometimes it is aware that there are going to be multiple memory requests back-to-back to the same memory page within the stream of instructions. That can often be the nature of the access, including big block accesses that make many requests to single page in memory. Conventionally, without awareness of the upcoming memory requests to the same page, the arbitration components of the system (e.g., muxes, crossbars, etc.) switch to a different EU or sub-slice right away as part of a protocol to balance requests across all the agents (e.g., EUs). This conventional protocol mixes up the requests and loses the locality of the requests, resulting in increased latency and slower processing speed. To address the technical problems encountered by the conventional systems, embodiments provide for fabric block creation for memory paging. In some embodiments, an arbitration policy is aligned to recognize a set (or block) of on page memory streaming requests and preserve atomicity of such streaming requests. Embodiments provide a hint bit or marker that is included with the set of requests. In one implementation, an on page detector circuit can assist with providing the hint bit or marker with the request(s). This hint bit or marker can cause arbitration components (e.g., muxes, cross bars, etc.) to remain with the same EU for while processing the set of requests. Embodiments provide for the technical advantage of improving latency and processing speed of memory requests by keeping a black of requests together throughout arbitration components of the fabric. Fabric as used herein may refer to set of components and/or circuitry utilized to enable communication to and from memory components, such as the cache(s), memory controller(s), and memory including DRAM, high-bandwidth memory (HBM), and so on. For example, the fabric may include one or more multiplexors (muxes) and one or more crossbar switches to enable communication between the various components of the system, such as a graphic processor. FIG. 38 is a block diagram illustrating a computing system 3800 for fabric block creation for memory paging, in accordance with embodiments. In one embodiment, computing system 3800 may include a graphics processor, such as a GPU or GPGPU discussed herein. In some embodiments, computing system 3800 may include other types of processing devices, such as a CPU or a hardware accelerator (e.g., FPGA, etc.). As illustrated, computing system 3800 processes an instruction stream 3810 for execution by components of the computing system 3800 . The instruction stream 3810 may include one or more instructions requesting access to or from memory that is communicated with via fabric components 3840 . As noted above, fabric components 3840 may include the set of components utilized to enable communication to and from memory components, such as the cache(s), memory controller(s), and the memory itself, including DRAM, high-bandwidth memory (HBM), and so on. The memory paging from one or more of the instructions of the instruction stream 3810 may pass through a set of arbitration components 3820 , such as muxes 3825 and/or crossbar units that route access to the fabric components 3840 . In some embodiments, arbitration components 3820 can be referred to as arbitration circuitry. In one embodiment, an on-page detector circuit 3830 is provided to recognize a set (or block) of on page memory requests originating from the instruction stream 3810 and preserve atomicity of such requests. In one implementation, the on-page detector circuit 3830 provides a marker or hint bit with the request(s). This marker or hint bit can cause the arbitration components 3820 to process the set of on-page memory requests as a block of requests that is routed to the fabric components 3840 . FIG. 39 is a block diagram illustrating a fabric block creation system 3900 for memory paging, according to embodiments. In one embodiment, the fabric block creation system may be implemented in a GPU, such as a GPU or a GPGPU described herein. As shown, the fabric block creation system 3900 includes multiple sub-slices including sub-slice 0 3910 A and sub-slice 1 3910 B. More or less sub-slices 3910 A-B may be included in fabric block creation system 3900 . Each of the sub-slices 3910 A-B can include one or more EUs 3912 A, 3912 B and an L1 cache 3915 A, 3915 B. The EUs 3912 A, 3912 B may generate memory requests that pass through the L1 cache 3915 A, 3915 and, upon a cache miss, can proceed through arbitration using arbitration components 3920 . Arbitration components may include components such as multiplexers (muxes) 3922 , 3924 , 3926 and crossbars, for example. Arbitration components 3920 provide access (e.g., route requests) to and from memory, such as DRAM, a global cache 3930 (e.g., L2/L3 cache), and/or high-bandwidth memory (HBM) 3950 . In some embodiments, the L1 caches 3915 A, 3915 B include an on-page detector circuit 3917 A, 3917 B to identify a stream of memory requests that access the same memory location. In one embodiment, the on-page detector circuit 3917 A, 3917 B is the same as on-page detector circuit 3830 described with respect to FIG. 38 . The on-page detector circuit may be implemented via dedicated hardware such as fixed function circuitry, programmable circuitry, or the like. Fixed function circuitry may include dedicated logic or circuitry and may provide a set of fixed function entry points that may map to the dedicated logic for a fixed purpose or function. In one embodiment, the L1 cache 3915 A, 3915 B can break an instruction memory access request packet received from a corresponding EU 3912 A into individual memory requests for the down-stream fabric. The on-page detector circuit 3917 A-B can analyze the individual memory requests to identify if the memory requests are accessing a same page in memory. If the on-page detector circuit 3917 A-B determines that a set of requests are accessing a same page in memory, it can generate one or more markers that are included with the requests to cause the requests to be processed as a block by the downstream fabric. This is referred to herein a fabric block creation. In some embodiments, the marker may be a bit that is set with the memory request. In some embodiments, the on-page detector circuit 3917 A-B sets the marker bit in a first request in the identified set of memory requests to the same memory location and sets the marker bit in the last request of the identified set of memory requests. For example, when sending the first request in the set of identified memory requests to the same location, the on-page detector circuit 3917 A-B sets the marker bit in the first request to indicate that more requests to the same memory page are following the first request. This marker bit can cause the arbitration components 3920 to remain with the same EU 3912 A-B for multiple requests. In one embodiment, the marker bits causes the arbitration components 3920 to remain with the same EU 3912 A-B until another marker bit is encountered (e.g., with the last request in the set of requests). In some embodiments, the on-page detector circuit 3917 A-B sets the marker bit in the first and the last requests in the identified set of memory requests. In this case, the arbitration components 3920 are configured to stay with the same EU 3912 A-B upon encountering a first market bit in a request until another request is received with a set marker bit. In some embodiments, each request in the set of requests may include a set marker bit and the arbitration components 3920 are configured to stay with the same EU 3912 A-B for all memory requests that have the marker bit set. In another embodiment, the marker may include further bits that indicate a size and/or a number of requests in the set of requests. The arbitration components 3920 may be configured to stay with the same EU 3912 A-B for a duration of the size and/or number of requests indicated with the marker bit. Other embodiments of the marker bit and the information imparted by the marker bit may be implemented by embodiments herein and are not limited to those described herein. As such, the marker indicates to the arbitration components 3920 that there are more requests coming for the same memory page. If a marker is encountered, then a switch to another EU 3912 A-B or sub-slice 3910 A-B does not occur. Instead, the arbitration components 3920 should remain with the same EU 3912 A-B and keep the subsequent requests together that are being sent back-to-back to the memory (e.g., global cache 3930 , memory controller 3940 , HBM 3950 ). This fabric block creation proceeds through to the global cache 3930 , and from global cache 3930 it goes to the memory controller 3940 (e.g., of the HBM 3950 ) and attempts to keep the memory accesses together through the memory controller 3940 /HBM 3950 . As such, embodiments cause a first memory page to be opened from the first memory request and then that memory page is maintained for accesses associated with several subsequent requests. A benefit of embodiments is that if these memory requests are kept together through the arbitration to memory, an improved access pattern in the memory pages is achieved, resulting in less power consumption and improved page hit statistics. FIG. 40 is a flow diagram illustrating an embodiment of a method 4000 for fabric block creation for DRAM paging. Method 4000 may be performed by processing logic that may comprise hardware (e.g., circuitry, dedicated logic, programmable logic, etc.), software (such as instructions run on a processing device), or a combination thereof. The process of method 4000 is illustrated in linear sequences for brevity and clarity in presentation; however, it is contemplated that any number of them can be performed in parallel, asynchronously, or in different orders. Further, for brevity, clarity, and ease of understanding, many of the components and processes described with respect to FIGS. 1-39 may not be repeated or discussed hereafter. In one implementation, a fabric block creation system, such as fabric block creation system 3900 of FIG. 39 , may perform method 4000 . Method 4000 begins at processing block 4010 where a first request in an instruction stream is received. In one embodiment, the first request is for a memory access to a page. At processing block 4020 , it is determined that subsequent requests in the instruction stream access the same page as the first request. In one embodiment, an on page detector circuit of an L1 cache makes the determination that subsequent request in the instruction stream access the same page as the first request. Subsequently, at processing block 4030 , a marker is placed or set in the first request. The marker indicates that the subsequent requests access the same page as the first request. Lastly, at processing block 4040 , in response to the marker being set in the first request, arbitration components cause a same EU that processes the first request to process the subsequent requests. In one embodiment, the arbitration components are to process the subsequent requests without interleaving other requests outside of the set of requests into the set of requests. As such, if the requests to the same memory page in the instruction stream are kept together the entire way to memory, an improved access pattern in the memory, less power consumption, and improved page hit statistics can result. The following examples pertain to further embodiments. Example 1 is an apparatus to facilitate efficient data sharing for graphics data processing operations. In one embodiment, Example 1 is an apparatus to facilitate fabric block creation for memory paging. The apparatus of Example 1 includes a processing resource to generate a stream of instructions; an L1 cache communicably coupled to the processing resource and comprising an on-page detector circuit to: determine that a set of memory requests in the stream of instructions access a same memory page; and set a marker in a first request of the set of memory requests; and arbitration circuitry communicably coupled to the L1 cache, the arbitration circuitry to route the set of memory requests to memory comprising the memory page and to, in response to receiving the first request with the marker set, remain with the processing resource to process the set of memory requests. In Example 2, the subject matter of Example 1 can optionally include wherein the processing resource is an execution unit in a graphics processing unit (GPU). In Example 3, the subject matter of any one of Examples 1-2 can optionally include wherein the marker comprises a bit that is set at least one of the set of memory requests. In Example 4, the subject matter of any one of Examples 1-3 can optionally include wherein the on-page detector circuit is further to set the marker in a last request of the set of requests, and wherein the arbitration circuitry are to remain with processing resource to process the set of memory requests until the receiving the last request with the marker set. In Example 5, the subject matter of any one of Examples 1-4 can optionally include wherein the arbitration circuitry comprise at least one of multiplexors or crossbars. In Example 6, the subject matter of any one of Examples 1-5 can optionally include wherein the marker comprises data indicating a size of the set of memory requests, and wherein the arbitration circuitry are to remain with processing resource to process the set of memory requests until the size of the set of memory requests is processed. In Example 7, the subject matter of any one of Examples 1-6 can optionally include wherein the size comprises a number of requests in the set of memory requests. In Example 8, the subject matter of any one of Examples 1-7 can optionally include wherein the arbitration circuitry to remain with the processing resource to process the set of memory requests further comprises the arbitration circuitry to process the set of memory requests without interleaving other requests outside of the set of memory requests into the set of memory requests. In Example 9, the subject matter of any one of Examples 1-8 can optionally include wherein the apparatus is at least one of a single instruction multiple data (SIMD) machine or a single instruction multiple thread (SIMT) machine. Example 10 is a method for facilitating efficient data sharing for graphics data processing operations, the method comprising generating, by a processing resource of a graphics processor, a stream of instructions; determining, by an on-page detector circuit of an L1 cache of the graphics processor, that a set of memory requests in the stream of instructions access a same memory page; setting, by the on-page detector circuit, a marker in a first request of the set of memory requests; and in response to receiving the first request with the marker set, remaining, by arbitration circuitry of the graphics processor, with the processing resource to process the set of memory requests, wherein the arbitration circuitry to route the set of memory requests to memory comprising the memory page. In Example 11, the subject matter of Example 10 can optionally include wherein the marker comprises a bit that is set at least one of the set of memory requests. In Example 12, the subject matter of any one of Examples 10-11 can optionally include further comprising setting, by the on-page detector circuit, the marker in a last request of the set of memory requests, wherein the arbitration circuitry are to remain with processing resource to process the set of memory requests until the receiving the last request with the marker set. In Example 13, the subject matter of any one of Examples 10-12 can optionally include wherein the arbitration circuitry comprise at least one of multiplexors or crossbars. In Example 14, the subject matter of any one of Examples 10-13 can optionally include wherein the marker comprises data indicating a size of the set of memory requests, and wherein the arbitration circuitry are to remain with processing resource to process the set of memory requests until the size of the set of memory requests is processed. In Example 15, the subject matter of any one of Examples 10-14 can optionally include wherein remaining with the processing resource to process the set of memory requests further comprises processing, by the arbitration circuitry, the set of memory requests without interleaving other requests outside of the set of memory requests into the set of memory requests. Example 16 is a non-transitory computer-readable medium for facilitating efficient data sharing for graphics data processing operations. In Example 16, the non-transitory computer-readable medium can have instructions stored thereon, which when executed by one or more processors, cause the processors to: generate, by a processing resource of the one or more processors, a stream of instructions; determine, by an on-page detector circuit of an L1 cache of the one or more processors, that a set of memory requests in the stream of instructions access a same memory page; set, by the on-page detector circuit, a marker in a first request of the set of memory requests; and in response to receiving the first request with the marker set, remain, by arbitration circuitry of the one or more processors, with the processing resource to process the set of memory requests, wherein the arbitration circuitry to route the set of memory requests to memory comprising the memory page. In Example 17, the subject matter of Example 16 can optionally include wherein the marker comprises a bit that is set at least one of the set of memory requests. In Example 18, the subject matter of any one of Examples 16-17 can optionally include wherein the instructions are further to cause the one or more processors to set, by the on-page detector circuit, the marker in a last request of the set of memory requests, wherein the arbitration circuitry are to remain with processing resource to process the set of memory requests until the receiving the last request with the marker set. In Example 19, the subject matter of any one of Examples 16-18 can optionally include wherein the marker comprises data indicating a size of the set of memory requests, and wherein the arbitration circuitry are to remain with processing resource to process the set of memory requests until the size of the set of memory requests is processed. In Example 20, the subject matter of any one of Examples 16-19 can optionally include wherein remaining with the processing resource to process the set of memory requests further comprises processing, by the arbitration circuitry, the set of memory requests without interleaving other requests outside of the set of memory requests into the set of memory requests. Example 21 is a system for facilitating efficient data sharing for graphics data processing operations. In Example 21, the system includes a memory and one or more processors of a plurality of GPUs. The one or more processors of Example 21 are communicably coupled to the memory and are to generate a stream of instructions. The system of Example 21 includes an L1 cache communicably coupled to the one or more processors and comprising an on-page detector circuit to: determine that a set of memory requests in the stream of instructions access a same memory page; and set a marker in a first request of the set of memory requests. The system of Example 21 includes arbitration circuitry communicably coupled to the L1 cache, the arbitration circuitry to route the set of memory requests to memory comprising the memory page and to, in response to receiving the first request with the marker set, remain with the processing resource to process the set of memory requests. In Example 22, the subject matter of Example 21 can optionally include wherein the processing resource is an execution unit in a graphics processing unit (GPU). In Example 23, the subject matter of any one of Examples 21-22 can optionally include wherein the marker comprises a bit that is set at least one of the set of memory requests. In Example 24, the subject matter of any one of Examples 21-23 can optionally include wherein the on-page detector circuit is further to set the marker in a last request of the set of memory requests, and wherein the arbitration circuitry are to remain with processing resource to process the set of memory requests until the receiving the last request with the marker set. In Example 25, the subject matter of any one of Examples 21-24 can optionally include wherein the arbitration circuitry comprise at least one of multiplexors or crossbars. In Example 26, the subject matter of any one of Examples 21-25 can optionally include wherein the marker comprises data indicating a size of the set of memory requests, and wherein the arbitration circuitry are to remain with processing resource to process the set of memory requests until the size of the set of memory requests is processed. In Example 27, the subject matter of any one of Examples 21-26 can optionally include wherein the size comprises a number of requests in the set of memory requests. In Example 28, the subject matter of any one of Examples 21-27 can optionally include wherein the arbitration circuitry to remain with the processing resource to process the set of memory requests further comprises the arbitration circuitry to process the set of memory requests without interleaving other requests outside of the set of memory requests into the set of memory requests. In Example 29, the subject matter of any one of Examples 21-28 can optionally include wherein the apparatus is at least one of a single instruction multiple data (SIMD) machine or a single instruction multiple thread (SIMT) machine. Example 30 is an apparatus for facilitating efficient data sharing for graphics data processing operations comprising means for generating, by a processing resource of a graphics processor, a stream of instructions; means for determining, by an on-page detector circuit of an L1 cache of the graphics processor, that a set of memory requests in the stream of instructions access a same memory page; means for setting, by the on-page circuit detector, a marker in a first request of the set of memory requests; and in response to receiving the first request with the marker set, means for remaining, by arbitration circuitry of the graphics processor, with the processing resource to process the set of memory requests, wherein the arbitration circuitry to route the set of memory requests to memory comprising the memory page. In Example 31, the subject matter of Example 30 can optionally include the apparatus further configured to perform the method of any one of the Examples 11 to 15. Example 32 is at least one machine readable medium comprising a plurality of instructions that in response to being executed on a computing device, cause the computing device to carry out a method according to any one of Examples 10-15. Example 33 is an apparatus for facilitating efficient data sharing for graphics data processing operations, configured to perform the method of any one of Examples 10-15. Example 34 is an apparatus for facilitating efficient data sharing for graphics data processing operations comprising means for performing the method of any one of claims 10 to 15 . Specifics in the Examples may be used anywhere in one or more embodiments. Example 35 is an apparatus to facilitate efficient data sharing for graphics data processing operations, and in particular to facilitate utilization of scalar registers for GPU threads. The apparatus of Example 35 includes a processor of a plurality of graphics processing units (GPUs), the processor to allocate a bank of global scalar general purpose registers (GPRs) in the GPU; determine that a same constant is loaded by more than one thread in the GPU; load the constant in a scalar GPR of the bank of global scalar GPRs; and access, by one or more threads in the GPU, the scalar GPR storing the constant using one or more scalar arithmetic logic units (ALUs) of the GPU. Example 36 is an apparatus to facilitate efficient data sharing for graphics data processing operations, and in particular to facilitate cross-slice direct data communications using L1 to L1 communications. The apparatus of Example 36 includes a processor of a plurality of graphics processing units (GPUs), the processor to allocate a first thread group as a member of a super thread group (SGT) comprising a collection of thread groups running on a same chiplet; receive from a second thread group within the SGT, a request to communicate with the first thread group identified by a thread group identifier (ID) within the SGT; access a routing table to determine a location of the first thread group based on the thread group ID; and route the request to the determined location of the first thread group using communication links between L1 caches of the chiplet. Example 37 is an apparatus to facilitate efficient data sharing for graphics data processing operations, and in particular to facilitate energy aware SRAM address mapping. The apparatus of Example 37 includes a processor of a plurality of graphics processing units (GPUs), the processor to receive an address range hint that indicates a range of addresses that are identified as frequently-accessed addresses of a set of instructions; identify one or more SRAM banks closest to one or more EUs executing the set of instructions; and map the range of addresses corresponding to the address range hint to the identified one or more SRAM banks. The foregoing description and drawings are to be regarded in an illustrative rather than a restrictive sense. Persons skilled in the art can understand that various modifications and changes may be made to the embodiments described herein without departing from the broader spirit and scope of the features set forth in the appended claims.",en,PATENT_APPLICATION
013-811-945-215-774,US,20240384346,A1,2024-11-21,US_20240384346_A1_20241121,en,US,20240384346,A1,2024-11-21,US,18556044,2022-04-22,"ORAL SWAB-BASED TEST FOR THE DETECTION OF DENTAL DISEASE STATES IN DOMESTIC CATS, DOGS AND OTHER MAMMALS",en,US,Basepaws,"Torrance, CA",US,YULIANA MIHAYLOVA,"PLAYA VISTA, CA",US,1,DAMIAN KAO,"PLAYA VISTA, CA",C12Q1/6883,I,F,C12Q1/689,I,L,G16H50/30,I,L,G16H50/70,I,L,C12Q1/6883,I,F,C12Q1/689,I,L,G16H50/30,I,L,G16H50/70,I,L,C12Q2600/118,A,L,C12Q2600/158,A,L,US,20240384346,A1,2024-11-21,013-811-945-215-774,1,US,20240384346,A1,2024-11-21,013-811-945-215-774,1,UNKNOWN,"Systems and methods for screening for and identifying oral disease states in domestic cats, dogs and other mammals.",en,"1 . A method for screening for, detecting, and/or preventing oral disease in non-human, mammalian animals, the method comprising: obtaining an oral microbial profile for a non-human, mammalian animal, the oral microbial profile comprising one or more microbial species present in an oral sample of the non-human, mammalian animal and a quantity or abundance of the one or more microbial species in the oral sample; comparing the oral microbial profile to information in a database that identifies weighted correlations between: (i) occurrence and/or prevalence of one or more oral diseases in animals in a classification of the non-human, mammalian animal; and (ii) presence and/or abundance of various microbial species in the oral microbiome of animals in the classification of the non-human, mammalian animal, wherein the various microbial species comprise the one or more microbial species in the oral sample; generating a risk score indicating a likelihood that the non-human, mammalian animal has the one or more oral diseases based on one or more matches between the oral microbial profile and the information in the database; and categorizing the non-human, mammalian animal as developing the one or more oral diseases when the risk score meets or exceeds a predetermined threshold and, optionally, prescribing a therapeutic treatment protocol suitable for treating, mitigating, or preventing the development, advancement, or recurrence of the one or more oral diseases when the risk score meets or exceeds a predetermined threshold.","2 . The method of claim 1 further comprising administering the therapeutic treatment protocol to the non-human, mammalian animal or confirming that the therapeutic treatment protocol has been administered to the non-human, mammalian animal, wherein the therapeutic treatment protocol is sufficient to alter the oral microbial profile of the non-human, mammalian animal.","3 . The method of claim 1 , wherein obtaining the oral microbial profile for the non-human, mammalian animal comprises: obtaining nucleic acid sequence data corresponding to microbial nucleic acid obtained from the oral sample; analyzing the nucleic acid sequence data to identify the one or more microbial species present in the oral sample and quantifying the one or more microbial species; and generating the oral microbial profile for the non-human, mammalian animal based on the identified and, optionally, quantified one or more microbial species.","4 . The method of claim 3 , wherein obtaining the microbial nucleic acid sequence data comprises: sequencing microbial nucleic acid from the oral sample; and, optionally, isolating the microbial nucleic acid from the oral sample.","5 . The method of claim 4 , wherein isolating the microbial nucleic acid from the oral sample comprises: performing heat treatment on the oral sample; and performing magnetic SPRI beads-based nucleic acid extraction on the heat-treated oral sample, with or without the addition of protein digesting reagents and detergents, to extract the microbial nucleic acid from the oral sample.","6 . The method of claim 3 , wherein analyzing the microbial nucleic acid sequence data comprises one or more of: demultiplexing the nucleic acid sequence data; trimming the nucleic acid sequence data; mapping one or more unmapped reads onto a reference genome of the non-human, mammalian animal and/or onto existing microbial reference genomes; classifying one or more reads as mammalian from the nucleic acid sequence data after mapping; classifying one or more reads as microbial from the nucleic acid sequence data after mapping; quantifying the one or more microbial reads; transforming the quantified one or more microbial reads to account for sequence coverage biases using methods such as pairwise log ratio transformation; and comparing compositional abundance patterns in the transformed one or more microbial reads against compositional abundance patterns in the transformed data in a reference database comprising samples from non-human, mammalian animals that do not suffer from dental diseases, as well as samples from non-human, mammalian animals that suffer from specific dental diseases.","7 . The method of claim 1 , wherein comparing the oral microbial profile to the information in the database comprises one or more of: calculating the abundance of the one or more microbial species in the oral sample; identifying the one or more microbial species in the oral sample; and comparing the abundance of the identified one or more microbial species in the oral sample to the presence and/or abundance of various microbial species in the oral microbiome of animals in the classification of the non-human, mammalian animal contained in the database.","8 . The method of claim 1 , wherein generating the risk score comprises one or more of: identifying one or more similarities between compositional abundance of the one or more microbial species in the oral sample and compositional abundance of various microbial species in the oral microbiome of animals in the classification of the non-human, mammalian animal contained in the database; identifying one or more matches between the identity of the one or more microbial species in the oral sample and the presence of various microbial species in the oral microbiome of animals in the classification of the non-human, mammalian animal contained in the database; quantifying the identified one or more similarities between the compositional abundance of the one or more microbial species in the oral sample and the compositional abundance of the one or more microbial species in the oral microbiome of animals in the classification of the non-human, mammalian animal contained in the database; and identifying a presence of one or more predictive microbial species in the oral sample.","9 . The method of claim 1 , wherein the one or more oral diseases is selected from the group consisting of periodontal disease, tooth resorption, gingivostomatitis, and halitosis.","10 . The method of claim 1 further comprising: generating a report presenting (i) the risk score, (ii) an indication of developing the one or more oral diseases when the risk score meets or exceeds the predetermined threshold, (iii) a timing recommendation, (iv) optionally, one or more at home practices to improve dental health, (v) optionally, one or more diagnostic steps to diagnose the one or more oral diseases when the risk score meets or exceeds the predetermined threshold, and (vi) optionally, a prescription for the therapeutic treatment protocol; and, optionally, communicating the generated report electronically to an owner of the non-human, mammalian animal and/or their veterinarian.","11 . The method of claim 1 , wherein the therapeutic treatment protocol is sufficient to alter the oral microbial profile of the non-human, mammalian animal.","12 . A computer system configured to indicate or predict oral disease in mammalian animals, the computer system comprising: one or more processors; and one or more computer-readable hardware storage devices having stored thereon instructions that are executable by the one or more processors to configure the computer system to: receive microbial nucleic acid sequence data corresponding to microbial nucleic acid obtained from an oral sample taken from a mammalian animal; analyze the microbial nucleic acid sequence data to identify one or more microbial species present in the oral sample and quantify the one or more microbial species; generate an oral microbial profile for the mammalian animal based on the identified one or more microbial species and their respective abundances; compare the oral microbial profile to information in a database that identifies weighted correlations between: (i) occurrence and/or prevalence of one or more oral diseases in animals in a classification of the mammalian animal; and (ii) presence and/or abundance of various microbial species in the oral microbiome of animals in the classification of the mammalian animal, wherein the various microbial species comprise the one or more microbial species in the oral sample; identify one or more matches between the oral microbial profile and the information in the database; generate a risk score indicating a likelihood that the mammalian animal has the one or more oral diseases based on the one or more matches between the oral microbial profile and the information in the database; and, optionally, diagnose the mammalian animal as “developing” the one or more oral diseases when the risk score meets or exceeds a predetermined threshold, prescribe a therapeutic treatment protocol suitable for treating or preventing the one or more oral diseases when the risk score meets or exceeds the predetermined threshold, generate a report indicating (i) the risk score, (ii) an indication of developing the one or more oral diseases when the risk score meets or exceeds the predetermined threshold, (iii) a timing recommendation, (iv) optionally, one or more at home practices to improve dental health, (v) optionally, one or more diagnostic steps to diagnose the one or more oral diseases when the risk score meets or exceeds the predetermined threshold, and (vi) a prescription for the therapeutic treatment protocol, and/or communicate the generated report electronically to an owner of the mammalian animal and/or their veterinarian.","13 . The computer system of claim 12 , wherein the instructions further configure the computer system to map one or more unmapped reads to a mammalian reference genome and/or map one or more reads to microbial reference genomes and, optionally, classify the reads as microbial or mammalian.","14 . The computer system of claim 13 , wherein the instructions further configure the computer system to identify at least one unmapped sequence read of the metagenomic sequence data and, optionally, classify the at least one unmapped read.","15 . The computer system of claim 13 , wherein mammalian oral microbiome samples having fewer than 10,000 classified microbial reads or more than 500,000 classified microbial reads are excluded from the comparison of the oral microbial profile for the mammalian animal against a database of defined microbial profiles.","16 . The computer system of claim 12 , wherein the instructions further configure the computer system to calculate an abundance of the one or more microbial species present in the oral sample.","17 . The computer system of claim 16 , wherein the abundance of the specific one or more microbial species present in the oral sample correlates to whether the specific one or more microbial species is a predictive microbial species for the specific oral disease.","18 . The computer system of claim 16 , wherein the instructions further configure the computer system to perform a pairwise log ratio comparison of the microbial abundance of the mammalian animal's oral sample against the information in the database.","19 . The system of claim 18 , wherein the specific one or more microbial species is a predictive microbial species when 50% or more of the maximum possible pairwise log ratio comparisons involving this microbe are significantly different when compared between a disease and a control cohort.","20 . A method for predicting the development of an oral disease in a mammalian animal, the method comprising: obtaining an oral sample from a mammalian animal, the oral sample containing one or more microbial species; isolating, from the oral sample, microbial nucleic acid of the one or more microbial species; obtaining microbial nucleic acid sequence data corresponding to the microbial nucleic acid; analyzing the microbial nucleic acid sequence data to identify one or more microbial species present in the oral sample and, optionally, quantifying the one or more microbial species; generating an oral microbial profile for the mammalian animal based on the identified and, optionally, quantified one or more microbial species, the oral microbial profile comprising the one or more microbial species and, optionally, a quantity or relative abundance of the one or more microbial species in the oral sample; comparing the oral microbial profile to information in a database that identifies weighted correlations between: (i) occurrence and/or prevalence of one or more oral diseases in animals in a classification of the mammalian animal; and (ii) presence and/or abundance of various microbial species in the oral microbiome of animals in the classification of the mammalian animal, wherein the various microbial species comprise the one or more microbial species in the oral sample; generating a risk score indicating a likelihood of the mammalian animal developing the one or more oral diseases based on one or more matches between the oral microbial profile and the information in the database; and indicating the mammalian animal as developing the one or more oral diseases when the risk score meets or exceeds a predetermined threshold.",en,"CROSS-REFERENCE TO RELATED APPLICATIONS This application claims the benefit of and priority to: (1) U.S. Provisional Application No. 63/178,395, filed Apr. 22, 2021, titled “Development of an Oral Swab Based Microbiome Test for the Detection of Feline Dental Disease,” and (2) U.S. Provisional Application No. 63/221,554, filed Jul. 14, 2021, titled “Oral Swab-Based Test for the Detection of Dental Disease States in Domestic Cats, Dogs, and Other Mammals,” the entirety of each of which is incorporated herein by specific reference. BACKGROUND Technical Field This disclosure relates to systems and methods for screening for, detecting and identifying oral disease states in domestic cats, dogs and other mammals. Related Technology Dental health in cats and dogs, and mammals, in general, is known to be linked to the overall health and wellbeing of the individual animal. That is, dental health may be a good proxy for overall health and wellbeing in cats, dogs and mammals more generally. Dental conditions may be indicative of wider, more serious systemic conditions and may impact an individual animal's level of comfort while living with a particular dental condition. For example, animals suffering from dental disease conditions may experience pain, loss of sleep, loss of appetite, decreased activity, and depression, among other things. One of the most prevalent forms of mammalian dental disease, periodontal disease, can generally be broken down into four stages, where the gingiva (gums) becomes inflamed in stage 1. In stages 2-4, varying degrees of tooth support are lost until, in stage 4, over 50% of the tooth support is lost. This can result in loss of teeth for the mammal and pain when using the teeth (such as during eating). Many mammalian animals, such as cats and dogs, are incapable of communicating this pain and discomfort to their owners. Moreover, by the time dental disease-associated pain begins to manifest, it is too late for prevention focused regimens to significantly improve oral health and some treatment options may be unavailable or ineffective, resulting in increased owner spending for emergency veterinary services. Many mammalian animals, such as cats and dogs, do not receive routine veterinary care, meaning that early signs of dental disease can often be missed. To compound this problem, extensive assessment of oral health is rarely a part of any routine veterinary visit. The typical veterinary oral health examination relies on a visual inspection of the mouth during an awake exam. Ensuring that early signs of developing dental disease are not missed during an examination requires the animal to be put under anesthesia and undergo X-ray imaging of the oral cavity, since signs of dental disease are often invisible to the naked eye. Due to the high cost of this procedure and the risks associated with cats and dogs undergoing anesthesia, this is not standard practice in most veterinary hospitals and clinics. Accordingly, there is a need for robust and accurate, yet safe, painless and affordable means that can be used on a recurring basis for detecting dental disease in mammalian animals, such as cats and dogs. Using such a tool to guide and complement veterinary oral health assessment can significantly improve oral health outcomes and lead to detecting signs of deteriorating oral health and implementing treatment earlier compared to relying on veterinary visits alone. SUMMARY Embodiments of the present disclosure include systems and methods for screening for, detecting and identifying oral disease states in cats, dogs and/or other mammalian companion animals. Embodiments of the disclosed subject matter describe a method for interrogating the oral microbiome of a mammalian companion animal. The disclosed methods interrogate the oral microbiome to detect microbe compositional abundance trends that may be associated with dental disease in cats, dogs and other mammalian animals. Detecting, identifying and/or quantifying microbial compositional abundance trends enables a practitioner to screen for and/or indicate whether a cat, dog and/or other mammal has a particular oral and/or dental disease state. Detecting and identifying oral and/or dental disease states enables the practitioner and/or the mammalian animal's owner to treat and/or prevent the dental disease state. Treating and/or preventing oral disease states enables treatment and/or prevention of wider, systemic conditions, beneficially resulting in a healthier and more comfortable life for the mammalian animal. In some embodiments, a method is disclosed for detecting and/or indicating oral disease in mammalian animals. The method may include receiving an oral swab sample taken from a mammalian animal; manipulating the sample, such as heat treatment of the oral sample; and extracting microbial deoxyribonucleic acids (DNA) from the heat-treated sample. The method may additionally include sequencing the microbial DNA to identify which specific one or more microbes are present in the oral sample (and in what relative proportions), wherein identifying the specific one or more microbes enables generation of an oral microbial profile for the mammalian animal. The method may additionally include comparing the oral microbial profile for the mammalian animal against a reference database including defined microbial profiles, wherein the database identifies correlations between (i) profiles that include one or more microbes and (ii) corresponding oral diseases; and based on a result of comparing the oral microbial profile against the database of defined microbial profiles, generating a risk score indicating a likelihood that the mammalian animal has a specific oral disease. The method may further include treating the specific oral disease and/or administering a therapeutic treatment. In some embodiments, the therapeutic treatment may include administering a therapeutic compound, such as a compound designed to inhibit or encourage growth of a specific one or more microbes present in the oral microbiome of the mammal. In some embodiments, the therapeutic compound includes a pre-biotic, a post-biotic, a pro-biotic, a medicament or a combination thereof. In some embodiments, the therapeutic treatment may include brushing the mammal's teeth with a topical treatment. In some embodiments, a method for indicating oral disease in mammalian animals includes receiving an oral swab sample taken from a mammalian animal and performing heat treatment on the oral sample. The method may also include performing magnetic beads-based deoxyribonucleic acid (DNA) extraction on the heat treated oral sample to extract microbial DNA that is present in the oral swab sample and sequencing the microbial DNA to identify which specific one or more microbes are present in the oral sample (and in what compositional abundance), wherein identifying the specific one or more microbe(s) enables generation of an oral microbial profile for the mammalian animal. The method may additionally include comparing the oral microbial profile for the mammalian animal against a database of defined microbial profiles, wherein the database identifies correlations between (i) profiles that include one or more microbes (and their compositional abundance) and (ii) corresponding oral diseases; and based on a result of comparing the oral microbial profile against the database of defined microbial profiles, generating a risk score indicating a likelihood that the mammalian animal has a specific oral disease. The method may include, in response to generating the risk score and identifying the specific oral disease, administering a therapeutic treatment designed to treat the specific oral disease, recommending veterinary attention or follow-up examination, and/or recommending at-home care for specific oral diseases. Also disclosed are computer systems. In some embodiments, a computer system is configured to indicate oral disease in mammalian animals and includes one or more processors and one or more computer-readable hardware storage devices that store instructions executable by the one or more processors. The instructions may configure the computer system to receive sequenced microbial DNA data from an oral swab sample taken from a mammalian animal; map the sequenced microbial DNA to identify which specific one or more microbial species are present in the oral sample, wherein identifying the specific one or more microbial species results in generation of an oral microbial profile for the mammalian animal; calculate relative abundance of different microbial species to further build the oral microbial profile for the mammalian animal; compare the oral microbial profile for the mammalian animal against a database of defined microbial profiles, wherein the database identifies correlations between (i) profiles that include one or more microbial species and their relative abundance and (ii) corresponding oral diseases; and based on a result of comparing the oral microbial profile against the database of defined microbial profiles, generate a risk score indicating a likelihood that the mammalian animal has a specific oral disease. In response to generating the risk score, the instructions may further configure the computer system to generate a report outlining and/or presenting the risk score and prescribing a therapeutic treatment and/or at-home treatment protocol suitable for addressing (e.g., treating and/or preventing) the specific oral disease. The therapeutic treatment protocol may be influenced by the severity of the oral disease state, which is indicated by or correlated to the risk score. In some embodiments, the therapeutic treatment or at-home care protocol is designed to alter the composition of the oral microbiome of the mammalian animal. In some embodiments, altering the composition of the mammalian animal's oral microbiome treats and/or addresses the specific oral disease. In some embodiments, the therapeutic treatment repairs the mammalian animal's oral microbiome. In some embodiments, the therapeutic treatment or at-home care protocol is designed to maintain the composition of the oral microbiome of the mammalian animal. In some embodiments, the therapeutic treatment protocol is designed to stimulate a metabolic output of the mammalian animal's oral microbiome. Stimulating a metabolic output of the mammalian animal's oral microbiome may include using known enzymatic pathway analysis tools to provide an additional dimension to the existing microbial composition data to further characterize disease signatures and improve predictive disease models. Illustrative embodiments and non-limiting examples of the present disclosure include: Example 1. A method for screening for, detecting, and/or preventing oral disease in non-human, mammalian animals, the method comprising: obtaining an oral microbial profile for a non-human, mammalian animal, the oral microbial profile comprising one or more microbial species present in an oral sample of the non-human, mammalian animal and a quantity or abundance of the one or more microbial species in the oral sample;comparing the oral microbial profile to information in a database that identifies weighted correlations between:(i) occurrence and/or prevalence of one or more oral diseases in animals in a classification of the non-human, mammalian animal; and(ii) presence and/or abundance of various microbial species in the oral microbiome of animals in the classification of the non-human, mammalian animal, wherein the various microbial species comprise the one or more microbial species in the oral sample;generating a risk score indicating a likelihood that the non-human, mammalian animal has the one or more oral diseases based on one or more matches between the oral microbial profile and the information in the database; andcategorizing the non-human, mammalian animal as developing the one or more oral diseases when the risk score meets or exceeds a predetermined threshold and, optionally, prescribing a therapeutic treatment protocol suitable for treating, mitigating, or preventing the development, advancement, or recurrence of the one or more oral diseases when the risk score meets or exceeds a predetermined threshold.Example 2. The method of Example 1 further comprising administering the therapeutic treatment protocol to the non-human, mammalian animal or confirming that the therapeutic treatment protocol has been administered to the non-human, mammalian animal, wherein the therapeutic treatment protocol is sufficient to alter the oral microbial profile of the non-human, mammalian animal.Example 3. The method of Example 1, wherein obtaining the oral microbial profile for the non-human, mammalian animal comprises: obtaining nucleic acid sequence data corresponding to microbial nucleic acid obtained from the oral sample;analyzing the nucleic acid sequence data to identify the one or more microbial species present in the oral sample and quantifying the one or more microbial species; andgenerating the oral microbial profile for the non-human, mammalian animal based on the identified and, optionally, quantified one or more microbial species.Example 4. The method of Example 3, wherein obtaining the microbial nucleic acid sequence data comprises: sequencing microbial nucleic acid from the oral sample; and, optionally,isolating the microbial nucleic acid from the oral sample.Example 5. The method of Example 4, wherein isolating the microbial nucleic acid from the oral sample comprises: performing heat treatment on the oral sample; andperforming magnetic SPRI beads-based nucleic acid extraction on the heat-treated oral sample, with or without the addition of protein digesting reagents and detergents, to extract the microbial nucleic acid from the oral sample.Example 6. The method of Example 3, wherein analyzing the microbial nucleic acid sequence data comprises one or more of: demultiplexing the nucleic acid sequence data;trimming the nucleic acid sequence data;mapping one or more unmapped reads onto a reference genome of the non-human, mammalian animal and/or onto existing microbial reference genomes;classifying one or more reads as mammalian from the nucleic acid sequence data after mapping;classifying one or more reads as microbial from the nucleic acid sequence data after mapping;quantifying the one or more microbial reads;transforming the quantified one or more microbial reads to account for sequence coverage biases using methods such as pairwise log ratio transformation; andcomparing compositional abundance patterns in the transformed one or more microbial reads against compositional abundance patterns in the transformed data in a reference database comprising samples from non-human, mammalian animals that do not suffer from dental diseases, as well as samples from non-human, mammalian animals that suffer from specific dental diseases.Example 7. The method of Example 1, wherein comparing the oral microbial profile to the information in the database comprises one or more of: calculating the abundance of the one or more microbial species in the oral sample;identifying the one or more microbial species in the oral sample; andcomparing the abundance of the identified one or more microbial species in the oral sample to the presence and/or abundance of various microbial species in the oral microbiome of animals in the classification of the non-human, mammalian animal contained in the database.Example 8. The method of Example 1, wherein generating the risk score comprises one or more of: identifying one or more similarities between compositional abundance of the one or more microbial species in the oral sample and compositional abundance of various microbial species in the oral microbiome of animals in the classification of the non-human, mammalian animal contained in the database;identifying one or more matches between the identity of the one or more microbial species in the oral sample and the presence of various microbial species in the oral microbiome of animals in the classification of the non-human, mammalian animal contained in the database;quantifying the identified one or more similarities between the compositional abundance of the one or more microbial species in the oral sample and the compositional abundance of the one or more microbial species in the oral microbiome of animals in the classification of the non-human, mammalian animal contained in the database; andidentifying a presence of one or more predictive microbial species in the oral sample.Example 9. The method of Example 1, wherein the one or more oral diseases is selected from the group consisting of periodontal disease, tooth resorption, gingivostomatitis, and halitosis.Example 10. The method of Example 1 further comprising: generating a report presenting (i) the risk score, (ii) an indication of developing the one or more oral diseases when the risk score meets or exceeds the predetermined threshold, (iii) a timing recommendation, (iv) optionally, one or more at home practices to improve dental health, (v) optionally, one or more diagnostic steps to diagnose the one or more oral diseases when the risk score meets or exceeds the predetermined threshold, and (vi) optionally, a prescription for the therapeutic treatment protocol; and, optionally,communicating the generated report electronically to an owner of the non-human, mammalian animal and/or their veterinarian.Example 11. The method of Example 1, wherein the therapeutic treatment protocol is sufficient to alter the oral microbial profile of the non-human, mammalian animal.Example 12. A computer system configured to indicate or predict oral disease in mammalian animals, the computer system comprising: one or more processors; andone or more computer-readable hardware storage devices having stored thereon instructions that are executable by the one or more processors to configure the computer system to:receive microbial nucleic acid sequence data corresponding to microbial nucleic acid obtained from an oral sample taken from a mammalian animal;analyze the microbial nucleic acid sequence data to identify one or more microbial species present in the oral sample and quantify the one or more microbial species;generate an oral microbial profile for the mammalian animal based on the identified one or more microbial species and their respective abundances;compare the oral microbial profile to information in a database that identifies weighted correlations between:(i) occurrence and/or prevalence of one or more oral diseases in animals in a classification of the mammalian animal; and(ii) presence and/or abundance of various microbial species in the oral microbiome of animals in the classification of the mammalian animal, wherein the various microbial species comprise the one or more microbial species in the oral sample;identify one or more matches between the oral microbial profile and the information in the database;generate a risk score indicating a likelihood that the mammalian animal has the one or more oral diseases based on the one or more matches between the oral microbial profile and the information in the database; and, optionally,diagnose the mammalian animal as “developing” the one or more oral diseases when the risk score meets or exceeds a predetermined threshold,prescribe a therapeutic treatment protocol suitable for treating or preventing the one or more oral diseases when the risk score meets or exceeds the predetermined threshold,generate a report indicating (i) the risk score, (ii) an indication of developing the one or more oral diseases when the risk score meets or exceeds the predetermined threshold, (iii) a timing recommendation, (iv) optionally, one or more at home practices to improve dental health, (v) optionally, one or more diagnostic steps to diagnose the one or more oral diseases when the risk score meets or exceeds the predetermined threshold, and (vi) a prescription for the therapeutic treatment protocol, and/orcommunicate the generated report electronically to an owner of the mammalian animal and/or their veterinarian.Example 13. The computer system of Example 12, wherein the instructions further configure the computer system to map one or more unmapped reads to a mammalian reference genome and/or map one or more reads to microbial reference genomes and, optionally, classify the reads as microbial or mammalian.Example 14. The computer system of Example 13, wherein the instructions further configure the computer system to identify at least one unmapped sequence read of the metagenomic sequence data and, optionally, classify the at least one unmapped read.Example 15. The computer system of Example 13, wherein mammalian oral microbiome samples having fewer than 10,000 classified microbial reads or more than 500,000 classified microbial reads are excluded from the comparison of the oral microbial profile for the mammalian animal against a database of defined microbial profiles.Example 16. The computer system of Example 12, wherein the instructions further configure the computer system to calculate an abundance of the one or more microbial species present in the oral sample.Example 17. The computer system of Example 16, wherein the abundance of the specific one or more microbial species present in the oral sample correlates to whether the specific one or more microbial species is a predictive microbial species for the specific oral disease.Example 18. The computer system of Example 16, wherein the instructions further configure the computer system to perform a pairwise log ratio comparison of the microbial abundance of the mammalian animal's oral sample against the information in the database.Example 19. The system of Example 18, wherein the specific one or more microbial species is a predictive microbial species when 50% or more of the maximum possible pairwise log ratio comparisons involving this microbe are significantly different when compared between a disease and a control cohort.Example 20. A method for predicting the development of an oral disease in a mammalian animal, the method comprising: obtaining an oral sample from a mammalian animal, the oral sample containing one or more microbial species;isolating, from the oral sample, microbial nucleic acid of the one or more microbial species;obtaining microbial nucleic acid sequence data corresponding to the microbial nucleic acid;analyzing the microbial nucleic acid sequence data to identify one or more microbial species present in the oral sample and, optionally, quantifying the one or more microbial species;generating an oral microbial profile for the mammalian animal based on the identified and, optionally, quantified one or more microbial species, the oral microbial profile comprising the one or more microbial species and, optionally, a quantity or relative abundance of the one or more microbial species in the oral sample;comparing the oral microbial profile to information in a database that identifies weighted correlations between:(i) occurrence and/or prevalence of one or more oral diseases in animals in a classification of the mammalian animal; and(ii) presence and/or abundance of various microbial species in the oral microbiome of animals in the classification of the mammalian animal, wherein the various microbial species comprise the one or more microbial species in the oral sample;generating a risk score indicating a likelihood of the mammalian animal developing the one or more oral diseases based on one or more matches between the oral microbial profile and the information in the database; andindicating the mammalian animal as developing the one or more oral diseases when the risk score meets or exceeds a predetermined threshold. This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the detailed description. This summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used as an indication of the scope of the claimed subject matter. BRIEF DESCRIPTION OF THE DRAWINGS Various objects, features, characteristics, and advantages of the invention will become apparent and more readily appreciated from the following description of the embodiments, taken in conjunction with the accompanying drawings and the appended claims, all of which form a part of this specification. In the Figures, like reference numerals may be utilized to designate corresponding or similar parts in the various Figures, and the various elements depicted are not necessarily drawn to scale, wherein: FIG. 1A-1B illustrates a dental health test workflow and oral microbiome reference database construction. FIG. 2A-2C illustrates a distribution of the average log ratio difference scores between pairwise microbial interactions associated with (A) periodontal disease (PD) and healthy cohorts, (B) tooth resorption (TR) and healthy cohorts, and (C) bad breath (BB) and typical breath (TB) cohorts. FIGS. 3A-3D illustrate sensitivity and specificity of the feline dental health test based on a 2-component Gaussian mixture model. FIG. 4 illustrates overlap of oral microbiome predictive microbes characteristic of feline periodontal disease, tooth resorption and halitosis. FIGS. 5A-5B illustrate sampling location effect and reproducibility of the feline dental health test results. FIG. 6 illustrates microbial species richness as a function of number of sequencing reads, comparing data from two different types of metagenomic whole genome sequencing (WGS) library preparations-a ligation-based approach versus a tagmentation-based approach (such as the Illumina Nextera DNA Flex Library Preparation Kit). FIG. 7 illustrates an oral microbiome-based periodontal disease risk assessment in clinically recruited cohorts of cats suffering from gingivitis (no alveolar bone loss), periodontal disease and alveolar bone loss, and citizen science recruited healthy controls. FIG. 8 illustrates an oral microbiome-based bad breath (halitosis) risk assessment in clinically recruited cohorts of cats suffering from gingivitis (no alveolar bone loss), periodontal disease and alveolar bone loss, and citizen science recruited healthy controls. FIG. 9A illustrates an oral microbiome-based tooth resorption risk assessment in clinically recruited cohorts of cats suffering from tooth resorption and citizen science recruited healthy controls. FIG. 9B illustrates an oral microbiome-based tooth resorption risk assessment in clinically recruited cohorts of cats suffering from tooth resorption, incorporating tooth resorption staging information, and citizen science recruited healthy controls. FIG. 10A illustrates a distribution of the average log ratio difference scores between pairwise microbial interactions associated with gingivostomatitis and healthy cohorts. FIG. 10B illustrates the sensitivity and specificity of the feline gingivostomatitis test based on a 2-component Gaussian mixture model. Distribution of the probability of cats from the gingivostomatitis and healthy cohorts being classified as having gingivostomatitis or being healthy according to a 2-component Gaussian mixture model. Sensitivity and specificity of the feline gingivostomatitis test based on the ability to detect oral microbiome signatures characteristic of this disease are also shown. DETAILED DESCRIPTION Variations in the microbial composition of the mouth (the oral microbiome) may have associations with certain dental and systemic diseases. This research area is still young and studies on human subjects demonstrating these associations in a comprehensive manner have only been published in the last decade or less. Studies on this topic in companion animals, such as cats and dogs, have been limited. Nutritional and environmental factors, as well as present disease states, may play an important role in the dynamic microbial composition of a mammal's mouth (their oral microbiome). With the mouth being the first line of defense from a constant exposure to foreign microbes, the oral microbiome has evolved to be competitive and territorial. It is comprised of microbes that excel at defending their territory and are typically able to avoid being replaced by foreign invaders, including pathogens. However, dysbiosis inducing events such as poor diet or poor dental hygiene, can lead to pathogenic microbes colonizing disproportionately large parts of the oral cavity (and, thus, altering the oral microbiome), which can be associated with pathology. Understanding the composition of the oral microbiome can provide information about the health of oral tissues and point to potential dental and gum diseases. This information may also be used to manage the health and wellbeing of a pet. Dental diseases may be associated with complex interactions involving a multitude of microbes, as opposed to a single microbe. The field of oral microbiome research in companion animals has received little focus and it is still in its infancy. Existing studies base their conclusions on small sample sizes and outdated culture-based techniques for querying the microbiome. It is estimated that only around 2% of all existing bacteria can be cultured in the laboratory, meaning that in studies relying on this method for microbial classification, many important microbial organisms will likely be missed, while false emphasis might be placed on particular species, simply because they could be cultured and measured. This problem is compounded by the fact that lab culturing provides a very bacteria-centric view of the microbiome, often ignoring other microorganisms such as fungi, protozoa, archaea and viruses. Interrogating the oral microbiome of a mammalian animal can be accomplished using an oral (saliva) sample. Saliva sampling kits have gained popularity in recent years as tests for ancestry and microbial infection have become more prevalent. Available direct-to-consumer microbiome tests typically rely on a technique called ‘16S rRNA gene sequencing,’ which utilizes Next Generation Sequencing (NGS). While this technique provides substantially more information than early bacterial culturing efforts, it can only be used for identifying bacterial species (and some archaea) present in the microbiome. In most cases, these tests do not provide sufficient resolution to reliably, and consistently, identify bacteria beyond the genus level of taxonomic classification. Therefore, in most cases, the test results do not provide the exact species or strain of bacteria comprising the microbiome, making data-driven conclusions vague and relying on approximation. Moreover, it is well-known that the microbiomes of different sites of the body can be composed of viruses, protozoa, and fungal species, in addition to bacteria and archaea. This means that the 16S rRNA gene sequencing approach zooms in on just one part of the microbiome, ignoring the rest. Before describing various embodiments of the present disclosure in detail, it is to be understood that this disclosure is not limited only to the specific parameters, verbiage, and description of the particularly exemplified systems, methods, and/or products that may vary from one embodiment to the next. Thus, while certain embodiments of the present disclosure will be described in detail, with reference to specific features (e.g., configurations, parameters, properties, steps, components, ingredients, members, elements, parts, and/or portions, etc.), the descriptions are illustrative and are not to be construed as limiting the scope of the present disclosure and/or the claimed invention. In addition, the terminology used herein is for the purpose of describing the embodiments and is not necessarily intended to limit the scope of the present disclosure and/or the claimed invention. Presently disclosed are computer systems, systems and methods for the identification, screening, indication and/or treatment of oral disease states in cats, dogs and/or other mammalian companion animals. Oral disease states are understood to encompass dental disease states, while not being limited to dental disease states. Embodiments of the disclosed subject matter describe a method for interrogating the oral microbiome of a mammalian companion animal for the purpose of detecting microbe compositional abundance trends associated with dental disease in cats, dogs and other mammals. Detecting, identifying and/or quantifying microbe compositional abundance trends enables a practitioner to screen for and/or indicate whether a cat, dog and/or other mammalian animal has a particular oral disease state. Detecting and identifying oral and/or dental disease states enables the practitioner or pet owner to treat and prevent the future recurrence of the dental disease state. Treating and/or preventing oral and/or dental disease states enables treatment and prevention of wider, systemic conditions, beneficially resulting in a healthier and more comfortable life for the pet. The degree to which the disclosed systems and methods enable detection, identification and indication of disease states such as periodontal disease, is further enabling for the detection, identification and indication of other disease states such as tooth resorption, feline gingivostomatitis and halitosis, among others. Similarly, the degree to which the disclosed systems and methods enable detection, identification and indication of disease states in felines, is further enabling for the detection, identification and indication of disease states in other mammalian animals, such as dogs (and other canines), horses (and other equines), sheep (and other ovines), cows (and other bovines and/or ruminates), pigs (and other porcine animals), guinea pigs, hamsters, etc. Disclosed methods may compare, for example, a cat's oral microbiome to the oral microbiomes of cats reported by their owners and/or a veterinary professional to have been diagnosed with tooth resorption, periodontal disease, feline gingivostomatitis, or to have bad breath characterized by a ‘death and decay’ odor. The comparison is carried out using a reference database containing defined microbial profiles, associating one or more microbial species and their respective compositional abundance with one or more oral dental conditions. Disclosed systems and methods can comprise a painless oral swab sample collection. Accordingly, the oral microbiome can be surveyed via buccal, supragingival or subgingival sampling. Such sampling does not require anesthetizing the animal and can be performed by the pet owner at their home or by the veterinarian at the clinic. The disclosed systems and methods can serve as an early indicator of dental disease-associated processes not yet visible to the naked eye or not easily recognizable by the average veterinarian general practitioner who does not have extensive dentistry training. Routine use may enable identification of early-stage dental diseases, driving more pets to the veterinary office early on and reducing the number of emergency dental vet visits in the long run. Earlier identification of oral disease states beneficially saves costs in emergency visits and further saves the lives of mammalian companion animals. It is already known that the colonization dynamics and influence of microorganisms (and their relative abundance) inhabiting organs and systems, such as the gastrointestinal tract (from mouth to anus), show many similarities between dogs (canines), cats (felines) and humans. Periodontal disease, which is a disease associated with microbial imbalance in the oral cavity, is prevalent among cats, dogs and humans, with some notable commonalities. The porphyromonas and tannerella genera of bacteria, for example, plays a critical role in periodontal disease pathogenesis in cats, dogs and humans. This demonstrated overlap between cats, dogs and humans in both a disease state and microbial culprits suggests there is overlap in additional disease states/microbial culprits in cats, dogs and humans. The disclosed systems and methods use an oral swab collection device that has previously been successfully used for extracting genomic material from an oral swab sample from either cats or dogs. Additionally, per the manufacturer of the oral swab collection devices, the same swab collection device used is ideal for use with livestock (bovine, ovine, caprine), companion animals (canine, feline, equine) and other species by researchers, breeders, laboratories and consumers. The oral swab collection devices support use across different mammalian species. It has been established that extraction of both host and microbial DNA from such sample collection devices is possible (see the Examples below). It logically follows that the capability for this extraction on feline samples would extend to canine and other mammalian samples. The disclosed systems and methods demonstrate analysis of microbial species identity and abundance in feline oral microbiome samples for the purpose of screening for dental diseases in cats. Given that dogs and other mammals have oral cavities and oral microbiomes and are, in many cases, predisposed to the same dental disease pathologies (e.g., periodontal disease), our method should be readily applicable to dogs and other mammals. This is because the model for each species is based on a comparison between a disease and a healthy animal cohort in order to derive the precise trends in microbial identities and abundances in each state for each species. Defined Microbial Profiles Contained in the Reference Database With the mouth being the first line of defense from constant exposure to foreign microbes, the oral microbiome has evolved to be competitive and territorial. It is comprised of microbes that excel at defending their territory and are typically able to resist being replaced by foreign invaders, including pathogens. These microbes are generally present when a mammalian animal (e.g., a cat or dog) is healthy and would represent a healthy microbial profile of the oral microbiome. When the mammalian animal is suffering from a dental condition, the composition of the oral microbiome may be altered by the presence of foreign or pathogenic microbial species and/or altered abundance ratios between different microbes. Such an alteration in the composition of the oral microbiome might be represented by a pathogenic profile. In some cases, the presence of particular foreign and/or pathogenic microbial species, and their abundance relative to other microbes in the oral cavity, is correlated with the mammalian animal suffering from a particular dental condition. Identification of the particular (one or more) microbial species (and their respective relative abundances) correlated with particular oral disease states enables pre-diagnostic screening for the oral disease state in a mammalian animal exhibiting the presence of the identified (one or more) microbial species. In other words, identification and/or indication of the oral disease state may be correlated to the mammalian animal exhibiting a particular pathogenic profile. The gold standard for the comprehensive study of the microbiome is shotgun metagenomic sequencing, which allows capturing complete or near-complete genomes of organisms across all domains of life, not just bacteria and archaea. The disclosed methods also enable microbial identification and classification down to the species or, in some instances, the strain level, unlike 16S gene sequencing. In veterinary practice, dental disease is sometimes thought of as a syndrome where halitosis, tooth resorption and periodontal disease are rarely seen separately from each other, even though they can have different underlying pathologies and/or microbial culprits. As discussed more fully below, this view is, to some extent, reflected in obtained data where some overlap in microbial species between conditions is observed. The largest overlap observed is between halitosis and periodontal disease, which is consistent with observations from the clinic where halitosis is often a harbinger of periodontal disease. However, also identified were a plethora of microbes whose compositional abundance in the oral microbiome is predictive specifically of halitosis, tooth resorption, feline gingivostomatitis or periodontal disease. This suggests that there are microbial profiles associated with specific dental pathologies, in addition to the existence of a core set of microbes associated with dental disease in general. Using shotgun metagenomic oral microbiome sequencing of 38,000 domestic cats and compositional data analysis techniques, a comprehensive survey of the feline oral microbiome was executed, identifying 8,344 microbial species present in the feline oral microbiome. Whether a domestic cat included in the shotgun metagenomic sequencing suffered from a particular dental condition was identified in two ways. Cats were either reported by their owners to have been formally diagnosed by a veterinarian as suffering from a particular dental condition (e.g., periodontal disease, tooth resorption, gingivostomatitis, etc.) or were informally diagnosed by their owner (as is the case for halitosis). The reference database is a weighted correlation database and contains at least the identified 8,344 microbial species present in the feline oral microbiome. On average, 606 microbial species per cat were identified, 97% of which were classified as bacteria and archaea, 0.27% as DNA viruses (RNA viruses cannot be detected with shotgun metagenomic sequencing), 0.02% as phages and <2% as fungi. The various microbial species identified as being involved in and contributing to a specific dental disease are compiled into a “defined microbial profile.” The defined microbial profile is a list or collection of identified one or more microbial species and their respective relative abundances known to contribute to and/or be involved in a specific dental disease condition. For example, a defined microbial profile may include a set of 27 microbes that are predictive for three dental conditions (halitosis, tooth resorption and periodontal disease), as well as microbes specifically predictive for one of the four dental conditions (halitosis, feline gingivostomatitis, tooth resorption and periodontal disease). “Predictive microbes” are discussed more fully below. The defined microbial profile may rank and/or weigh each included microbial species by how frequently and in what proportions a certain microbe is observed in animals suffering from the specific dental disease condition, as deduced by consulting a reference database. How much any one microbial species contributes to a specific dental disease condition is correlated to how often a microbial species shows up (or is present) in the oral microbiome while an animal is suffering from a specific dental disease condition, as well as how consistently such microbial species demonstrates significantly different relative abundance from other oral microbes when compared to healthy control samples. The defined microbial profiles contained in the reference database also include defined microbial profiles of healthy mammalian animals that are not suffering from a dental condition. For example, the defined microbial profile of healthy cats lists and identifies the microbial species present in the oral microbiome, as well as their relative abundances, when no dental condition is present. A healthy defined microbial profile may establish a baseline or control for the microbial species present and their relative abundance. Any deviations from this profile may enable a practitioner to predict and/or indicate, for example, a cat's likelihood of suffering from a dental condition. Similarly, deviations from the healthy defined microbial profile may enable a practitioner in diagnosing a cat as suffering from a dental condition prior to the onset of symptoms for that dental condition. The defined microbial profile for each dental disease state is compared to the defined microbial profile for a healthy mammal to determine any differences between the dental disease states and a healthy state. In some embodiments, the comparisons are pairwise log ratio comparisons. For example, there may be some overlap in the oral microbiome of a healthy cat and a cat suffering from periodontal disease. A comparison of the healthy defined microbial profile to the periodontal disease defined microbial profile would identify common microbial species seen in similar abundances between the two. Any microbial species not common between the two microbial profiles, or any microbial species seen in significantly different proportions between the two profiles, would confirm the involvement of that microbial species in the development of periodontal disease. Identification of such a microbial species in a cat's oral microbiome would be indicative of the cat having periodontal disease. FIGS. 1A-1B illustrate a dental health test workflow and construction of the oral microbiome reference database using feline subjects. In FIG. 1A , the feline dental health test workflow includes collecting an oral swab from the cat in a DNA preservation solution, extracting and preparing the DNA for shotgun metagenomic next generation sequencing (NGS), sequencing, data analysis and the generation of a report presenting risk assessment for different dental diseases based on the state of the oral microbiome, accompanied by treatment recommendations tailored to the results. In FIG. 1B , the feline oral microbiome reference database was constructed through applying sequential filters on the initial database of 38,000 cats. First, all data from tagmentation-based NGS library preparation samples was removed. This was done due to an observed effect of the library preparation method on microbial species richness ( FIG. 6 ). The ligation-based method was preferred because the number of sequencing reads per sample had minimal impact on the number of microbial species detected. In addition, Tn5 transposase assisted tagmentation is known to introduce GC sequencing bias, particularly in metagenomic communities. However, tagmentation-based NGS library preparation may be included in some embodiments. Next, samples lacking an accompanying phenotype/health history record for the cat were excluded. After classification of the microbial reads in each sample using KRAKEN2 and Bracken, samples with fewer than 10,000 and more than 500,000 classified microbial reads were removed. The remaining cats/samples were placed into cohorts. This resulted in a periodontal disease (PD) cohort of 570 cats, tooth resorption (TR) cohort of 111 cats, feline gingivostomatitis (FG) cohort of 115 cats, bad breath (BB) cohort of 173 cats, healthy cohort of 1,147 cats and typical breath (TB) cohort of 4,109 cats. Identifying Predictive Microbes As a first step towards identifying microbes significantly correlated with each dental condition, Pairwise Log-Ratio (PLR) transformation was performed on the Bracken output species level read counts. Next, the significant PLR comparisons (p-value <0.01) were identified between the control and a condition by performing a z-test. The healthy cohort was compared to the PD, TR and FG cohorts; the typical breath (TB) cohort was compared to the BB cohort. The frequency of each microbial species in all significant PLRs was assessed. Only microbial species where 50% or more of their maximum possible comparisons with other species were significant were kept. This measure was used as a proxy for the importance of different microbial species in the four dental conditions of interest. These microbial species are “predictive microbial species” for each dental condition. In order to identify population-wide microbial compositional abundance patterns characteristic of periodontal disease, tooth resorption, feline gingivostomatitis, or halitosis, for each of the conditions, each sample was scored by comparing the predictive pairwise log-ratios (pPLRs) of the sample to the mean pPLRs of controls, taking into account the direction and magnitude of the difference. FIGS. 2A-2C illustrate a distribution of the average log ratio difference scores between pairwise microbial interactions associated with periodontal disease and healthy cohorts, tooth resorption (TR) and healthy cohorts, and bad breath (BB) and typical breath (TB) cohorts. FIG. 10A illustrates the distribution of the average log ratio difference scores between pairwise microbial interactions associated with feline gingivostomatitis (FG) and healthy cohorts. Next, we fitted four (4) Gaussian mixture models (one for each dental condition) with two (2) components each-healthy cohort (or TB cohort) and dental condition-onto the distribution of the average log ratio difference score between pairwise microbial interactions. This modeling approach generates a 0 to 1 score for each sample, which represents the probability that the sample belongs to the control cohort or to the respective dental condition cohort. FIGS. 3A-3C plot the probability that samples belonging to three of the dental disease cohorts (periodontal disease, tooth resorption and halitosis) and the control samples would be classified as belonging to their respective cohorts based on each sample's compositional abundance of predictive microbes. FIG. 10B plots the probability that feline gingivostomatitis and control samples would be classified as belonging to the feline gingivostomatitis category or to the control category based on each sample's compositional abundance of predictive microbes. A bimodal probability distribution consistent with sample identity was observed between the dental condition and control in all cases. The clearest bimodal pattern was for periodontal disease and halitosis, and a weaker bimodal pattern for tooth resorption and feline gingivostomatitis was observed. In all four instances, there was a minority of disease samples forming a small peak closer to 0 and a small set of control samples forming a slight peak closer to 1. The defined microbial profile for each dental disease state (periodontal disease, tooth resorption, feline gingivostomatitis and halitosis) is compared to the defined microbial profile for a healthy mammal to determine and quantify differences and commonalities in microbial species and their abundance between the dental disease states and a healthy state. The defined microbial profiles for each dental disease state are also compared to each other to identify overlapping microbial species common to each dental disease state. The defined microbial profiles for each dental disease state and a healthy control state undergo a pairwise log ratio (PLR) transformation. The PLR transformation corrects for potential sequencing coverage differences between samples by scaling microbial abundances relative to each microbe instead of a constant scaling factor. Next, a z-test between PLRs from each disease state versus the control state is performed. A p-value of approximately <0.01 serves as a threshold value for significant PLR comparisons. For each microbial species identified in a defined microbial profile for a dental disease state, the number of significant PLR comparisons (as defined by the p-value) that microbial species shows up in is counted. If the number of significant PLR comparisons is at least 50% of all possible PLR comparisons for that microbe, the microbial species is deemed a “predictive microbe.” This process may be repeated for each dental disease state of interest. In other words, through z-test identification of significant PLR comparisons, predictive microbes can be identified for periodontal disease, halitosis, feline gingivostomatitis and/or tooth resorption. Table 2 provides examples of identified predictive microbes for periodontal disease, halitosis and tooth resorption. As outlined in Table 2, 108 predictive microbes for periodontal disease, 74 for tooth resorption and 182 for halitosis were identified. The predictive microbes for each dental disease were identified based on PLR microbial abundance comparisons between healthy/control defined microbial profiles and the defined microbial profiles of cats suffering from one of three dental conditions (See FIG. 4 ). 27 microbes were identified as predictive for three dental conditions (periodontal disease, tooth resorption and halitosis), though each condition has its own specific set of predictive microbes, differentiating it from other conditions. Plotting the average log ratio difference between significant pairwise microbial interactions in a dental condition versus control samples allowed separation of sample populations based on their dental disease status. (See FIGS. 2A-2C ). However, some overlap between the populations was observed, meaning that for a certain set of samples, their compositional abundance of predictive microbes could be interpreted as either consistent with the control population or the respective dental disease population. It is important to note that the use of the word ‘predictive’ is not meant to be interpreted as ‘causative’, it simply reflects the fact that a microbe has a significantly different compositional abundance in a particular dental condition compared to control. This could either mean that the microbe has an active role in the disease's pathology or that the changes of its compositional abundance are a byproduct of pathology. In either scenario, presence of the microbe in a specific abundance relative to other microbes directly correlates with an oral and/or dental disease state. A significantly increased compositional abundance of P. gingivalis, T. forsythia, B. zoogleoformans, D. orale, D. fairfieldensis and T. denticola (among other microbes) was observed in the microbiomes of cats suffering from periodontal disease. Furthermore, significantly decreased compositional abundance of the genera Moraxella and Capnocytophaga , as well as the bacterial species P. multocida , was also observed (Table 1). These observations are all consistent with previous findings from studies focused on the oral microbiome of cats, humans and dogs suffering from periodontal disease. Sequencing and Extraction Protocols At least one oral swab of a mammalian animal may be taken to provide a sample for testing. The oral swabs may target the gum lines of the animal (top and bottom) and/or target the entire mouth of the animal. Microbial DNA may be extracted from the oral swab samples in order to identify which microbial species, and in what relative abundance, are present in the mammalian animal's oral microbiome. Metagenomic DNA may be extracted from the oral samples via heat treatment for approximately one hour on a shaker, with or without bead-beating or the addition of detergents and protein degradation reagents such as proteinase K. In some embodiments, the oral samples are heat treated at approximately 45° C. to 75° C., such as 50° C., 55° C., 60° C., 65° C., 70° C. or within a range defined by any two of the foregoing values. After heat treatment of the oral sample, metagenomic DNA may be extracted by SPRI magnetic beads-based DNA extraction (MCLAB, MBC-200) using 80% ethanol for purification. The DNA may be quantified using a GloMax Plate Reader (Promega). Following metagenomic DNA extraction and quantification, the oral samples may be prepared for NGS using the LOTUS DNA library prep kit (IDT), the Next Ultra II FS DNA library prep kit (NEB), or another ligation or tagmentation based DNA library prep kit, following the manufacturer's instructions. The oral samples may be dual-barcoded with iTRU indices. The prepared sequencing libraries may be quantified using a GloMax Plate Reader (Promega) and equal-mass pooled into 96-sample pools. The pools may then be visualized (to assess fragment size distribution) and quantified using a 2100 Bioanalyzer instrument (Agilent). Following standard QC steps, the 96-sample pools may be loaded onto an Illumina HiSeq X or NovaSeq 6000 Next Generation Sequencing machine. The raw sequencing data may be demultiplexed and trimmed to remove low-quality data using, for example, the program Trimmomatic 0.32. The data may then be mapped to the latest version of, for example, the feline genome Felis_catus_9.0, or the reference genome of the mammalian species of interest. For every oral sample, there may be approximately 5-7% sequencing reads that do not map to the mammalian genome of interest. The unmapped reads may be classified using the KRAKEN2 metagenomic sequence classifier (or a suitable alternative) to identify the microbial organisms present in each sample. Bracken, a statistical method for calculating species abundance in DNA sequencing data from a metagenomic sample, was used on the sequenced data in conjunction with the KRAKEN2 analysis. Bracken may output species level read counts. Based on the outcome of the KRAKEN2 metagenomic sequence classifier and the Bracken calculations, an oral microbial profile for the mammalian animal may be generated. The oral microbial profile generated may include data regarding the identity of the microbial species present as well as their relative abundance. A confidence score of approximately 0.1 may be used as a cutoff (or threshold value) for the KRAKEN2 classification algorithm. All samples with fewer than 10,000 classified microbial reads or more than 500,000 classified microbial reads may be filtered out. The reads for microbial species with a non-zero mean of fewer than 10 reads may also be filtered out. Methods of Indication and Comparison Indication of whether a cat is suffering from a dental disease relies on a comparison of the cat's current oral microbiome state to the oral microbiomes of cats reported by their pet owners to have been diagnosed by a veterinarian with periodontal disease, gingivostomatitis or tooth resorption or to suffer from bad breath (halitosis). The comparison is based on the compositional abundance of microbes determined by the analysis to be predictive of each of the three dental conditions. Computational analysis of the compositional abundance of different microbes present in the oral microbiome involves comparison of the sample against a database of samples from mammals of the same species known to suffer from different dental conditions, as well as mammals of the same species who do not suffer from any known dental conditions. In other words, the computational analysis compares the oral microbiome identified from the oral swab sample to the defined microbial profiles contained in the reference database (discussed more fully above). In some embodiments, a method for indicating oral disease in mammalian animals includes receiving an oral swab sample taken from a mammalian animal; performing heat treatment on the oral sample; and performing magnetic beads-based deoxyribonucleic acid (DNA) extraction on the heat-treated oral sample to extract microbial DNA that is present in the oral swab sample. The method may also include sequencing the microbial DNA to identify which specific one or more microbes are present in the oral sample and in what proportions, wherein identifying the specific one or more microbes and their abundances results in generation of an oral microbial profile for the mammalian animal; and comparing the oral microbial profile for the mammalian animal against a database of defined microbial profiles, wherein the database identifies correlations between (i) profiles that include one or more microbes and (ii) corresponding oral diseases. Based on a result of comparing the oral microbial profile against the database of defined microbial profiles, the method may further include generating a risk score indicating a likelihood that the mammalian animal has a specific oral disease. In some embodiments, a method for indicating oral disease in mammalian animals includes receiving an oral swab sample taken from a mammalian animal; performing heat treatment on the oral sample; and performing magnetic beads-based deoxyribonucleic acid (DNA) extraction on the heat-treated oral sample to extract microbial DNA that is present in the oral swab sample. The method may also include sequencing the microbial DNA to identify which specific one or more microbes are present in the oral sample, wherein identifying the specific one or more microbes and their abundance results in generation of an oral microbial profile for the mammalian animal. The method may further include comparing the oral microbial profile for the mammalian animal against a database of defined microbial profiles, wherein the database identifies correlations between (i) profiles that include one or more microbes and (ii) corresponding oral diseases; based on a result of comparing the oral microbial profile against the database of defined microbial profiles, generating a risk score indicating a likelihood that the mammalian animal has a specific oral disease; and in response to generating the risk score and identifying the specific oral disease, administering a therapeutic treatment designed to treat the specific oral disease. In some embodiments, the therapeutic treatment may include administering a therapeutic compound, such as a compound designed to inhibit or encourage growth of a specific one or more microbial species present in the oral microbiome of the mammal. In some embodiments, the therapeutic compound includes a pre-biotic, a post-biotic, a pro-biotic, a medicament or a combination thereof. In some embodiments, the therapeutic treatment may include brushing the mammal's teeth with a topical treatment. In some embodiments, the therapeutic treatment protocol is designed to alter the composition of the oral microbiome of the mammal. In some embodiments, altering the composition of the mammal's oral microbiome treats and/or addresses the specific oral disease. In some embodiments, the therapeutic treatment repairs the mammal's oral microbiome. In some embodiments, repairing the mammal's oral microbiome brings the mammal's oral microbiome more in line with the oral microbiome (or defined oral microbial profile) of a healthy mammal—both in terms of the specific one or more microbial species present and their relative abundance. In some embodiments, the therapeutic treatment protocol is designed to maintain the composition of the oral microbiome of the mammal. In some embodiments, the therapeutic treatment protocol is designed to stimulate a metabolic output of the mammalian animal's oral microbiome. Stimulating a metabolic output of the mammalian animal's oral microbiome may include using known enzymatic pathway analysis tools to provide an additional dimension to the existing microbial composition data to further characterize disease signatures and improve predictive disease models. EXAMPLES The examples below were performed with cats/feline data, though it is to be understood that the methods outlined are expected to be accurate and appropriate with regards to other mammalian data (e.g., canine, or another mammal). Confirmatory Study of the Database To assess over- or under-representation of particular microbial species in periodontal disease, the Bracken output data underwent Centered Log-Ratio (CLR) transformation. This was done to account for potential compositional biases, which are a well-established problem in microbiome data analysis. A z-test was then performed on the CLR transformed data to identify microbial species with statistically significant increased and decreased compositional abundance in periodontal disease compared to control. Table 1 shows microbes with upregulated and downregulated abundance in the periodontal disease cohort compared to control. The results expand on and agree with previous findings from oral microbiome studies of periodontal disease in humans, dogs and cats. These results validate the sample collection, DNA extraction, metagenomic sequencing and compositional abundance-based analysis methodology. These results also enable a similar identification of microbes with upregulated and downregulated abundances in other disease states, such as halitosis, gingivostomatitis and tooth resorption. To start building computational oral disease classification algorithms, Pairwise Log-Ratio (PLR) transformation was performed on the Bracken output species level read counts. Next, the significant PLR comparisons (a threshold p-value <0.01) were identified between the control and a condition by performing a z-test. The healthy cohort was compared to the PD, TR and FG cohorts; the typical breath (TB) cohort was compared to the BB cohort. The frequency of each microbial species in all significant PLRs was assessed. Only microbial species where 50% or more of their maximum possible comparisons with other species were significant were kept. This measure was used as a proxy for the importance of different microbial species in the three dental conditions of interest. These microbial species are “predictive microbial species” for each dental condition. In order to identify population-wide microbial compositional abundance patterns characteristic of periodontal disease, tooth resorption, feline gingivostomatitis, or halitosis, for each of the conditions, each sample was scored by comparing the predictive pairwise log-ratios (pPLRs) of the sample to the mean pPLRs of controls, taking into account the direction and magnitude of the difference. FIGS. 2A-2C illustrate a distribution of the average log ratio difference scores between pairwise microbial interactions associated with periodontal disease and healthy cohorts, tooth resorption (TR) and healthy cohorts, and bad breath (BB) and typical breath (TB) cohorts. FIG. 10A illustrates the distribution of the average log ratio difference scores between pairwise microbial interactions associated with feline gingivostomatitis (FG) and healthy cohorts. Next, we fitted four (4) Gaussian mixture models (one for each dental condition) with two (2) components each—healthy cohort (or TB cohort) and dental condition—onto the distribution of the average log ratio difference score between pairwise microbial interactions. This modeling approach generates a 0 to 1 score for each sample, which represents the probability that the sample belongs to the control cohort or to the respective dental condition cohort. FIGS. 3A-3C plot the probability that samples belonging to three of the dental disease cohorts (periodontal disease, tooth resorption and halitosis) and the control samples would be classified as belonging to their respective cohorts based on each sample's compositional abundance of predictive microbes. FIG. 10B plots the probability that feline gingivostomatitis and control samples would be classified as belonging to the feline gingivostomatitis category or to the control category based on each sample's compositional abundance of predictive microbes. A bimodal probability distribution consistent with sample identity was observed between dental condition and control in all cases, with the clearest bimodal pattern for periodontal disease and halitosis and a weaker bimodal pattern for tooth resorption and feline gingivostomatitis. In all four instances, there was a minority of disease samples forming a small peak closer to 0 and a small set of control samples forming a slight peak closer to 1. This suggests that it is possible that a small proportion of cats in the dental disease cohorts might actually be healthy or in remission (due to old, wrong or incomplete health information provided by the pet owner), while some cats in the control cohorts could be suffering from a dental condition that has not yet been diagnosed or noticed. The sensitivity (ability to detect cats known to suffer from a dental condition) and specificity (ability to detect cats in the control cohort as not suffering from a dental condition) of the risk classification method for each dental condition was tested (see FIG. 3D and FIG. 10 B). The method's sensitivity is highest for halitosis and gingivostomatitis and lowest for tooth resorption, while the specificity is highest for tooth resorption and lowest for halitosis. This relatively low sensitivity for tooth resorption could be due to the nature of the pathology behind this condition. It tends to originate inside of the tooth and, as it enters more advanced stages, it then reaches the surface of the tooth. It is possible that the microbes associated with tooth resorption can be detected most reliably when the resorptive process has reached the surface of the tooth. The specificity for periodontal disease and bad breath is lower (70% and 62%, respectively) compared to the specificity for detecting tooth resorption associated changes in the microbiome (78%). This observation could be explained by the possibility that the healthy and TB cohorts include some cats with periodontal disease or bad breath, respectively, that have not yet been diagnosed by a veterinarian or noticed by the pet owner. However, even with these caveats in mind, the specificity and sensitivity of the disclosed methods for all four conditions are comparable to (or better than) previously reported human microbiome-based disease risk assessment algorithms. Even though a sizable domestic cat cohort (n=6,110) was used to develop the reference database, the health history data for these cats was provided by the pet owner. Despite the fact that pet owners were asked if their cats had been diagnosed by a veterinarian with periodontal disease, gingivostomatitis or tooth resorption, some of the diagnostic precision would have, undoubtedly, suffered, having been relayed by the pet owner. To alleviate this problem and limit instances where a cat reported by their pet owner to be healthy (i.e., not suffering from any known systemic or dental conditions) had actually started developing a yet undiagnosed dental disease, an age limit was set to the control healthy cohort of 1-3 years. This limit was set due to the well-established connection between age and dental disease. Cats below one year of age were intentionally excluded from this group with the purpose of avoiding any potential kitten-specific oral microbiome bias. The healthy control cohort could potentially be biased towards the oral microbiomes of younger cats and not be representative of older cats with no dental or systemic diseases. The assessment of whether cats in the BB and TB cohorts had halitosis or not was based on the subjective evaluation of the pet owner, which could have potentially added another source of bias. Study 1 The present study was of observational nature and did not utilize any invasive procedures. All feline oral swab samples and accompanying health history information used in this study were provided voluntarily by pet owners who agreed in electronic form for their cat's data to be used in an aggregated de-identified format for research purposes. Participants were recruited through an email inviting participation in studies focused on feline dental health. Twelve (12) cats took part in Study 1. The pet owners of the feline participants received two (2) DNAGenotek's PERFORMAgene (PG-100) oral swab collection devices and were instructed to swab their cat once using each swab. The first swab was used to collect a sample from the whole mouth, while the second one targeted the gum line specifically. Participants were also asked to collect the two samples in one sitting. The vast majority of feline oral swab samples were collected by pet owners at their respective homes, with a small proportion of sample collections performed by a veterinarian. Pet owners and veterinarians were instructed to collect the samples at least 30 minutes to an hour after the cat had had anything to eat or drink. They were also instructed to keep the oral swab sample collection device in the cat's mouth for at least 5 seconds. Metagenomic DNA was extracted from feline oral samples via heat treatment (55° C.) for an hour on a shaker, followed by SPRI magnetic beads-based DNA extraction (MCLAB, MBC-200) using 80% ethanol for purification. The DNA was quantified using a GloMax Plate Reader (Promega). Following metagenomic DNA extraction and quantification, each sample was prepared for NGS using the LOTUS DNA library prep kit (IDT) following the manufacturer's instructions. Each sample was dual-barcoded with iTRU indices. The prepared sequencing libraries were quantified using a GloMax Plate Reader (Promega) and equal-mass pooled. The pools were then visualized (to assess fragment size distribution) and quantified using a 2100 Bioanalyzer instrument (Agilent). Following standard QC steps, the sample pool was loaded onto an Illumina HiSeq X or NovaSeq 6000 Next Generation Sequencing machine. The raw sequencing data was demultiplexed and trimmed to remove low-quality data using the program Trimmomatic 0.32. The data was then mapped to the latest version of the feline genome Felis_catus_9.0. For every sample, there were 5-7% sequencing reads that did not map to the feline genome. The unmapped reads were classified using the KRAKEN2 metagenomic sequence classifier to identify the microbial organisms present in each sample. A confidence score of 0.1 was used as a cutoff for the KRAKEN2 classification algorithm. One sample from a cat undergoing antibiotic treatment at the time of sample collection had fewer than 10,000 classified microbial reads and was therefore excluded from the analysis. Bracken, a statistical method for calculating species abundance in DNA sequencing data from a metagenomic sample, was used on the sequenced data. Each cat's risk of having tooth resorption, periodontal disease or halitosis was calculated based on the pattern of predictive microbe PLRs observed in their oral microbiome. Briefly, the Bracken output microbial abundance data for each sample was transformed into PLRs and the PLRs associated with predictive microbes were compared to the mean predictive microbes PLRs for the healthy reference cohort. This comparison resulted in a list of deviation scores from the healthy cohort mean for each predictive microbe PLR in the sample of interest. This list of deviations was compared to the list of mean deviations in predictive microbe PLRs for a disease cohort of interest (e.g., tooth resorption, periodontal disease or halitosis) from a healthy control cohort with the purpose of assessing whether the sample shows a similar deviation profile from the healthy cohort as does the reference disease cohort. Assessing this similarity takes into account the directionality of deviations for each predictive microbe PLR, ‘punishing’ deviations that are in the opposite direction of the respective PLR deviation in the disease cohort compared to the healthy cohort. In other words, if a PLR deviation is in the opposite direction, this is assumed to bring the sample closer to the healthy profile. After summing up all the sample PLR deviation scores, the final deviation score was transformed to fit the probability space in the 2-component Gaussian model for the disease of interest versus healthy. Therefore, each sample had a probability score between 0 and 1 for each dental condition. The following three (3) risk assessment categories based on the probability score generated for each sample were applied: the 0.0-0.33 bracket is classified as ‘low risk’ of having a dental condition; >0.33-0.66 is classified as ‘medium risk’ for having a dental condition; and >0.66-1.0 is classified as ‘high risk’ for having a dental condition. FIG. 5A illustrates results from Study 1 comparing the oral microbiome profiles of 11 cats based on sample collection methods targeting the whole mouth area or the gum line specifically. The dendrogram shows sample clustering based on Spearman's rank correlation of the oral microbiome profiles. The table shows each participating cat's risk assessment for periodontal disease, tooth resorption and halitosis based on the swabbing condition. Green color indicates low risk, light orange-medium risk and dark orange-high risk. Cat #7's ‘whole mouth’ sample was excluded from the analysis because the number of classified microbial reads was <10,000. Study 2 The present study was of observational nature and did not utilize any invasive procedures. All feline oral swab samples and accompanying health history information used in this study were provided voluntarily by pet owners who agreed in electronic form for their cat's data to be used in an aggregated de-identified format for research purposes. Participants were recruited through an email inviting participation in studies focused on feline dental health. Eleven (11) cats took part in this study. Participants received three (3) DNAGenotek's PERFORMAgene (PG-100) oral swab collection devices and were instructed to perform three oral swab collections targeting the whole oral cavity, with all three swab collections performed in one sitting. The vast majority of feline oral swab samples were collected by pet owners at their respective homes, with a small proportion of sample collections performed by a veterinarian. Pet owners and veterinarians were instructed to collect the samples at least 30 minutes to an hour after the cat had had anything to eat or drink. They were also instructed to keep the oral swab sample collection device in the cat's mouth for at least 5 seconds. The feline oral swab samples underwent the same extraction, sequencing and analysis protocols outline above for Study 1. FIG. 5B illustrates the results from Study 2 comparing the oral microbiome profiles of 11 cats based on three separate sample collections targeting the whole mouth, not just focusing on the gum line. The dendrogram shows sample clustering based on Spearman's rank correlation of the oral microbiome profiles. The table shows each participating cat's risk assessment for periodontal disease, tooth resorption and halitosis based on each replicate. Some owners only provided two (2) samples rather than the requested three (3); and some samples were excluded because the number of classified microbial reads was <10,000. Study 3 Following obtainment of written consent from pet owners, thirty-six (36) feline oral swab samples from felines (cats) suffering from various degrees of periodontal disease and tooth resorption were collected by a licensed veterinary technician at a feline-only animal hospital using DNAGenotek PERFORMAGENE P-100 collection devices, with the sample collection method targeting the gum line (gingiva). Each cat participating in this trial had accompanying veterinary records and dental radiographs performed within a week of sample collection. A boarded veterinary dentist blindly assessed the dental radiographs for each sample to confirm diagnosis. The cats were categorized into three groups-cats with mild/moderate gingivitis and no radiographic evidence of alveolar bone loss (n=10), cats with periodontal disease and radiographic evidence of bone loss (n=11) and cats with radiographic evidence of tooth resorption, stages 2-4, affecting one or more teeth (n=15). DNA was extracted from these samples, after which shotgun metagenomic sequencing was performed and the data was analyzed using the computational dental disease risk assessment methods and/or computer systems described previously. The algorithm-produced dental disease risk assessments for these clinically recruited cohorts of cats were compared against risk assessments for a randomly selected cohort of cats whose owners had reported them to not have been diagnosed by a veterinarian with any dental disease and to have fresh or typical cat breath (healthy citizen science-based cohort). Basic demographic statistics of all cohorts are presented in Table 3. The generated periodontal disease risk assessment was significantly higher for the gingivitis (no evidence of bone loss) and periodontal disease with evidence of bone loss cohorts compared to the healthy control cohort (p<0.01 and p<0.0001, respectively). FIG. 7 illustrates an oral microbiome based periodontal disease risk assessment in clinically recruited cohorts of cats suffering from gingivitis (no bone loss), periodontal disease and bone loss, and citizen science recruited healthy controls. The horizontal lines represent the mean risk score for each cohort (the risk score range is from 0 to 1, with higher values representing increased risk of disease) and the error bars represent the Standard Error of the Mean (SEM). A 2-tailed t-test assuming unequal variance was used for each comparison; *p<0.01, ***<0.0001. The algorithm generated bad breath (halitosis) risk assessment was significantly higher for the gingivitis (no evidence of bone loss) cohort compared to the periodontal disease with evidence of bone loss cohort (p<0.05). The halitosis risk assessment was also significantly higher for the periodontal disease with evidence of bone loss cohort compared to the healthy control cohort (p<0.01). Halitosis is a known hallmark of gingivitis and later stages of periodontal disease. FIG. 8 illustrates an oral microbiome based bad breath (halitosis) risk assessment in clinically recruited cohorts of cats suffering from gingivitis (no bone loss), periodontal disease and bone loss, and citizen science recruited healthy controls. The horizontal lines represent the mean risk score for each cohort (the risk score range is from 0 to 1, with higher values representing increased risk of disease) and the error bars represent the Standard Error of the Mean (SEM). A 2-tailed t-test assuming unequal variance was used for each comparison; *p<0.05. FIG. 9A illustrates an oral microbiome-based tooth resorption risk assessment in clinically recruited cohorts of cats suffering from tooth resorption and citizen science recruited healthy controls. FIG. 9B illustrates an oral microbiome-based tooth resorption risk assessment in clinically recruited cohorts of cats suffering from tooth resorption, incorporating tooth resorption staging information, and citizen science recruited healthy controls. There was a significant difference between the mean risk score generated for the healthy control cohort and the stage 4 tooth resorption cohort. Discussion The disclosed methods' and systems' specificity and sensitivity are potentially influenced by the sample collection method. Current risk prediction models are based on pet owner-provided oral swab samples where the whole mouth was targeted for sample collection, focusing on no particular area of interest. As replicate studies have shown, risk assessments based on a ‘whole mouth’ swab sample can occasionally show variability (specific examples are Cat V's and Cat IX's samples from Study 2). This is probably due to the fact that when the pet owner is instructed to collect a ‘whole mouth’ swab sample, different mouth areas get preferentially swabbed each time. Since the easiest area to swab is the tongue, it is possible that some ‘whole mouth’ sample collection attempts focus on the tongue area where the presence of dental disease-associated microbes is more variable compared to the gum line. Interestingly, when there was a discrepancy between ‘whole mouth’ and ‘gum line’ targeted oral swab collection in Study 1, the ‘gum line’ risk score was always higher than the ‘whole mouth’ risk score (Cat #4, Cat #5, Cat #8, Cat #10). This supports the hypothesis that sample collection targeted at the gum line is likely to more accurately represent microbiome states linked to dental diseases. For this reason, in Study 3, the veterinary technician was instructed to target the gum line for sample collection from cats suffering from periodontal disease and tooth resorption. The disclosed method was able to identify cats with early stages of periodontal disease (i.e., gingivitis with no evidence of alveolar bone loss) and cats with more advanced periodontal disease (with evidence of alveolar bone loss) as being at a significantly higher risk of periodontal disease than cats from the healthy citizen science-recruited cohort. Additionally, the disclosed method identified cats with periodontal disease and evidence of alveolar bone loss as being at a significantly higher risk of halitosis, compared to healthy controls. Similarly, cats with initial stages of periodontal disease (i.e., gingivitis with no evidence of alveolar bone loss) were found to be at a significantly higher risk of halitosis, compared to controls. Halitosis is commonly known to be a harbinger of periodontal disease. These results demonstrate the utility and accuracy of the disclosed methods and systems as a screening tool for cats at both early and late stages of periodontal disease. Study 3 failed to demonstrate a significant difference in tooth resorption risk between cats with radiographic evidence of tooth resorption and the healthy citizen science-recruited cohort, unless the stage of tooth resorption was taken into account. The disclosed method identified cats with stage 4 tooth resorption as being at a significantly higher risk of the disease compared to healthy controls. As previously hypothesized, the disclosed method had the highest sensitivity when the resorptive lesion had reached the surface of the tooth and the tooth's integrity had already been compromised (stage 4). Stages 1 and 2 of tooth resorption are characterized with mild or moderate dental hard tissue loss that does not extend into the pulp cavity. In stage 3, the dentin loss extends into the pulp cavity, but most of the tooth still retains its integrity. Stage 5, on the other hand, is characterized by remnants of dental hard tissue and re-establishing of gingival covering over the affected area. The results of Study 3 indicate that the oral microbiome is most significantly altered as a consequence of tooth resorption at stage 4 (and leading to stage 5) of the disease. It is already known that the colonization dynamics and influence of microorganisms (and their relative abundance) inhabiting organs and systems, such as the gastrointestinal tract (from mouth to anus), show many similarities between dogs (canines), cats (felines) and humans. Periodontal disease, which is a disease associated with microbial imbalance in the oral cavity, is prevalent among cats, dogs and humans, with some notable commonalities. The porphyromonas and treponema genera of bacteria, for example, play a critical role in periodontal disease pathogenesis in cats, dogs and humans. The disclosed systems and methods use an oral swab collection device that has previously been successfully used for extracting genomic material from an oral swab sample from either cats or dogs. Additionally, per the manufacturer of the oral swab collection devices, the same swab collection device used is ideal for use with livestock (bovine, ovine, caprine), companion animals (canine, feline, equine) and other species by researchers, breeders, laboratories and consumers. The oral swab collection devices support use across different mammalian species. It has already been established that extraction of both host and microbial DNA from such sample collection devices is a possibility. It logically follows that the capability for this extraction on feline samples would extend to canine and other mammalian samples. The disclosed systems and methods demonstrate analysis of the microbial species identity and abundance in feline oral microbiome samples for the purpose of screening for dental diseases in cats. Given that dogs and other mammals have oral cavities and oral microbiomes and are, in many cases, predisposed to the same dental disease pathologies (e.g., periodontal disease), our method should be readily applicable to dogs and other mammals. This is given that the model for each species is based on a comparison between a disease and a healthy animal cohort in order to derive the precise trends in microbial identities and abundances in each state for each species. The degree to which the disclosed systems and methods enable detection, identification and indication of disease states such as periodontal disease, is further enabling for the detection, identification and indication of other disease states such as tooth resorption, feline gingivostomatitis and halitosis, among others. Similarly, the degree to which the disclosed systems and methods enable detection, identification and indication of disease states in felines, is further enabling for the detection, identification and indication of disease states in other mammalian animals, such as dogs (and other canines), horses (and other equines), sheep (and other ovines), cows (and other bovines and/or ruminates), pigs (and other porcine animals), guinea pigs, hamsters, etc. The risk score generation methodology disclosed herein is based on oral microbiome compositional analysis. Other embodiments of the disclosed methods may also include incorporating predictions of the metabolic output of the oral microbiome (generated by enzymatic pathway analysis tools or metabolomics), alongside the oral microbiome compositional abundance analysis for the purpose of predictive risk of dental conditions. Additional Terms and Definitions Unless defined otherwise, all technical and scientific terms used herein have the same meaning as commonly understood by one of ordinary skill in the art to which the present disclosure pertains. Various “aspects” of the present disclosure, including systems, methods, and/or products may be illustrated with reference to one or more “embodiments,” which are exemplary in nature. As used herein, the terms “aspect” and “embodiment” may be used interchangeably. The term “embodiment” can also mean “serving as an example, instance, or illustration,” and should not necessarily be construed as preferred or advantageous over other aspects disclosed herein. In addition, reference to an “embodiment” of the present disclosure or invention is intended to provide an illustrative example without limiting the scope of the invention, which is indicated by the appended claims. As used in this specification and the appended claims, the singular forms “a,” “an” and “the” each contemplate, include, and specifically disclose both the singular and plural referents, unless the context clearly dictates otherwise. For example, reference to a “protein” contemplates and specifically discloses one, as well as a plurality of (e.g., two or more, three or more, etc.) proteins. Similarly, use of a plural referent does not necessarily require a plurality of such referents, but contemplates, includes, specifically discloses, and/or provides support for a single, as well as a plurality of such referents, unless the context clearly dictates otherwise. As used throughout this disclosure, the words “can” and “may” are used in a permissive sense (i.e., meaning having the potential to), rather than the mandatory sense (i.e., meaning must). Additionally, the terms “including,” “having,” “involving,” “containing,” “characterized by,” variants thereof (e.g., “includes,” “has,” and “involves,” “contains,” etc.), and similar terms as used herein, including the claims, shall be inclusive and/or open-ended, shall have the same meaning as the word “comprising” and variants thereof (e.g., “comprise” and “comprises”), and do not exclude additional, un-recited elements or method steps, illustratively. The term “condition” refers to any disorder, disease, injury, or illness, as understood by those skilled in the art, that is manifested or anticipated in a patient. Manifestation of such a condition can be an early, middle, or late stage manifestation, as known in the art, including pre-condition symptoms, signs, or markers. Anticipation of such a condition can be or include the predicted, expected, envisioned, presumed, supposed, and/or speculated occurrence of the same, whether founded in scientific or medical evidence, risk assessment, or mere apprehension or trepidation. The term “patient,” as used herein, is synonymous with the term “subject” and generally refers to any animal under the care of a medical professional, as that term is defined herein, with particular reference to (i) humans (under the care of a doctor, nurse, or medical assistant or volunteer) and (ii) non-human animals, such as non-human mammals (under the care of a veterinarian or other veterinary professional, assistant, or volunteer). “Mammal” includes humans and both domestic animals such as laboratory animals and household pets (e.g., cats, dogs, swine, cattle, sheep, goats, horses, rabbits), and non-domestic animals such as wildlife and the like. “Treating” or “treatment” as used herein covers the treatment of the disease or condition of interest in a mammal, preferably a cat or dog, having the disease or condition of interest, and includes: (i) preventing the disease or condition from occurring in a mammal, in particular, when such mammal is actually starting to develop the condition but has not yet been diagnosed as having it; (ii) inhibiting the disease or condition, i.e., arresting its development; (iii) relieving the disease or condition, i.e., causing regression of the disease or condition; or (iv) relieving the symptoms resulting from the disease or condition, i.e., relieving pain without addressing the underlying disease or condition. As used herein, the terms “disease” and “condition” may be used interchangeably or may be different in that the particular malady or condition may not have a known causative agent (so that etiology has not yet been worked out) and it is therefore not yet recognized as a disease but only as an undesirable condition or syndrome, wherein a more or less specific set of symptoms have been identified by clinicians. For the sake of brevity, the present disclosure may recite a list or range of numerical values. It will be appreciated, however, that where such a list or range of numerical values (e.g., greater than, less than, up to, at least, and/or about a certain value, and/or between two recited values) is disclosed or recited, any specific value or range of values falling within the disclosed values or list or range of values is likewise specifically disclosed and contemplated herein. To facilitate understanding, like references (i.e., like naming of components and/or elements) have been used, where possible, to designate like elements common to different embodiments of the present disclosure. Similarly, like components, or components with like functions, will be provided with similar reference designations, where possible. Specific language will be used herein to describe the exemplary embodiments. Nevertheless, it will be understood that no limitation of the scope of the disclosure is thereby intended. Rather, it is to be understood that the language used to describe the exemplary embodiments is illustrative only and is not to be construed as limiting the scope of the disclosure (unless such language is expressly described herein as essential). While the detailed description is separated into sections, the section headers and contents within each section are for organizational purposes only and are not intended to be self-contained descriptions and embodiments or to limit the scope of the description or the claims. Rather, the contents of each section within the detailed description are intended to be read and understood as a collective whole, where elements of one section may pertain to and/or inform other sections. Accordingly, embodiments specifically disclosed within one section may also relate to and/or serve as additional and/or alternative embodiments in another section having the same and/or similar products, methods, and/or terminology. While certain embodiments of the present disclosure have been described in detail, with reference to specific configurations, parameters, components, elements, etcetera, the descriptions are illustrative and are not to be construed as limiting the scope of the claimed invention. Furthermore, it should be understood that for any given element of component of a described embodiment, any of the possible alternatives listed for that element or component may generally be used individually or in combination with one another, unless implicitly or explicitly stated otherwise. In addition, unless otherwise indicated, numbers expressing quantities, constituents, distances, or other measurements used in the specification and claims are to be understood as optionally being modified by the term “about” or its synonyms. When the terms “about,” “approximately,” “substantially,” or the like are used in conjunction with a stated amount, value, or condition, it may be taken to mean an amount, value or condition that deviates by less than 20%, less than 10%, less than 5%, less than 1%, less than 0.1%, or less than 0.01% of the stated amount, value, or condition. At the very least, and not as an attempt to limit the application of the doctrine of equivalents to the scope of the claims, each numerical parameter should be construed in light of the number of reported significant digits and by applying ordinary rounding techniques. Any headings and subheadings used herein are for organizational purposes only and are not meant to be used to limit the scope of the description or the claims. It will also be noted that, as used in this specification and the appended claims, the singular forms “a,” “an” and “the” do not exclude plural referents unless the context clearly dictates otherwise. Thus, for example, an embodiment referencing a singular referent (e.g., “widget”) may also include two or more such referents. It will also be appreciated that embodiments described herein may also include properties and/or features (e.g., ingredients, components, members, elements, parts, and/or portions) described in one or more separate embodiments and are not necessarily limited strictly to the features expressly described for that particular embodiment. Accordingly, the various features of a given embodiment can be combined with and/or incorporated into other embodiments of the present disclosure. Thus, disclosure of certain features relative to a specific embodiment of the present disclosure should not be construed as limiting application or inclusion of said features to the specific embodiment. Rather, it will be appreciated that other embodiments can also include such features. TABLES Table 1. Selected microbial species which show significantly increased or decreased compositional abundance in periodontal disease compared to control (p<0.05). The average percentage increased or decreased abundance for each microbial species when compared to a healthy control (calculated using a centered log-ratio transformation) is shown. Microbial species previously described in scientific literature as misregulated in periodontal disease are shown in bold font. TABLE 1% increase% decreaseMicrobes with increasedcompared toMicrobes with decreasedcomparedabundance in PD cohortHealthy cohortabundance in PD cohortHealthy cohortBacteroides sp. HF-5287+49%Frederiksenia canicola−47%Bacteroides zoogleoformans+47%Moraxella bovis−33%Bacteroides sp. M10+41%Mannheimia haemolytica−32%Odoribacter splanchnicus+38%Pseudoleptotrichia−32%goodfellowiiDesulfobulbus oralis+36%Streptobacillus moniliformis−32%Bacteroides caccae+35%Capnocytophaga sp. H4358−29%Desulfomicrobium orale+35%Capnocytophaga sp. H2931−29%Bacteroides sp. CBA7301+33%Moraxella catarrhalis−28%Bacteroides uniformis+33%Alysiella filiformis−28%Parabacteroides distasonis+33%Moraxella cuniculi−27%Bacteroides ovatus+32%Moraxella ovis−27%Bacteroides caecimuris+32%Moraxella bovoculi−27%Desulfovibrio fairfieldensis+26%Neisseria zoodegmatis−27%Porphyromonas gingivalis+26%Neisseria weaveri−26%Bacteroides heparinolyticus+25%Capnocytophaga−25%cynodegmiActinomyces sp. Chiba101+25%Neisseria animaloris−25%Bacteroides thetaiotaomicron+25%Cutibacterium acnes−24%Paraprevotella xylaniphila+25%Neisseria chenwenguii−23%Actinomyces howellii+25%Neisseria elongata−22%Bacteroides xylanisolvens+24%Neisseria dentiae−22%Bacteroides helcogenes+24%Kingella oralis−22%Petrimonas mucosa+24%Neisseria canis−22%Desulfovibrio desulfuricans+24%Pelistega sp. NLN63−21%Bacteroides fragilis+23%Neisseria wadsworthii−21%Bacteroides sp. A1C1+22%Moraxella osloensis−21%Treponema sp. OMZ 838+22%Capnocytophaga−21%canimorsusProteiniphilum+22%Epilithonimonas vandammei−19%saccharofermentansTreponema brennaborense+22%Lysobacter oculi−19%Treponema putidum+22%Streptococcus dysgalactiae−18%Treponema denticola+21%Riemerella anatipestifer−18%Treponema pedis+11%Capnocytophaga stomatis−17%Acidovorax monticola+11%Fusobacterium hwasookii−17%Propionibacterium freudenreichii+11%Cardiobacterium hominis−17%Treponema phagedenis+11%Acinetobacter johnsonii−17%Prevotella denticola+10%Neisseria shayeganii−16%Acidovorax sp. RAC01+10%Fusobacterium−16%pseudoperiodonticumTannerella forsythia+10%Pasteurella multocida−16% TABLE 2TABLE 2. Predictive microbes for periodontal disease, tooth resorptionand halitosis based on pairwise log-ratio microbial abundancecomparisons between healthy/control oral microbiomes and thoseof cats suffering from one of the three dental conditions. Identificationof the predictive microbes is dynamic and will evolve as thereference database and the cohorts evolve. ‘1’ indicatesthat the microbe is considered predictive of a particular dentalcondition, while ‘0’ indicates that it is not.peri-odon-resorp-badMicrobial speciestaltionbreathHaemophilus influenzae: 727100Actinomyces howellii: 52771100Saccharomyces cerevisiae: 4932100Bacteroides intestinalis: 329854100Bacteroides ovatus: 28116100Tessaracoccus timonensis: 2161816100Petrimonas mucosa: 1642646100Porphyromonas asaccharolytica: 28123100Prevotella scopos: 589437100Acinetobacter guillouiae: 106649100Corynebacterium tuberculostearicum: 38304100Acinetobacter johnsonii: 40214100Actinomyces sp. ZJ750: 2744574100Kocuria indica: 1049583100Bacteroides uniformis: 820100Salmonella enterica: 28901100Corynebacterium sp. ATCC 6931: 1487956100Neisseria polysaccharea: 489100Proteiniphilum saccharofermentans: 1642647100Actinomyces sp. Chiba101: 1851395100Odoribacter splanchnicus: 28118100Flavonifractor plautii: 292800010Tessaracoccus flavescens: 399497010Psychrobacter sp. P11G5: 1699624010Klebsiella grimontii: 2058152010Escherichia coli: 562010Ruthenibacterium lactatiformans: 1550024010Streptococcus equi: 1336010Enterocloster clostridioformis: 1531010Parabacteroides distasonis: 823010Lachnoanaerobaculum umeaense: 617123010Aeromonas sp. ASNIH1: 1636606010Acetoanaerobium sticklandii: 1511010Histophilus somni: 731110Alloprevotella sp. E39: 2133944110Cruoricaptor ignavus: 1118202110Pasteurella dagmatis: 754110Psychrobacter alimentarius: 261164110Fusobacterium pseudoperiodonticum: 2663009110Fusobacterium gonidiaformans: 849110Streptococcus dysgalactiae: 1334110Streptococcus suis: 1307110Dichelobacter nodosus: 870110Acinetobacter junii: 40215110Streptobacillus moniliformis: 34105110Wolinella succinogenes: 844110Moraxella bovoculi: 386891110Streptococcus ruminantium: 1917441110Saccharomyces eubayanus: 1080349110Psychrobacter sp. PRwf-1: 349106110Roseburia hominis: 301301110Fusobacterium periodonticum: 860110Corynebacterium xerosis: 1725110Mycoplasma felis: 33923110Leptotrichia sp. oral taxon 212: 712357110Pelistega sp. NLN63: 2652177110Streptococcus oralis: 1303110Bacteroides xylanisolvens: 371601110Moraxella osloensis: 34062110Neisseria zoodegmatis: 326523110Porphyromonas crevioricanis: 393921001Prevotella jejuni: 1177574001Campylobacter sp. RM16192: 1660080001Serratia sp. FS14: 1327989001Gemella morbillorum: 29391001Schaalia odontolytica: 1660001Acidovorax ebreus: 721785001Luteimonas granuli: 1176533001Citrobacter sp. RHBSTW-00599: 2742657001Brucella intermedia: 94625001Acidovorax sp. KKS102: 358220001Treponema pedis: 409322001Stenotrophomonas sp. SXG-1: 2682487001Bordetella genomo sp. 6: 463024001Streptococcus intermedius: 1338001Diaphorobacter aerolatus: 1288495001Filifactor alocis: 143361001Pseudoxanthomonas spadix: 415229001Citrobacter sp. RHBSTW-00570: 2742655001Pseudomonas stutzeri: 316001Arsenicicoccus sp. oral taxon 190: 1658671001Thermomonas sp. HDW16: 2714945001Treponema phagedenis: 162001Acetobacter pasteurianus: 438001Xanthomonas sacchari: 56458001Bordetella parapertussis: 519001Citrobacter sp. RHBSTW-00229: 2742641001Xanthomonas hortorum: 56454001Pseudoxanthomonas mexicana: 128785001Xanthomonas euvesicatoria: 456327001Pseudomonas otitidis: 319939001Acidovorax sp. JS42: 232721001Treponema sp. OMZ 838: 1539298001Polaromonas sp. JS666: 296591001Ralstonia mannitolilytica: 105219001Thermomonas sp. XSG: 2771436001Pseudomonas plecoglossicida: 70775001Prevotella oris: 28135001Variovorax boronicumulans: 436515001Comamonas serinivorans: 1082851001Enterobacter cloacae complex sp. FDA-CDC-001AR_0132: 2077137Stenotrophomonas indicatrix: 2045451001Micrococcus luteus: 1270001Polaromonas naphthalenivorans: 216465001Selenomonas sputigena: 69823001Hydrogenophaga sp. NH-16: 2184519001Acidovorax sp. T1: 1858609001Treponema denticola: 158001Ralstonia pseudosolanacearum: 1310165001Citrobacter sp. RHB36-C18: 2742627001Mycoplasma arginini: 2094001Acidovorax carolinensis: 553814001Pseudomonas denitrificans (nom. rej.): 43306001Eubacterium callanderi: 53442001Xanthomonas citri: 346001Stenotrophomonas maltophilia: 40324001Achromobacter ruhlandii: 72557001Paracoccus yeei: 147645001Hydrogenophaga sp. PBC: 795665001Xanthomonas translucens: 343001Xanthomonas arboricola: 56448001Cutibacterium acnes: 1747001Streptococcus pseudoporcinus: 361101001Xanthomonas campestris: 339001Campylobacter showae: 204001Alcaligenes faecalis: 511001Lysobacter enzymogenes: 69001Acidovorax sp. 16-35-5: 2743470001Streptococcus milleri: 33040001Achromobacter xylosoxidans: 85698001Prevotella intermedia: 28131001Acidovorax sp. HDW3: 2714923001Streptococcus anginosus: 1328001Diaphorobacter sp. JS3050: 2735554001Streptomyces sp. 2114.2: 1881022001Aeromonas caviae: 648001Comamonas sp. NLF-7-7: 2597701001Citrobacter pasteurii: 1563222001Enterobacter sp. DSM 30060: 2747372001Bordetella hinzii: 103855001Stenotrophomonas rhizophila: 216778001Serpentinomonas raichei: 1458425001Thermomonas brevis: 215691001Enterobacter sp. RHBSTW-00593: 2742656001Variovorax sp. HW608: 1034889001Luteimonas sp. YGD11-2: 2508168001Serpentinomonas mccroryi: 1458426001Comamonas thiooxydans: 363952001Lysobacter maris: 1605891001Campylobacter rectus: 203001Fusobacterium necrophorum: 859001Bacteroides cellulosilyticus: 246787001Serratia sp. LS-1: 2485839001Aerosticca soli: 2010829001Stenotrophomonas nitritireducens: 83617001Pulveribacter suum: 2116657001Delftia tsuruhatensis: 180282001Phocaeicola dorei: 357276001Achromobacter spanius: 217203001Stenotrophomonas acidaminiphila: 128780001Melaminivora sp. SC2-9: 2109913001Citrobacter sp. CF971: 2566012001Citrobacter freundii: 546001Comamonas aquatica: 225991001Hydrogenophaga sp. PBL-H3: 434010001Propioniciclava sp. HDW11: 2714937001Comamonas testosteroni: 285001Treponema brennaborense: 81028001Treponema putidum: 221027001Alicycliphilus denitrificans: 179636001Thauera hydrothermalis: 2184083001Treponema socranskii: 53419001Enterobacter roggenkampii: 1812935001Proteus terrae: 1574161001Bacteroides fragilis: 817101Desulfobulbus oralis: 1986146101Neisseria canis: 493101Bacteroides caecimuris: 1796613101Cardiobacterium hominis: 2718101Lysobacter oculi: 2698682101Prevotella denticola: 28129101Pseudoleptotrichia goodfellowii: 157692101Ottowia sp. oral taxon 894: 1658672101Capnocytophaga sputigena: 1019101Hydrogenophaga sp. BA0156: 2716225101Tannerella forsythia: 28112101Capnocytophaga cynodegmi: 28189101Neisseria subflava: 28449101Conchiformibius steedae: 153493101Desulfovibrio sp. G11: 631220101Desulfovibrio fairfieldensis: 44742101Porphyromonas gingivalis: 837101Actinomyces israelii: 1659101Neisseria chenwenguii: 1853278101Neisseria meningitidis: 487101Neisseria mucosa: 488101Neisseria elongata: 495101Neisseria shayeganii: 607712101Bacteroides heparinolyticus: 28113101Neisseria bacilliformis: 267212101Lautropia mirabilis: 47671101Eikenella corrodens: 539101Desulfomicrobium orale: 132132101Bacteroides caccae: 47678101Neisseria dentiae: 194197101Alysiella filiformis: 194196101Neisseria sicca: 490101Streptococcus canis: 1329011Gemella sp. oral taxon 928: 1785995011Aerococcus sanguinicola: 119206011Ottowia oryzae: 2109914011Treponema sp. Marseille-Q4132: 2766701011Parvimonas micra: 33033011Treponema sp. OMZ 804: 120683011Pseudopropionibacterium propionicum: 1750011Bacteroides sp. HF-5287: 2650157111Actinobacillus pleuropneumoniae: 715111Capnocytophaga sp. H4358: 1945658111Capnocytophaga canimorsus: 28188111Rodentibacter pneumotropicus: 758111Glaesserella sp. 15-184: 2030797111Riemerella anatipestifer: 34085111Bacteroides zoogleoformans: 28119111Mannheimia sp. USDA-ARS-USMARC-1111261: 1432056Moraxella cuniculi: 34061111Neisseria animaloris: 326522111Haemophilus haemolyticus: 726111Mannheimia pernigra: 111844111Neisseria weaveri: 28091111Frederiksenia canicola: 123824111Pasteurella multocida: 747111Capnocytophaga sp. H2931: 1945657111Neisseria musculi: 1815583111Avibacterium paragallinarum: 728111Actinobacillus indolicus: 51049111Glaesserella parasuis: 738111Aggregatibacter aphrophilus: 732111Moraxella catarrhalis: 480111Capnocytophaga stomatis: 1848904111Moraxella ovis: 29433111Neisseria wadsworthii: 607711111Rodentibacter heylii: 1906744111TOTAL10874182 TABLE 3TABLE 3. Demographic statistics forthe cohorts of felines in Study 3.AverageGenderCohortagesplit (M/F)Gingivitis (no radiographic evidence of bone loss)6 years2/8Periodontal disease (radiographic evidence of bone9 years6/5loss)Tooth resorption (with radiographic evidence)9 years7/8Healthy - owner provided information4 years10/5 CONCLUSION While the foregoing detailed description makes reference to specific exemplary embodiments, the present disclosure may be embodied in other specific forms without departing from its spirit or essential characteristics. Accordingly, the described embodiments are to be considered in all respects only as illustrative and not restrictive. For instance, various substitutions, alterations, and/or modifications of the inventive features described and/or illustrated herein, and additional applications of the principles described and/or illustrated herein, which would occur to one skilled in the relevant art and having possession of this disclosure, can be made to the described and/or illustrated embodiments without departing from the spirit and scope of the disclosure as defined by the appended claims. Such substitutions, alterations, and/or modifications are to be considered within the scope of this disclosure. The scope of the invention is, therefore, indicated by the appended claims rather than by the foregoing description. The limitations recited in the claims are to be interpreted broadly based on the language employed in the claims and not limited to specific examples described in the foregoing detailed description, which examples are to be construed as non-exclusive and non-exhaustive. All changes which come within the meaning and range of equivalency of the claims are to be embraced within their scope. It will also be appreciated that various features of certain embodiments can be compatible with, combined with, included in, and/or incorporated into other embodiments of the present disclosure. For instance, systems, methods, and/or products according to certain embodiments of the present disclosure may include, incorporate, or otherwise comprise features described in other embodiments disclosed and/or described herein. Thus, disclosure of certain features relative to a specific embodiment of the present disclosure should not be construed as limiting application or inclusion of said features to the specific embodiment. In addition, unless a feature is described as being requiring in a particular embodiment, features described in the various embodiments can be optional and may not be included in other embodiments of the present disclosure. Moreover, unless a feature is described as requiring another feature in combination therewith, any feature herein may be combined with any other feature of a same or different embodiment disclosed herein. It will be appreciated that while features may be optional in certain embodiments, when features are included in such embodiments, they can be required to have a specific configuration as described in the present disclosure. Likewise, any steps recited in any method or process described herein and/or recited in the claims can be executed in any suitable order and are not necessarily limited to the order described and/or recited, unless otherwise stated (explicitly or implicitly). Such steps can, however, also be required to be performed in a specific order or any suitable order in certain embodiments of the present disclosure. Furthermore, various well-known aspects of illustrative systems, methods, products, and the like are not described herein in particular detail in order to avoid obscuring aspects of the example embodiments. Such aspects are, however, also contemplated herein.",en,PATENT_APPLICATION
017-195-007-149-343,US,20240384395,A1,2024-11-21,US_20240384395_A1_20241121,en,US,20240384395,A1,2024-11-21,US,18785244,2024-07-26,LOW CARBON DEFECT COPPER-MANGANESE SPUTTERING TARGET AND METHOD FOR PRODUCING THE SAME,en,US,"Tosoh SMD, Inc.","Grove City, OH",US,Eduardo del Rio Perez,"Dublin, OH",US,1,Erich Walter Theado,"Columbus, OH",US,2,Lora Birkefeld Thrun,"Grove City, OH",C23C14/34,I,F,C22C1/02,I,L,C22C9/00,I,L,C22C9/05,I,L,H01J37/34,I,L,C23C14/3414,I,F,C22C9/00,I,L,C22C9/05,I,L,H01J37/3429,I,L,C22C1/02,A,L,US,20240384395,A1,2024-11-21,017-195-007-149-343,1,US,20240384395,A1,2024-11-21,017-195-007-149-343,1,UNKNOWN,"Provided is a low carbon defect copper-manganese (CuMn) sputtering target and systems and methods for producing the same. The low carbon defect CuMn sputtering target may comprise of copper with a purity of at least about 99.9999%, manganese with a purity of about 99.9% to about 99.999%, and one or more active elements comprising of oxygen (O) at about 100 parts per million (ppm) to about 4000 ppm, iron (Fe) at about 5 parts per billion (ppb) to about 100 ppm, sulfur (S) at about 5 ppm to about 400 ppm, hydrogen (H) at about 1 ppm to about 10 ppm, and chromium (Cr) at about 5 ppb to about 200 ppm, wherein the manganese has a compositional range of up to about 5 wt %.",en,"1 . A vacuum induction melting (VIM) furnace comprising: a controller; and memory storing executable code when executed by the controller performs actions comprising: receiving predetermined parameters for the creation of a low carbon defect Cu—Mn ingot using an I/O device of the VIM furnace; pausing for a charging of raw material, the raw material comprising of copper (Cu) with a purity of at least about 99.9999% and an alloy addition, said alloy addition comprising: manganese (Mn) with a purity of about 99.9% to about 99.999%, and one or more active elements; and the one or more active elements including one or more of oxygen (O) at about 100 parts per million (ppm) to about 4000 ppm, iron (Fe) at about 5 parts per billion (ppb) to about 100 ppm, sulfur (S) at about 5 ppm to about 400 ppm, hydrogen (H) at about 1 ppm to about 10 ppm, and chromium (Cr) at about 5 ppb to about 200 ppm, wherein the manganese has a compositional range of up to about 5 wt %; pumping down a chamber of the VIM furnace using vacuum pump of the VIM furnace; melting the raw materials in a crucible using an induction coil and a temperature sensor of the VIM furnace, such that the raw materials in the crucible form a melt having a predetermined temperature value; maintaining the predetermined temperature value of the melt using the induction coil and temperature sensor until a predetermined soak time has elapsed; and casting an ingot by pouring the melt into a mold.","2 . The VIM furnace of claim 1 , wherein the code when executed by said controller performs additional actions comprising: charging the alloy addition and Cu into the crucible prior to pumping down the chamber of the VIM furnace.","3 . The VIM furnace of claim 1 , wherein the code when executed by said controller performs additional actions comprising: pumping down the chamber of the VIM furnace after the Cu is charged into the crucible and the alloy addition is charged into a dissolution device.","4 . The VIM furnace of claim 3 , wherein the code when executed by said controller performs additional actions comprising: dispensing the alloy addition, using the dissolution device, into the crucible after the Cu has melted, wherein the alloy addition is directionally dispensed into a stirring wake of the melted Cu.","5 . The VIM furnace of claim 4 , wherein the code when executed by said controller performs additional actions comprising: wherein the alloy addition is dispensed at a rate of about 17 grams/second (g/s) to about 167 g/s.","6 . The VIM furnace of claim 1 , wherein the code when executed by said controller performs additional actions comprising: wherein the predetermined soak time value is about 30 minutes (min) to about 120 min.",en,"CROSS REFERENCE TO RELATED APPLICATIONS This application is a divisional of U.S. Utility application Ser. No. 18/120,358 filed on Mar. 10, 2023, entitled “Low Carbon Defect Copper-Manganese Sputtering Target and Method for Producing the Same” which claims priority to U.S. Provisional Patent Application No. 63/318,775, filed on Mar. 10, 2022, entitled “Low Carbon Defect Copper-Manganese Sputtering Target and Method for Producing the Same”, each of which are incorporated herein by reference in their entireties. TECHNICAL FIELD This application relates to a low carbon defect copper-manganese sputtering target and method for producing the same and, more particularly, to a vacuum induction method for producing the low carbon defect copper-manganese sputtering target. BACKGROUND The ongoing development to further shrink features in semiconductor integrated systems while improving performance required in the advanced 7 nm, 5 nm, 3 nm and beyond nodes require a reduction of defects generated during the thin film deposition process. Specifically, carbon-based defects, either as pure carbon and allotropes or as carbon-based compounds (e.g., carbides) encountered during the deposition in copper-manganese (CuMn) films cause detrimental performance, affecting the adhesion of the CuMn seed layer to the Tantalum Nitride/Tantalum (TaN/Ta) barrier. Since the development of the CuMn self-forming barrier and seed by Usui et al. (2005) and Nogami et al. (2010) for copper metal interconnects, semiconductor factories and chip designers adopted CuMn as the preferred alloy for seed layers prior to copper metallization in integrated circuits. SUMMARY The following presents a summary of this disclosure to provide a basic understanding of some aspects. This summary is intended to neither identify key or critical elements nor define any limitations of embodiments or claims. This summary may provide a simplified overview of some aspects that may be described in greater detail in other portions of this disclosure. Furthermore, any of the described aspects may be isolated or combined with other described aspects without limitation to the same effect as if they had been described separately and in every possible combination explicitly. In one aspect of the invention, a low carbon defect copper-manganese (CuMn) sputtering target is made of copper (Cu) with a purity of at least about 99.9999%; and an alloy addition. The alloy addition includes manganese (Mn) with a purity of about 99.9% to about 99.999%, with the manganese having a compositional range of up to about 5 wt %. The alloy addition also includes one or more active elements. The active elements include one or more of oxygen (O) at about 100 parts per million (ppm) to about 4000 ppm, iron (Fe) at about 5 parts per billion (ppb) to about 100 ppm, sulfur (S) at about 5 ppm to about 400 ppm, hydrogen (H) at about 1 ppm to about 10 ppm, and chromium (Cr) at about 5 ppb to about 200 ppm. In another aspect of the invention, the low carbon defect CuMn sputtering target has a purity of at least about 99.999%. In another aspect of the invention, the Cu and the alloy addition are charged into a crucible prior to melting the Cu, thereby forming a molten bath. In another aspect of the invention, the alloy addition is added as a late addition by a dissolution device after the Cu has melted, the melted Cu forming a molten bath to which the alloy addition is added later. In another aspect of the invention, the alloy addition is directionally dispensed by the dissolution device into a stirring wake of the molten bath in a crucible during a creation of a low carbon defect CuMn ingot used to form the sputtering target. In another aspect of the invention, the dissolution device dispenses the alloy addition at a dissolution rate of about 17 grams/second (g/s) to about 167 g/s. In another aspect of the invention, a resulting molten bath of the Cu and the alloy addition is homogenized for about 30 minutes (min) to about 120 min. In yet another aspect, a method of forming a low carbon defect copper-manganese (CuMn) sputtering target includes, selecting raw material comprising of copper (Cu) with a purity of at least about 99.9999% and an alloy addition. The alloy addition including manganese (Mn) with a purity of about 99.9% to about 99.999%, and one or more active elements. The one or more active elements including one or more of oxygen (O) at about 100 parts per million (ppm) to about 4000 ppm, iron (Fe) at about 5 parts per billion (ppb) to about 100 ppm, sulfur (S) at about 5 ppm to about 400 ppm, hydrogen (H) at about 1 ppm to about 10 ppm, and chromium (Cr) at about 5 ppb to about 200 ppm, wherein the manganese has a compositional range of up to about 5 wt %. The method further including, melting the raw material into a molten alloy, casting the molten alloy into an ingot, thermomechanical processing the ingot into a target blank; and assembling the target blank into a sputtering target by joining the target blank onto a backing plate. In another aspect of the invention, the method further includes charging the alloy addition and the Cu into a crucible. In another aspect of the invention, the method further includes, adding the alloy addition as a late addition by a dissolution device into a molten Cu, thereby forming the molten alloy. In another aspect of the invention, the method further includes, dispensing, by the dissolution device, the alloy addition into a stirring wake of the molten Cu. In another aspect of the invention, the method further includes, dispensing of the alloy addition is at a dissolution rate of about 17 grams/second (g/s) to about 167 g/s. In another aspect of the invention, the method further includes, homogenizing a resulting molten bath of the Cu and the alloy addition for about 30 minutes (min) to about 120 min. In yet another aspect of the invention, a vacuum induction melting (VIM) furnace includes a controller and memory storing executable code when executed by the controller performs actions including, receiving predetermined parameters for the creation of a low carbon defect Cu—Mn ingot using an I/O device of the VIM furnace; pausing for a charging of raw material, the raw material comprising of copper (Cu) with a purity of at least about 99.9999% and an alloy addition, said alloy addition including manganese (Mn) with a purity of about 99.9% to about 99.999%, and one or more active elements. The one or more active elements including one or more of oxygen (O) at about 100 parts per million (ppm) to about 4000 ppm, iron (Fe) at about 5 parts per billion (ppb) to about 100 ppm, sulfur (S) at about 5 ppm to about 400 ppm, hydrogen (H) at about 1 ppm to about 10 ppm, and chromium (Cr) at about 5 ppb to about 200 ppm, wherein the manganese has a compositional range of up to about 5 wt %; pumping down a chamber of the VIM furnace using vacuum pump of the VIM furnace; melting the raw materials in a crucible using an induction coil and a temperature sensor of the VIM furnace, such that the raw materials in the crucible form a melt having a predetermined temperature value; maintaining the predetermined temperature value of the melt using the induction coil and temperature sensor until a predetermined soak time has elapsed; and casting an ingot by pouring the melt into a mold. In another aspect of the invention, the code when executed by the controller performs additional actions including, charging the alloy addition and Cu into the crucible prior to pumping down the chamber of the VIM furnace. In another aspect of the invention, the code when executed by the controller performs additional actions including, pumping down the chamber of the VIM furnace after the Cu is charged into the crucible and the alloy addition is charged into a dissolution device. In another aspect of the invention, wherein the code when executed by the controller performs additional actions including, dispensing the alloy addition, using the dissolution device, into the crucible after the Cu has melted, wherein the alloy addition is directionally dispensed into a stirring wake of the melted Cu. In another aspect of the invention, wherein the code when executed by the controller performs additional actions including, dispensing the alloy addition at a rate of about 17 grams/second (g/s) to about 167 g/s. In another aspect of the invention, wherein the code when executed by the controller performs additional actions including, wherein the predetermined soak time value is about 30 minutes (min) to about 120 min. According to an aspect of the embodiments disclosed herein, a low carbon defect copper-manganese (CuMn) sputtering target is provided. The low carbon defect CuMn sputtering target may comprise of copper (Cu) with a purity of at least about 99.9999% and an alloy addition. The alloy addition may comprise of manganese (Mn) with a purity of about 99.9% to about 99.999%, wherein the manganese has a compositional range of up to about 5 wt %, and one or more active elements, where the active elements include one or more of oxygen (O) at about 100 parts per million (ppm) to about 4000 ppm, iron (Fe) at about 5 parts per billion (ppb) to about 100 ppm, sulfur (S) at about 5 ppm to about 400 ppm, hydrogen (H) at about 1 ppm to about 10 ppm, and chromium (Cr) at about 5 ppb to about 200 ppm. According to another aspect of the embodiments disclosed herein, a method of producing a CuMn sputtering target is provided. The method may comprise selecting raw material comprising of copper (Cu) with a purity of at least about 99.9999% and a combination of alloy addition comprising: manganese (Mn) with a purity of about 99.9% to about 99.999% and one or more active elements, the active elements including one or more of oxygen (O) at about 100 parts per million (ppm) to about 4000 ppm, iron (Fe) at about 5 parts per billion (ppb) to about 100 ppm, sulfur (S) at about 5 ppm to about 400 ppm, hydrogen (H) at about 1 ppm to about 10 ppm, and chromium (Cr) at about 5 ppb to about 200 ppm, wherein the manganese has a compositional range of up to about 5 wt %. The method may further comprise melting the raw material into a molten alloy, casting the molten alloy into an ingot, thermomechanical processing the ingot into a target blank, and assembling the target blank into a sputtering target by joining the target blank onto a backing plate. According to another aspect of the embodiments disclosed herein, a system is provided. The system may comprise a processor coupled to a memory that stores processes executable by the processor. The processes may comprise determining whether a molten bath of copper (Cu) requires a late addition of alloy addition comprising manganese and one or more active elements. The processes may further comprise dispensing the alloy addition into the stirring wakes at a predetermined rate. The following description and the drawings disclose various illustrative aspects. Some improvements and novel aspects may be expressly identified, while others may be apparent from the description and drawings. DESCRIPTION OF THE DRAWINGS The accompanying drawings illustrate various systems, apparatuses, devices and methods in which like reference characters refer to like parts throughout, and in which: FIG. 1 illustrates an example low carbon defect CuMn ingot transformed into a low carbon defect CuMn sputtering target in accordance with various disclosed aspects herein; FIG. 2 is illustrates a vacuum induction melting furnace in accordance with various disclosed aspects herein; FIG. 3 is a flow diagram of an embodiment of a method of forming a low carbon defect CuMn sputtering target in accordance with various disclosed aspects herein; FIG. 4 is a flow diagram of an embodiment of a method of casting a low carbon defect copper-manganese (CuMn) ingot in accordance with various disclosed aspects herein; FIG. 5 illustrates a dissolution device and the directional Mn alloying addition to the stirring wake; FIG. 6 is a flow diagram of an example method of using a VIM to manufacture a low carbon defect copper-manganese (CuMn) ingot in accordance with various disclosed aspects herein; FIG. 7 is a bar graph of the defect density comparison between the traditional VIM process and the VIM process disclosed herein; and FIG. 8 is an ultrasonic scan visual comparison between the traditional VIM process and the VIM process disclosed herein. The disclosed embodiments may be embodied in several forms without departing from its spirit or essential characteristics. The scope of the invention is defined in the appended claims, rather than in the specific description preceding them. All embodiments that fall within the meaning and range of equivalency of the claims are therefore intended to be embraced by the claims. DETAILED DESCRIPTION Reference will now be made to exemplary embodiments, examples of which are illustrated in the accompanying drawings. It is to be understood that other embodiments may be utilized and structural and functional changes may be made. Moreover, features of the various embodiments may be combined or altered. As such, the following description is presented by way of illustration only and should not limit in any way the various alternatives and modifications that may be made to the illustrated embodiments. In this disclosure, numerous specific details provide a thorough understanding of the subject disclosure. It should be understood that aspects of this disclosure may be practiced with other embodiments not necessarily including all aspects described herein, etc. Approximating language, as used herein throughout the specification and claims, may be applied to modify any quantitative representation that could permissibly vary without resulting in a change in the basic function to which it is related. Accordingly, a value modified by a term or terms, such as “about”, is not limited to the precise value specified. In at least some instances, the approximating language may correspond to the precision of an instrument for measuring the value. Range limitations may be combined and/or interchanged, and such ranges are identified and include all the sub-ranges stated herein unless context or language indicates otherwise. Other than in the operating examples or where otherwise indicated, all numbers or expressions referring to quantities of ingredients, reaction conditions and the like, used in the specification and the claims, are to be understood as modified in all instances by the term “about”. “Optional” or “optionally” means that the subsequently described event or circumstance may or may not occur, or that the subsequently identified material may or may not be present, and that the description includes instances where the event or circumstance occurs or where the material is present, and instances where the event or circumstance does not occur or the material is not present. As used herein, the terms “comprises”, “comprising”, “includes”, “including”, “has”, “having”, or any other variation thereof, are intended to cover a non-exclusive inclusion. For example, a process, method, article or apparatus that comprises a list of elements is not necessarily limited to only those elements, but may include other elements not expressly listed or inherent to such process, method, article, or apparatus. The singular forms “a”, “an”, and “the” include plural referents unless the context clearly dictates otherwise. A “processor”, as used herein, processes signals and performs general computing and arithmetic functions. Signals processed by the processor can include digital signals, data signals, computer instructions, processor instructions, messages, a bit, a bit stream, or other means that can be received, transmitted and/or detected. Generally, the processor can be a variety of various processors including multiple single and multicore processors and co-processors and other multiple single and multicore processor and co-processor architectures. The processor can include various modules to execute various functions. A “memory”, as used herein can include volatile memory and/or nonvolatile memory. Non-volatile memory can include, for example, ROM (read only memory), PROM (programmable read only memory), EPROM (erasable PROM), and EEPROM (electrically erasable PROM). Volatile memory can include, for example, RAM (random access memory), synchronous RAM (SRAM), dynamic RAM (DRAM), synchronous DRAM (SDRAM), double data rate SDRAM (DDRSDRAM), and direct RAM bus RAM (DRRAM). The memory can also include a disk. The memory can store an operating system that controls or allocates resources of a computing device. The memory can also store data for use by the processor. A “disk”, as used herein can be, for example, a magnetic disk drive, a solid state disk drive, a floppy disk drive, a tape drive, a Zip drive, a flash memory card, and/or a memory stick. Furthermore, the disk can be a CD-ROM (compact disk ROM), a CD recordable drive (CD-R drive), a CD rewritable drive (CD-RW drive), and/or a digital video ROM drive (DVD ROM). The disk can store an operating system and/or program that controls or allocates resources of a computing device. Some portions of the detailed description that follows are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here, and generally, conceived to be a self-consistent sequence of steps (instructions) leading to a desired result. The steps are those requiring physical manipulations of physical quantities. Usually, though not necessarily, these quantities take the form of electrical, magnetic or optical non-transitory signals capable of being stored, transferred, combined, compared and otherwise manipulated. It is convenient at times, principally for reasons of common usage, to refer to these signals as bits, values, elements, symbols, characters, terms, numbers, or the like. Furthermore, it is also convenient at times, to refer to certain arrangements of steps requiring physical manipulations or transformation of physical quantities or representations of physical quantities as modules or code devices, without loss of generality. However, all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the following discussion, it is appreciated that throughout the description, discussions utilizing terms such as “processing” or “computing” or “calculating” or “determining” or “displaying” or “determining” or the like, refer to the action and processes of a computer system, or similar electronic computing device (such as a specific computing machine), that manipulates and transforms data represented as physical (electronic) quantities within the computer system memories or registers or other such information storage, transmission or display devices. Certain aspects of the embodiments described herein include process steps and instructions described herein in the form of an algorithm. It should be noted that the process steps and instructions of the embodiments could be embodied in software, firmware or hardware, and when embodied in software, could be downloaded to reside on and be operated from different platforms used by a variety of operating systems. The embodiments can also be in a computer program product which can be executed on a computing system. The embodiments also relates to an apparatus for performing the operations herein. This apparatus can be specially constructed for the purposes, e.g., a specific computer, or it can comprise a general-purpose computer selectively activated or reconfigured by a computer program stored in the computer. Such a computer program can be stored in a non-transitory computer readable storage medium, such as, but is not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMS, EEPROMs, magnetic or optical cards, application specific integrated circuits (ASICs), or any type of media suitable for storing electronic instructions, and each electrically connected to a computer system bus. Furthermore, the computers referred to in the specification can include a single processor or can be architectures employing multiple processor designs for increased computing capability. The algorithms and displays presented herein are not inherently related to any particular computer or other apparatus. Various general-purpose systems can also be used with programs in accordance with the teachings herein, or it can prove convenient to construct more specialized apparatus to perform the method steps. The structure for a variety of these systems will appear from the description below. In addition, the embodiments are not described with reference to any particular programming language. It will be appreciated that a variety of programming languages can be used to implement the teachings of the embodiments as described herein, and any references below to specific languages are provided for disclosure of enablement and best mode of the embodiments. In addition, the language used in the specification has been principally selected for readability and instructional purposes, and may not have been selected to delineate or circumscribe the inventive subject matter. Accordingly, the disclosure of the embodiments is intended to be illustrative, but not limiting, of the scope of the embodiments, which is set forth in the claims. Traditional copper (Cu) alloy manufacturing methods involve melting and casting ingots using processes such as vacuum induction melting (VIM), electron beam melting (EBM), cold crucible induction melting or induction skull melting (CCIM/ISM). However, EBM and CCIM/ISM suffer from excessive copper vaporization due to the low melting point of Cu at about 1084° C. and alloy microstructure and composition non-uniformities due to the high electrical and thermal conductivity of Cu. In as much, VIM is an ideal process to produce Cu and Cu alloys, more specifically CuMn, due to the ability to control the melting process by controlling the physical properties of the molten metal alloy (e.g., melting point and vapor pressure) and produce high quality ingots. However, VIM processes have inherited potential issues with defectivity generated from the melting process in the form of secondary phase particles assimilated during melting from ancillary refractory tooling. Refractory crucibles, molds, and insulation either by reaction or homogeneous nucleation drive the formation of defects capable of increasing the defectivity count in the produced ingot. Such defects can be transferred to the production of sputtering target and consequently generate defect during sputtering. Therefore, there is a need to eliminate the defects generated from the melting process in the form of secondary phase particles from ancillary refractory tooling by reaction or homogenous nucleation. Additionally, there is a need to eliminate the defects caused by the reaction of Mn with graphite crucibles which results in Mn-based carbide compounds formed during the melting process and the carbon-based defects due to homogeneous nucleation at the refractory wall surface morphology of the graphite crucibles. The embodiments disclosed herein provide a method to produce a low carbon defectivity copper-manganese (CuMn) sputtering target with a manganese (Mn) concentration up to about 5 wt % having a purity of about 99.999% or higher. Disclosed herein is a method to melt and cast high volume CuMn alloys utilizing vacuum induction melting (VIM) capable of producing a low defect sputtering target with improved performance in comparison with traditional VIM melting/casting process. The process begins with a chemical selection of the Cu raw material, Mn raw material and an active elements combination which includes certain elements capable of producing the overall metal purity range desired for the alloy, maintain the required compositional range of up to about 5 wt % Mn, while producing the desired active defect reduction. The Cu raw material range require the overall purity to be at least about 99.9999%. The Mn raw material range require the overall purity to be about 99.9% to 99.999% (5) with the following active elements in these specific ranges: oxygen (O) a minimum of about 100 parts per million (ppm) to a maximum of about 4000 ppm, iron (Fe) a minimum of about 5 parts per billion (ppb) to a maximum of about 100 ppm, sulfur (S) a minimum of about 5 ppm to a maximum of about 400 ppm, hydrogen (H) a minimum of about 1 ppm to a maximum of about 10 ppm, and chromium (Cr) a minimum of about 5 ppb to a maximum of about 200 ppm. These elements, alone and/or in combination, act as carbon getters to form evaporable gaseous forms of CO x from the melt during the casting process. The Mn and one or more of the active elements (e.g., carbon getters) may be added as a late addition after the Cu has been charged and melted. The Mn and the active elements may be added as a powder, flake, sponge, chunks, ingot, or a combination thereof. The active role of these elements (e.g., carbon getters) was proven with the reduction of defects in sputtering targets. The Mn and one or more of the active elements may be added as an alloy addition charged in the crucible along with the Cu raw material. The alloy addition may also be added to the crucible later as a late addition. The disclosed VIM method is an improvement over the traditional VIM process by enabling the utilization of known refractory ancillary equipment like graphite, magnesium oxide (MgO), aluminum oxide (Al 2 O 3 , zirconium dioxide (ZrO 2 ) as well as refractory metals like tantalum (Ta), tungsten (W), and molybdenum (Mo). Refractory ancillary equipment, refractory metals, and refractory tools made of graphite is the preferred embodiment as carbon getters are utilized to remove carbon-based defects. However, the embodiments disclosed herein is designed to eliminate defects caused by reaction of the late alloy addition with the crucible walls so that other types of refractory ancillary equipment, refractory metals, and refractory tools may also be used. This method eliminates the potential generation of defects caused by the reaction of the alloying Mn with the crucible walls or by physical breakdown of crucible material with the melt. Specifically, the reaction of Mn with graphite crucibles results in Mn-based carbide compounds formed during melting and carbon-based defects due to homogeneous nucleation at the refractory wall surface morphology of graphite crucibles. This improved process employs a custom-designed Programmable Logic Controller (PLC) controlled process, and a device that maintains a time-based Mn alloying addition defined for each alloy composition. The alloying addition comprises of Mn and an active element or a combination of the active elements (e.g., carbon getters). The PLC-controlled Mn alloying directionality inserts the alloying addition away from the crucible wall and into the one or more electromagnetically induced “stirring wakes” generated by the VIM process. The one or more stirring wakes are induced away from the crucible wall and towards the center of the crucible, such that they are not located by the crucible wall. This keeps the Mn away from the crucible wall, thereby preventing the introduction of carbide impurities by the Mn contacting the crucible wall. The PLC program, alloying addition, and dispersion device are synchronously tuned to produce a uniform dissolution of alloy addition with a predetermined dispensing rate between about 17 to about 167 gram/second (g/s). Stated alternatively, the PLC program, using the dispersion device, introduces the alloying addition into the stirring wakes of the melt at a predetermined rate of about 17 to about 167 grams/second (g/s). The disclosed VIM method produces a measurable improvement in defect levels of about 50% or greater in comparison with traditional VIM process. This improvement is evaluated utilizing ultrasonic defect inspection described in United States Patent Number U.S. Pat. No. 6,739,196 B2. Turning to FIG. 1 , which illustrates a low carbon defect CuMn sputtering target 130 (e.g., sputtering target 130 ) that may be produced by the methods disclosed herein to achieve low carbon defectivity as compared to the traditional VIM process. The disclosed methods may comprise selecting raw materials to melt and cast utilizing a VIM process that employs a Programmable Logic Controller (PLC) process to add the alloy addition at a predetermined rate and position. The raw material may comprise of Cu and an alloy comprising of Mn and an active element or a combination of active elements (e.g., carbon getters) such as, one or more of, O, Fe, S, H, and Cr. The VIM process may begin with charging the Cu into a crucible 210 located inside a chamber 202 of a VIM furnace 200 , as shown in FIG. 2 . The chamber 202 may be closed and processing parameters for the VIM furnace 200 such as chamber pressure value, melt temperature value, and soak time value may be set for the casting process. Setting the chamber pressure value allows air to be removed from the chamber 202 using vacuum pump 225 . The chamber pressure value may range from about 0 torr (vacuum in Table I below) to about 500 torr. The temperature value of the melt in the crucible 210 of the VIM furnace 200 may set at about 1100° C. to about 1400° C. A soak time of about 20 minutes (min) to about 120 min may be set to let the molten metal or molten metal alloy homogenize. The alloy addition may be charged into the crucible along with the raw Cu material or the alloy addition may be later as a late addition, such as by using dissolution device 212 . A dissolution device 212 may be operatively positioned above the crucible 210 for dispensing the alloy addition into the crucible 210 . The dissolution device 212 may be controlled by the PLC process to dispense the alloy addition into the molten metal of melted Cu to achieve a predetermined dissolution (dispensing) rate of about 17 grams/second (g/s) to about 167 g/s and direct the insertion of the alloy addition away from a crucible wall 214 . The alloy addition may be added to the dissolution device 212 during setup (e.g., charging the crucible 210 with raw materials and setting the processing parameters) for automatic dispensing if it is determined that a late addition is required. The molten bath comprising of Cu, Mn, and a carbon getter or a combination of carbon getters (e.g., active elements) may be homogenized (e.g., soaked) for a predetermined period of time (e.g., soak time, hold time, etc.). After the molten bath (melt) has soaked for a predetermined length of time at the predetermined temperature, the molten bath (melt) may be poured into a mold 220 to cast an ingot 100 , as illustrated in FIG. 1 . The ingot 100 as produced by the method disclosed herein may have fewer defects 102 a compared to the traditional VIM process, and thus, the resulting sputtering target 130 may also have fewer defects 102 b. The amount (e.g. number and severity) of defects in the ingot 100 determines the amount of defects in the sputtering target 130 , since the ingot 100 is formed into the sputtering target 130 . Therefore, decreasing the amount of defects 102 a in the ingot 100 also decreases the amount of defects 102 b in the resulting sputtering target 130 . The ingot 100 may be thermomechanically processed and assembled into the sputtering target 130 . The thermomechanical processing may compress the ingot 100 along a dimension 104 to form a disc-shaped target blank 110 . The defects 102 a from the ingot 100 are retained in the target blank 110 as defects 102 b. The target blank 110 may be joined to a backing plate 120 to produce sputtering target 130 , which also retains the defects 102 b. For this reason, the selection of the raw material and the casting process of the ingot 100 may be essential for the quality of the sputtering target 130 . The ingot 100 , and therefore the sputtering target 130 as well, should be composed of a CuMn metal alloy comprising of Mn at a concentration of up to about 5 wt % with an overall CuMn metal alloy purity of about 99.999% or higher. The VIM furnace 200 also includes induction coil 216 for heating the melt within crucible 210 and creating stirring wakes within the melt. VIM furnace 200 also includes a temperature sensor 250 for monitoring the temperature of the melt within crucible 210 . VIM furnace 200 also includes a PLC 230 having a memory 235 and a processor 240 . A user enters parameter values for the VIM furnace 200 using the I/O device 245 that interfaces with the PLC. The I/O device 245 also permits a user to monitor the status of the VIM furnace 200 . The parameter values are stored in the memory of PLC 230 and executed by processor 240 of PLC 230 . The parameters and parameter values may include those listed below in Table I. Stated alternatively, the parameter values stored in the memory 235 of PLC 230 may include, but are not limited to, chamber pressure, allot addition method, temperature, and soak time, as well as whether the dissolution device 212 will be used to add the alloy additions (e.g. whether there will be late additions), and the dispensing (dispersion rate) of the dissolution device 212 when used. With reference to FIG. 3 , an exemplary method 300 is provided for producing the low carbon defect sputtering target 130 . At step 302 , the method 300 may comprise of selecting raw materials. The raw material may comprise of Cu and an alloy addition. The Cu raw material require a purity range of at least about 99.9999%. The alloy addition may comprise of Mn and a reactive element or a combination of reactive elements, which acts as carbon getters that forms evaporable gaseous forms of CO x from the melt during the casting process. The Mn may have a compositional range of up to about 5 wt % with a purity of about 99.9% to 99.999%. The active elements (e.g., carbon getters) may comprise of either O a minimum of about 100 parts per million (ppm) to a maximum of about 4000 ppm, Fe a minimum of about 5 ppb to a maximum of about 100 ppm, S a minimum of about 5 ppm to a maximum of about 400 ppm, H a minimum of about 1 ppm to a maximum of about 10 ppm, or Cr a minimum of about 5 ppb to a maximum of about 200 ppm, or a combination thereof. The Mn and these active elements (e.g., carbon getters) may come in various forms such as, but not limited to powder, flake, sponge, chunks, and ingot. At step 304 , the method 300 may comprise of melting the selected raw material. The Cu may be charged into the crucible 210 and melted at a predetermined temperature. The alloy addition of Mn and an active element or a combination of active elements (e.g., one or more carbon getters) may be added as an alloy addition to form a homogenized molten bath of molten alloy. The alloy addition may charged into the crucible 210 along with the Cu raw material, or alternatively, the alloy addition may be added to the dissolution device 212 for automatic dispensing of the alloy addition at a later time as a late addition. The dissolution device 212 may programmed to dispense the alloy addition into the crucible 210 away from the crucible wall 214 at a predetermined rate of about 17 g/s to about 167 g/s. Once the molten bath (melt) of raw material (Cu and alloy addition) reaches the predetermined temperature, the melt is permitted to soak for a predetermined length of time. In embodiments where the dissolution device 212 is used, the molten bath is permitted to reach the predetermined temperature prior to the dissolution device 212 dispersing the alloy addition into the molten bath (Cu melt). The dissolution device is controlled by the PLC 230 and directionally dispenses the alloy addition in a direction away from the crucible wall 214 , such as into a stirring wake of the molten bath. Stated alternatively, the molten bath may be homogenized (e.g., soak) for a predetermined length of time (e.g. about 20 min to about 120 min) prior to casting the molten allot into the ingot 100 . At step 306 , the method 300 may comprise of casting the molten alloy into the ingot 100 once the soak time of the molten alloy has elapsed. At step 308 , the method 300 may comprise of thermomechanical processing the ingot 100 into the target blank 110 whereby the ingot 100 may be compressed along the dimension 104 into a disc shape. At step 310 , the method 300 may comprise of assembling the target blank 100 into the sputtering target 130 by joining the target blank 110 to the backing plate 120 . The resulting sputtering target 130 may be a low carbon defect CuMn sputtering target with a compositional range of up to about 5 wt % Mn and an overall purity of about 99.999% or higher. FIG. 4 further illustrates an example casting process 400 to cast the selected raw materials into the ingot 100 . The selected raw materials may comprise of Cu having an overall purity of at least about 99.9999% and an alloy addition comprising of Mn with a concentration up to about 5 wt % (e.g., compositional range of up to about 5 wt %) having an overall purity of about 99.9% to about 99.999% and one or more active elements (e.g., carbon getters) comprising of O at a minimum of about 100 ppm to a maximum of about 4000 ppm, Fe at a minimum of about 5 ppb to a maximum of about 100 ppm, S at a minimum of about 5 ppm to about a maximum of about 400 ppm, H at a minimum of about 1 ppm to a maximum of about 10 ppm, or Cr at a minimum of about 5 ppb to a maximum of about 200 ppm. Thus, the ingot 100 will have an Mn content between about 0 wt %-about 5 wt %. At step 402 , the casting process 400 may comprise of charging the selected raw material of Cu into the crucible 210 . The crucible 210 may be wrapped with induction coil 216 , which is positioned to induce 200 stirring wakes directed away from the crucible walls 214 . At this point, in an example preferred embodiment, the alloy addition (e.g., late addition) may be placed in the dissolution device 212 for inclusion as a late addition into the melt, and the predetermined processing parameters may be set. In other embodiments, the alloy addition may be charged into the crucible 210 along with the raw Cu material and the processing parameters may be set. Table I further depicts the processing parameters for a preferred embodiment and additional embodiments Alt A-E (e.g., Alternative A-E). These processing parameters may be entered into the PLC 230 using the I/O device 245 , stored in the memory 235 of the PLC, and executed using the processor 240 of the PLC. It is contemplated that the PLC 230 may be any suitable controller. TABLE IProcessing ParametersAlloySoakPressureAdditionTemperatureTimeConditions(Torr)Method(C.)(Min)Preferred50-300Late addition1250 +/− 10030-60Alt A100-500Late addition1250 +/− 15020-120Alt BVacuumLate addition1250 +/− 15020-120Alt C50-300Charged1250 +/− 15020-120crucibleAlt D100-500Charged1250 +/− 15020-120crucibleAlt EVacuumCharged1250 +/− 15020-120crucible For example, in a preferred embodiment, the vacuum level of the chamber 202 may be about 50 torr to about 300 torr, the method of alloy addition may be a late addition (e.g., added later via the dissolution device 212 ), the temperature for the melt may be about 1150° C. to about 1350° C., and the soak time (e.g., hold time) may be about 30 min to 60 min. In an another embodiment, Alt A (e.g., Alternative A), the vacuum level of the chamber 202 may be about 100 torr to about 500 torr, the method of alloy addition may be a late addition (e.g., added later via the dissolution device 212 ), the temperature for the melt may be about 1100° C. to about 1400° C., and the soak time (e.g., hold time) may be about 20 min to about 120 min. In an another embodiment, Alt B (e.g., Alternative B), the vacuum level of the chamber 202 may be a vacuum (e.g., about 0 torr), the method of alloy addition may be a late addition (e.g., added later via the dissolution device 212 ), the temperature for the melt may be about 1100° C. to about 1400° C., and the soak time (e.g., hold time) may be about 20 min to about 120 min. In an another embodiment, Alt C (e.g., Alternative C), the vacuum level of the chamber 202 may be about 50 torr to about 300 torr, the method of alloy addition may be charged crucible (e.g., charged into the crucible 210 with the raw Cu material at step 402 ), the temperature of the melt may be about 1100° C. to about 1400° C., and the soak time (e.g., hold time) may be about 20 min to about 120 min. In an another embodiment, Alt D (e.g., Alternative D), the vacuum level of the chamber 202 may be about 100 torr to about 500 torr, the method of alloy addition may be charged crucible (e.g., charged into the crucible 210 with the raw Cu material at step 402 ), the temperature for the melt may be about 1100° C. to about 1400° C., and the soak time (e.g., hold time) may be about 20 min to about 120 min. In an another embodiment, Alt E (e.g., Alternative E), the vacuum level of the chamber 202 may be a vacuum (e.g., about 0 torr), the method of alloy addition may be late addition (e.g., added later via the dissolution device 212 ), the temperature for the melt may be about 1100° C. to about 1400° C., and the soak time (e.g., hold time) may be about 20 min to about 120 min. At step 404 , the casting process 400 may comprise pumping down the chamber 210 , to remove air, to achieve a predetermined pressure within chamber 210 as described in Table I. The pumping down of the chamber 210 may be controlled by the PLC and a vacuum pump 225 of the VIM furnace 200 . At step 406 , the casting process 400 may comprise melting the raw material charged into the crucible 210 . The temperature for the melt is described in the processing parameters in Table I. If the alloy addition method is a late addition, there is no alloy addition yet and only copper is contained in the melt. If the alloy addition method is not a late addition (e.g., a charged crucible alloy addition method), the alloy addition is already charged into the crucible 210 and the raw metallic material in the melt includes both the Cu and alloy addition. The raw metallic material may be melted through the induction of eddy currents which leads to resistive heating of the raw metallic metals and stirring wakes inside the crucible 210 within the induction coil 216 . The stirring wakes are directed away from the crucible walls 214 . At step 408 , the casting process 400 may comprise determining whether the melt require late additions, e.g., whether the alloy addition method of the processing parameters as shown in Table I is a late addition (e.g., the alloy addition is placed in the dissolution device 212 to be alloyed later as a late addition) or a charged crucible (e.g., the alloy addition is charged into the crucible along with the raw Cu material). If late additions are not required in step 408 , the casting process 400 may proceed to step 410 . If late additions are required in step 408 , the casting process may proceed to step 414 . At step 410 , temperature of the melt is maintained at the predetermined temperature value for the predetermined soak time value, such as those shown in Table I, during which time the melt homogenizes. In an embodiment, the predetermined soak time may range between about 20 min-about 120 min. At step 412 , the casting process 400 may include adjusting a current value of the induction coil 216 to maintain the predetermined temperature value of the melt for the predetermined soak time value. The casting process 400 proceeds to step 422 once the predetermined soak time value has elapsed. At step 414 , the casting process 400 may comprise adding material (e.g., alloy addition) at a predetermined rate to the melt. The dissolution device 212 may controlled by the PLC to add (e.g., dispense) the alloy addition into the melt contained in the crucible 210 at a predetermined rate of about 17 g/s to about 167 g/s. The dissolution device 212 may also be controlled to add the alloy addition into the crucible 210 away from the crucible wall 214 . During step 414 , the temperature of the melt in the crucible 210 is maintained by the PLC 230 using the induction coil 216 and melt temperature sensor 250 . At step 416 , the casting process 400 may comprise maintaining the temperature of the melt (molten bath) at the predetermined temperature value for the predetermined soak time value, such as those described in Table I. For example, in a preferred embodiment, the molten bath may be soaked for about 30 min-about 60 min. In other embodiments, the molten bath may be soaked for about 20 min-about 120 min. This soak time may be described as a hold time to allow the molten bath to homogenize. During the soak time, the casting process 400 may include measuring the temperature value of the melt using a suitable measuring device and adjusting a current value of the induction coil 216 , using the PLC 230 , to maintain the predetermined temperature value of the melt throughout the predetermined soak time (e.g. until the predetermined soak time has elapsed). The casting process 400 proceeds to step 422 once the predetermined soak time value has elapsed. At step 422 , the casting process 400 may comprise casting the ingot 100 . The melt, a molten alloy comprising of Cu and alloy addition, may be poured from the crucible 210 into the mold 220 to cast the ingot 100 . The ingot 100 casted using the casting process 400 may be a low carbon defect CuMn ingot with an Mn concentration up to about 5 wt % having a purity of about 99.999% or higher. The PLC 230 may control the casting process (e.g. pouring of the melt from the crucible 210 into the mold 220 ). FIG. 5 further illustrates the dissolution of the alloy addition (e.g., dissolution process) by the dissolution device 212 . FIG. 5 illustrates the dissolution device 212 and the direction at which the alloy addition 502 (e.g., a late addition) is inserted into the electromagnetic stirring wakes 504 of the molten bath 500 . The disclosed dissolution process enables the utilization of known refractory ancillary equipment, refractory metals, and refractory tools such as graphite, MgO, Al 2 O 3 , ZrO 2 , Ta, W, and Mo. However, graphite is preferred, and other ceramic type of crucible is nonconductive and can create inclusion contamination in the melt that are nonconductive that can create arcing. More specifically, the dissolution device 212 may dispense the alloy addition 502 away from the crucible wall 214 and into the electromagnetically induced stirring wakes 504 (e.g., stirring wakes 504 ) imparted by the induction coil 216 . The dissolution device 212 may be centrally located over the crucible 210 , which is wrapped with the induction coil 216 . The dissolution device 212 may be controlled by programmable logic controlled (PLC) 230 to dispense (e.g., disperse or discharge) the alloy addition 502 into the stirring wakes 504 of the molten bath 500 , away from the crucible wall 214 , at a predetermined dissolution rate of about 17 g/s to about 167 g/s. The alloy addition 502 (e.g., a late addition) may comprise of Mn with a concentration up to about 5 wt % (e.g., compositional range up to about 5 wt %) having an overall purity of about 99.9% to about 99.999% and one or more active elements (e.g., carbon getters). The active elements can include O at a minimum of about 100 ppm to a maximum of about 4000 ppm, Fe at a minimum of about 5 ppb to a maximum of about 100 ppm, S at a minimum of about 5 ppm to about a maximum of about 400 ppm, H at a minimum of about 1 ppm to a maximum of about 10 ppm, or Cr at a minimum of about 5 ppb to a maximum of about 200 ppm. The alloy addition 502 may comprise of one or more active elements or the alloy addition may comprise all of these active elements at the range described. The alloy addition may be a powder, flake, sponge, chunks, ingot, or a combination thereof. FIG. 6 is a program for PLC 230 to create a low carbon defect Cu—Mn ingot 100 using selected raw materials. The ingot is then formed into a low carbon defect Cu—Mn sputtering target blank 110 that is attached to a backing plate 120 to form a low carbon defect Cu—Mn sputtering target 130 . Further, since PLC 230 includes a processor 240 and memory 235 , the term PLC 230 is intended to encompass embodiments of VIM furnace 200 that are implemented with processor 240 and memory 235 . The program of FIG. 6 is stored in memory 235 and executed by processor 240 and directed to a method for creating a low carbon defect Cu—Mn ingot 100 for use in forming a low carbon defect Cu—Mn sputtering target 130 . In step 602 the VIM furnace 200 receives predetermined values for parameters using the I/O device 245 of PLC 230 . The predetermined parameter values are stored in memory 235 . The parameters and predetermined parameter values may include those listed below in Table I. Stated alternatively, the predetermined parameter values stored in the memory 235 of PLC 230 may include, but are not limited to, chamber pressure, allot addition method, temperature, and soak time, as well as whether the dissolution device 212 will be used to add the alloy additions (e.g. whether there will be late additions), and the dispensing (dispersion rate) of the dissolution device 212 when used. In an exemplary embodiment, the dispending rate of the dissolution device 212 may between about 17 g/s to about 167 g/s. Once the predetermined parameter values are stored in memory 235 , the program proceeds to step 604 where the program pauses for the charging of raw material in the crucible 210 . The raw material includes Cu placed in the crucible 210 . The raw materials also includes alloy additions, which may also be placed in the crucible 210 or the dissolution device 212 , in accordance with the predetermined parameter value for late additions. In 606 the chamber 202 is pumped down to the predetermined chamber pressure value using the vacuum pump 225 . In 608 , the raw materials in the crucible 210 are melted using the induction coil 216 and temperature sensor 250 , such that the melt achieves a predetermined temperature value. In 610 , the late additions parameter value is examined. If the late additions parameter is set to “NO”, then the program proceeds to 612 , where the temperature value of the melt in the crucible 210 is maintained at the predetermined temperature value, using the induction coil 216 and temperature sensor 250 , until a predetermined soak time value has elapsed. In the late additions parameter is set to “YES”, then the programs proceeds to 614 , where alloy additions are dispensed into the stirring wakes 504 of the melt in the crucible 210 using the dissolution device 212 as a predetermined rate until all of the alloy additions in the dissolution device 212 have been dispensed. The alloy additions are directed toward the stirring wakes 504 and away from the crucible walls 214 . Once the allow additions have been added to the melt, the program proceeds to step 612 . After the predetermined sock time value has elapsed in step 612 , the program proceeds to step 616 where the ingot is cast by pouring the melt from the crucible 210 into the mold 220 . In some embodiments, the PLC 230 may direct a mechanism attached to the crucible to pour the melt from the crucible 210 into the mold 220 . Referring now to FIG. 7 , which illustrates a bar graph 700 comparing the defect density in concentration units of sputtering targets made using the traditional VIM process and sputtering targets (e.g., sputtering target 130 ) made using the herein disclosed VIM method, which comprise of the selection of raw material and the casting process (e.g., casting process 400 ). The bar 702 represents the traditional VIM process and the bar 704 represents the herein disclosed VIM method. The defects is measured by concentration units, and as illustrated, for every 2+ defects in the sputtering targets made using the disclosed VIM method there are 7+ defects in the sputtering targets using the traditional VIM process. FIG. 8 further provides measurable improvements using the herein disclosed VIM method compared to the traditional VIM process. FIG. 8 illustrates an ultrasonic scan visual comparison between the traditional VIM process compared to the disclosed VIM method. Improvements using the disclosed VIM method is evaluated utilizing the ultrasonic defect inspection described in U.S. Patent No. U.S. Pat. No. 6,739,196. FIG. 8 presents a visual comparison map from the ultrasonic through-thickness test. The amplitude (AMP) represents the percentage of signal and the higher the amplitude the greater the defect. As illustrated, the traditional VIM process has many more signals with a higher AMP (%) compared to the disclosed VIM method. For example, the signal 702 has a higher amplitude than the signal 704 . The disclosed VIM method produces a measurable reduction in defect level of about 50% or greater in comparison with the traditional VIM process. It is unexpected that the addition of the carbon getters into the melt in accordance with the disclosed VIM method results in a measureable reduction in defect level of the ingot 100 , when compared with the traditional VIM process. As was stated above, a reduction in the defect level of the ingot 100 naturally results in a reduction in the defect level of the sputtering target 130 that is produced using the ingot 100 . What has been described above includes examples of the present specification. It is, of course, not possible to describe every conceivable combination of components or methodologies for purposes of describing the present specification, but one of ordinary skill in the art may recognize that many further combinations and permutations of the present specification are possible. Each of the components or methodologies described above may be combined or added together in any permutation. Accordingly, the present specification is intended to embrace all such alterations, modifications and variations that fall within the spirit and scope of the appended claims. Furthermore, to the extent that the term “includes” is used in either the detailed description or the claims, such term is intended to be inclusive in a manner similar to the term “comprising” as “comprising” is interpreted when employed as a transitional word in a claim.",en,PATENT_APPLICATION
040-156-328-126-208,US,20240385684,A1,2024-11-21,US_20240385684_A1_20241121,en,US,20240385684,A1,2024-11-21,US,18785875,2024-07-26,VR ENVIRONMENT FOR ACCIDENT RECONSTRUCTION,en,US,STATE FARM MUTUAL AUTOMOBILE INSURANCE COMPANY,"Bloomington, IL",US,Aaron Williams,"Congerville, IL",US,1,Joseph Robert Brannan,"Bloomington, IL",US,2,John Donovan,"Bloomington, IL",US,3,Brian N. Harvey,"Bloomington, IL",G06F3/01,I,F,G06T11/00,I,L,G08G1/16,I,L,G06F3/011,I,F,G06T11/00,I,L,G08G1/162,I,L,US,20240385684,A1,2024-11-21,040-156-328-126-208,1,US,20240385684,A1,2024-11-21,040-156-328-126-208,1,UNKNOWN,"The following relates generally to providing virtual reality (VR) alerts to a driver of an autonomous vehicle. For example, a vehicle may be driving autonomously while the driver is watching a VR movie (e.g., on a pair of VR goggles); the driver may then receive a VR alert recommending that the driver take control of the vehicle (e.g., switch the vehicle from autonomous to manual mode). The following also relates to generating a VR feed for presenting real-time road conditions so that a user may preview a road segment. The following also relates to generating a VR feed corresponding to an event (e.g., a vehicle collision, a crime, a weather event, and/or a natural disaster).",en,"1 . A computer-implemented method for generating a virtual reality (VR) feed corresponding to an event, the method comprising: obtaining, via one or more processors, an indication of an event occurring in a geographic area, wherein the event is at least one of: a vehicle collision, a crime, a weather event, or a natural disaster; in response to obtaining the indication of the event, generating, via the one or more processors, a real-time VR feed of the geographic area at a time of the event based upon real-time condition data from the geographic area, the real-time VR feed including a virtual representation of the geographic area at the time of the event to reflect the real-time conditions at the geographic area; and providing, via the one or more processors, the real-time VR feed for presentation to a user within a virtual reality display for the user to experience the geographic area where the event occurred within a VR environment.","2 . The computer-implemented method of claim 1 , wherein the real-time VR feed is generated based upon data generated by: (i) a camera of a vehicle in the geographic area, (ii) a drone in the geographic area, and/or (iii) an infrastructure camera in the geographic area.","3 . The computer-implemented method of claim 1 , wherein: the event is the vehicle collision; the indication is obtained from a vehicle in the geographic area; and the real-time VR feed is generated based upon data received from a vehicle camera of the vehicle in the geographic area.","4 . The computer-implemented method of claim 1 , wherein generating the real-time VR feed further comprises blurring out: a face of an individual, identifying information of an individual, and/or a license plate.","5 . The computer-implemented method of claim 1 , wherein the event comprises the natural disaster event, and the natural disaster event comprises a forest fire, or a hurricane.","6 . The computer-implemented method of claim 1 , wherein the event comprises the weather event, and the weather event comprises a hail event.","7 . The computer-implemented method of claim 1 , wherein the real-time condition data includes light detection and ranging (LIDAR) data.","8 . A computer system configured to generate a virtual reality (VR) feed corresponding to an event, the computer system comprising one or more local or remote processors, transceivers, and/or sensors configured to: obtain an indication of an event occurring in a geographic area, wherein the event is at least one of: a vehicle collision, a crime, a weather event, or a natural disaster; in response to obtaining the indication of the event, generate a real-time VR feed of the geographic area at a time of the event based upon real-time condition data from the geographic area, the real-time VR feed including a virtual representation of the geographic area at the time of the event to reflect the real-time conditions at the geographic area; and provide the real-time VR feed for presentation to a user within a virtual reality display for the user to experience the geographic area where the event occurred within a VR environment.","9 . The computer system of claim 8 , wherein the real-time VR feed is generated based upon data generated by: (i) a camera of a vehicle in the geographic area, (ii) a drone in the geographic area, and/or (iii) an infrastructure camera in the geographic area.","10 . The computer system of claim 8 , wherein: the event is the vehicle collision; the indication is obtained from a vehicle in the geographic area; and the real-time VR feed is generated based upon data received from a vehicle camera of the vehicle in the geographic area.","11 . The computer system of claim 8 , wherein generating the real-time VR feed further comprises blurring out: a face of an individual, identifying information of an individual, and/or a license plate.","12 . The computer system of claim 8 , wherein: the event comprises the natural disaster event, and the natural disaster event comprises a forest fire, or a hurricane.","13 . The computer system of claim 8 , wherein the event comprises the weather event, and the weather event comprises a hail event.","14 . The computer system of claim 8 , wherein the real-time condition data includes light detection and ranging (LIDAR) data.","15 . A computer device for generating a virtual reality (VR) feed corresponding to an event, the computer device comprising: one or more processors; and one or more non-transitory memories coupled to the one or more processors; the one or more non-transitory memories including computer executable instructions stored therein that, when executed by the one or more processors, cause the one or more processors to: obtain an indication of an event occurring in a geographic area, wherein the event is at least one of: a vehicle collision, a crime, a weather event, or a natural disaster; in response to obtaining the indication of the event, generate a real-time VR feed of the geographic area at a time of the event based upon real-time condition data from the geographic area, the real-time VR feed including a virtual representation of the geographic area at the time of the event to reflect the real-time conditions at the geographic area; and provide the real-time VR feed for presentation to a user within a virtual reality display for the user to experience the geographic area where the event occurred within a VR environment.","16 . The computer device of claim 15 , wherein the real-time VR feed is generated based upon data generated by: (i) a camera of a vehicle in the geographic area, (ii) a drone in the geographic area, and/or (iii) an infrastructure camera in the geographic area.","17 . The computer device of claim 15 , wherein: the event is the vehicle collision; the indication is obtained from a vehicle in the geographic area; and the real-time VR feed is generated based upon data received from a vehicle camera of the vehicle in the geographic area.","18 . The computer device of claim 15 , wherein generating the real-time VR feed further comprises blurring out: a face of an individual, identifying information of the individual, and/or a license plate.","19 . The computer device of claim 15 , wherein: the event comprises the natural disaster event, and the natural disaster event comprises a forest fire, or a hurricane.","20 . The computer device of claim 15 , wherein the event comprises the weather event, and the weather event comprises a hail event.",en,"CROSS REFERENCE TO RELATED APPLICATIONS This application is a continuation of U.S. patent application Ser. No. 17/874,227, entitled “VR Environment for Accident Reconstruction,” filed Jul. 26, 2022, which claims the benefit of claims the benefit of U.S. Provisional Application No. 63/358,002, entitled “Generating Virtual Reality (VR) Alerts for Challenging Streets” (filed Jul. 1, 2022), the entirety of each of which is incorporated by reference herein. FIELD The present disclosure generally relates to, inter alia: (i) providing virtual reality (VR) alerts to a driver of an autonomous vehicle; (ii) generating a VR feed for presenting real-time road conditions; and (iii) generating a VR feed corresponding to an event. BACKGROUND In some scenarios, the driver of an autonomous vehicle may be watching a VR movie while the vehicle is driving autonomously. However, that the driver is watching a VR movie presents a problem when the vehicle approaches an area where the driver should take control of the vehicle (e.g., an area where it would be difficult for the vehicle to drive autonomously). In other scenarios, for a person who is determining whether or not to take an upcoming trip, it may be useful to know the road conditions on upcoming portions of a potential route to a destination. However, it may be difficult and/or cumbersome for the person to learn the road conditions prior to embarking on the trip. In still other scenarios, for the person who is determining whether or not to take the upcoming trip, it may be useful to know if an event (e.g., a vehicle collision, a crime, a weather event, or a natural disaster) has occurred in a geographic area of the trip. However, it may be difficult and/or cumbersome for the person to learn if an event has occurred, and also difficult and/or cumbersome to obtain information of the event. The systems and methods disclosed herein provide solutions to these problems and may provide solutions to other drawbacks of conventional techniques. SUMMARY In general, first, the present embodiments may relate to, inter alia, generating Virtual Reality (VR) alerts for challenging streets. For instance, a VR environment may be provided in which VR alerts are generated for challenging streets or areas for delivery drivers/passengers, truck drivers/passengers, or other vehicles. The environment provides audible or visual alerts for the driver to pay attention in certain areas. As an example, an alert may interrupt a viewing of movie or playing of a video game using a VR headset, to alert the passenger that the Autonomous Vehicle (AV) is approaching construction, congestion, an accident, or tight city streets and the passenger should take manual control of the AV. The VR environment may also provide for VR driver training for the challenging streets/areas. For instance, virtual street/driving training of scenes of tight city streets may be provided via a VR headset prior to the driver traveling to that area of the city. Second, the present embodiments may relate to, inter alia, a VR environment for presenting real-time road conditions, such as on an AR (Augmented Reality)/VR headset or AR/VR display. The VR environment may live-stream what current weather/road or traffic conditions look like from the perspective of other drivers (e.g., gather feeds from smart glasses, AR or VR glasses/headsets, smart vehicle cameras, and post the images on the internet or for viewing on a VR headset). A user may go into the Metaverse or other virtual environment, and preview roads for driving along pre-determined routes and/or in certain difficult areas based on sensor data and/or images from vehicles in that area. For instance, if the user is going to travel from Denver to Cheyenne in winter and snow is forecasted, or driving into Chicago, allow the user to view current road and traffic conditions. In certain embodiments, a VR headset or smart windshield may also be able to display road conditions from vehicles directly ahead of the user, e.g., collect and display images from vehicles or passengers traveling a few miles ahead and along the same route of the user. Third, the present embodiments may relate to, inter alia, a VR environment for accident reconstruction. A VR environment may be provided for representing a real-time view of a certain geographical area where users can go in and experience what is happening in the area. This may include real-time viewing of an accident scene, crime scene, or other real-time event. The scene may include real-time weather conditions or natural disaster conditions (e.g., forest fires, hurricanes) which may be provided from sensors or cameras within the area (e.g., vehicle sensors, infrastructure sensors, drones, etc.). For privacy reasons, the VR environment may blur out individuals (e.g., mask faces) within the scene, license plates, or other identifying information, or replace with the individuals with generic avatars. The VR environment may also provide a more photorealistic stream of individuals to emergency services to show the extent of injuries from the event. More specifically, in one aspect, a computer-implemented method for providing virtual reality (VR) alerts to a driver of an autonomous vehicle may be provided. The method may be implemented via one or more local or remote processors, transceivers, sensors, servers, virtual headsets or displays, and/or other electric or electronic components. In one instance, the method may include: (1) receiving, via one or more processors, an indication that a driver of a vehicle is accessing a VR feed on a VR display; (2) receiving, via the one or more processors, an indication that the vehicle is driving in an autonomous mode; (3) determining, via the one or more processors, a complexity score for traversing an upcoming area which the vehicle is approaching; and/or (4) in response to determining that the complexity score is above a predetermined threshold, providing, via the one or more processors, a VR alert to the driver through the VR display warning the driver of the upcoming area. The method may include additional, fewer, or alternate actions, including those discussed elsewhere herein. In another aspect, a computer system configured to provide virtual reality (VR) alerts to a driver of an autonomous vehicle may be provided. The computer system may include one or more local or remote processors, transceivers, VR headsets or displays, servers, and/or sensors configured to: (1) receive an indication that a driver of a vehicle is accessing a VR feed on a VR display; (2) receive an indication that the vehicle is driving in an autonomous mode; (3) determine a complexity score for traversing an upcoming area which the vehicle is approaching; and/or (4) in response to a determination that the complexity score is above a predetermined threshold, provide a VR alert to the driver through the VR display warning the driver of the upcoming area. The computer system may include additional, less, or alternate functionality, including that discussed elsewhere herein. In yet another aspect, a computer device for providing virtual reality (VR) alerts to a driver of an autonomous vehicle may be provided. The computer device may include: one or more processors; and one or more memories coupled to the one or more processors. The one or more memories including computer executable instructions stored therein that, when executed by the one or more processors, cause the one or more processors to: (1) receive an indication that a driver of a vehicle is accessing a VR feed on a VR display; (2) receive an indication that the vehicle is driving in an autonomous mode; (3) determine a complexity score for traversing an upcoming area which the vehicle is approaching; and/or (4) in response to a determination that the complexity score is above a predetermined threshold, provide a VR alert to the driver through the VR display warning the driver of the upcoming area. The computer device may include additional, less, or alternate functionality, including that discussed elsewhere herein. In one aspect, a computer-implemented method for generating a virtual reality (VR) feed for presenting real-time road conditions may be provided. The method may be implemented via one or more local or remote processors, servers, transceivers, sensors, VR headsets or displays, and/or other electric or electronic components. In one instance, the method may include: (1) obtaining, via one or more processors, real-time condition data indicating conditions of a road segment in a geographic area; (2) generating, via the one or more processors, a VR feed of the road segment based upon the real-time condition data, the VR feed including a virtual representation of the road segment to reflect the real-time conditions at the road segment; and/or (3) providing, via the one or more processors, the generated VR feed for presentation to a user within a VR display for the user to preview the road segment. The method may include additional, fewer, or alternate actions, including those discussed elsewhere herein. In another aspect, a computer system configured to generate a virtual reality (VR) feed for presenting real-time road conditions may be provided. The computer system may include one or more local or remote processors, transceivers, servers, VR headsets or displays, and/or sensors configured to: (1) obtain real-time condition data indicating conditions of a road segment in a geographic area; (2) generate a VR feed of the road segment based upon the real-time condition data, the VR feed including a virtual representation of the road segment to reflect the real-time conditions at the road segment; and/or (3) provide the generated VR feed for presentation to a user within a VR display for the user to preview the road segment. The computer system may include additional, less, or alternate functionality, including that discussed elsewhere herein. In yet another aspect, a computer device for generating a virtual reality (VR) feed for presenting real-time road conditions may be provided. The computer device including: one or more processors; and one or more memories coupled to the one or more processors. The one or more memories including computer executable instructions stored therein that, when executed by the one or more processors, cause the one or more processors to: (1) obtain real-time condition data indicating conditions of a road segment in a geographic area; (2) generate a VR feed of the road segment based upon the real-time condition data, the VR feed including a virtual representation of the road segment to reflect the real-time conditions at the road segment; and/or (3) provide the generated VR feed for presentation to a user within a VR display for the user to preview the road segment. The computer device may include additional, less, or alternate functionality, including that discussed elsewhere herein. In one aspect, a computer-implemented method for generating a virtual reality (VR) feed corresponding to an event may be provided. The method may be implemented via one or more local or remote processors, servers, transceivers, sensors, VR headsets or displays, and/or other electric or electronic components. In one instance, the method may include: (1) obtaining, via one or more processors, an indication of an event occurring in a geographic area, wherein the event is at least one of: a vehicle collision, a crime, a weather event, or a natural disaster; (2) generating, via the one or more processors, a VR feed of the geographic area at a time of the event based upon real-time condition data from the geographic area, the VR feed including a virtual representation of the geographic area at the time of the event to reflect the real-time conditions at the geographic area; and/or (3) providing, via the one or more processors, the generated VR feed for presentation to a user within a virtual reality display for the user to experience the geographic area where the event occurred within a VR environment. The method may include additional, fewer, or alternate actions, including those discussed elsewhere herein. In another aspect, a computer system configured to generate a virtual reality (VR) feed corresponding to an event may be provided. The computer system may comprise one or more local or remote processors, transceivers, servers, VR headsets or displays, and/or sensors configured to: (1) obtain an indication of an event occurring in a geographic area, wherein the event is at least one of: a vehicle collision, a crime, a weather event, or a natural disaster; (2) generate a VR feed of the geographic area at a time of the event based upon real-time condition data from the geographic area, the VR feed including a virtual representation of the geographic area at the time of the event to reflect the real-time conditions at the geographic area; and/or (3) provide the generated VR feed for presentation to a user within a virtual reality display for the user to experience the geographic area where the event occurred within a VR environment. The computer system may include additional, less, or alternate functionality, including that discussed elsewhere herein. In yet another aspect, a computer device for generating a virtual reality (VR) feed corresponding to an event may be provided. The computer device may include: one or more processors; and one or more memories coupled to the one or more processors. The one or more memories including computer executable instructions stored therein that, when executed by the one or more processors, may cause the one or more processors to: (1) obtain an indication of an event occurring in a geographic area, wherein the event is at least one of: a vehicle collision, a crime, a weather event, or a natural disaster; (2) generate a VR feed of the geographic area at a time of the event based upon real-time condition data from the geographic area, the VR feed including a virtual representation of the geographic area at the time of the event to reflect the real-time conditions at the geographic area; and/or (3) provide the generated VR feed for presentation to a user within a virtual reality display for the user to experience the geographic area where the event occurred within a VR environment. The computer device may include additional, less, or alternate functionality, including that discussed elsewhere herein. BRIEF DESCRIPTION OF THE DRAWINGS Advantages will become more apparent to those skilled in the art from the following description of the preferred embodiments which have been shown and described by way of illustration. As will be realized, the present embodiments may be capable of other and different embodiments, and their details are capable of modification in various respects. Accordingly, the drawings and description are to be regarded as illustrative in nature and not as restrictive. The figures described below depict various aspects of the applications, methods, and systems disclosed herein. It should be understood that each figure depicts an embodiment of a particular aspect of the disclosed applications, systems and methods, and that each of the figures is intended to accord with a possible embodiment thereof. Furthermore, wherever possible, the following description refers to the reference numerals included in the following figures, in which features depicted in multiple figures are designated with consistent reference numerals. FIG. 1 illustrates an exemplary system for providing VR alerts to a driver of an autonomous vehicle. FIG. 2 illustrates an exemplary VR display including an example VR alert overlaid onto a VR feed. FIG. 3 illustrates an exemplary display where a VR feed to the VR display is stopped, and only a VR alert is displayed on the VR display. FIG. 4 illustrates an exemplary display including an example VR alert overlaid onto a VR feed, including a request to the driver to receive training for traversing an upcoming area. FIG. 5 illustrates a flow diagram of an exemplary computer-implemented method for providing VR alerts to a driver of an autonomous vehicle. FIG. 6 illustrates a flow diagram of an exemplary computer-implemented method for providing VR alerts to a driver of an autonomous vehicle, including providing a driver with training for traversing an upcoming area. FIG. 7 illustrates an exemplary system for presenting real-time road conditions. FIG. 8 illustrates an exemplary VR display depicting real-time rainy conditions of a real-world road. FIG. 9 illustrates an exemplary VR display depicting real-time traffic conditions of a real-world road. FIG. 10 illustrates a flow diagram of an exemplary computer-implemented method for presenting real-time road conditions. FIG. 11 illustrates a flow diagram of an exemplary computer-implemented method for presenting real-time road conditions, including selecting a geographic area by the user. FIG. 12 illustrates a flow diagram of an exemplary computer-implemented method for presenting real-time road conditions, including determining a route that the vehicle is on. FIG. 13 shows an exemplary computer system for generating a VR feed corresponding to an event. FIG. 14 shows an exemplary display of a VR feed including a vehicle collision event. FIG. 15 shows an exemplary display of a VR feed including a crime event. FIG. 16 shows an exemplary display of a VR feed including a weather event. FIG. 17 shows an exemplary display of a VR feed including a natural disaster event. FIG. 18 shows an exemplary display of a VR feed where certain items are blurred out. FIG. 19 shows an exemplary display of a VR feed where certain items are replaced with avatars. FIG. 20 illustrates a flow diagram of an exemplary computer-implemented method for generating a VR feed corresponding to an event. FIG. 21 illustrates a flow diagram of an exemplary computer-implemented method for generating a VR feed where a second event follows a first event. While the systems and methods disclosed herein is susceptible of being embodied in many different forms, it is shown in the drawings and will be described herein in detail specific exemplary embodiments thereof, with the understanding that the present disclosure is to be considered as an exemplification of the principles of the systems and methods disclosed herein and is not intended to limit the systems and methods disclosed herein to the specific embodiments illustrated. In this respect, before explaining at least one embodiment consistent with the present systems and methods disclosed herein in detail, it is to be understood that the systems and methods disclosed herein is not limited in its application to the details of construction and to the arrangements of components set forth above and below, illustrated in the drawings, or as described in the examples. Methods and apparatuses consistent with the systems and methods disclosed herein are capable of other embodiments and of being practiced and carried out in various ways. Also, it is to be understood that the phraseology and terminology employed herein, as well as the abstract included below, are for the purposes of description and should not be regarded as limiting. DETAILED DESCRIPTION In general, the present embodiments relate to, inter alia: (i) providing VR alerts to a driver of an autonomous vehicle; (ii) generating a VR feed for presenting real-time road conditions; and/or (iii) generating a VR feed corresponding to an event. More specifically, first, the present embodiments may relate to, inter alia, generating Virtual Reality (VR) alerts for challenging streets. For instance, a VR environment may be provided in which VR alerts are generated for challenging streets or areas for delivery drivers/passengers, truck drivers/passengers, or other vehicles. The environment provides audible or visual alerts for the driver to pay attention in certain areas. As an example, an alert may interrupt a viewing of movie or playing of a video game using a VR headset, to alert the passenger that the Autonomous Vehicle (AV) is approaching construction, congestion, an accident, or tight city streets and the passenger should take manual control of the AV. The VR environment may also provide for VR driver training for the challenging streets/areas. For instance, virtual street/driving training of scenes of tight city streets may be provided via a VR headset prior to the driver traveling to that area of the city. Second, the present embodiments may relate to, inter alia, a VR environment for presenting real-time road conditions, such as on an AR (Augmented Reality)/VR headset or AR/VR display. The VR environment may live-stream what current weather/road or traffic conditions look like from the perspective of other drivers (e.g., gather feeds from smart glasses, AR or VR glasses/headsets, smart vehicle cameras, and post the images on the internet or for viewing on a VR headset). A user may go into the Metaverse or other virtual environment, and preview roads for driving along pre-determined routes and/or in certain difficult areas based on sensor data and/or images from vehicles in that area. For instance, if the user is going to travel from Denver to Cheyenne in winter and snow is forecasted, or driving into Chicago, allow the user to view current road and traffic conditions. In certain embodiments, a VR headset or smart windshield may also be able to display road conditions from vehicles directly ahead of the user, e.g., collect and display images from vehicles or passengers traveling a few miles ahead and along the same route of the user. Third, the present embodiments may relate to, inter alia, a VR environment for accident reconstruction. A VR environment may be provided for representing a real-time view of a certain geographical area where users can go in and experience what is happening in the area. This may include real-time viewing of an accident scene, crime scene, or other real-time event. The scene may include real-time weather conditions or natural disaster conditions (e.g., forest fires, hurricanes) which may be provided from sensors or cameras within the area (e.g., vehicle sensors, infrastructure sensors, drones, etc.). For privacy reasons, the VR environment may blur out individuals (e.g., mask faces) within the scene, license plates, or other identifying information, or replace with the individuals with generic avatars. The VR environment may also provide a more photorealistic stream of individuals to emergency services to show the extent of injuries from the event. Exemplary System for Providing VR Alerts to a Driver of an Autonomous Vehicle Some embodiments disclosed herein advantageously provide VR alerts to a driver of an autonomous vehicle. For example, a vehicle may be driving autonomously while the human driver uses VR goggles to play a VR video game or watch a VR movie. In this example, if the vehicle approaches an area that it will be difficult for the vehicle to traverse autonomously (e.g., because area is a construction area, or because of a weather condition, etc.), it may be advantageous (e.g., to reduce the likelihood of an accident) for the human to take control of the vehicle while driving through the area. To this end, FIG. 1 shows an example system 100 for providing VR alerts to a driver of an autonomous vehicle. The high-level architecture includes both hardware and software applications, as well as various data communications channels for communicating data between the various hardware and software components. With reference thereto, vehicle 150 may be an autonomous vehicle (e.g., a vehicle capable of driving autonomously, semi-autonomously, or in a manual mode, etc.). In this regard, the vehicle 150 may have autonomous operation features that may take full control of the vehicle under certain conditions, viz. fully autonomous operation, or the autonomous operation features may assist the vehicle operator in operating the vehicle, viz. partially autonomous operation. Fully autonomous operation features may include systems within the vehicle that pilot the vehicle to a destination with or without a vehicle operator present (e.g., an operating system for a driverless car). Partially autonomous operation features may assist the vehicle operator in limited ways (e.g., automatic braking or collision avoidance systems). Fully or partially autonomous operation features may perform specific functions to control or assist in controlling some aspect of vehicle operation, or such features may manage or control other autonomous operation features. For example, a vehicle operating system may control numerous subsystems that each fully or partially control aspects of vehicle operation. In addition to information regarding the position or movement of a vehicle, autonomous operation features may collect and utilize other information, such as data about other vehicles or control decisions of the vehicle. Such additional information may be used to improve vehicle operation, route the vehicle to a destination, warn of component malfunctions, advise others of potential hazards, or for other purposes described herein. Information may be collected, assessed, and/or shared via applications installed and executing on computing devices associated with various vehicles or vehicle operators, such as on-board computers of vehicles or smartphones of vehicle operators. By using computer applications to obtain data, the additional information generated by autonomous vehicles or features may be used to assess the autonomous features themselves while in operation or to provide pertinent information to non-autonomous vehicles through an electronic communication network 104 (which may be a wired and/or wireless network, such as the internet). These and other advantages are further described below. Some autonomous operation features may be adapted for use under particular conditions, such as city driving or highway driving. Additionally, the vehicle operator may be able to configure settings relating to the features or may enable or disable the features at will. Therefore, some embodiments monitor use of the autonomous operation features, which may include the settings or levels of feature use during vehicle operation. Information obtained by monitoring feature usage may be used to determine risk levels associated with vehicle operation, either generally or in relation to a vehicle operator. In such situations, total risk may be determined by a weighted combination of the risk levels associated with operation while autonomous operation features are enabled (with relevant settings) and the risk levels associated with operation while autonomous operation features are disabled. For fully autonomous vehicles, settings or configurations relating to vehicle operation may be monitored and used in determining vehicle operating risk. In some embodiments, information regarding the risks associated with vehicle operation with and without the autonomous operation features may be used to determine risk categories or premiums for a vehicle insurance policy covering a vehicle with autonomous operation features, as described elsewhere herein. Risk category or price may be determined based upon factors relating to the evaluated effectiveness of the autonomous vehicle features. The risk or price determination may also include traditional factors, such as location, vehicle type, and level of vehicle use. The vehicle 150 may have various vehicle sensors 152 . The vehicle sensors 152 may be any kind of sensors. Examples of the vehicle sensors 152 include: cameras (e.g., for capturing images and/or video), light detection and ranging (LIDAR) cameras, radio detection and ranging (RADAR) devices, accelerometers, gyroscopes, compasses, speedometers, magnetometers, barometers, thermometers, proximity sensors, light sensors (e.g., light intensity detectors), electromagnetic radiation sensors (e.g., infrared and/or ultraviolet radiation sensors), ultrasonic and/or infrared range detectors, humistors, hygrometers, altimeters, microphones, audio or video recorders, etc. Additional examples vehicle sensors 152 include advanced sensors, for example, that detect and/or receive data associated with temperature measurements, thermal imaging, weather conditions, traffic conditions, etc. The vehicle 150 may include any number or combination of vehicle sensors 152 . The vehicle 150 may further include one or more processors 151 such as one or more microprocessors, controllers, and/or any other suitable type of processor. The one or more processors 151 may perform any functions. For example, the one or more processors 151 may control the vehicle 151 while it is driving in an autonomous or semi-autonomous mode. In another example, the one or more processors 151 may switch the vehicle 150 between manual, autonomous, and semi-autonomous modes. As will be discussed further below, the one or more processors 151 may perform any of the functions of the VR alert generator application 124 and/or the complexity score application 126 . The vehicle 150 may further include a smart windshield 154 . The smart windshield 154 may be configured to produce a VR or augmented reality (AR) display, respectively, from a VR feed or AR feed. The vehicle 150 may be driven by driver 160 . For example, the driver 160 may operate the vehicle 150 when the vehicle 150 is in a manual mode or a semi-autonomous mode. When the vehicle is in an autonomous mode, the driver may simply sit in the vehicle without operating the vehicle. While in the vehicle 150 , the driver 160 may view a VR display (e.g., the smart windshield 154 or VR goggles 162 ). The VR display may be viewed, for instance, by accessing a VR feed. In some examples, the VR feed comprises a VR movie or a VR video game. As mentioned above, while the driver 160 is watching a VR movie or playing a VR videogame and the vehicle 150 is driving autonomously, it may happen that the vehicle 150 approaches an upcoming area that it would be difficult for the vehicle 150 to drive autonomously through. As such, it would be advantageous for the driver 160 to stop interacting with the VR display, and take manual control of the vehicle through the upcoming area. To this end, the example system 100 includes VR alert computing device 102 for generating and sending VR alerts to the vehicle 150 . The VR alert computing device 102 may further include one or more processors 120 such as one or more microprocessors, controllers, and/or any other suitable type of processor. The VR alert computing device 102 may further include a memory 122 (e.g., volatile memory, non-volatile memory) accessible by the one or more processors 120 , (e.g., via a memory controller). The one or more processors 120 may interact with the memory 122 to obtain, for example, computer-readable instructions stored in the memory 122 . Additionally or alternatively, computer-readable instructions may be stored on one or more removable media (e.g., a compact disc, a digital versatile disc, removable flash memory, etc.) that may be coupled to the VR alert computing device 102 to provide access to the computer-readable instructions stored thereon. In particular, the computer-readable instructions stored on the memory 122 may include instructions for executing various applications, such as a VR alert generator application 124 , and/or a complexity score application 126 . In some examples, the VR alert generator application 124 may generate VR alerts, such as the VR alert 210 on the example display 200 of FIG. 2 . In the example of FIG. 2 , the vehicle 150 is driving autonomously while the driver 160 is watching a VR movie via the VR goggles 162 . In this example, the VR alert generator application 124 generates a VR alert 210 which is overlaid on to the VR movie (e.g., overlaid onto the VR feed comprising the VR movie). The generated VR alert, in this example, comprises the text, “Warning: recommended that you take manual control of vehicle in XYZ miles.” FIG. 3 illustrates another example of an VR alert 310 that may be generated by the VR alert generator application 124 . In the example of FIG. 3 , to display the VR alert 310 , the VR feed (e.g., of an AR movie, or AR videogame) is stopped, and replaced with the VR alert 310 in the example display 300 . The generated VR alert, in this example, comprises the text, “Warning: recommended that you take manual control of vehicle in XYZ miles.” FIG. 4 illustrates another example of an VR alert 410 that may be generated by the VR alert generator application 124 . In particular, the example display 400 includes a request to the driver 160 to receive training for traversing the upcoming area. Specifically, the alert 410 states, “Warning: Difficult driving area ahead. Would you like to receive training for the upcoming difficult driving area?” In this regard, as will be discussed elsewhere herein, providing training to a driver about specific areas can be particularly useful in certain situations. For example, when an area is difficult because of narrow streets or sharp turns, providing training to the driver tailored specifically to the upcoming narrow streets or sharp turns can be particularly useful. In the example of FIG. 4 , the driver 160 is viewing the example display 400 on the smart windshield 154 . In some embodiments, the VR alert generator application 124 determines to generate a VR alert based upon a complexity score for traversing an upcoming area, which may be generated by the complexity score application 126 . For example, if an upcoming area would be difficult for the vehicle to traverse autonomously, the complexity score application 126 may generate a higher complexity score for the upcoming area. In some embodiments, the complexity score may be determined based upon at least one of: construction, congestion, road curvature, a traffic accident, a weather condition, and/or narrow streets in the upcoming area. The data that the complexity score is determined from may come from any source. For example, the data may come from a database, such as VR alert data base 118 (e.g., a proprietary database of a company of the VR alert computing device 102 ), and/or the external database 180 (e.g., a third party data bases, such as that of a third party aggregator, a road infrastructure data base, a weather database, etc.). Additionally or alternatively, the data may come from smart infrastructure devices 170 . Examples of the smart infrastructure devices include road camera 171 , smart stoplight 172 , smart stop sign 173 , and infrastructure camera 174 . Any of the smart infrastructure devices 170 may include any kind of sensors. For example, any of the smart infrastructure devices 170 may include: cameras (e.g., for capturing images and/or video), light detection and ranging (LIDAR) cameras, radio detection and ranging (RADAR) devices, accelerometers, gyroscopes, compasses, speedometers, magnetometers, barometers, thermometers, proximity sensors, light sensors (e.g., light intensity detectors), electromagnetic radiation sensors (e.g., infrared and/or ultraviolet radiation sensors), ultrasonic and/or infrared range detectors, humistors, hygrometers, altimeters, microphones, audio or video recorders, thermal imaging devices, etc. Furthermore, any of the smart infrastructure devices 170 may include multiple sensors (e.g., any combination of the example sensors just listed). Exemplary Methods for Providing VR Alerts to a Driver of an Autonomous Vehicle FIG. 5 shows an exemplary implementation 500 for providing VR alerts to a driver of an autonomous vehicle. Although the following discussion refers to many of the blocks of the example implementation 500 as being performed by the one or more processors 120 , it should be understood that any of the blocks or functions of the example implantation 500 may be performed by either of the one or more processors 120 of the VR alert computing device 102 , or the one or more processors 151 of the vehicle 150 . The exemplary implementation 500 begins at block 505 when the one or more processors 120 receive an indication that the driver 160 of the vehicle 150 is accessing a VR feed on a VR display (e.g., the VR googles 162 or the smart windshield 154 ). In some examples, the VR feed comprises a VR movie or a VR videogame. At block 510 , the one or more processors 120 receive an indication that the vehicle is driving in an autonomous mode. Along with the indication that the vehicle is driving in an autonomous mode, the one or more processors 120 may also receive an indication of a route that the vehicle is traveling on. For example, the driver 160 may have input a route or destination into a GPS device of the vehicle 150 , or a smartphone device; and this route or destination may be sent to the one or more processors 120 along with the indication of a route that the vehicle is traveling on. However, the route or destination of the vehicle 150 may also be sent to the one or more processors 120 separately from the indication that the vehicle is driving in an autonomous mode. At block 515 , the one or more processors 120 receive data from the smart infrastructure devices 170 and/or the vehicle sensors 152 . At block 520 , the one or more processors 120 determine a complexity score for traversing an upcoming area which the vehicle is approaching. In some embodiments, this first involves determining upcoming areas that the vehicle is approaching. This determination may be made based upon the route or destination received by the one or more processors 120 from the vehicle 150 . Additionally or alternatively, the upcoming areas may be determined based upon a prediction of a route that the vehicle 150 will take. For example, the one or more processors 120 may predict a route (and possibly a destination) based upon any criteria, such as a known location of the vehicle 150 , a driving history of the vehicle 150 , known previous destinations of the vehicle 150 , etc. The complexity score may be determined based upon any data, and determined in any suitable manner. In some examples, the complexity score is determined based upon the data received from the smart infrastructure devices 170 and/or the vehicle sensors 152 at block 515 . In one example of this, the one or more processors use image and/or video data (e.g., received from any of the road camera 171 , the smart stoplight 172 , the smart stop sign 173 , and/or the infrastructure camera 174 ) to determine construction, congestion, road curvature, a traffic accident, a weather condition, or narrow streets in the upcoming area. Additionally or alternatively, the complexity score may be determined based upon data received from VR alert database 118 and/or external database 180 . For example, the VR alert database 118 and/or external database 180 may send construction data, congestion data, road curvature data, data of a traffic accident, weather condition data, or data of narrow streets to the one or more processors 120 ; and the one or more processors 120 may use any of this data to determine the complexity score. In some embodiments, advantageously, the complexity score is determined only from data from the smart infrastructure devices 170 , and not from data from the vehicle sensors 152 . For example, if the upcoming area is more than a predetermined distance ahead of the vehicle 150 , then the one or more processors 120 may not use data from the vehicle sensors 152 . Advantageously, this may increase accuracy of the complexity score, and decrease the amount of time it takes the one or more processors 120 to determine the complexity score. In some embodiments, the complexity score is determined via a machine learning algorithm. The machine learning algorithm may take any of the data discussed above as inputs. Furthermore, the machine learning algorithm may have been trained by any suitable technique (e.g., supervised learning, unsupervised learning, semi-supervised learning). Examples of the machine learning algorithm may include neural networks, deep learning algorithms, etc. At block 525 , the one or more processors 120 determine if the complexity score is above a predetermined threshold. If not, the processes returns to block 505 . However, it should be understood that implementation 500 is only an example; and, in other examples, the process may return to any of blocks 510 , 515 , or 520 , rather than return to block 505 . If the complexity score is above a predetermined threshold, a VR alert is provided to the VR display warning the driver 160 of the upcoming area (block 530 ). Examples of providing the alert include overlaying the VR alert onto a VR feed (e.g., FIG. 2 ), and stopping the VR feed while displaying the VR alert on the VR display (e.g., FIG. 3 ). The VR alert may also include an indication that the driver should take control of the vehicle. In this regard, there may be different levels of the VR alert. In one example, the different levels of the VR alert correspond to the different ranges of the complexity score (e.g., high complexity score indicating a high level of VR alert). In this regard, the text of the VR alert may change depending on the different levels of the VR alert (e.g., a high VR alert has text of “strongly recommended that you switch to manual control,” whereas a low VR alert has text of “consider switching to manual control”). The different VR alert levels may also be color coded in the VR alert (e.g., high VR alert indicated with red text; low VR alert indicated with green text; etc.). The alert may also indicate a distance to the upcoming area (e.g., text indicating “recommended that you take manual control of vehicle in XYZ miles”). The alert may also be color coded based on the distance to the upcoming area (e.g., red text indicating a shorter distance to the upcoming area; green indicating a longer distance to the upcoming area; etc.). Additionally or alternatively, in some embodiments, the alert may be haptic. In one example, the driver 160 is provided the haptic VR alert through the VR goggles 162 (e.g., a tapping, vibrating, or rubbing provided by the VR goggles 162 ). In another example, there may be a pair of VR gloves 163 with the VR goggles 162 (e.g., VR gloves 163 that are used to control the VR goggles or headset 162 ), and the haptic VR alert is provided through the VR gloves 163 (e.g., VR gloves 163 vibrating, etc.). Additionally or alternatively, in some embodiments, the alert may be audible. In some examples, the text of any of the VR alerts (e.g., the text of any of VR alerts 210 , 310 , 410 ) is read aloud through speakers of the vehicle 150 , or VR goggles 162 . Additionally or alternatively to the VR alert, an augmented reality (AR) alert may be provided. For example, if the driver 160 is watching a VR video on the smart windshield 154 , the VR video feed may be stopped, and an AR alert may be provided. For instance, the AR alert may indicate, “vehicle is approaching complex area ahead. Would you like to switch to manual control?” Advantageously, displaying the alert in AR form shows the driver 160 an additional view through the smart windshield 154 , thus allowing the driver 160 to know specifically where he is (if he is familiar with the route the vehicle is driving along). Moreover, in such embodiments, the driver 160 may control the position or other aspects of the AR alert or any other AR information. For example, the driver 160 may use the VR/AR gloves 163 to control the position of the AR alert on the smart windshield 154 . In another example, the driver 160 may use the VR/AR gloves 163 to remove the AR alert from the smart windshield 154 (e.g., with a swiping motion). The VR (or AR) alert may also include a request for the driver 160 to switch the vehicle 150 to a manual mode (e.g., VR alert has text “would you like to switch to manual mode”). At block 535 , the one or more processors 120 receive acceptance of the request to switch the vehicle to manual mode. At block 540 , the one or more processors 120 switch the vehicle to manual mode (e.g., provide a command to the one or more processors 151 to switch the vehicle 150 to manual mode). However, as noted above, any of the blocks may be performed by the one or more processors 151 . Thus, in some embodiments, the acceptance of the request (e.g., block 535 ) is sent to the one or more processors 151 (rather than the one or more processors 120 ), accordingly advantageously saving bandwidth and computational resources by eliminating unnecessary signals to the VR alert computing device 102 . FIG. 6 illustrates an exemplary implementation 600 of providing VR alerts to a driver of an autonomous vehicle, including providing a driver with training for traversing an upcoming area. Although the following discussion refers to many of the blocks of the example implementation 600 as being performed by the one or more processors 120 , it should be understood that any of the blocks or functions of the example implantation 600 may be performed by either of the one or more processors 120 of the VR alert computing device 102 , or the one or more processors 151 of the vehicle 150 . In the example implementation 600 , blocks 505 - 525 may be performed as in FIG. 5 . At block 610 , the one or more processors 120 may, additionally or alternatively to the providing a VR alert as in block 530 of FIG. 5 , provide the VR alert with a request to the driver to receive training for traversing the upcoming area. The training may relate to any condition in the upcoming area (e.g., construction, congestion, road curvature, a traffic accident, a weather condition, or narrow streets in the upcoming area). In this regard, the VR alert may also indicate the condition (e.g., VR alert indicating “There will be a storm in approximately XYZ miles. Would you like training on driving through a storm?”). At block 620 , the one or more processors 120 receive acceptance of the driving training. At block 630 , the one or more processors 120 provide the driving training. In one example, the driver 160 is wearing a pair of VR goggles 162 with a pair of VR gloves 163 that control the VR goggles 162 . The driver 160 may complete the training using the VR goggles 162 and VR gloves 163 . In another example, the training is displayed on the smart windshield 154 , and the driver 160 completes the training using the vehicle controls (e.g., the vehicle's steering wheel, accelerator pedal, etc.). In some implementations of this, the vehicle is still driving autonomously while the driver 160 completes the training (e.g., the driver 160 turning the steering wheel as part of the training does not affect the vehicle's actual steering because the vehicle 150 is driving autonomously). It should be understood that not all blocks of the exemplary flowcharts 500 , 600 are required to be performed. Moreover, the example flowcharts 500 , 600 are not mutually exclusive (e.g., block(s) from each example flowchart 500 , 600 may be performed in any other flowchart). The exemplary flowcharts 500 , 600 may include additional, less, or alternate actions, including those discussed elsewhere herein. Applicability to the Insurance Industry Some embodiments have particular applicability to the insurance industry. For example, discounts to insurance premiums may be provided by the techniques described herein. For instance, if a driver 160 competes training (e.g., as provided in FIG. 6 ), the driver 160 may receive a discount on an insurance premium. In another example, a driver 160 may receive a discount on an insurance premium for agreeing to have VR alerts provided to her or her vehicle 150 . In one aspect, data from the vehicle 150 , and/or other data, including the types of data discussed elsewhere herein, may be collected or received by an insurance provider remote server, such as via direct or indirect wireless communication or data transmission from a smart home controller, mobile device, or other customer computing device, after a customer affirmatively consents or otherwise opts-in to an insurance discount, reward, or other program. The insurance provider may then analyze the data received with the customer's permission to provide benefits to the customer. As a result, risk averse customers may receive insurance discounts or other insurance cost savings based upon data that reflects low risk behavior and/or technology that mitigates or prevents risk to autonomous vehicles. Exemplary Use of Providing VR Alerts to a Driver of an Autonomous Vehicle In one aspect, a computer-implemented method for providing virtual reality (VR) alerts to a driver of an autonomous vehicle may be provided. The method may include: (1) receiving, via one or more processors, an indication that a driver of a vehicle is accessing a VR feed on a VR display; (2) receiving, via the one or more processors, an indication that the vehicle is driving in an autonomous mode; (3) determining, via the one or more processors, a complexity score for traversing an upcoming area which the vehicle is approaching; and/or (4) in response to determining that the complexity score is above a predetermined threshold, providing, via the one or more processors, a VR alert to the driver through the VR display warning the driver of the upcoming area. The method may include additional, fewer, or alternate actions, including those discussed elsewhere herein. For instance, the VR feed may include a VR movie or a VR video game; and/or providing the VR alert may include, via the one or more processors: (i) stopping the VR feed, and/or (ii) displaying the VR alert on the VR display. In some embodiments, providing the VR alert may include, via the one or more processors, overlaying the VR alert onto the VR feed. Additionally or alternatively, the VR alert may include an indication that the driver should take control of the vehicle. In certain embodiments, the VR alert may include a request to the driver to switch the vehicle to a manual mode; and/or the method may further include: in response to the driver accepting the request to switch to the manual mode, switching, via the one or more processors, control of the vehicle from the autonomous mode to the manual mode. In some embodiments, the complexity score for traversing the upcoming area may be determined based upon at least one of: construction, congestion, traffic density, road conditions, road curvature, a traffic accident, a weather condition, or narrow streets in the upcoming area. In other embodiments, the complexity score for traversing the upcoming area may not be determined based upon data generated from sensors in the vehicle. In certain embodiments, providing the VR alert may include presenting, via the one or more processors, a request to the driver to receive training for traversing the upcoming area; and/or the method may further include: in response to the driver accepting the request to receive the training, providing, via the one or more processors, the training for traversing the upcoming area on the VR display. In some embodiments, the complexity score for traversing the upcoming area may be based upon narrow streets in the upcoming area, the narrow streets including a particular narrow street; providing the VR alert may include presenting, via the one or more processors, a request to the driver to receive training for traversing the particular narrow street; and/or the method may further include: in response to the driver accepting the request to receive the training, providing, via the one or more processors, the training for the particular narrow street on the VR display. In another aspect, a computer system configured to provide virtual reality (VR) alerts to a driver of an autonomous vehicle may be provided. The computer system may include one or more local or remote processors, transceivers, and/or sensors configured to: (1) receive an indication that a driver of a vehicle is accessing a VR feed on a VR display; (2) receive an indication that the vehicle is driving in an autonomous mode; (3) determine a complexity score for traversing an upcoming area which the vehicle is approaching; and/or (4) in response to a determination that the complexity score is above a predetermined threshold, provide a VR alert to the driver through the VR display warning the driver of the upcoming area. The computer system may include additional, less, or alternate functionality, including that discussed elsewhere herein. For instance, the VR feed may include a VR movie or a VR video game; and/or providing the VR alert may include: (i) stopping the VR feed, and/or (ii) displaying the VR alert on the VR display. In some embodiments, providing the VR alert may include overlaying the VR alert onto the VR feed. Additionally or alternatively, the VR alert may include an indication that the driver should take control of the vehicle. In some embodiments, the VR alert may include a request to the driver to switch the vehicle to a manual mode; and the one or more local or remote processors, transceivers, and/or sensors may be further configured to: in response to the driver accepting the request to switch to the manual mode, switch control of the vehicle from the autonomous mode to the manual mode. In some embodiments, the complexity score for traversing the upcoming area may be determined based upon at least one of: construction, congestion, traffic density, road conditions, road curvature, a traffic accident, a weather condition, or narrow streets in the upcoming area. In yet another aspect, a computer device for providing virtual reality (VR) alerts to a driver of an autonomous vehicle may be provided. The computer device may include: one or more processors; and one or more memories coupled to the one or more processors. The one or more memories including computer executable instructions stored therein that, when executed by the one or more processors, cause the one or more processors to: (1) receive an indication that a driver of a vehicle is accessing a VR feed on a VR display; (2) receive an indication that the vehicle is driving in an autonomous mode; (3) determine a complexity score for traversing an upcoming area which the vehicle is approaching; and/or (4) in response to a determination that the complexity score is above a predetermined threshold, provide a VR alert to the driver through the VR display warning the driver of the upcoming area. The computer device may include additional, less, or alternate functionality, including that discussed elsewhere herein. For instance, the VR feed may include a VR movie or a VR video game; and/or providing the VR alert may include: (i) stopping the VR feed, and/or (ii) displaying the VR alert on the VR display. In some embodiments, providing the VR alert may include overlaying the VR alert onto the VR feed. Additionally or alternatively, the VR alert may include an indication that the driver should take control of the vehicle. In some embodiments, the VR alert may include a request to the driver to switch the vehicle to a manual mode; and/or the one or more memories including computer executable instructions stored therein that, when executed by the one or more processors, may further cause the one or more processors to: in response to the driver accepting the request to switch to the manual mode, switch control of the vehicle from the autonomous mode to the manual mode. Exemplary System for Generating a VR Feed for Presenting Real-Time Road Conditions Some embodiments disclosed herein advantageously generate a VR feed for presenting real-time road conditions. To illustrate, in one example, a user may be in an autonomous vehicle driving to a destination. Here, it is advantageous for the user to know the road conditions on upcoming portions of the route to a destination. For instance, if there is traffic on the route, the user may wish to reroute the vehicle in order to avoid the traffic. Furthermore, in a second example, a user may be at home about to leave for a trip in a vehicle. Prior to departure, the user may wish to check the road conditions of the route she will take on the trip. To this end, some embodiments disclosed herein advantageously generate a VR feed of a road segment based upon real-time condition data, and provide the generated VR feed for presentation to a user within a VR display for the user to preview the road segment. FIG. 7 shows an exemplary computer system 700 for presenting real-time road conditions. The high-level architecture includes both hardware and software applications, as well as various data communications channels for communicating data between the various hardware and software components. With reference thereto, a user 760 may have VR goggles or a VR headset 762 , which may be controlled by VR gloves 763 . The user 760 may potentially be the driver of the vehicle 750 . The vehicle 750 may be an autonomous vehicle (e.g., a vehicle capable of driving autonomously, semi-autonomously, or in a manual mode, etc.). In this regard, the vehicle 750 may have autonomous operation features that may take full control of the vehicle under certain conditions, viz. fully autonomous operation, or the autonomous operation features may assist the vehicle operator in operating the vehicle, viz. partially autonomous operation. Fully autonomous operation features may include systems within the vehicle that pilot the vehicle to a destination with or without a vehicle operator present (e.g., an operating system for a driverless car). Partially autonomous operation features may assist the vehicle operator in limited ways (e.g., automatic braking or collision avoidance systems). Fully or partially autonomous operation features may perform specific functions to control or assist in controlling some aspect of vehicle operation, or such features may manage or control other autonomous operation features. For example, a vehicle operating system may control numerous subsystems that each fully or partially control aspects of vehicle operation. In addition to information regarding the position or movement of a vehicle, autonomous operation features may collect and utilize other information, such as data about other vehicles or control decisions of the vehicle. Such additional information may be used to improve vehicle operation, route the vehicle to a destination, warn of component malfunctions, advise others of potential hazards, or for other purposes described herein. Information may be collected, assessed, and/or shared via applications installed and executing on computing devices associated with various vehicles or vehicle operators, such as on-board computers of vehicles or smartphones of vehicle operators. By using computer applications to obtain data, the additional information generated by autonomous vehicles or features may be used to assess the autonomous features themselves while in operation or to provide pertinent information to non-autonomous vehicles through an electronic communication network 704 (which may be a wired and/or wireless network, such as the internet). These and other advantages are further described below. Some autonomous operation features may be adapted for use under particular conditions, such as city driving or highway driving. Additionally, the vehicle operator may be able to configure settings relating to the features or may enable or disable the features at will. Therefore, some embodiments monitor use of the autonomous operation features, which may include the settings or levels of feature use during vehicle operation. Information obtained by monitoring feature usage may be used to determine risk levels associated with vehicle operation, either generally or in relation to a vehicle operator. In such situations, total risk may be determined by a weighted combination of the risk levels associated with operation while autonomous operation features are enabled (with relevant settings) and the risk levels associated with operation while autonomous operation features are disabled. For fully autonomous vehicles, settings or configurations relating to vehicle operation may be monitored and used in determining vehicle operating risk. In some embodiments, information regarding the risks associated with vehicle operation with and without the autonomous operation features may be used to determine risk categories or premiums for a vehicle insurance policy covering a vehicle with autonomous operation features, as described elsewhere herein. Risk category or price may be determined based upon factors relating to the evaluated effectiveness of the autonomous vehicle features. The risk or price determination may also include traditional factors, such as location, vehicle type, and level of vehicle use. The vehicle 750 may have various vehicle sensors 752 . The vehicle sensors 752 may be any kind of sensors. Examples of the vehicle sensors 752 include: cameras (e.g., for capturing images and/or video), light detection and ranging (LIDAR) cameras, radio detection and ranging (RADAR) devices, accelerometers, gyroscopes, compasses, speedometers, magnetometers, barometers, thermometers, proximity sensors, light sensors (e.g., light intensity detectors), electromagnetic radiation sensors (e.g., infrared and/or ultraviolet radiation sensors), ultrasonic and/or infrared range detectors, humistors, hygrometers, altimeters, microphones, audio or video recorders, etc. Additional examples vehicle sensors 752 include advanced sensors, for example, that detect and/or receive data associated with temperature measurements, thermal imaging, weather conditions, traffic conditions, etc. The vehicle 750 may include any number or combination of vehicle sensors 752 . The vehicle 750 may further include one or more processors 751 , such as one or more microprocessors, controllers, and/or any other suitable type of processor. The one or more processors 751 may perform any functions. For example, the one or more processors 751 may control the vehicle 751 while it is driving in an autonomous or semi-autonomous mode. In another example, the one or more processors 751 may switch the vehicle 750 between manual, autonomous, and semi-autonomous modes. As will be discussed further below, the one or more processors 751 may perform any of the functions of the VR feed generator application 724 and/or the condition determiner application 726 . The vehicle 750 may further include a smart windshield 754 . The smart windshield 754 may be configured to produce a VR or augmented reality (AR) display, respectively, from a VR feed or AR feed. The example of FIG. 7 also illustrates vehicle 790 , which, in some examples, may be a vehicle on a road traveling ahead of vehicle 750 . Similarly to the vehicle 750 , the vehicle 790 may be an autonomous vehicle. The vehicle 790 may also have sensors 792 , which may be any kind of sensors, such as those discussed above with respect to vehicle sensors 752 . The vehicle 790 may also have one or more processors 791 , such as one or more microprocessors, controllers, and/or any other suitable type of processor. The one or more processors 791 may perform any functions. For example, the one or more processors 791 may control the vehicle 790 while it is driving in an autonomous or semi-autonomous mode. In another example, the one or more processors 791 may switch the vehicle 790 between manual, autonomous, and semi-autonomous modes. It should be understood that the discussions above regarding autonomous vehicles, processors, and sensors with respect to vehicle 750 apply also to vehicle 790 . One or both of the vehicles 750 , 790 may be in communication with smart infrastructure devices 770 . Examples of the smart infrastructure devices include road camera 771 , smart stoplight 772 , smart stop sign 773 , and infrastructure camera 774 . Any of the smart infrastructure devices 770 may include any kind of sensors. For example, any of the smart infrastructure devices 770 may include: cameras (e.g., for capturing images and/or video), light detection and ranging (LIDAR) cameras, radio detection and ranging (RADAR) devices, accelerometers, gyroscopes, compasses, speedometers, magnetometers, barometers, thermometers, proximity sensors, light sensors (e.g., light intensity detectors), electromagnetic radiation sensors (e.g., infrared and/or ultraviolet radiation sensors), ultrasonic and/or infrared range detectors, humistors, hygrometers, altimeters, microphones, audio or video recorders, thermal imaging devices, etc. Furthermore, any of the smart infrastructure devices 770 may include multiple sensors (e.g., any combination of the example sensors just listed). As mentioned above, the techniques described herein advantageously allow the user 760 to use a VR display, such as the VR goggles 760 and/or the smart windshield 754 , to preview a road segment. (Regarding the use of the smart windshield 754 as a VR display, it may be noted that although the example of FIG. 7 illustrates the user 760 outside of the vehicle 750 , in some examples, the user 760 enters the vehicle 750 , e.g., to become the driver of the vehicle 750 .) To this end, VR feed computing device 702 may be used to generate a VR feed of a road segment based upon the real-time condition data. The VR feed computing device 702 may include one or more processors 720 such as one or more microprocessors, controllers, and/or any other suitable type of processor. The VR feed computing device 702 may further include a memory 722 (e.g., volatile memory, non-volatile memory) accessible by the one or more processors 720 , (e.g., via a memory controller). The one or more processors 720 may interact with the memory 722 to obtain, for example, computer-readable instructions stored in the memory 722 . Additionally or alternatively, computer-readable instructions may be stored on one or more removable media (e.g., a compact disc, a digital versatile disc, removable flash memory, etc.) that may be coupled to the VR feed computing device 702 to provide access to the computer-readable instructions stored thereon. In particular, the computer-readable instructions stored on the memory 722 may include instructions for executing various applications, such as a VR feed generator application 724 , and/or a condition determiner application 726 . In some examples, the VR feed generator application 724 may generate a VR feed to preview a road segment. The data that the VR feed generator application 724 uses to generate the VR feed may come from any suitable source, such as the smart infrastructure devices 770 , VR feed database 718 , and/or the external database 780 . To this end, the external database 780 may hold any suitable data. Examples of the data held by external database 780 include historical image data of road segments, historical video data of road segments, and/or historical VR data of road segments. Additional examples include data relating to current road conditions, such as traffic data, weather data, road condition data, etc. The VR feed database 718 may also hold any suitable data. Examples of the data held by external database 780 include historical image data of road segments, historical video data of road segments, and/or historical VR data of road segments. Additional examples include data relating to current road conditions, such as traffic data, weather data, road condition data, etc. The VR feed database 718 may also store information of the VR feed as it is generated. For example, the VR feed database 718 may store a copy of the generated VR feed itself. Additionally, or alternatively, the VR feed database 718 may store information of when and where the VR feed was sent. The VR feed generated by the VR feed generator application 724 may include a virtual representation of the road segment to reflect the real-time conditions at the road segment, and thus may be provided as a display to the user 760 . In this regard, FIG. 8 illustrates an exemplary display 800 showing the real-time conditions at a road segment. The illustrated example further shows that there is a weather condition (e.g., rain) occurring on the road segment. An additional example of the VR feed generated by the VR feed generator application 724 is shown by FIG. 9 . Specifically, FIG. 9 illustrates an exemplary display 900 showing the real-time conditions at a road segment. The illustrated example further shows that there is a traffic condition occurring on the road segment. In the example of FIG. 9 , the driver 760 is viewing the example display 900 on the smart windshield 754 . Conditions, such as the weather condition of FIG. 8 and the traffic condition of FIG. 9 , may be determined by the condition determiner application 726 . More broadly, the condition determiner application 726 may determine any kind of condition. Examples of conditions include weather conditions (e.g., rain, snow, hail, natural disaster, etc.), traffic conditions (e.g., a light traffic condition, a medium traffic condition, a heavy traffic condition, etc.), accident conditions, road conditions, congestion, construction, etc. Any suitable data may be used to determine the conditions. For instance image and/or video data from a smart infrastructure device 770 may be analyzed to determine any of the conditions. The analysis may be done with or without a machine learning algorithm. Exemplary Methods for Generating a VR Feed for Presenting Real-Time Road Conditions FIG. 10 shows an exemplary implementation 1000 of generating a VR feed for presenting real-time road conditions. Although the following discussion refers to many of the blocks of the exemplary implementation 1000 , as well as the example implementations 1100 , 1200 of FIGS. 11 and 12 , as being performed by the one or more processors 720 , it should be understood that any of the blocks or functions of the example implantations 1000 , 1100 , 1200 may be performed by either of the one or more processors 720 of the VR feed computing device 702 , or the one or more processors 751 of the vehicle 750 . The exemplary implementation 1000 begins at block 1005 when the one or more processors 720 obtain real-time condition data indicating condition of a road segment in a geographic area. The real-time condition data may be obtained from any suitable source. For example, the real-time condition data may be obtained from any smart infrastructure device 770 (e.g., road camera 771 , smart stoplight 772 , smart stop sign 773 , infrastructure camera 774 , etc.), external database 780 , sensors of a vehicle (e.g., vehicle sensors 792 ), AR or VR headsets (e.g., camera(s) mounted on AR or VR headsets), etc. Examples of the real-time condition data include imagery data (e.g., image data, video data, LIDAR data, RADAR data, infrared data, etc.), audio data, weather data, traffic data, etc. At block 1010 , the one or more processors 720 determine a condition occurring on the road segment. Examples of the condition include a weather condition (e.g., a rain condition [e.g., FIG. 8 ], a snow condition, an ice condition, a hail condition, a natural disaster condition, etc.), a traffic condition (e.g., FIG. 9 ), a construction condition, a poor road condition, etc. Furthermore, the conditions may have grades associated with them. For example, a traffic condition may be a light traffic condition, a medium traffic condition, a heavy traffic condition, etc. In another example, the weather condition may be a light weather condition, a severe weather condition, etc. The condition may be determined using any suitable technique. For example, the determination may be made based on the real-time condition data obtained at block 1005 . For instance, imagery data, and/or audio data of the real-time condition data may be analyzed to determine the condition. In one example, audio data may be analyzed to determine a weather condition (e.g., a rain, snow, or ice condition or a hail, storm, or wind condition). In another example, imagery data may be used to determine a traffic condition (e.g., imagery data indicates that the density of vehicles on the road segment (or a portion thereof) is above a threshold). In yet another example, a poor road condition may be determined when it is determined that a road has a density of potholes with a depth greater than a predetermined value. To this end, a machine learning algorithm(s) may be used to determine the condition. For example, the real-time condition data may be input into a trained machine learning algorithm to determine the condition. Furthermore, the machine learning algorithm may have been trained by any suitable technique (e.g., supervised learning, unsupervised learning, semi-supervised learning). Examples of the machine learning algorithm may include neural networks, deep learning algorithms, etc. At block 1015 , the one or more processors 720 generate a VR feed of the road segment based upon the real-time condition data. In some embodiments, the generation of the VR feed of the road segment occurs in response to the determination that there is a condition at block 1010 . For example, the one or more processors 720 may be continually analyzing the real-time condition data; and, when a condition is found, the VR feed is generated. Advantageously, generating the VR feed upon the determination of the condition saves processing power (e.g., of the one or more processors 720 ), and/or bandwidth (e.g., the VR feed is only generated/transmitted when necessary, thus saving processing power, and bandwidth). However, in other embodiments, the VR feed is continually generated, and then transmitted only upon the determination of the condition. For example, the VR feed may be continually generated from real-time condition data from smart infrastructure devices 770 , but then transmitted (e.g., to the vehicle 750 , and/or VR goggles 762 , etc.) only upon a determination of a condition (e.g., at block 1010 ). It may be noted that these embodiments advantageously save bandwidth (e.g., because the VR feed does not need to be continuously transmitted), but have the drawback of not saving processing power (e.g., because the VR feed is continually being generated). In some embodiments, it may be useful for the driver of a vehicle to view what is happening ahead of his vehicle. As such, in some embodiments, the VR feed may be generated from data from a vehicle directly ahead of a vehicle that the user 760 is traveling in, such that the user 760 may have a VR view of what is happening ahead of his vehicle. Furthermore, the generation of the VR feed may be triggered by the particular type of condition. For instance, a poor road condition (e.g., due to a high density of potholes) shortly ahead of the vehicle 750 may trigger generation of a VR feed from vehicle(s) ahead of the vehicle 750 . The VR feed may be generated by any suitable technique. For example, the VR feed may be generated based upon data from a single device (e.g., from any of the smart infrastructure devices 770 , from a sensor of the vehicle 790 , etc.). Alternatively, the VR feed may be generated based upon data from multiple devices. In some embodiments, the VR feed is generated based upon different devices with different device types. For example, the VR feed may be generated from: (i) a first smart infrastructure device 770 comprising a video camera, and (ii) a second smart infrastructure device 770 comprising a LIDAR camera. At block 1020 , the generated VR feed is provided to the user 760 for presentation within a VR display (e.g., the VR goggles 762 , smart windshield 754 , etc.). It may be noted that the user 760 may or may not be located in the vehicle 750 . Advantageously, if the user is not located in the vehicle 750 (e.g., the user is at home with the VR goggles 762 ), the user 760 may determine if she wants to embark on a trip based on the condition depicted in the VR feed. FIG. 11 shows an exemplary implementation 1100 , including a selection of a geographic area by the user 760 . The exemplary implementation 1100 begins at block 1105 when the user 760 is presented with a virtual map on the virtual display (e.g., the VR goggles 762 , smart windshield 754 , etc.). In some embodiments, the virtual map may be partitioned into geographic areas (e.g., sections), therefore allowing the user 760 to easily select any of the geographic areas by clicking on a desired geographic area. In some examples, the geographic areas are geometric sections (e.g., squares, rectangles, polygons, etc.) on the map. In other embodiments, the geographic areas are roads (e.g., a section of road a quarter mile long). In yet other embodiments, the geographic areas are based upon geographical boundaries (e.g., based upon neighborhoods, counties, communities, city boundaries, etc.). At block 1100 , the one or more processors 720 receives a selection of a geographic area from the user 760 . If the virtual map has been segmented in any way, the user 760 may select the geographic area by clicking on the geographic area. In some embodiments where the virtual map has not been segmented, the user may select the geographic area by “drawing” on the map. For example, the user may circle an area on the map to create (and therefore select) the geographic area. In some embodiments, the user 760 may make a voice selection of the geographic area. For example, the user 760 may say, “select southwest corner of Chicago.” This voice selection is advantageous in implementations where the user 760 is wearing VR goggles 762 that do not receive inputs easily allowing for a selection on the presented virtual map. Following block 1110 , at block 1005 , the one or more processors 720 obtain real-time condition data indicating conditions of the road segment in the geographic area (e.g., selected at block 1110 ). This occurs substantially as in block 1005 of FIG. 10 ; however, in some embodiments, the obtaining of the real-time condition data at block 1005 occurs in response to the selection at block 1110 . Blocks 1010 - 1020 occur substantially as in the exemplary implementation 1000 of FIG. 10 . FIG. 12 shows an exemplary implementation 1200 , including determining a route that the vehicle 750 is on. The exemplary implementation 1200 begins at block 1205 when the one or more processors 720 determine a route that a vehicle 750 is on. The route may be determined by any suitable method. For example, the route may be received as input (e.g., into a smartphone of the user 760 , into a GPS device of the vehicle 750 , etc.) by the user 760 . In other examples, the route may be determined via a prediction made by the one or more processors 120 . For example, the one or more processors may predict the route (possibly including predicting the destination) based upon current location, trip staring location, time of day, previous routes of travel, previous destinations, profile information (e.g., of the vehicle 750 , and/or user 760 ), direction of travel, speed of travel, etc. Once the route has been determined, the one or more processors 120 may receive an input of a range of miles from the user (block 1210 ). The range of miles may be a range of miles on the route ahead of the user. For example, the user may input a range 2-4 miles. In some embodiments, such as in the example implementation 1200 , the range of miles on the route is the road segment (e.g., of the geographic area) that the VR feed will preview. At block 1215 , the one or more processors 720 may determine a second vehicle (e.g., vehicle 790 ) within the range of miles (received at block 1210 ) ahead of the vehicle 750 on the route. The one or more processors 720 may also make a determination(s) of whether or not the second vehicle 790 (determined at block 1210 ) is: capable of transmitting real-time condition data, and/or (ii) has consented to transmit real-time condition data. For example, the one or more processors 720 may receive automatically broadcast data from the second vehicle 790 (e.g., through the network 704 ), and make this determination based upon the received data. Additionally or alternatively, the one or more processors 720 may make this determination by using identification information of the second vehicle 790 to look up this information in a database (e.g., external database 780 ). Following block 1150 , at block 1205 , the one or more processors 720 obtain real-time condition data indicating conditions of the road segment (e.g., the range of miles input at block 1210 ) in the geographic area. For example, the obtained real time condition data may include data generated by a smart camera of the second vehicle 790 . Block 1205 occurs substantially as in block 1005 of FIG. 10 ; however, in some embodiments, the obtaining of the real-time condition data at block 1205 occurs in response to receiving the input at block 1210 , and determining the second vehicle at block 1215 . Further, at block 1215 , as part of obtaining the real-time condition data, the one or more processors 720 may send a request to the second vehicle 790 for real-time condition data. In response, the second vehicle 790 may send the real-time condition data to the vehicle 750 . Alternatively, in some embodiments, the second vehicle automatically sends real-time condition data to the external database 780 ; thus, in these embodiments, the one or more processors 720 may obtain the real-time condition data from the external database 780 , rather than request permission from the second vehicle 790 . Blocks 1010 - 1020 occur substantially as in the exemplary implementations 1000 , 1100 of FIGS. 10 and 11 . It should be understood that not all blocks of the exemplary flowcharts 500 , 600 , 1000 , 1100 , 1200 are required to be performed. Moreover, the exemplary flowcharts 500 , 600 , 1000 , 1100 , 1200 are not mutually exclusive (e.g., block(s) from each exemplary flowchart 500 , 600 , 1000 , 1100 , 1200 may be performed in any other flowchart). The exemplary flowcharts 500 , 600 , 1000 , 1100 , 1200 may include additional, less, or alternate functionality, including that discussed elsewhere herein. Applicability to the Insurance Industry Some embodiments have particular applicability to the insurance industry. For example, discounts to insurance premiums may be provided by the techniques described herein. For instance, if a user 760 agrees to allow her vehicle to send real-time condition data (e.g., to the external database 780 , thereby allowing others to preview severe weather and/or other dangerous conditions), the user 760 may receive a discount on an insurance premium. In one aspect, data from the vehicle 750 , and/or other data, including the types of data discussed elsewhere herein, may be collected or received by an insurance provider remote server, such as via direct or indirect wireless communication or data transmission from a smart home controller, mobile device, or other customer computing device, after a customer affirmatively consents or otherwise opts-in to an insurance discount, reward, or other program. The insurance provider may then analyze the data received with the customer's permission to provide benefits to the customer. As a result, risk averse customers may receive insurance discounts or other insurance cost savings based upon data that reflects low risk behavior and/or technology that mitigates or prevents risk to autonomous vehicles. Exemplary Use of Generating a VR Feed for Presenting Real-Time Road Conditions In one aspect, a computer-implemented method for generating a virtual reality (VR) feed for presenting real-time road conditions may be provided. The method may include: (1) obtaining, via one or more processors, real-time condition data indicating conditions of a road segment in a geographic area; (2) generating, via the one or more processors, a VR feed of the road segment based upon the real-time condition data, the VR feed including a virtual representation of the road segment to reflect the real-time conditions at the road segment; and/or (3) providing, via the one or more processors, the generated VR feed for presentation to a user within a VR display for the user to preview the road segment. The method may include additional, fewer, or alternate actions, including those discussed elsewhere herein. For instance, the real-time condition data may include (i) weather data, (ii) traffic data, and/or (iii) imagery data from: smart glasses, AR/VR headsets, smart vehicle cameras, and/or vehicles or passengers ahead of the user. In some embodiments, the VR display may include a display via VR goggles or a smart windshield display. In some embodiments, the method may further include: determining, via the one or more processors, that a weather condition is occurring on the road segment; and/or wherein generating the VR feed of the road segment occurs in response to the determination that the weather condition is occurring on the road segment. In certain embodiments, the method may further include determining, via the one or more processors, that a traffic condition is occurring on the road segment; and/or wherein generating the VR feed of the road segment occurs in response to the determination that the traffic condition is occurring on the road segment. In some embodiments, the VR display may include a smart windshield display; and/or the real-time condition data may include data generated by a smart vehicle camera of a vehicle directly ahead of a vehicle that the user is traveling in. Additionally or alternatively, the VR display may include a smart windshield display, and/or the method may further include: determining, via the one or more processors, a route that a vehicle of the user is on, wherein the vehicle is a first vehicle; receiving, via the one or more processors, an input of a range of miles from the user; and/or determining, via the one or more processors, a second vehicle, the second vehicle being on the route within the range of miles ahead of the first vehicle; and/or wherein the real-time condition data includes data generated by a smart camera of the second vehicle. In some embodiments, obtaining the real-time condition data indicating the conditions of the road segment in the geographic area may occur in response to a selection from the user of the geographic area. In certain embodiments, the method may further include, prior to obtaining the real-time condition data: presenting, via the one or more processors, a virtual map to the user on the VR display, wherein the virtual map includes the geographic area; and/or receiving, via the one or more processors, a selection of the geographic area by the user; and/or wherein obtaining the real-time condition data occurs in response to the selection of the geographic area by the user. In another aspect, a computer system configured to generate a virtual reality (VR) feed for presenting real-time road conditions may be provided. The computer system may include one or more local or remote processors, transceivers, and/or sensors configured to: (1) obtain real-time condition data indicating conditions of a road segment in a geographic area; (2) generate a VR feed of the road segment based upon the real-time condition data, the VR feed including a virtual representation of the road segment to reflect the real-time conditions at the road segment; and/or (3) provide the generated VR feed for presentation to a user within a VR display for the user to preview the road segment. The computer system may include additional, less, or alternate functionality, including that discussed elsewhere herein. For instance, the real-time condition data may include (i) weather data, (ii) traffic data, and/or (iii) imagery data from: smart glasses, AR/VR headsets, smart vehicle cameras, and/or vehicles or passengers ahead of the user. In some embodiments, the VR display may include a display via VR goggles or a smart windshield display. In some embodiments, the one or more local or remote processors, transceivers, and/or sensors may be further configured to: determine that a weather condition is occurring on the road segment; and/or wherein generation of the VR feed of the road segment occurs in response to the determination that the weather condition is occurring on the road segment. In certain embodiments, the one or more local or remote processors, transceivers, and/or sensors may be further configured to: determine that a traffic condition is occurring on the road segment; and/or wherein generating the VR feed of the road segment occurs in response to the determination that the traffic condition is occurring on the road segment. Additionally or alternatively, the VR display may include a smart windshield display; and/or the real-time condition data may include data generated by a smart vehicle camera of a vehicle directly ahead of a vehicle that the user is traveling in. In yet another aspect, a computer device for generating a virtual reality (VR) feed for presenting real-time road conditions, the computer device comprising: one or more processors; and one or more memories coupled to the one or more processors. The one or more memories including computer executable instructions stored therein that, when executed by the one or more processors, cause the one or more processors to: (1) obtain real-time condition data indicating conditions of a road segment in a geographic area; (2) generate a VR feed of the road segment based upon the real-time condition data, the VR feed including a virtual representation of the road segment to reflect the real-time conditions at the road segment; and/or (3) provide the generated VR feed for presentation to a user within a VR display for the user to preview the road segment. The computer device may include additional, less, or alternate functionality, including that discussed elsewhere herein. For instance, the real-time condition data may include (i) weather data, (ii) traffic data, and/or (iii) imagery data from: smart glasses, AR/VR headsets, smart vehicle cameras, and/or vehicles or passengers ahead of the user. In some embodiments, the VR display may include a display via VR goggles or a smart windshield display. In some embodiments, the one or more memories including computer executable instructions stored therein that, when executed by the one or more processors, may further cause the one or more processors to: determine that a weather condition is occurring on the road segment; and/or generation of the VR feed of the road segment occurs in response to the determination that the weather condition is occurring on the road segment. In certain embodiments, the one or more memories including computer executable instructions stored therein that, when executed by the one or more processors, may further cause the one or more processors to: determine that a traffic condition is occurring on the road segment; and/or generating the VR feed of the road segment occurs in response to the determination that the traffic condition is occurring on the road segment. Exemplary System for Generating a VR Feed Corresponding to an Event Some embodiments relate to generating a VR feed corresponding to an event. For example, a user may be preparing to leave her home for an upcoming trip. However, shortly before the user is about to leave, there may be a vehicle collision on a route that the user was about to take. As such, it would be useful to the user to obtain information of the vehicle collision so that he may determine if an alternate route (or even cancelling or postponing the trip) is desirable. As disclosed herein, in this example, a VR feed may be provided to the user so that the user can experience the geographic area where the collision occurred within a VR environment, thus allowing the user to determine if taking an alternate route is warranted. To this end, FIG. 13 shows an exemplary computer system 1300 for generating a VR feed corresponding to an event (e.g., a vehicle collision, a crime, a weather event, or a natural disaster). The high-level architecture includes both hardware and software applications, as well as various data communications channels for communicating data between the various hardware and software components. Broadly speaking, the VR feed computing device 1302 may obtain an indication of an event occurring in a geographic area. The VR feed computing device 1302 may then generate a VR feed of the geographic area at a time of the event based upon real-time condition data from the geographic area, thereby allowing a user 1360 to experience the geographic area where the event occurred within a VR environment (e.g., view the geographic area on a VR display, such as VR goggles 1362 possibly controlled by VR gloves 1363 , smart windshield 1354 , etc.). The VR feed computing device 1302 may include one or more processors 1320 such as one or more microprocessors, controllers, and/or any other suitable type of processor. The VR feed computing device 1302 may further include a memory 1322 (e.g., volatile memory, non-volatile memory) accessible by the one or more processors 1320 , (e.g., via a memory controller). The one or more processors 1320 may interact with the memory 1322 to obtain, for example, computer-readable instructions stored in the memory 1322 . Additionally or alternatively, computer-readable instructions may be stored on one or more removable media (e.g., a compact disc, a digital versatile disc, removable flash memory, etc.) that may be coupled to the VR feed computing device 1302 to provide access to the computer-readable instructions stored thereon. In particular, the computer-readable instructions stored on the memory 1322 may include instructions for executing various applications, such as a VR feed generator application 1324 , and/or an event determiner application 1326 . In some examples, the VR feed generator application 1324 may generate a VR feed to allow a user 1360 to experience the geographic area where the event occurred within a VR environment. The data that the VR feed generator application 1324 uses to generate the VR feed may come from any suitable source, such as the smart infrastructure devices 1370 , VR feed and event database 1318 , and/or the external database 1380 . To this end, the external database 1380 may hold any suitable data. Examples of the data held by external database 1380 include historical image data of geographic areas, historical video data of geographic areas, and/or historical VR data of geographic areas. Additional examples include data relating to current conditions in geographic areas, such as traffic data, crime data, weather data, road condition data, etc. The VR feed and event database 1318 may also hold any suitable data. Examples of the data held by the VR feed and event database 1318 include historical image data of road segments, historical video data of road segments, and/or historical VR data of road segments. Additional examples include current data of geographic areas, such as traffic data, vehicle collision data, crime data, weather data, road condition data, etc. The VR feed and event database 1318 may also store information of the VR feed as it is generated. For example, the VR feed and event database 1318 may store a copy of the generated VR feed itself. Additionally, or alternatively, the VR feed and event database 1318 may store information of when and where the VR feed was sent. In some embodiments, the user 1360 may experience the geographic area (via the VR feed provided by the VR feed generator application 1324 ) while she is at home, or not in any vehicle. However, in some examples, the user 1360 may be inside of a vehicle, such as vehicle 1350 . The vehicle 1350 may be an autonomous vehicle (e.g., a vehicle capable of driving autonomously, semi-autonomously, or in a manual mode, etc.). In this regard, the vehicle 1350 may have autonomous operation features that may take full control of the vehicle under certain conditions, viz. fully autonomous operation, or the autonomous operation features may assist the vehicle operator in operating the vehicle, viz. partially autonomous operation. Fully autonomous operation features may include systems within the vehicle that pilot the vehicle to a destination with or without a vehicle operator present (e.g., an operating system for a driverless car). Partially autonomous operation features may assist the vehicle operator in limited ways (e.g., automatic braking or collision avoidance systems). Fully or partially autonomous operation features may perform specific functions to control or assist in controlling some aspect of vehicle operation, or such features may manage or control other autonomous operation features. For example, a vehicle operating system may control numerous subsystems that each fully or partially control aspects of vehicle operation. In addition to information regarding the position or movement of a vehicle, autonomous operation features may collect and utilize other information, such as data about other vehicles or control decisions of the vehicle. Such additional information may be used to improve vehicle operation, route the vehicle to a destination, warn of component malfunctions, advise others of potential hazards, or for other purposes described herein. Information may be collected, assessed, and/or shared via applications installed and executing on computing devices associated with various vehicles or vehicle operators, such as on-board computers of vehicles or smartphones of vehicle operators. By using computer applications to obtain data, the additional information generated by autonomous vehicles or features may be used to assess the autonomous features themselves while in operation or to provide pertinent information to non-autonomous vehicles through an electronic communication network 1304 (which may be a wired and/or wireless network, such as the internet). These and other advantages are further described below. Some autonomous operation features may be adapted for use under particular conditions, such as city driving or highway driving. Additionally, the vehicle operator may be able to configure settings relating to the features or may enable or disable the features at will. Therefore, some embodiments monitor use of the autonomous operation features, which may include the settings or levels of feature use during vehicle operation. Information obtained by monitoring feature usage may be used to determine risk levels associated with vehicle operation, either generally or in relation to a vehicle operator. In such situations, total risk may be determined by a weighted combination of the risk levels associated with operation while autonomous operation features are enabled (with relevant settings) and the risk levels associated with operation while autonomous operation features are disabled. For fully autonomous vehicles, settings or configurations relating to vehicle operation may be monitored and used in determining vehicle operating risk. In some embodiments, information regarding the risks associated with vehicle operation with and without the autonomous operation features may be used to determine risk categories or premiums for a vehicle insurance policy covering a vehicle with autonomous operation features, as described elsewhere herein. Risk category or price may be determined based upon factors relating to the evaluated effectiveness of the autonomous vehicle features. The risk or price determination may also include traditional factors, such as location, vehicle type, and level of vehicle use. The vehicle 1350 may have various vehicle sensors 1352 . The vehicle sensors 1352 may be any kind of sensors. Examples of the vehicle sensors 1352 include: cameras (e.g., for capturing images and/or video), light detection and ranging (LIDAR) cameras, radio detection and ranging (RADAR) devices, accelerometers, gyroscopes, compasses, speedometers, magnetometers, barometers, thermometers, proximity sensors, light sensors (e.g., light intensity detectors), electromagnetic radiation sensors (e.g., infrared and/or ultraviolet radiation sensors), ultrasonic and/or infrared range detectors, humistors, hygrometers, altimeters, microphones, audio or video recorders, etc. Additional examples vehicle sensors 1352 include advanced sensors, for example, that detect and/or receive data associated with temperature measurements, thermal imaging, weather conditions, traffic conditions, etc. The vehicle 1350 may include any number or combination of vehicle sensors 1352 . The vehicle 1350 may further include one or more processors 1351 , such as one or more microprocessors, controllers, and/or any other suitable type of processor. The one or more processors 1351 may perform any functions. For example, the one or more processors 1351 may control the vehicle 1351 while it is driving in an autonomous or semi-autonomous mode. In another example, the one or more processors 1351 may switch the vehicle 1350 between manual, autonomous, and semi-autonomous modes. As will be discussed further below, the one or more processors 1351 may perform any of the functions of the VR feed generator application 1324 and/or the event determiner application 1326 . The vehicle 1350 may further include a smart windshield 1354 . The smart windshield 1354 may be configured to produce a VR or augmented reality (AR) display, respectively, from a VR feed or AR feed. The example of FIG. 13 also illustrates vehicle 1390 , which, in some examples, may be a vehicle on a road traveling ahead of vehicle 1350 . Similarly to the vehicle 1350 , the vehicle 1390 may be an autonomous vehicle. The vehicle 1390 may also have sensors 1392 , which may be any kind of sensors, such as those discussed above with respect to vehicle sensors 1352 . The vehicle 1390 may also have one or more processors 1391 , such as one or more microprocessors, controllers, and/or any other suitable type of processor. The one or more processors 1391 may perform any functions. For example, the one or more processors 1391 may control the vehicle 1390 while it is driving in an autonomous or semi-autonomous mode. In another example, the one or more processors 1391 may switch the vehicle 1390 between manual, autonomous, and semi-autonomous modes. It should be understood that the discussions above regarding autonomous vehicles, processors, and sensors with respect to vehicle 1350 apply also to vehicle 1390 . One or both of the vehicles 1350 , 1390 may be in communication with smart infrastructure devices 1370 . Examples of the smart infrastructure devices 1370 include road camera 1371 , smart stoplight 1372 , smart stop sign 1373 , and infrastructure camera 1374 . Any of the smart infrastructure devices 1370 may include any kind of sensors. For example, any of the smart infrastructure devices 1370 may include: cameras (e.g., for capturing images and/or video), light detection and ranging (LIDAR) cameras, radio detection and ranging (RADAR) devices, accelerometers, gyroscopes, compasses, speedometers, magnetometers, barometers, thermometers, proximity sensors, light sensors (e.g., light intensity detectors), electromagnetic radiation sensors (e.g., infrared and/or ultraviolet radiation sensors), ultrasonic and/or infrared range detectors, humistors, hygrometers, altimeters, microphones, audio or video recorders, thermal imaging devices, etc. Furthermore, any of the smart infrastructure devices 1370 may include multiple sensors (e.g., any combination of the example sensors just listed). In some embodiments, the VR feed computing device 1302 receives the indication of the event occurring in a geographic area from drone 1395 , which may be equipped with any kind of sensors. Examples of sensors that the drone 1395 may be equipped with include: cameras (e.g., for capturing images and/or video), light detection and ranging (LIDAR) cameras, radio detection and ranging (RADAR) devices, accelerometers, gyroscopes, compasses, speedometers, magnetometers, barometers, thermometers, proximity sensors, light sensors (e.g., light intensity detectors), electromagnetic radiation sensors (e.g., infrared and/or ultraviolet radiation sensors), ultrasonic and/or infrared range detectors, humistors, hygrometers, altimeters, microphones, audio or video recorders, thermal imaging devices, etc. Furthermore, the drone 1395 may include multiple sensors (e.g., any combination of the example sensors just listed). The drone 1395 may also send data (e.g., generated by any of its sensors) to the VR feed computing device 1302 to be used for generating the VR feed. In some embodiments, a VR feed generated by the VR computing device 1302 may be sent to an emergency response entity 1399 . Examples of the emergency response entity 1399 include a police station, a fire station, a government office, a helicopter pad, etc. Exemplary Presented VR Feeds As previously mentioned, the one or more processors 1320 (or the one or more processors 1351 ) may generate a VR feed (e.g., in response to an indication of an event). In this regard, FIG. 14 shows an exemplary display 1400 of a generated VR feed. More specifically, in this example, the event is a vehicle collision event; in particular, vehicle 1410 and vehicle 1420 have collided. FIG. 15 shows an exemplary display 1500 of another generated VR feed. More specifically, in this example, the event is a crime event. In particular, burglar 1510 is running out of bank 1520 while carrying a stolen bag of money 1530 . In the example of FIG. 15 , the display 1500 is displayed on the smart windshield 1354 . FIG. 16 shows an exemplary display 1600 of yet another generated VR feed. More specifically, in this example, the event is a weather event; in particular, a hail event. To further elaborate, the exemplary display 1600 shows a road 1610 occupied by vehicles 1620 with hail 1630 falling on them. FIG. 17 shows an exemplary display 1700 of another generated VR feed. More specifically, in this example, the event is a natural disaster event; in particular, a forest fire event. To further elaborate, the exemplary display 1700 shows a road 1710 with vehicles 1720 traveling through a forest fire 1730 . Furthermore, in some embodiments, and as will be described further below, the VR feed generator application 1324 may anonymize particular items in the VR feed. Examples of items that may be anonymized are a face of an individual, identifying information of an individual, and/or license plates. To this end, FIG. 18 illustrates an exemplary display 1800 of a generated VR feed, including anonymized items. In particular, in the illustrated example, the indicated event is a crime event. Vehicle 1810 , driven by driver 1820 , is being driven near a crime scene. In particular, a bank 1845 is being robbed by bank robber 1830 , who is carrying a stolen bag of money 1840 . To anonymize the data, the one or more processors 1320 have blurred out: (i) the face 1825 of the driver 1820 , (ii) the face 1835 of the bank robber 1830 , and/or (iii) the license plate 1815 of the vehicle 1810 . In another example, rather than blurring out items, the items may be replaced with other items, such as avatars. In this regard, FIG. 19 shows an exemplary display 1900 relating to a crime event. In the illustrated example, vehicle 1910 is being driven by driver 1920 near a crime scene. In particular, a bank 1945 is being robbed by bank robber 1930 , who is carrying a stolen bag of money 1940 . To anonymize the identities of the driver 1920 and bank robber 1930 , the driver 1920 has been replaced with avatar 1921 , and the bank robber 1930 has been replaced with avatar 1931 . The license plate 1915 has been blurred out. Exemplary Methods for Generating a VR Feed Corresponding to an Event FIG. 20 shows an exemplary implementation 2000 of generating a VR feed corresponding to an event. Although the following discussion refers to many of the blocks of the exemplary implementation 2000 , as well as the exemplary implementation 2100 of FIG. 21 , as being performed by the one or more processors 1320 , it should be understood that any of the blocks or functions of the example implantations 2000 , 2100 may be performed by either of the one or more processors 1320 of the VR feed computing device 1302 , or the one or more processors 1351 of the vehicle 1350 . The exemplary implementation 2000 begins at block 2005 when the one or more processors 1320 obtain an indication of an event occurring in a geographic area. Examples of the event include a vehicle collision, a crime, a weather event, or a natural disaster. The indication of the event may be obtained from any suitable source. For example, the indication may be obtained from: any of the smart infrastructure devices 1370 (e.g., road camera 1371 , smart stoplight 1372 , smart stop sign 1373 , infrastructure camera 1374 , etc.), a vehicle (e.g., vehicle 1390 and/or 1350 ), external database 1380 , drone 1395 , etc. In some examples, these sources are continually monitoring for events. For example, a smart infrastructure device 1370 may be continuously analyzing data (e.g., imagery data) that it generates to determine if an event has occurred. If so, it sends an indication to the one or more processors 1320 that the event has occurred. The indication may also include the type of event. In another example, the smart infrastructure devices do not analyze the data that they generate, and rather send their raw data (e.g., imagery data) to the one or more processors 1320 . In these examples, the one or more processors 1320 (e.g., by using the event determiner application 1326 ) analyzes the raw data to determine that an event has occurred (e.g., thus the one or more processers 1320 analyze the data to obtain the indication that the event has occurred). The one or more processors 1320 may use any suitable technique to determine that the event has occurred. For example, any data sent to the one or more processors 1320 , such as data sent from a smart infrastructure device 1370 and/or drone 1395 , may be input into a trained machine learning algorithm to determine the event. Furthermore, the machine learning algorithm may have been trained by any suitable technique (e.g., supervised learning, unsupervised learning, semi-supervised learning). Examples of the machine learning algorithm may include neural networks, deep learning algorithms, etc. In some examples, the geographic area corresponds to a geometric section on a map (e.g., a squares, a rectangle, a polygon, etc.) on the map. In other embodiments, the geographic area corresponds to a road (e.g., a section of road a quarter mile long). In other embodiments, the geographic area is based upon geographical boundaries (e.g., based upon neighborhoods, counties, communities, city boundaries, etc.). At block 2010 , the one or more processors 1320 generate a VR feed of the geographic area at a time of the event based upon real-time condition data from the geographic area. The real-time condition data may be obtained from any suitable source. For example, the real-time condition data may be obtained from any smart infrastructure device 1370 (e.g., road camera 1371 , smart stoplight 1372 , smart stop sign 1373 , infrastructure camera 1374 , etc.), external database 1380 , sensors of a vehicle (e.g., vehicle sensors 1392 ), AR or VR headsets (e.g., camera(s) mounted on AR or VR headsets), drone 1395 , etc. Examples of the real-time condition data include imagery data (e.g., image data, video data, LIDAR data, RADAR data, infrared data, etc.), audio data, weather data, traffic data, etc. The generated VR feed may include a virtual representation of the geographic area at the time of the event to reflect the real-time conditions at the geographic area. In some embodiments, the VR feed is generated in response to obtaining the indication of the event. In one example, vehicle 1390 is traveling ahead of vehicle 1350 on a route. The processors 1391 determine (e.g., from data from the vehicle sensors 1392 ) that a vehicle collision has occurred ahead of the vehicle 1390 . The one or more processors 1320 may then obtain an indication that the event has occurred, and then generate a VR feed of the road segment based upon data received form the vehicle 1390 (and/or any other source, e.g., a smart infrastructure device 1370 , the external database 1380 , the drone 1395 , etc.). Advantageously, generating the VR feed based upon data from multiple sources (e.g., cameras from multiple vehicles, a camera of a vehicle plus a smart infrastructure device 1370 , etc.) creates a higher quality VR feed. At block 2015 , the one or more processors 1320 anonymize (or partially anonymize) identifying information (e.g., items that could potentially be used to identify an individual). For example, the one or more processors 1320 may first identify, in the VR feed, items that could be used to identify an individual; subsequently, the one or more processors 1320 may blur the identified items. Examples of items that could be used to identify an individual include a face of an individual, a body of an individual (e.g., a representation of the individual), items with identifying information (e.g., a name tag, a license plate, etc.), etc. In this regard, FIG. 18 shows an example display 1800 with faces 1825 , 1835 , and license plate 1815 all blurred out. Alternatively to blurring, identified items may be replaced by objects, such as avatars. In this regard, FIG. 19 shows an example display 1900 with the body of driver 1920 replaced by avatar 1921 , and the body of bank robber 1930 replaced with avatar 1931 . Such embodiments are particularly advantageous when the event is a crime event. For example, if a person depicted in a VR feed is a victim of a crime or a criminal suspect, it may be desirable to remove identifying information of the person. As such, in some embodiments, the one or more processors 1320 first determine the type of event; and then, if the event is a crime event, apply the blurring or replacement of the items. Furthermore, in some embodiments, if the type of event is a crime event, the one or more processors 1320 make a further determination of if an individual in the VR feed is a victim of a crime; and, if the individual is the victim of a crime, then the one or more processors 1320 performs the blurring or replacement of the items that could be used to identify the individual. At block 2020 , the one or more processors 1320 provide the generated VR feed for presentation to the user 1360 within a virtual reality display (e.g., the VR goggles 1362 , the smart windshield 1354 , etc.) for the user 1360 to experience the geographic area where the event occurred within a VR environment. In some embodiments, the user 1320 experiences the VR environment by navigating through the VR environment. For example, if the VR display comprises the VR goggles 1362 , the user 1320 may use the VR gloves 1363 to navigate through the VR environment. In another example, if the VR display comprises the smart windshield 1354 , the user 1360 may use her smartphone, or a dashboard in the vehicle 1350 to navigate through the VR environment. To this end, navigation though the VR environment is sometimes only possible if real-time condition data is obtained from more than one source. For example, if the real-time condition data is obtained only from a single road camera 1371 , it might not be possible to navigate, in the VR environment, outside of a field of view (FOV) of the road camera 1371 . Thus, in some embodiments, the one or more processors 1320 allow the user to navigate through the VR environment only if the real-time condition data comes from more than one source (e.g., a smart infrastructure device 1370 and a vehicle sensor 1392 ). Furthermore, advantageously, the presentation may be improved if the real-time condition data comes from different kinds of sensors. For example, real-time condition data from a LIDAR camera (to provide superior depth information) may be combined with video information (to provide color information) to produce an improved presentation. At block 2025 , the one or more processors 1320 send the VR feed to an emergency response entity 1399 . Examples of the emergency response entity 1399 include a police station, a fire station, a government office, a helicopter pad, etc. Further, the one or more processors 1320 may indicate to the user 1360 that the VR feed has been sent to the emergency response entity 1399 . For example, an indication indicating that the VR feed has been sent to the emergency response entity 1399 may be superimposed onto the VR feed that the user 1360 is viewing. The indication may also specify which emergency response entity 1399 the VR feed has been sent to (e.g., an indication indicating “VR feed sent to local police,” “VR feed sent to state police,” “VR feed sent to local fire station,” etc.). In addition, the feed sent to the emergency response entity 1399 may or may not have information anonymized. In one example, the VR feed experienced by the user 1320 has a suspect's face (as identified by the one or more processors 1320 ) blurred out, but the VR feed sent to the emergency response entity 1399 (e.g., a law enforcement agency) does not have the suspect's face blurred out. FIG. 21 illustrates an exemplary implementation 2100 , including an example where a second event occurs. With reference thereto, blocks 2005 - 2020 occur substantially as in the example of FIG. 20 . It should be understood that the event referred to in blocks 2005 - 2020 of FIG. 21 is a first event, and the indication referred to in blocks 2005 - 2020 of FIG. 21 is a first indication. At block 2105 , the one or more processors 1320 obtain a second indication of a second event. The second indication may be obtained any way the first indication was obtained (e.g., at block 2005 ). At block 2110 , the one or more processors 1320 interrupt the providing of the generated VR feed by providing an option to the user to experience the second event. The interruption may be provided by any suitable technique. For example, a visual message may be superimposed onto the VR feed. Examples of the messages include messages indicating: “A fire has been detected in the area. Would you like to experience the newly detected fire?” “Additional rain has been detected in the area. Would you like to change experiences to the newly detected rain?” etc. In other examples of the visual message, the VR feed may be stopped, and the visual message (such as the example messages given above) may be displayed on the VR display, rather than superimposed onto the VR feed. Additionally or alternatively, the interruption may be auditory. For example, a voice may read the words “A fire has been detected in the area. Would you like to experience the newly detected fire?” “Additional rain has been detected in the area. Would you like to change experiences to the newly detected rain?” etc. Advantageously, an interruption comprising an audio component, but not a visual component, may cause less of a disruption to the experience of the first event that the user 1360 is experiencing. However, it should be understood that the interruption may comprise both an audio and a visual component. It should be understood that not all blocks of the exemplary flowcharts 500 , 600 , 1000 , 1100 , 1200 , 2000 , 2100 are required to be performed. Moreover, the exemplary flowcharts 500 , 600 , 1000 , 1100 , 1200 , 2000 , 2100 are not mutually exclusive (e.g., block(s) from each example flowchart 500 , 600 , 1000 , 1100 , 1200 , 2000 , 2100 may be performed in any other flowchart). The exemplary flowcharts may include additional, less, or alternate functionality, including that discussed elsewhere herein. Applicability to the Insurance Industry Some embodiments have particular applicability to the insurance industry. For example, discounts to insurance premiums may be provided by the techniques described herein. For instance, if a user 1360 agrees to allow her vehicle to send real-time condition data (e.g., to the external database 1380 , thereby allowing others to preview severe weather and/or other dangerous conditions), the user 1360 may receive a discount on an insurance premium. In one aspect, data from the vehicle 1350 , and/or other data, including the types of data discussed elsewhere herein, may be collected or received by an insurance provider remote server, such as via direct or indirect wireless communication or data transmission from a smart home controller, mobile device, or other customer computing device, after a customer affirmatively consents or otherwise opts-in to an insurance discount, reward, or other program. The insurance provider may then analyze the data received with the customer's permission to provide benefits to the customer. As a result, risk averse customers may receive insurance discounts or other insurance cost savings based upon data that reflects low risk behavior and/or technology that mitigates or prevents risk to autonomous vehicles. Exemplary Use of Generating a VR Feed Corresponding to an Event In one aspect, a computer-implemented method for generating a virtual reality (VR) feed corresponding to an event may be provided. The method may include: (1) obtaining, via one or more processors, an indication of an event occurring in a geographic area, wherein the event is at least one of: a vehicle collision, a crime, a weather event, or a natural disaster; (2) generating, via the one or more processors, a VR feed of the geographic area at a time of the event based upon real-time condition data from the geographic area, the VR feed including a virtual representation of the geographic area at the time of the event to reflect the real-time conditions at the geographic area; and/or (3) providing, via the one or more processors, the generated VR feed for presentation to a user within a virtual reality display for the user to experience the geographic area where the event occurred within a VR environment. The method may include additional, fewer, or alternate actions, including those discussed elsewhere herein. In some embodiments, the VR feed may be generated based upon data generated by: (i) a camera of a vehicle in the geographic area, (ii) a drone in the geographic area, and/or (iii) an infrastructure camera in the geographic area. In certain embodiments, the event may be the vehicle collision; the indication may be obtained from a vehicle in the geographic area; and/or the VR feed may be generated based upon data received from a vehicle camera of the vehicle in the geographic area. In some embodiments, the indication may be a first indication, the event may be a first event, and/or the method may further include: obtaining, via the one or more processors, a second indication of a second event occurring in the geographic area; and/or interrupting, via the one or more processors, the providing of the generated VR feed by providing an option to the user to experience the second event. Additionally or alternatively, generating the VR feed may further comprise blurring out: a face of an individual, identifying information of an individual, and/or a license plate. In some embodiments, generating the VR feed may further include identifying, via the one or more processors, a representation of an individual in the virtual representation of the geographic area, and/or replacing the representation of the individual with an avatar. The method may further include sending, via the one or more processors and to an emergency response entity, the virtual representation of the geographic area including the representation of the individual. In some embodiments, the event may include the natural disaster event, and/or the natural disaster event may comprise a forest fire, or a hurricane. In another aspect, a computer system configured to generate a virtual reality (VR) feed corresponding to an event may be provided. The computer system may include one or more local or remote processors, transceivers, and/or sensors configured to: (1) obtain an indication of an event occurring in a geographic area, wherein the event is at least one of: a vehicle collision, a crime, a weather event, or a natural disaster; (2) generate a VR feed of the geographic area at a time of the event based upon real-time condition data from the geographic area, the VR feed including a virtual representation of the geographic area at the time of the event to reflect the real-time conditions at the geographic area; and/or (3) provide the generated VR feed for presentation to a user within a virtual reality display for the user to experience the geographic area where the event occurred within a VR environment. The computer system may include additional, less, or alternate functionality, including that discussed elsewhere herein. For instance, the VR feed may be generated based upon data generated by: (i) a camera of a vehicle in the geographic area, (ii) a drone in the geographic area, and/or (iii) an infrastructure camera in the geographic area. In some embodiments, the event may be the vehicle collision; the indication may be obtained from a vehicle in the geographic area; and/or the VR feed may be generated based upon data received from a vehicle camera of the vehicle in the geographic area. In some embodiments, the indication may be a first indication, the event may be a first event, and/or the one or more local or remote processors, transceivers, and/or sensors may be further configured to: obtain a second indication of a second event occurring in the geographic area; and/or interrupt the providing of the generated VR feed by providing an option to the user to experience the second event. Generating the VR feed may further include blurring out: a face of an individual, identifying information of an individual, and/or a license plate. In some embodiments, the event may include the natural disaster event, and the natural disaster event may comprise a forest fire, or a hurricane. In yet another aspect, a computer device for generating a virtual reality (VR) feed corresponding to an event may be provided. The computer device may include: one or more processors; and one or more memories coupled to the one or more processors. The one or more memories including computer executable instructions stored therein that, when executed by the one or more processors, may cause the one or more processors to: (1) obtain an indication of an event occurring in a geographic area, wherein the event is at least one of: a vehicle collision, a crime, a weather event, or a natural disaster; (2) generate a VR feed of the geographic area at a time of the event based upon real-time condition data from the geographic area, the VR feed including a virtual representation of the geographic area at the time of the event to reflect the real-time conditions at the geographic area; and/or (3) provide the generated VR feed for presentation to a user within a virtual reality display for the user to experience the geographic area where the event occurred within a VR environment. The computer device may include additional, less, or alternate functionality, including that discussed elsewhere herein. For instance, the VR feed may be generated based upon data generated by: (i) a camera of a vehicle in the geographic area, (ii) a drone in the geographic area, and/or (iii) an infrastructure camera in the geographic area. In some embodiments: the event may be the vehicle collision; the indication may be obtained from a vehicle in the geographic area; and/or the VR feed may be generated based upon data received from a vehicle camera of the vehicle in the geographic area. In some embodiments, the indication may be a first indication, the event may be a first event, and/or wherein the one or more memories including computer executable instructions stored therein that, when executed by the one or more processors, may further cause the one or more processors to: obtain a second indication of a second event occurring in the geographic area; and/or interrupt the providing of the generated VR feed by providing an option to the user to experience the second event. Generating the VR feed may further include blurring out: a face of an individual, identifying information of an individual, and/or a license plate. In some embodiments, the event may include the natural disaster event, and/or the natural disaster event may comprise a forest fire, or a hurricane. Other Matters Although the text herein sets forth a detailed description of numerous different embodiments, it should be understood that the legal scope of the invention is defined by the words of the claims set forth at the end of this patent. The detailed description is to be construed as exemplary only and does not describe every possible embodiment, as describing every possible embodiment would be impractical, if not impossible. One could implement numerous alternate embodiments, using either current technology or technology developed after the filing date of this patent, which would still fall within the scope of the claims. It should also be understood that, unless a term is expressly defined in this patent using the sentence “As used herein, the term ‘ ’ is hereby defined to mean . . . ” or a similar sentence, there is no intent to limit the meaning of that term, either expressly or by implication, beyond its plain or ordinary meaning, and such term should not be interpreted to be limited in scope based upon any statement made in any section of this patent (other than the language of the claims). To the extent that any term recited in the claims at the end of this disclosure is referred to in this disclosure in a manner consistent with a single meaning, that is done for sake of clarity only so as to not confuse the reader, and it is not intended that such claim term be limited, by implication or otherwise, to that single meaning. Throughout this specification, plural instances may implement components, operations, or structures described as a single instance. Although individual operations of one or more methods are illustrated and described as separate operations, one or more of the individual operations may be performed concurrently, and nothing requires that the operations be performed in the order illustrated. Structures and functionality presented as separate components in example configurations may be implemented as a combined structure or component. Similarly, structures and functionality presented as a single component may be implemented as separate components. These and other variations, modifications, additions, and improvements fall within the scope of the subject matter herein. Additionally, certain embodiments are described herein as including logic or a number of routines, subroutines, applications, or instructions. These may constitute either software (code embodied on a non-transitory, tangible machine-readable medium) or hardware. In hardware, the routines, etc., are tangible units capable of performing certain operations and may be configured or arranged in a certain manner. In example embodiments, one or more computer systems (e.g., a standalone, client or server computer system) or one or more hardware modules of a computer system (e.g., a processor or a group of processors) may be configured by software (e.g., an application or application portion) as a hardware module that operates to perform certain operations as described herein. In various embodiments, a hardware module may be implemented mechanically or electronically. For example, a hardware module may comprise dedicated circuitry or logic that is permanently configured (e.g., as a special-purpose processor, such as a field programmable gate array (FPGA) or an application-specific integrated circuit (ASIC) to perform certain operations). A hardware module may also comprise programmable logic or circuitry (e.g., as encompassed within a general-purpose processor or other programmable processor) that is temporarily configured by software to perform certain operations. It will be appreciated that the decision to implement a hardware module mechanically, in dedicated and permanently configured circuitry, or in temporarily configured circuitry (e.g., configured by software) may be driven by cost and time considerations. Accordingly, the term “hardware module” should be understood to encompass a tangible entity, be that an entity that is physically constructed, permanently configured (e.g., hardwired), or temporarily configured (e.g., programmed) to operate in a certain manner or to perform certain operations described herein. Considering embodiments in which hardware modules are temporarily configured (e.g., programmed), each of the hardware modules need not be configured or instantiated at any one instance in time. For example, where the hardware modules comprise a general-purpose processor configured using software, the general-purpose processor may be configured as respective different hardware modules at different times. Software may accordingly configure a processor, for example, to constitute a particular hardware module at one instance of time and to constitute a different hardware module at a different instance of time. Hardware modules can provide information to, and receive information from, other hardware modules. Accordingly, the described hardware modules may be regarded as being communicatively coupled. Where multiple of such hardware modules exist contemporaneously, communications may be achieved through signal transmission (e.g., over appropriate circuits and buses) that connect the hardware modules. In embodiments in which multiple hardware modules are configured or instantiated at different times, communications between such hardware modules may be achieved, for example, through the storage and retrieval of information in memory structures to which the multiple hardware modules have access. For example, one hardware module may perform an operation and store the output of that operation in a memory device to which it is communicatively coupled. A further hardware module may then, at a later time, access the memory device to retrieve and process the stored output. Hardware modules may also initiate communications with input or output devices, and can operate on a resource (e.g., a collection of information). The various operations of example methods described herein may be performed, at least partially, by one or more processors that are temporarily configured (e.g., by software) or permanently configured to perform the relevant operations. Whether temporarily or permanently configured, such processors may constitute processor-implemented modules that operate to perform one or more operations or functions. The modules referred to herein may, in some example embodiments, comprise processor-implemented modules. Similarly, the methods or routines described herein may be at least partially processor-implemented. For example, at least some of the operations of a method may be performed by one or more processors or processor-implemented hardware modules. The performance of certain of the operations may be distributed among the one or more processors, not only residing within a single machine, but deployed across a number of machines. In some example embodiments, the processor or processors may be located in a single location (e.g., within a home environment, an office environment or as a server farm), while in other embodiments the processors may be distributed across a number of geographic locations. Unless specifically stated otherwise, discussions herein using words such as “processing,” “computing,” “calculating,” “determining,” “presenting,” “displaying,” or the like may refer to actions or processes of a machine (e.g., a computer) that manipulates or transforms data represented as physical (e.g., electronic, magnetic, or optical) quantities within one or more memories (e.g., volatile memory, non-volatile memory, or a combination thereof), registers, or other machine components that receive, store, transmit, or display information. As used herein any reference to “one embodiment” or “an embodiment” means that a particular element, feature, structure, or characteristic described in connection with the embodiment may be included in at least one embodiment. The appearances of the phrase “in one embodiment” in various places in the specification are not necessarily all referring to the same embodiment. Some embodiments may be described using the expression “coupled” and “connected” along with their derivatives. For example, some embodiments may be described using the term “coupled” to indicate that two or more elements are in direct physical or electrical contact. The term “coupled,” however, may also mean that two or more elements are not in direct contact with each other, but yet still co-operate or interact with each other. The embodiments are not limited in this context. As used herein, the terms “comprises,” “comprising,” “includes,” “including,” “has,” “having” or any other variation thereof, are intended to cover a non-exclusive inclusion. For example, a process, method, article, or apparatus that comprises a list of elements is not necessarily limited to only those elements but may include other elements not expressly listed or inherent to such process, method, article, or apparatus. Further, unless expressly stated to the contrary, “or” refers to an inclusive or and not to an exclusive or. For example, a condition A or B is satisfied by any one of the following: A is true (or present) and B is false (or not present), A is false (or not present) and B is true (or present), and both A and B are true (or present). In addition, use of the “a” or “an” are employed to describe elements and components of the embodiments herein. This is done merely for convenience and to give a general sense of the description. This description, and the claims that follow, should be read to include one or at least one and the singular also includes the plural unless it is obvious that it is meant otherwise. Upon reading this disclosure, those of skill in the art will appreciate still additional alternative structural and functional designs for the approaches described herein. Thus, while particular embodiments and applications have been illustrated and described, it is to be understood that the disclosed embodiments are not limited to the precise construction and components disclosed herein. Various modifications, changes and variations, which will be apparent to those skilled in the art, may be made in the arrangement, operation and details of the method and apparatus disclosed herein without departing from the spirit and scope defined in the appended claims. The particular features, structures, or characteristics of any specific embodiment may be combined in any suitable manner and in any suitable combination with one or more other embodiments, including the use of selected features without corresponding use of other features. In addition, many modifications may be made to adapt a particular application, situation or material to the essential scope and spirit of the present invention. It is to be understood that other variations and modifications of the embodiments of the present invention described and illustrated herein are possible in light of the teachings herein and are to be considered part of the spirit and scope of the present invention. While the preferred embodiments of the invention have been described, it should be understood that the invention is not so limited and modifications may be made without departing from the invention. The scope of the invention is defined by the appended claims, and all devices that come within the meaning of the claims, either literally or by equivalence, are intended to be embraced therein. It is therefore intended that the foregoing detailed description be regarded as illustrative rather than limiting, and that it be understood that it is the following claims, including all equivalents, that are intended to define the spirit and scope of this invention. Furthermore, the patent claims at the end of this patent application are not intended to be construed under 35 U.S.C. § 112(f) unless traditional means-plus-function language is expressly recited, such as “means for” or “step for” language being explicitly recited in the claim(s). The systems and methods described herein are directed to an improvement to computer functionality, and improve the functioning of conventional computers.",en,PATENT_APPLICATION
062-400-044-151-022,US,20240388404,A1,2024-11-21,US_20240388404_A1_20241121,en,US,20240388404,A1,2024-11-21,US,18785768,2024-07-26,REFERENCE SIGNAL COMMUNICATION IN A WIRELESS NETWORK,en,US,"InterDigital Patent Holdings, Inc.","Wilmington, DE",CA,Benoit Pelletier,Roxboro,US,1,Lujing Cai,"Morganville, NJ",CA,2,Diana Pani,Montreal,US,3,Hong O. Zhang,"Acton, MA",US,4,Damian C. Hamme,"Horsham, PA",H04L5/00,I,F,H04W72/23,I,L,H04L5/0051,I,F,H04L5/0023,I,L,H04L5/0035,I,L,H04L5/0053,I,L,H04L5/0091,I,L,H04W72/23,I,L,US,20240388404,A1,2024-11-21,062-400-044-151-022,1,US,20240388404,A1,2024-11-21,062-400-044-151-022,1,UNKNOWN,"A method performed by a WTRU may comprise receiving a beamformed physical control channel using a first beamformed pilot signal. The beamformed physical control channel may include an indication of a beam for a physical shared channel. The indicated beam of the physical shared channel and a beam of the beamformed physical control channel may be different. The method may further comprise receiving, based on the indicated beam, the physical shared channel using a second beamformed pilot signal. In an embodiment, the WTRU may receive an RRC message including an indication of a code of the first beamformed pilot signal.",en,"1 . A wireless transmit/receive unit (WTRU) comprising a processor configured to: receive a radio resource control (RRC) message indicating first pilot sequence information and second pilot sequence information, wherein the first pilot sequence information corresponds to a first set of configuration parameters used to generate a first pilot sequence and the second pilot sequence information corresponds to a second set of configuration parameters used to generate a second pilot sequence; receive a physical downlink control channel (PDCCH) transmission wherein the PDCCH transmission comprises a value indicating whether the first pilot sequence information or the second pilot sequence information is used for generating a pilot sequence for a pilot signal associated with a physical downlink shared channel (PDSCH) transmission; and receive the PDSCH transmission using at least the pilot signal associated with the pilot sequence generated using the first pilot sequence information or the second pilot sequence information as indicated in the PDCCH transmission.","2 . The WTRU of claim 1 , wherein the PDCCH transmission is associated with a radio network temporary identifier (RNTI) of the WTRU.","3 . The WTRU of claim 1 , wherein the PDCCH transmission is beamformed, and wherein the PDSCH transmission is beamformed.","4 . The WTRU of claim 3 , wherein the processor is configured to: receive the PDCCH transmission using receive side beamform reception techniques; and receive the PDSCH transmission using receive side beamform reception techniques.","5 . The WTRU of claim 1 , wherein the processor is further configured to: determine a number of transport blocks in the PDSCH transmission; and determine a number of layers for each transport block.","6 . The WTRU of claim 1 , wherein the first set of configuration parameters comprises a first channelization code and a first pilot signal index and the second set of configuration parameters comprises a second channelization code and a second pilot signal index.","7 . The WTRU of claim 6 , wherein the first channelization code and the first pilot signal index uniquely identify the first pilot sequence information and the second channelization code and the second pilot signal index uniquely identify the second pilot sequence information.","8 . The WTRU of claim 6 , wherein the first set of configuration parameters is organized for indexing in order of a first channelization code list and the first pilot signal index for each channelization code, and the second set of configuration parameters is organized for indexing in order of a second channelization code list and the second pilot signal index for each channelization code.","9 . The WTRU of claim 1 , wherein the PDCCH transmission comprises information indicating a number of codewords and a number of layers associated with a PDSCH transmission.","10 . The WTRU of claim 1 , wherein the PDCCH transmission indicates that the PDSCH transmission is associated with two codewords.","11 . A method performed by a wireless transmit/receive unit (WTRU), the method comprising: receiving a radio resource control (RRC) message indicating first pilot sequence information and second pilot sequence information, wherein the first pilot sequence information corresponds to a first set of configuration parameters used to generate a pilot sequence and the second pilot sequence information corresponds to a second set of configuration parameters used to generate the pilot sequence; receiving a physical downlink control channel (PDCCH) transmission, wherein the physical downlink control channel PDCCH transmission comprises a value indicating whether the first pilot sequence information or the second pilot sequence information is used for generating a pilot sequence for a pilot signal associated with a physical downlink shared channel (PDSCH) transmission; and receiving the PDSCH transmission using at least the pilot signal associated with the pilot sequence generated using the first pilot sequence information or the second pilot sequence information as indicated in the PDCCH transmission.","12 . The method of claim 11 , wherein the PDCCH transmission is associated with a radio network temporary identifier (RNTI) of the WTRU.","13 . The method of claim 11 , wherein the PDCCH transmission is beamformed, and wherein the PDSCH transmission is beamformed.","14 . The method of claim 13 , further comprising: receiving the PDCCH transmission using receive side beamform reception techniques; and receiving the PDSCH transmission using receive side beamform reception techniques.","15 . The method of claim 11 , further comprising: determining a number of transport blocks in the PDSCH transmission; and determining a number of layers for each transport block.","16 . The method of claim 11 , wherein the first set of configuration parameters comprises a first channelization code and a first pilot signal index and the second set of configuration parameters comprises a second channelization code and a second pilot signal index.","17 . The method of claim 16 , wherein the first channelization code and the first pilot signal index uniquely identify the first pilot sequence information and the second channelization code and the second pilot signal index uniquely identify the second pilot sequence information.","18 . The method of claim 16 , wherein the first set of configuration parameters is organized for indexing in order of a first channelization code list and the first pilot signal index for each channelization code, and the second set of configuration parameters is organized for indexing in order of a second channelization code list and the second pilot signal index for each channelization code.","19 . The method of claim 11 , wherein the PDCCH transmission comprises information indicating a number of codewords and a number of layers associated with a PDSCH transmission. 20 The method of claim 11 , wherein the PDCCH transmission indicates that the PDSCH transmission is associated with two codewords.",en,"CROSS REFERENCE TO RELATED APPLICATIONS This application is a continuation of U.S. non-provisional application Ser. No. 16/848,016, filed Apr. 14, 2020, which is a continuation of U.S. non-provisional application Ser. No. 15/431,305, filed Feb. 13, 2017, which issued as U.S. Pat. No. 10,623,160 on Apr. 14, 2020, which is a continuation of U.S. non-provisional application Ser. No. 13/572,040, filed Aug. 10, 2012, now abandoned, which claims the benefit of U.S. Provisional Application No. 61/591,577, filed Jan. 27, 2012, and claims the benefit of U.S. Provisional Application No. 61/555,840, filed Nov. 4, 2011, and claims the benefit of U.S. Provisional Application No. 61/541,714, filed Sep. 30, 2011, and claims the benefit of U.S. Provisional Application No. 61/522,842, filed Aug. 12, 2011, the disclosure of which is incorporated herein by reference in its entirety. BACKGROUND In response to increasing demand in terms of higher peak data rate and better user experience from end users, third generation partnership project (3GPP) wireless communication systems involving wideband code division multiple access (WCDMA) technologies have been evolving, whereby many new features have been proposed and specified. For example, a new feature that allows the simultaneous use of two high speed downlink (DL) packet access (HSDPA) downlink carriers has been introduced. This new feature essentially improves the bandwidth usage and user peak downlink rate via frequency aggregation and resource pooling, and was extended to include a multiple-input multiple-output (MIMO) function. Later, four (4) carrier HSDPA (4C-HSDPA) was introduced which allows up to four (4) carriers to operate simultaneously to increase the downlink throughput. As efforts to improve user experience at cell edge continue, coordinated HSDPA transmission involving multiple cells operates in the same frequency to deploy and support multipoint (MP) downlink transmission. Remote radio head (RRH) is an important technology that may simplify the deployment of the multipoint downlink transmission. SUMMARY A method performed by a wireless transmit/receive unit (WTRU) may comprise receiving a beamformed physical control channel using a first beamformed pilot signal. The beamformed physical control channel may include an indication of a beam for a physical shared channel. The indicated beam of the physical shared channel and a beam of the beamformed physical control channel may be different. The method may further comprise receiving, based on the indicated beam, the physical shared channel using a second beamformed pilot signal. In an embodiment, the WTRU may receive a radio resource control (RRC) message including an indication of a code of the first beamformed pilot signal. BRIEF DESCRIPTION OF THE DRAWINGS A more detailed understanding may be had from the following description, given by way of example in conjunction with the accompanying drawings wherein: FIG. 1A is a system diagram of an example communications system in which one or more disclosed embodiments may be implemented; FIG. 1B is a system diagram of an example wireless transmit/receive unit (WTRU) that may be used within the communications system illustrated in FIG. 1A ; FIG. 1C is a system diagram of an example radio access network and an example core network that may be used within the communications system illustrated in FIG. 1A ; FIG. 2 is an example of a conventional homogeneous network deployment; FIG. 3 is an example of a network deployment with RRH, wherein the RRH acts as an independent cell; FIG. 4 is an example of utilizing common scrambling code (CSC) among RRHs in UMTS; FIG. 5 is an example of joint MP-HSDPA transmission mode; FIG. 6 is an example of multiflow aggregation to the same WTRUs; FIG. 7 is an example of multiflow aggregation for single cell transmission to a single WTRU; FIG. 8 is an example of 4 branch DL-MIMO operating at individual RRH; FIG. 9 is an example of 4 branch DL-MIMO when RRHs are used as simple antenna extensions; FIG. 10 is an example modulation pattern and channelization code assignment for four common pilot channels; FIG. 11 is a first example modulation pattern and channelization code assignment for six common pilot channels; FIG. 12 is a second example modulation pattern and channelization code assignment for six common pilot channels; FIG. 13 is an example of pilot indexing with rank indication; FIG. 14 is an example of time multiplexing WTRU-specific pilot with a high speed-physical downlink shared channel (HS-PDSCH); FIG. 15 is an example of time multiplexing WTRU-specific pilot with HS-PDSCH on one channelization code; FIG. 16 is an example of time multiplexing WTRU-specific pilot with HS-PDSCH on one channelization code and discontinuously transmitting the pilot portion of HS_PDSCHs on all other channelization codes; FIG. 17 is an example of time multiplexing WTRU-specific pilot with HS-PDSCH on all assigned channelization codes (up to 15); FIG. 18 is an example of pilot resource allocation for demodulation of HS-SCCH and HS-PDSCH; FIG. 19 shows a time multiplexing WTRU-specific pilot with HS-PDSCH on one channelization code; FIG. 20 is an example of the coding chain for HS-SCCH type 4; FIG. 21 is an example of a coding chain for HS-SCCH for a non-codebook based MIMO scheme with 4 transport blocks; FIG. 22 is an example of a coding chain for HS-SCCH for a codebook based MIMO scheme with 4 transport blocks; and FIG. 23 is an example of a method for determining pilot information for each data stream. DETAILED DESCRIPTION FIG. 1A is a diagram of an example communications system 100 in which one or more disclosed embodiments may be implemented. The communications system 100 may be a multiple access system that provides content, such as voice, data, video, messaging, broadcast, etc., to multiple wireless users. The communications system 100 may enable multiple wireless users to access such content through the sharing of system resources, including wireless bandwidth. For example, the communications system 100 may employ one or more channel access methods, such as code division multiple access (CDMA), time division multiple access (TDMA), frequency division multiple access (FDMA), orthogonal FDMA (OFDMA), single-carrier FDMA (SC-FDMA), and the like. As shown in FIG. 1A , the communications system 100 may include wireless transmit/receive units (WTRUs) 102 a , 102 b , 102 c , 102 d , a radio access network (RAN) 104 , a core network 106 , a public switched telephone network (PSTN) 108 , the Internet 110 , and other networks 112 , though it will be appreciated that the disclosed embodiments contemplate any number of WTRUs, base stations, networks, and/or network elements. Each of the WTRUs 102 a , 102 b , 102 c , 102 d may be any type of device configured to operate and/or communicate in a wireless environment. By way of example, the WTRUs 102 a , 102 b , 102 c , 102 d may be configured to transmit and/or receive wireless signals and may include user equipment (UE), a mobile station, a fixed or mobile subscriber unit, a pager, a cellular telephone, a personal digital assistant (PDA), a smartphone, a laptop, a netbook, a personal computer, a wireless sensor, consumer electronics, and the like. The communications system 100 may also include a base station 114 a and a base station 114 b . Each of the base stations 114 a , 114 b may be any type of device configured to wirelessly interface with at least one of the WTRUs 102 a , 102 b , 102 c , 102 d to facilitate access to one or more communication networks, such as the core network 106 , the Internet 110 , and/or the networks 112 . By way of example, the base stations 114 a , 114 b may be a base transceiver station (BTS), a Node-B, an eNode B, a Home Node B, a Home eNode B, a site controller, an access point (AP), a wireless router, and the like. While the base stations 114 a , 114 b are each depicted as a single element, it will be appreciated that the base stations 114 a , 114 b may include any number of interconnected base stations and/or network elements. The base station 114 a may be part of the RAN 104 , which may also include other base stations and/or network elements (not shown), such as a base station controller (BSC), a radio network controller (RNC), relay nodes, etc. The base station 114 a and/or the base station 114 b may be configured to transmit and/or receive wireless signals within a particular geographic region, which may be referred to as a cell (not shown). The cell may further be divided into cell sectors. For example, the cell associated with the base station 114 a may be divided into three sectors. Thus, in one embodiment, the base station 114 a may include three transceivers, i.e., one for each sector of the cell. In another embodiment, the base station 114 a may employ multiple-input multiple output (MIMO) technology and, therefore, may utilize multiple transceivers for each sector of the cell. The base stations 114 a , 114 b may communicate with one or more of the WTRUs 102 a , 102 b , 102 c , 102 d over an air interface 116 , which may be any suitable wireless communication link (e.g., radio frequency (RF), microwave, infrared (IR), ultraviolet (UV), visible light, etc.). The air interface 116 may be established using any suitable radio access technology (RAT). More specifically, as noted above, the communications system 100 may be a multiple access system and may employ one or more channel access schemes, such as CDMA, TDMA, FDMA, OFDMA, SC-FDMA, and the like. For example, the base station 114 a in the RAN 104 and the WTRUs 102 a , 102 b , 102 c may implement a radio technology such as Universal Mobile Telecommunications System (UMTS) Terrestrial Radio Access (UTRA), which may establish the air interface 116 using wideband CDMA (WCDMA). WCDMA may include communication protocols such as High-Speed Packet Access (HSPA) and/or Evolved HSPA (HSPA+). HSPA may include High-Speed Downlink Packet Access (HSDPA) and/or High-Speed Uplink Packet Access (HSUPA). In another embodiment, the base station 114 a and the WTRUs 102 a , 102 b , 102 c may implement a radio technology such as Evolved UMTS Terrestrial Radio Access (E-UTRA), which may establish the air interface 116 using Long Term Evolution (LTE) and/or LTE-Advanced (LTE-A). In other embodiments, the base station 114 a and the WTRUs 102 a , 102 b , 102 c may implement radio technologies such as IEEE 802.16 (i.e., Worldwide Interoperability for Microwave Access (WiMAX)), CDMA2000, CDMA2000 1X, CDMA2000 EV-DO, Interim Standard 2000 (IS-2000), Interim Standard 95 (IS-95), Interim Standard 856 (IS-856), Global System for Mobile communications (GSM), Enhanced Data rates for GSM Evolution (EDGE), GSM EDGE (GERAN), and the like. The base station 114 b in FIG. 1A may be a wireless router, Home Node B, Home eNode B, or access point, for example, and may utilize any suitable RAT for facilitating wireless connectivity in a localized area, such as a place of business, a home, a vehicle, a campus, and the like. In one embodiment, the base station 114 b and the WTRUs 102 c , 102 d may implement a radio technology such as IEEE 802.11 to establish a wireless local area network (WLAN). In another embodiment, the base station 114 b and the WTRUs 102 c , 102 d may implement a radio technology such as IEEE 802.15 to establish a wireless personal area network (WPAN). In yet another embodiment, the base station 114 b and the WTRUs 102 c , 102 d may utilize a cellular-based RAT (e.g., WCDMA, CDMA2000, GSM, LTE, LTE-A, etc.) to establish a picocell or femtocell. As shown in FIG. 1A , the base station 114 b may have a direct connection to the Internet 110 . Thus, the base station 114 b may not be required to access the Internet 110 via the core network 106 . The RAN 104 may be in communication with the core network 106 , which may be any type of network configured to provide voice, data, applications, and/or voice over internet protocol (VoIP) services to one or more of the WTRUs 102 a , 102 b , 102 c , 102 d . For example, the core network 106 may provide call control, billing services, mobile location-based services, pre-paid calling, Internet connectivity, video distribution, etc., and/or perform high-level security functions, such as user authentication. Although not shown in FIG. 1A , it will be appreciated that the RAN 104 and/or the core network 106 may be in direct or indirect communication with other RANs that employ the same RAT as the RAN 104 or a different RAT. For example, in addition to being connected to the RAN 104 , which may be utilizing an E-UTRA radio technology, the core network 106 may also be in communication with another RAN (not shown) employing a GSM radio technology. The core network 106 may also serve as a gateway for the WTRUs 102 a , 102 b , 102 c , 102 d to access the PSTN 108 , the Internet 110 , and/or other networks 112 . The PSTN 108 may include circuit-switched telephone networks that provide plain old telephone service (POTS). The Internet 110 may include a global system of interconnected computer networks and devices that use common communication protocols, such as the transmission control protocol (TCP), user datagram protocol (UDP) and the internet protocol (IP) in the TCP/IP internet protocol suite. The networks 112 may include wired or wireless communications networks owned and/or operated by other service providers. For example, the networks 112 may include another core network connected to one or more RANs, which may employ the same RAT as the RAN 104 or a different RAT. Some or all of the WTRUs 102 a , 102 b , 102 c , 102 d in the communications system 100 may include multi-mode capabilities, i.e., the WTRUs 102 a , 102 b , 102 c , 102 d may include multiple transceivers for communicating with different wireless networks over different wireless links. For example, the WTRU 102 c shown in FIG. 1A may be configured to communicate with the base station 114 a , which may employ a cellular-based radio technology, and with the base station 114 b , which may employ an IEEE 802 radio technology. FIG. 1B is a system diagram of an example WTRU 102 . As shown in FIG. 1B , the WTRU 102 may include a processor 118 , a transceiver 120 , a transmit/receive element 122 , a speaker/microphone 124 , a keypad 126 , a display/touchpad 128 , non-removable memory 130 , removable memory 132 , a power source 134 , a global positioning system (GPS) chipset 136 , and other peripherals 138 . It will be appreciated that the WTRU 102 may include any sub-combination of the foregoing elements while remaining consistent with an embodiment. The processor 118 may be a general purpose processor, a special purpose processor, a conventional processor, a digital signal processor (DSP), a plurality of microprocessors, one or more microprocessors in association with a DSP core, a controller, a microcontroller, Application Specific Integrated Circuits (ASICs), Field Programmable Gate Array (FPGAs) circuits, any other type of integrated circuit (IC), a state machine, and the like. The processor 118 may perform signal coding, data processing, power control, input/output processing, and/or any other functionality that enables the WTRU 102 to operate in a wireless environment. The processor 118 may be coupled to the transceiver 120 , which may be coupled to the transmit/receive element 122 . While FIG. 1B depicts the processor 118 and the transceiver 120 as separate components, it will be appreciated that the processor 118 and the transceiver 120 may be integrated together in an electronic package or chip. The transmit/receive element 122 may be configured to transmit signals to, or receive signals from, a base station (e.g., the base station 114 a ) over the air interface 116 . For example, in one embodiment, the transmit/receive element 122 may be an antenna configured to transmit and/or receive RF signals. In another embodiment, the transmit/receive element 122 may be an emitter/detector configured to transmit and/or receive IR, UV, or visible light signals, for example. In yet another embodiment, the transmit/receive element 122 may be configured to transmit and receive both RF and light signals. It will be appreciated that the transmit/receive element 122 may be configured to transmit and/or receive any combination of wireless signals. In addition, although the transmit/receive element 122 is depicted in FIG. 1B as a single element, the WTRU 102 may include any number of transmit/receive elements 122 . More specifically, the WTRU 102 may employ MIMO technology. Thus, in one embodiment, the WTRU 102 may include two or more transmit/receive elements 122 (e.g., multiple antennas) for transmitting and receiving wireless signals over the air interface 116 . The transceiver 120 may be configured to modulate the signals that are to be transmitted by the transmit/receive element 122 and to demodulate the signals that are received by the transmit/receive element 122 . As noted above, the WTRU 102 may have multi-mode capabilities. Thus, the transceiver 120 may include multiple transceivers for enabling the WTRU 102 to communicate via multiple RATs, such as UTRA and IEEE 802.11, for example. The processor 118 of the WTRU 102 may be coupled to, and may receive user input data from, the speaker/microphone 124 , the keypad 126 , and/or the display/touchpad 128 (e.g., a liquid crystal display (LCD) display unit or organic light-emitting diode (OLED) display unit). The processor 118 may also output user data to the speaker/microphone 124 , the keypad 126 , and/or the display/touchpad 128 . In addition, the processor 118 may access information from, and store data in, any type of suitable memory, such as the non-removable memory 130 and/or the removable memory 132 . The non-removable memory 130 may include random-access memory (RAM), read-only memory (ROM), a hard disk, or any other type of memory storage device. The removable memory 132 may include a subscriber identity module (SIM) card, a memory stick, a secure digital (SD) memory card, and the like. In other embodiments, the processor 118 may access information from, and store data in, memory that is not physically located on the WTRU 102 , such as on a server or a home computer (not shown). The processor 118 may receive power from the power source 134 , and may be configured to distribute and/or control the power to the other components in the WTRU 102 . The power source 134 may be any suitable device for powering the WTRU 102 . For example, the power source 134 may include one or more dry cell batteries (e.g., nickel-cadmium (NiCd), nickel-zinc (NiZn), nickel metal hydride (NiMH), lithium-ion (Li-ion), etc.), solar cells, fuel cells, and the like. The processor 118 may also be coupled to the GPS chipset 136 , which may be configured to provide location information (e.g., longitude and latitude) regarding the current location of the WTRU 102 . In addition to, or in lieu of, the information from the GPS chipset 136 , the WTRU 102 may receive location information over the air interface 116 from a base station (e.g., base stations 114 a , 114 b ) and/or determine its location based on the timing of the signals being received from two or more nearby base stations. It will be appreciated that the WTRU 102 may acquire location information by way of any suitable location-determination method while remaining consistent with an embodiment. The processor 118 may further be coupled to other peripherals 138 , which may include one or more software and/or hardware modules that provide additional features, functionality and/or wired or wireless connectivity. For example, the peripherals 138 may include an accelerometer, an e-compass, a satellite transceiver, a digital camera (for photographs or video), a universal serial bus (USB) port, a vibration device, a television transceiver, a hands free headset, a Bluetooth® module, a frequency modulated (FM) radio unit, a digital music player, a media player, a video game player module, an Internet browser, and the like. FIG. 1C is a system diagram of the RAN 104 and the core network 106 according to an embodiment. As noted above, the RAN 104 may employ a UTRA radio technology to communicate with the WTRUs 102 a , 102 b , 102 c over the air interface 116 . The RAN 104 may also be in communication with the core network 106 . As shown in FIG. 1C , the RAN 104 may include Node-Bs 140 a , 140 b , 140 c , which may each include one or more transceivers for communicating with the WTRUs 102 a , 102 b , 102 c over the air interface 116 . The Node-Bs 140 a , 140 b , 140 c may each be associated with a particular cell (not shown) within the RAN 104 . The RAN 104 may also include RNCs 142 a , 142 b . It will be appreciated that the RAN 104 may include any number of Node-Bs and RNCs while remaining consistent with an embodiment. As shown in FIG. 1C , the Node-Bs 140 a , 140 b may be in communication with the RNC 142 a . Additionally, the Node-B 140 c may be in communication with the RNC 142 b . The Node-Bs 140 a , 140 b , 140 c may communicate with the respective RNCs 142 a , 142 b via an Iub interface. The RNCs 142 a , 142 b may be in communication with one another via an Iur interface. Each of the RNCs 142 a , 142 b may be configured to control the respective Node-Bs 140 a , 140 b , 140 c to which it is connected. In addition, each of the RNCs 142 a , 142 b may be configured to carry out or support other functionality, such as outer loop power control, load control, admission control, packet scheduling, handover control, macrodiversity, security functions, data encryption, and the like. The core network 106 shown in FIG. 1C may include a media gateway (MGW) 144 , a mobile switching center (MSC) 146 , a serving GPRS support node (SGSN) 148 , and/or a gateway GPRS support node (GGSN) 150 . While each of the foregoing elements are depicted as part of the core network 106 , it will be appreciated that any one of these elements may be owned and/or operated by an entity other than the core network operator. The RNC 142 a in the RAN 104 may be connected to the MSC 146 in the core network 106 via an IuCS interface. The MSC 146 may be connected to the MGW 144 . The MSC 146 and the MGW 144 may provide the WTRUs 102 a , 102 b , 102 c with access to circuit-switched networks, such as the PSTN 108 , to facilitate communications between the WTRUs 102 a , 102 b , 102 c and traditional land-line communications devices. The RNC 142 a in the RAN 104 may also be connected to the SGSN 148 in the core network 106 via an IuPS interface. The SGSN 148 may be connected to the GGSN 150 . The SGSN 148 and the GGSN 150 may provide the WTRUs 102 a , 102 b , 102 c with access to packet-switched networks, such as the Internet 110 , to facilitate communications between and the WTRUs 102 a , 102 b , 102 c and IP-enabled devices. As noted above, the core network 106 may also be connected to the networks 112 , which may include other wired or wireless networks that are owned and/or operated by other service providers. Remote Radio Head (RRH) is an important technology that may simplify the deployment of systems supporting multiple HSDPA transmission, as it allows a plurality of Node-Bs in coordination to be collocated while distributing the transmitted signal to the radio frequency (RF) units in different locations. RRH configurations may also be used for long term evolution (LTE) coordinated multiple point (COMP) transmission, such as in a homogeneous network with intra-site Comp, a homogeneous network with high transmit (Tx) power RRHs, a heterogeneous network with low power RRHs within the macro-cell coverage with different cell identities (IDs), and a heterogeneous network with low power RRHs within the macro-cell coverage with same cell IDs. A heterogeneous network with low power RRHs within the macro-cell coverage with same cell IDs may be of particular interest, where a common cell-ID is shared among the transmission points, (macro points and pico points), within the coverage area of a macro point. While maintaining similar cell splitting gain as independent multiple cells, this deployment configuration may offer the advantages of improved coverage of the sync and control channels as they may be commonly transmitted from the multiple points. In addition, the WTRU mobility may be greatly improved in the heterogeneous network, and the number of handovers may be considerably reduced, especially if aggressive use of range extension is employed. Furthermore, as a result of the improved WTRU mobility, the network may dynamically and seamlessly allocate data traffic to the WTRU among various macro and pico cells, leading to additional resource pooling gain for scheduling optimization. In another aspect, a more realistic deployment scenario relevant to the above cell configuration may consider the RRHs as, or actually consist of, multiple antennas in one base station. While current WCDMA downlink MIMO operations are specified for up to two (2) spatial multiplexing streams in an LTE system, spatial multiplexing on the order of eight (8) may be possible. In order to take advantage of MIMO operations, multiple antennas may be used at both the transmitter and the receiver. Since practical deployment may share some of the antennas for both LTE and WCDMA systems, many sites for WCDMA may have access to two or more antennas. In one example, four (4) MIMO streams may be supported for HSDPA. This new feature, (referred to hereinafter as “4DL-MIMO”), may have the potential to not only provide doubled peak rates when compared to existing specifications, but also improve spectral efficiency. For example, doubled peak rates may be up to 84 Mbps in a single carrier and potentially up to 672 Mbps when 8 downlink carriers are used simultaneously. FIG. 2 is an example of a conventional homogeneous network deployment. Each cell 201 ( a ), 201 ( b ), and 201 ( c ) has its own network scheduler 202 ( a ), 202 ( b ), and 202 ( c ). Each WTRU 204 ( a ), 204 ( b ), and 204 ( c ) receives a scrambling code 203 ( a ), 203 ( b ), and 203 ( c ) from its own network scheduler 202 ( a ), 202 ( b ), and 202 ( c ). In a homogenous network deployment in a UMTS wireless cellular system, the radio equipment (RE), which includes the functionalities of the baseband and layer 2 processing, may be co-located with the transmission point, as shown in FIG. 2 . Each cell may be associated with a transmission point that covers a geographic area where WTRUs in the area may be served with data transmission scheduled by a network scheduler located in the RE. In order to improve the use of the frequency spectrum, a frequency reuse factor of 1 may be adopted thereby allowing an adjacent cell to operate in the same frequency band. To assist the WTRU to identify a serving cell in the cell searching process and mitigate the inference from other cells, a unique scrambling code may be assigned to each cell that operates at the front end of the baseband process at the WTRU to suppress the signal from other cells. Common control physical channels (CCPCHs) may be broadcasted from each cell. The CCPCHs may carry important system configuration parameters associated with a cell that may be uniquely identified by the WTRU using a special scrambling code. The scrambling code may be used as a unique cell ID for that cell in a UMTS system. FIG. 3 is an example of a network deployment with RRH, wherein the RRH acts as an independent cell. Each cell 301 ( a ), 301 ( b ), and 301 ( c ) has its own network scheduler 302 ( a ), 302 ( b ), and 302 ( c ). Each WTRU 304 ( a ), 304 ( b ), and 304 ( c ) receives a scrambling code 303 ( a ), 303 ( b ), and 303 ( c ) from its own network scheduler 302 ( a ), 302 ( b ), and 302 ( c ), respectively. Cells 301 ( b ) and 301 ( c ) include an RRH 305 ( b ) and 305 ( c ), respectively, each with their own REs 306 located in a centralized location, cell 301 ( a ). By introducing RRH 305 ( b ) and 305 ( c ), the REs 306 may be separated from the transmission points, where the RRHs 305 ( b ) and 305 ( c ) are connected to REs 306 by high speed and low latency backhaul links. Without changing the cell configuration, a deployment strategy is illustrated in FIG. 3 , where an RRH 305 ( b ) or 305 ( c ) may serve as a completely independent cell, identified by its own scrambling code that serves its own scheduling area, though the REs 306 are centralized at different locations. FIG. 4 is an example of utilizing common scrambling code (CSC) among RRHs in UMTS. Cell 401 ( b ) and 401 ( c ) include an RRH 405 ( b ) and 405 ( c ), respectively, each with their own REs 406 located in a centralized location, cell 401 ( a ). All three cells 401 ( a ), 401 ( b ), and 401 ( c ) utilize a common scrambling code (CSC) 403 . Data may be transmitted simultaneously from different transmission points to the same WTRU 404 . In order to improve the throughput performance for the WTRUs at the cell edge and in order to enhance WTRU mobility, the concept of using common scrambling code (CSC) 403 among the RRHs 405 ( b ) and 405 ( c ) may be implemented as shown in FIG. 4 . A common scrambling code may be utilized among different RRHs using any one or a combination of the following six techniques. In a first technique, a common broadcast channel may be transmitted with the same scrambling code. For example, a common broadcast channel may be a primary/secondary (P/S) CCPCH. In a second technique, one or more physical channels may be similarly transmitted across the RRHs, while the other may be different. For example, one or more physical channels may be a high speed-physical downlink shared channel (HS-PDSCH) and a high speed dedicated physical control channel (HS-DPCCH). In a third technique, each RRH may be characterized as a single cell in terms of scheduling and may have its own resource management though sharing the common scrambling code. In a fourth technique, the schedulers in the CSC set may work jointly in a coordinated manner. In a fifth technique, the cells in the CSC set may operate in the same frequency. In a sixth technique, each RRH may transmit with a different transmission power that may be dynamically changed. Depending on various data scheduling options, a number of operation modes with CSC may be used. The different operation modes are described below. FIG. 5 is an example of joint MP-HSDPA transmission mode. Cell 501 ( b ) and 501 ( c ) include an RRH 505 ( b ) and 505 ( c ), respectively, each with their own REs 506 located in a centralized location, cell 501 ( a ). All three cells 501 ( a ), 501 ( b ), and 501 ( c ) utilize a CSC 503 . Each cell 501 ( a ), 501 ( b ), and 501 ( c ) has its own joint scheduler 502 ( a ), 502 ( b ), and 502 ( c ). Data may be transmitted simultaneously from different transmission points to the same WTRU 504 . In a joint MP-HSDPA transmission, identical downlink signals carrying the same data may be transmitted simultaneously from different transmission points to the same WTRU 504 , as illustrated in FIG. 5 . These signals may be combined over the air before arriving at the WTRU receiver, so that the WTRU receiver perceives an enhanced signal overall. The transmission mode may be of particular use for the WTRU at cell edge where the WTRU may suffer severe inter-cell interference. All physical channels, (P/S CCPCH, common pilot channel (CPICH), high speed shared control channel (HS-SCCH), HS-PDSCH, dedicated physical data channel (DPDCH), and the like), may be transmitted this way. Because the WTRU is capable of performing channel state information (CSI) estimation and data demodulation based on the combined pilot signal carried by the CPICH, the WTRU may operate as if it is served by a single cell. For joint MP-HSDPA transmission mode, the same data stream from a higher layer may be transmitted to REs of each cell and the schedulers for the cells involved in the joint transmission may be operating jointly to schedule the same data to the WTRU. To further enhance the downlink transmission reliability, different precoding weights may be applied across the transmission points to adjust the transmission phase or the amplitude individually. The selection of the precoding weights may require the WTRU to distinguish the signal path from each transmission point individually. Thus, the pilot may be identified for each cell and the WTRU may measure preferred precoding weights and signal the preferred precoding weights to the network via uplink feedback. FIG. 6 is an example of multiflow aggregation to the same WTRUs. Cell 601 ( b ) and 601 ( c ) include an RRH 605 ( b ) and 605 ( c ), respectively, each with their own REs 606 located in a centralized location, cell 601 ( a ). All three cells 601 ( a ), 601 ( b ), and 601 ( c ) utilize a CSC 603 . Each cell 601 ( a ), 601 ( b ), and 601 ( c ) has its own joint scheduler 602 ( a ), 602 ( b ), and 602 ( c ). Data may be transmitted simultaneously from different transmission points to the same WTRU 604 . In a mode of operation using multiflow aggregation to the same WTRUs, different data may be transmitted simultaneously from different transmission points to the same WTRU 604 as shown in FIG. 6 . The WTRU may demodulate the signals from each cell individually, and the data from each cell may be aggregated to get a higher throughput. Due to operation in the same frequency and the same scrambling code for all cells involved in the multiflow transmission, and because of the interference from other transmission points, it may be desirable to suppress the interference at the WTRU demodulator. This issue may be effectively resolved by exploring the spatial differences among the transmission points and realizing the spatial multiplexing gain like a MIMO system does. Thus, the WTRU may be equipped with multiple antennas and a MIMO type of receiver structure. For HSDPA transmission, high speed data may be transmitted via various HS-PDSCHs from each transmission point with different transport block sizes, or codeword sizes, which may be indicated to the WTRU by a corresponding HS-SCCH transmitted from that cell. The data stream may be split and fed to each RE differently. The schedulers at each cell may operate independently to schedule the data simultaneously or at different time instances. Alternatively, the schedulers may be coordinated to achieve a certain way of optimization, either in terms of interference reduction or other aspects. Implementing the MIMO receiver may require the accurate estimation of the signal paths for each transmission point. Thus, distinguishable pilots or CPICHs may be designed for each transmission point to perform the channel estimation. As a more advanced option, the multiple data flows may be processed by a precoding matrix and transmitted at each transmission point. This precoding matrix may be selected by the WTRU based on the channel conditions of each signal path, or selected by the network according to scheduling needs. As a result, a data flow may be transmitted across all the transmission points depending on the configuration of the precoding matrix. In an example of the scheduling option, one data flow may be transmitted to a WTRU. However, this data transmission may be dynamically switched among the cells depending on the channel conditions. The schedulers may jointly operate to select a transmission point based on signal quality. FIG. 7 is an example of multiflow aggregation for single cell transmission to a single WTRU. Cell 701 ( b ) and 701 ( c ) include an RRH 705 ( b ) and 705 ( c ), respectively, each with their own REs 706 located in a centralized location, cell 701 ( a ). All three cells 701 ( a ), 701 ( b ), and 701 ( c ) utilize a CSC 703 . Each cell 701 ( a ), 701 ( b ), and 701 ( c ) has its own network scheduler 702 ( a ), 702 ( b ), and 702 ( c ). Each WTRU 704 ( a ), 704 ( b ), and 704 ( c ) receives a CSC 703 from its own network scheduler 702 ( a ), 702 ( b ), and 702 ( c ), respectively. As shown in FIG. 7 , a single cell transmission to a single WTRU is similar to the aggregation transmission mode, except that multiple data flows are transmitted from the multiple cells and the multiple data flows are addressed to various WTRUs. Each WTRU may only need one receiver to demodulate the data addressed to it. The HS-SCCH that carries the control information for the corresponding HS-PDSCH data transmission may be identified by a unique ID of the receiving WTRU. The WTRU may be equipped with multiple antennas and a MIMO receiver in order to suppress the interference from other transmission points, or other data flows simultaneously transmitted in the same frequency and same scrambling code. This way of receiving data is similar to the concept of multi-user MIMO (MU-MIMO) in LTE, except that the data transmission is now carried over multiple transmission points using the same scrambling code. The advantage of this transmission mode is that it allows for the cell splitting gain to be realized thereby effectively improving the overall system capacity. The schedulers among the CSC set may work jointly to schedule the data to minimize cross interference between the WTRUs. The multiflow transmission may also be processed by a precoding matrix before transmission. A data flow addressed to a WTRU may be associated to a specific set of precoding weights, rather than a cell. The WTRU may be required to report to the network its preferred precoding weights based on a measured channel condition. For the purpose of reducing cross interference, it may be desirable for the WTRU to select precoding weights based on the CSI for all the signal paths for the transmission points involved in the transmission. To support legacy WTRUs that are not designed for the CSC operation, a network scheduler may rely on available channel quality indicator (CQI) and precoding control indicator (PCI) information estimated by the WTRU to make scheduling decisions for transmissions. This does not require that the WTRU to be aware of the multipoint operation. FIG. 8 is an example of 4 branch DL-MIMO operating at an individual RRH. Cell 802 ( b ) includes an RRH 805 with its own RE 806 located in a centralized location, cell 801 ( a ). RRH 805 includes multiple antennas 810 which support multiple data streams 815 . As a more practical deployment scenario, as illustrated in FIG. 8 , each RRH may also include multiple antennas that may support multiple data streams. In the single point multi-antenna transmission operation mode, 4-branch DL-MIMO, each RRH, supported by its own scheduler, may be considered as an independent single point transmission operation with downlink MIMO with more than two layers. FIG. 9 is an example of 4 branch DL-MIMO when RRHs are used as simple antenna extensions. Cell 901 includes RRHs 905 ( a ), 905 ( b ), and 905 ( c ). RRHs 905 ( a ), 905 ( b ), and 905 ( c ) utilize common network scheduler 902 . In a similar deployment scenario, the antennas of the RRHs are may be considered as the antenna extension of the primary base station shown in FIG. 9 . Here, additional schedulers associated with the individual RRHs may not be needed. Therefore, the combination of the primary base station and the RRHs may include a common scheduling area. The 4DL-MIMO design may also include a design where the antennas are co-located within the base station with optional RRH deployment. A similar concept of same cell ID with RRH configurations is proposed in LTE coordinated multipoint (CoMP) and may be extended to HSDPA in the UMTS cellular network. As a proposed embodiment, a common scrambling code may be shared among the cells connected with RRH in order to improve the WTRU mobility and enhance the coverage of the control signals. Pilot design may be introduced to multiple downlink antennas, specifically 4DL-MIMO, and the RRH deployment to HSDPA, as there are only two pilots in existing DL-MIMO that may support up to rank 2 transmission. Further, while the concepts described herein are described in the context of 4 DL antennas, the concepts may also be applied to other antenna configurations, for example, 8 or more DL antennas. Therefore, when referring to 4-branch MIMO operations, it should be understood that this also refers to more than 4-branch operations, (e.g. 8-branch MIMO operations). The CPICH is a common pilot channel designed in UMTS to aid the channel estimation at the WTRU for downlink data transmissions. The CPICH may be scrambled with a unique scrambling code for each cell. Therefore, the CPICH may be considered to be cell-specific. New types of pilot channels may be used to accommodate different transmission modes in the CSC operation. A common pilot channel is a pilot channel transmitted from all the transmission points and may be scrambled with a scrambling code used in the CSC operation. The common pilot channel may be received by all of the WTRUs being served in the CSC area. A P-CPICH may be used that has a slot format of 20 bits/slot and a modulating bit sequence of all 0s, may be directly used for this purpose. Optionally, other modulating bit sequences may be used to differentiate the CSC operation. When REs are not co-located with the transmission points, advanced timing adjustment may be required for the baseband processing occurring at REs to ensure that the signals from each of the transmission points are accurately synchronized. Because this common pilot channel is intended to serve all of the WTRUs in an area, no cross-site precoding weights may be applied to the common pilot channel, in case other physical channels are precoded for performance enhancement. For the joint transmission mode, the common pilot channel may be sufficient for the WTRU to perform channel estimation and to demodulate the data, if no cross-cell precoding is employed. In a cell-specific or transmission-point specific pilot channel, each cell may transmit a pilot channel distinguishable from other cells. The cell-specific pilot channel may be designed to allow a WTRU to perform channel estimation for each individual signal path to the transmission points. Therefore, the cell-specific pilot channels may be orthogonal or close to orthogonal to single out the desired signal for the channel estimation. The orthogonality of the cell-specific pilot channels may be maintained by using an orthogonal modulating bit sequence. The bit sequences fed into the modulation mapped may be pre-defined differently for each of the cell-specific pilot channels. These bit sequences may be selected from a pool of orthogonal binary sequences of the similar slot format of CPICH. Table 1 is an example of orthogonal binary bit sequence used in the pilot channel. TABLE 1even slotsodd slotsbit sequence 10000000000000000000000000000000000000000bit sequence 20011110000111100001111000011110000111100bit sequence 31100110011001100110011001100110011001100bit sequence 41111000011110000111100001111000011110000 As shown in Table 1, the binary bit sequences may have a pattern defined, where the length of the sequence covers two time slots. Bit sequences of longer length (such as 4 or 8 slots) may also be used, which may generate more orthogonal choices. Use of an orthogonal sequence by the CSC cells may be assigned by a UTRAN, and indicated to a Node-B and a WTRU at a radio resource control (RRC) configuration. For example, the Node-B and WTRU may be informed of the orthogonal sequence via dedicated signaling or system information blocks (SIBs). The orthogonality of the cell-specific pilot channels may be maintained by using different channelization codes. With a spreading factor made equal to 256, (same as the CPICH), different channelization codes may be applied to the cell-specific pilot channels. As the channelization codes are orthogonal by nature, the pilot channels may be orthogonal from each other. For example, channelization codes, C 256,2 and C 256,3 may be candidates for the new pilot channels. The WTRU may obtain the channelization code and pilot information via dedicated RRC signaling or via the SIBs. Alternatively, the actual channelization codes used and pilot patterns may be pre-defined. The orthogonality of the cell-specific pilot channels may be maintained by using different scrambling codes. The cell-specific pilot channels may be transmitted differently by each cell under a different scrambling code. Although the pilot channels may not be perfectly orthogonal, the residual may be small enough to carry out the channel estimation at the WTRU receiver. Use of orthogonal sequences by the CSC cells may be assigned by a UTRAN, informed to a Node-B and a WTRU at an RRC configuration. The orthogonality of the cell-specific pilot channels may be maintained by time division multiplexing (TDM) the pilot channels. The transmit points in the CSC set may be coordinated in transmitting the pilot channel in a time switched manner, using the same bit sequence, channelization code, and scrambling code. As long as the network informs the WTRU of the schedule of the pilot channel transmission, the WTRU may perform individual channel estimation for a cell for the specified duration. The orthogonality of the cell-specific pilot channels may be maintained by overhead reduction. Introducing additional pilot channels may increase the control channel overhead and thus reduce efficiency of the data transmission. To mitigate the impact, a gated transmission may be adopted that only allows the transmission to take place within a specified duty cycle. In addition, the transmit power of the cell-specific pilot channel may be scaled down. In such cases, the WTRU may be informed of the power difference between the primary CPICH and the other pilot(s) via RRC signaling. This may allow the WTRU to compensate for the difference in transmission power in order to estimate the true channel. The CPICH is a common pilot channel designed in UMTS to aid the channel estimation at the WTRU for downlink data transmission. In 2-Tx DL MIMO, each antenna may have a common pilot channel so that the WTRU may compute the channel estimate for the antenna. This concept may be extended to 4-Tx DL MIMO, or more antennas, with a separate common pilot channel for each antenna. When 4-Tx DL MIMO is supported, the four common pilot channels may be transmitted and used for both data demodulation and measuring CSI. A similar concept may apply when more than 4 antennas are configured. Each of the CPICHs may be spread using a unique channelization code. Since the channelization codes are orthogonal, the WTRU may be able to determine unique channel estimates for each antenna. However, this may reduce the number of available codes for other physical channels and code usage may be an issue. To avoid any issues related to code usage, each CPICH may transmit an orthogonal pilot sequence and use the same channelization code. This may be used in various combinations of channelization codes and orthogonal pilot sequences. For example, in 4-Tx DL MIMO, each CPICH may transmit an orthogonal pilot sequence, and all four CPICHs may be spread with the same channelization code (e.g., C 256,0 ), or two orthogonal pilot sequences and two different channelization codes. Another potential issue related to using one CPICH per antenna is the presence of increased control channel overhead. To reduce the impact of additional CPICHs, the Node-B may periodically transmit the new CPICHs and/or transmit them at a lower power than the legacy CPICHs. When the new CPICHs are transmitted at a lower power, they may be used for CSI measurements. If the new CPICHs are needed for data demodulation, the Node-B may increase the power on the new CPICHs. However, the WTRU may need to know the change in CPICH power when it occurs for accurate CSI measurements. Also, increasing the CPICH power may increase the inter Node-B interference and may require additional pilot interference cancellation at the WTRU. Common pilot design considerations may be made for co-scheduled 4-branch MIMO and legacy 2-branch MIMO systems. When downlink (2-branch) MIMO is configured, the two pilot channels P-CPICH and S-CPICH may use two different channelization codes. When 4-branch MIMO is configured, although it may be challenging to co-schedule a 4-branch MIMO WTRU, the legacy WTRU may use 4 physical antennas by using a virtual antenna, a 4-branch MIMO WTRU and a 2-branch WTRU that use only 2 physical antennas may be co-scheduled. Therefore, it may be beneficial to keep the P-CPICH and S-CPICH pilot channel setting the same as the one required by legacy 2-branch MIMO when 4-branch, or more, MIMO is configured. Consequently, for the 4-branch MIMO case, the third and fourth common pilots CPICH3 and CPICH4 may share the two channelization codes with the P-CPICH and S-CPICH, while the pilot bit patterns may be orthogonal to the one used in P-CPICH and S-CPICH as shown in FIG. 10 . FIG. 10 is an example modulation pattern and channelization code assignment for four common pilot channels. The four common pilot channels in FIG. 10 are P-CPICH 1001 , S-CPICH 1002 , CPICH3 1003 , and CPICH4 1004 . P-CPICH 1001 and CPICH3 1003 share channelization code A. S-CPICH 1002 and CPICH4 1004 share channelization code B. Instead of sharing the P-CPICH and S-CPICH pilot channels between 4-branch MIMO WTRUs and legacy 2-branch MIMO WTRUs, another example may be to introduce four new common pilot channels, on top of the existing P-CPICH and S-CPICH pilot channels. The benefit of this pilot configuration scheme is that the legacy 2-branch MIMO WTRUs may be co-scheduled with 4-branch MIMO WTRUS, and the legacy 2-branch MIMO WTRUs may make full use of the four physical transmit antennas at the same time. If the four new common pilot channels are labeled as CPICH1, CPICH2, CPICH3, and CPICH4, the configuration of P-CPICH and S-CPICH may be the same as for the legacy 2-branch MIMO WTRUs as shown in FIG. 10 . FIG. 11 is a first example modulation pattern and channelization code assignment for six common pilot channels. The 6 common pilot channels in FIG. 11 are P-CPICH 1101 , S-CPICH 1102 , CPICH1 1103 , CPICH2 1104 , CPICH3 1105 , and CPICH4 1106 . CPICH1 1103 and CPICH2 1104 share channelization code C. CPICH3 1105 and CPICH4 1106 share channelization code D. The orthogonality between CPICH1 and CPICH2 and the orthogonality between CPICH3 and CPICH4 may be guaranteed by applying two orthogonal pilot patterns, as shown in FIG. 11 . FIG. 12 is a second example modulation pattern and channelization code assignment for six common pilot channels. The 6 common pilot channels in FIG. 12 are P-CPICH 1201 , S-CPICH 1202 , CPICH1 1203 , CPICH2 1204 , CPICH3 1205 , and CPICH4 1206 . To save the usage of DL channelization codes, the new four common pilot channels CPICH1 1203 and CPICH2 1204 , CPICH3 1205 and CPICH4 1206 share the same channelization code C, while the orthogonality among the four new pilot channels may be kept by using orthogonal pilot sequences as shown in Table 1. The WTRU-specific pilot channel may be generated in a similar way as the cell-specific pilot channel. The difference between the WTRU-specific pilot channel and the cell-specific pilot channel is that the WTRU-specific pilot channel is introduced to serve a specific WTRU or specific group of WTRUs. Therefore the WTRU-specific pilot channel may be precoded with the precoding weights obtained from the channel conditions for that WTRU. The WTRU-specific pilot channel may be transmitted from one cell or jointly transmitted from multiple cells. For data demodulation, one pilot per stream may be needed, whereas for CSI reporting purposes, one pilot per antenna may be required. For example, for every scheduled 4-branch MIMO WTRU, up to 4 WTRU-specific pilot channels may need to be transmitted, and 4 new common pilot channels may also be needed for CSI feedback generation for 4-branch MIMO WTRUs in order to make the 4-branch MIMO fully transparent to legacy 2-branch MIMO WTRUs so that 4-branch MIMO WTRUs and 2-branch MIMO WTRUs may be co-scheduled in the same subframe. However, this may require a significant amount of channelization codes in the downlink if code multiplexing pilot channels are based on channelization codes. For the 4 new common pilot channels, the channels may be transmitted with a lower duty cycle than WTRU-specific pilot channels, and therefore they may be transmitted in a time multiplexing fashion so that the channels may share a common channelization code. Several of the embodiments described below may significantly reduce the amount of required channelization codes for transmission of WTRU-specific pilot channels. A first family of separate channelization code (code division multiplexed (CDM)) solutions may consist of the WTRU receiving the pilot symbols over a separate channelization code. In one embodiment, all WTRU-specific pilot channels may be transmitted over one common channelization code such that the WTRU-specific pilot channels may be orthogonal to all other legacy downlink channels, such as P-CPICH, S-CPICH, and HS-PDSCH, and the like. Orthogonality of the WTRU-specific pilots within each WTRU, and orthogonality of the WTRU-specific pilots among different WTRUs may be achieved by using orthogonal pilot sequences under the same channelization code. In this embodiment, a pilot resource may be uniquely identified by a (RRC configured, static) channelization code and a pilot sequence index. Using a static channelization code, the WTRU may only need to be signaled by the pilot sequence index on a dynamic basis. In another embodiment, for all 4-branch MIMO WTRUs that are co-scheduled within one subframe, the WTRU-specific pilot channels belonging to the same WTRU may share one common channelization code that is associated with that WTRU. The orthogonality of WTRU-specific pilot channels among different WTRUs may be achieved by applying different channelization codes to different WTRUs that are co-scheduled in the same subframe. This embodiment may be appropriate for the case where the number of the co-scheduled 4-branch MIMO WTRUs is not significant. The benefit of this embodiment is that four orthogonal pilot sequences are sufficient and there may be no need to signal the WTRU with pilot sequences that are used by the Node-B. A set of predefined pilot sequences may be used for all WTRUs. In such cases, while each pilot resource may consist of a pair of channelization code index and pilot sequence index, only the channelization code (and the transmission rank) may be dynamically signaled to the WTRU. In another embodiment, the multiple WTRU dedicated pilot sequences may be transmitted by the Node-B and received by the WTRU using a combination of channelization codes and pilot sequences. Thus each pilot resource consists of a pair of channelization code index and pilot sequence index. In an example of this embodiment, a fixed number of pilot sequences are defined and may be re-used for each channelization code. Thus the total number of pilot resources is given by the product of the number of channelization codes and the defined pilot sequences. In another embodiment, the WTRU may receive a list of channelization code resources for dedicated pilots via RRC signaling. The pilot resources may then be organized for indexing in order of channelization code list and pilot sequence indices for each channelization code. Table 2 is an example of pilot resource indexing. TABLE 2Channelization codePilot sequencePilot resource indexCC # 0Sequence #00Sequence #11. . .2Sequence #(N seq − 1)3CC #1Sequence #04Sequence #15. . .6Sequence #(N seq − 1)7. . .. . .. . .CC #(N cc − 1)Sequence #0(N cc − 1)*N seq + 1Sequence #1. . .. . .. . .Sequence #(N seq − 1)N cc *N seq − 1 Table 2 shows an example of how pilot resources may be indexed when a combination of multiple channelization codes and pilot sequences are available. Here, N cc is the number of channelization codes signaled by the network and N seq is the maximum number of pilot sequences supported for a single channelization code. For example, the maximum number of pilot sequences supported for a single channelization code may be pre-defined in the specifications or configured by RRC signaling. In one case of the embodiment described above, Nseq=1 and each channelization code may carry only 1 pilot sequence. In another case of the embodiment described above, Ncc=1 and thus all the pilots sequences may be carried using a single channelization code. To reduce the signaling load associated to populating this list, a set of rules may be implemented such that the pilot resource indices are not necessarily signaled but rather inferred from the RRC signal ordered list. For example, the pilot resource indices may be inferred based on the order of the signaled pilot resource information. In one example, the WTRU is signaled a list of dedicated pilot channelization codes via RRC signaling. Based on the knowledge of Nseq, either fixed in the specifications or signaled by the network, (e.g., also via RRC signaling), the WTRU may determine the pilot resource indices in the order of the channelization code list received via RRC signaling. Depending on the method used for transmission of a dedicated pilot, a number of approaches may be used for the WTRU to determine the pilot resource to use for the associated data transmission. It may be desirable to have a small signaling overhead while leaving sufficient flexibility to allocate the resources efficiently. The methods for determining pilot information may be categorized as “implicit” and “explicit” indication methods. The methods may be used with any applicable dedicated pilot resource allocation method in any order or combination. When using implicit indication methods, it may be assumed that no additional signaling is required and it may be assumed that the WTRU determines the pilot information for each data stream based on fixed rules. In one particular method of implicit indication, the WTRU may be configured via RRC dedicated signaling with a specific dedicated pilot resource or a set of resources linked to one or more HS-SCCH resource or HS-SCCH number configured for dedicated pilot use. When the WTRU detects its high speed downlink shared channel (HS-DSCH) radio network transaction identifier (H-RNTI) on one of the configured HS-SCCH resources, the WTRU may determine the pilot information for the associated HS-PDSCH by association with the HS-SCCH configuration. In an example of this method, the WTRU may be pre-configured with a set of pilot sequences. For example, the WTRU may be pre-configured with up to the maximum of layers supported. The WTRU may receive an RRC configuration for HSDPA with dedicated pilots. For each HS-SCCH resource indicated, the WTRU may also receive the associated pilot channelization code. In another example, the pilot channelization code may be indicated under the “HS-SCCH Channelization Code” IE in the “HS-SCCH Info” IE as specified in the RRC specifications. When the WTRU detects its H-RNTI in the HS-SCCH, the WTRU may determine the pilot resources by association with the HS-SCCH number or resource. More specifically, when the WTRU detects its H-RNTI on a specific HS-SCCH resource, the WTRU may determine the associated HS-SCCH number from the IE configuration index associated with that HS-SCCH resource. The number of pilots may be determined explicitly or based on a combination of the number of transport blocks or codewords and associated layers as signaled on the HS-SCCH, for example. Hereinafter transport block and codewords are used interchangeably. In another example of this method, the WTRU may be pre-configured with a set of pilot resources, which may be indexed sequentially (for example, see Table 2). The WTRU may receive an RRC configuration for HSDPA with dedicated pilots. For each HS-SCCH resource indicated, the WTRU may also receive a base pilot resource index. When the WTRU detects its H-RNTI in the HS-SCCH, the WTRU may determine the pilot resource information, (channelization code, pilot sequences), by using the pilot resource index associated to the HS-SCCH number or resource in the configured look-up table. Using the implicit approach with the HS-SCCH number/resource may have the advantage of the WTRU knowing the pilot resource before starting to receive the HS-PDSCH. When using explicit methods, the WTRU may be indicated explicitly by the Node-B which dedicated pilot resource(s) to use for the associated HS-PDSCH transmission. In an example of this method, the WTRU may receive the dedicated pilot resource information in part 1 of the HS-SCCH. For example, the dedicated pilot resource information may consist of one or more indices to pilot resources. In another example, the dedicated pilot resource information may consist of a single index indicating a dedicated pilot channelization code or an index to a dedicated pilot channelization code, in which case the WTRU may determine the set of dedicated pilots resources to use by using a known pre-defined set of pilot sequences and the number of layers as signaled or determined from other fields in the HS-SCCH. This may be appropriate, for example, when a single channelization code per WTRU is used for dedicated pilot transmission. Alternatively, this dedicated pilot resource information may consist of a single index indicating, for example, a base pilot resource index, in which case the WTRU may determine the set of dedicated pilot resources to use via a configured look-up table and the number of layers determined for example using the other fields in the HS-SCCH. In the following example, up to 4 pilots may be needed for rank-4transmission, up to 4 sets of pilot information need to be signaled. This may be achieved for example by signaling a starting index of a set of orthogonal pilot sequences, and the WTRU may derive the numbers or indices of the rest of pilot sequences by reading the rank information signaled from the Node-B. This approach may require that the Node-B use a consecutive pilot index or that a fixed rule is defined for the WTRU to determine the pilot indices, and that both the WTRU and the Node-B are aware of the list of pilot indices. Alternatively, the Node-B may signal both the starting index of a set of orthogonal pilot sequences and the number of the orthogonal pilot sequences to be used by that WTRU. In this case, the number of the orthogonal pilot sequences may also be used as a rank indication. Therefore, there may not be a need for the Node-B to signal additional rank information to the WTRU. FIG. 13 is an example of pilot indexing with rank indication. FIG. 13 includes pilot 1301 . A starting point 1305 indicates starting with pilot 1301 ( a ). The rank information 1310 is used by the WTRU for demodulation and indicates that pilots 1301 ( a ) through 1301 ( b ) should be used for the demodulation. FIG. 13 illustrates an example where N is the maximum available number of orthogonal pilot resources. In the example, the WTRU may indicate a starting index pointing to Pilot #1 and a rank information of 3 layers leading the WTRU to use pilot indices #1-3 for demodulation of the three layers. A wrap-around of the indices may also be used. This example may be extended to support more than rank-4 transmission where the Node-B and the WTRU support a larger number of antennas. After receiving the HS-SCCH, the WTRU may determine the number of transport blocks in the associated HS-PDSCH, and the number of layers for each transport block. The number of layers for each transport block may be determined by a combination of fixed rules and explicit information signaled on the HS-SCCH. In one example, each transport block may be limited to a single layer by the specifications. Then, there is a one-to-one mapping between the number of transport blocks and the number of layers. The WTRU may receive the information on the number of transport blocks or number of layers in the HS-SCCH (preferably in part 1). The WTRU may then associate each transport block, with the information signaled in the HS-SCCH, to a given layer in order of the dedicated pilot index, (for example as configured via RRC signaling or in the specifications). For example, the WTRU may be configured with a set of pilot resources, for example via RRC configuration. In one particular example, the pilot resource configuration may consist of a starting index to a table of pilot resources. Then a given transmission time interval (TTI) the WTRU may be indicated via the HS-SCCH that the HS-PDSCH carries N layers. The WTRU may further determine the association between each layer and pilot based on the number of layers and the pilot resource configuration. For example, layer 1 may be associated to the first pilot in the set, layer 2 to the second pilot in the set and so forth. In another example, each transport block may be carried using more than one layer. The actual transport configuration may be restricted by the specifications. Table 3 is an example of transport blocks to number of layer mapping with up to 4 simultaneous transport blocks. TABLE 3Config #Nb of layersNb of TBNb of layers for each TB11112212321, 14313522, 15A31, 1, 16414723, 1822, 28A32, 1, 18B41, 1, 1, 1 Table 3 includes a table that lists a number of possible transport configurations or transport blocks to number of layer mapping with up to 4 simultaneous transport blocks. In practice, the set of configurations supported may be smaller than what is listed in Table 3. In one practical example or configuration reduction, the WTRU may be limited to two transport blocks at each TTI, and each transport block may thus be carried by up to two layers. Table 4 is an example of a reduced configurations set for up to two simultaneous transport blocks with up to 4 layers. TABLE 4Nb of layers for each TBConfig #Nb of layersNb of TBL 1 , L 21111, 02221, 13321, 24422, 2 Table 4 illustrates an example of this concept, derived from Table 3, where only 4 transmission configurations are supported. The number of layers for the first and second transport block (L 1 , L 2 , respectively) is shown in the last column. In another example, up to two transport blocks may be multiplexed into a single code word. In this example, a codeword is not necessarily identical to a transport block. Codewords containing a single transport block may be transmitted with a single layer, whereas codewords containing two transport blocks may be transmitted using two layers. In the context of 4DL-MIMO, the WTRU may be configured to receive up to 2 codewords. In this example, the WTRU may be indicated the number of transport blocks or codewords and number of layers in the HS-SCCH, preferably in part 1. The WTRU may then associate each transport block to one or two layers in the order of dedicated pilot index. This association may be carried out by the WTRU using a table lookup, for example. In one example, the WTRU may determine the number of transport blocks based on the indicated number of codewords and number of layers in the HS-SCCH, preferably part 1 using a fixed set of rules. More generally, it may be assumed that the WTRU is configured with a set of dedicated pilots that may be indexed. This indexing may be achieved using, for example, any one or a combination of the methods described above. Then let p l,k be the dedicated pilot index associated to layer index k=0,1, . . . , N L of transport block index l=0,1, . . . , N tb , where N L and N tb are the maximum number of layers per transport block and the maximum number of simultaneous transport blocks for a subframe. Further it may be assumed that the WTRU dynamically receives a dedicated pilot base index offset b. In one embodiment, the pilots may be associated in order of transport block first and then layers for each transport block. For example, when two transport blocks are transmitted, each with two layers, the first and second dedicated pilots for that WTRU may be associated with the first and second layers of the first transport block, respectively, and the third and fourth dedicated pilots may be associated with the first and second layers of the second transport block, respectively. In this example approach, the dedicated pilot index associated to the layer l or transport block k may be expressed as: This expression may imply that the maximum number of layers is allocated to the first block before another transport block can be allocated a layer. Alternatively, the number of the layer for transport block k may be expressed as N L,k , and the following expression for the pilot index does not suffer from that restriction: Alternatively in another approach, the pilots may be associated first in order of layers for each transport block. For example, when two transport blocks are transmitted, each with two layers, the first and second dedicated pilots for that WTRU may be associated with the first layer of the first and second transport block, respectively. The third and fourth dedicated pilots may be associated with the second layer of the first and second transport block, respectively. Similarly in this example approach, the dedicated pilot index associated to the layer l or transport block k may be expressed as: This expression may ensure that the layers are allocated to each TB in turn. Finally, for convenience, wrap around of the dedicated pilot indices may also be used, and in such cases the actual index p l,k ′ may be determined via a modulo operation, for example as follows: In another embodiment, the WTRU-specific pilots may be time-multiplexed with an HS-PDSCH channel. FIG. 14 is an example of time multiplexing WTRU-specific pilot with HS-PDSCH. FIG. 14 includes slot #0 1401 ( a ), slot #1 1401 ( b ), and slot #2 1401 ( c ). Slot #1 1401 ( b ) is expanded to show that it includes data 1405 and a pilot 1410 . The pilot 1410 is inserted into the middle of slot #1 1401 ( b ). FIG. 14 illustrates an example where the pilots are inserted into the middle of each slot of an HS-PDSCH subframe. Since multicode transmission is possible, where multiple channelization codes may be assigned to a single WTRU for HS-PDSCH transmission, the WTRU-specific pilots may need to be transmitted on one of the assigned channelization codes, as shown in FIG. 15 , while the HS-PDSCH channels on other assigned channelization codes are transmitted in the legacy way. FIG. 15 is an example of time multiplexing WTRU-specific pilot with HS-PDSCH on one channelization code. FIG. 15 illustrates channelization code #1 1520 to code #15 1530 . In channelization code #1 1520 , slots #0 1501 ( a ), #1 1501 ( b ), and #2 1501 ( c ) include pilot 1510 inserted between data 1505 of an HS-PDSCH subframe. Channelization code #2 1525 to code #15 1530 only include data 1505 in each HS-PDSCH subframe. Alternatively, the WTRU-specific pilots may be transmitted on one of the assigned channelization codes, and the pilot portion of HS-PDSCHs on other assigned channelization codes are not transmitted or discontinuously transmitted (DTXed), as shown in FIG. 16 . FIG. 16 is an example of time multiplexing WTRU-specific pilot with HS-PDSCH on one channelization code and discontinuously transmitting the pilot portion of HS_PDSCHs on all other channelization codes. FIG. 16 illustrates channelization code #1 1620 to code #15 1630 . In channelization code #1 1620 , slots #0 1601 ( a ), #1 1601 ( b ), and #2 1601 ( c ) include pilot 1610 inserted between data 1605 of an HS-PDSCH subframe. Channelization code #2 1625 to code #15 1630 includes a discontinuous transmission (DTX) 1615 inserted between data 1605 in each HS-PDSCH subframe. Alternatively, the WTRU-specific pilot may be time multiplexed with the HS-PDSCH on each assigned channelization code, as shown in FIG. 17 . FIG. 17 is an example of time multiplexing WTRU-specific pilot with HS-PDSCH on all assigned channelization codes (up to 15). FIG. 17 illustrates channelization code #1 1720 to code #15 1730 . In channelization code #1 1720 to code #15 1730 , slots #0 1701 ( a ), #1 1701 ( b ), and #2 1701 ( c ) include pilot 1710 inserted between data 1705 of an HS-PDSCH subframe. Alternatively, the pilot symbols may be spread uniformly across the radio slot in the HS-PDSCH on one code or over all channelization codes. Any number of pilot symbols (N pilot ) may be inserted in a given HS-PDSCH radio slot. Table 5 is an example of traffic-to-pilot ratio for various numbers of pilot symbols per radio slot. TABLE 5Nb pilots symbols/slotT/P (dB)122.0219.0317.2415.9514.9614.1713.4812.8912.21011.81111.31210.91310.51410.2159.9169.5179.2189.0198.7208.5 Table 5 shows the resulting traffic-to-pilot power ratio (T/P) for each value of number of pilots (for 1 radio slot) in a spreading factor (SF)=16 HS-PDSCH. Nb pilot symbols, 4, 5, 6, 8, 9, 12, and 15 are entries that correspond to one example subset of values that may be used. These values were obtained by finding the entries corresponding to 10,11,12, . . . 16 dB T/P as it is currently defined for the uplink. The values are for a single radio-slot for a single code. To achieve the listed T/P the corresponding number of pilots must be inserted on each channelization code and for all slots in the subframe. As it may be more convenient in terms of hardware processing to have all the pilot symbols carried on a single channelization code (as illustrated by FIG. 17 ), the number of pilot symbols present on the channelization code carrying pilot may depend on the actual number of HS-PDSCH codes being used for the transmission. In one method, the WTRU may be configured, for example, via RRC signaling for a specific T/P. Then, assuming the same pilot power and modulation scheme for all symbols in a HS-PDSCH code, the WTRU may determine the actual number of pilots used. For example, assuming that N ch HS-PDSCH codes are being used (Nch=1, . . . , 15), then the WTRU may determine the number of pilot symbols N pilot (for one radio slot) on the HS-PDSCH code carrying the pilot symbols via this expression: where 160 corresponds to the number of SF=16 symbols in one radio slot, and TP is the T/P expressed in dB. While a floor operation is indicated in Equation (5), a ceiling or rounding up to the closest integer may also be used to determine the number of pilot symbols. The WTRU may also be configured with a fixed table indicating the number of pilot symbols for each possible configuration of T/P and number of HS-PDSCH codes being transmitted. Table 6 is an example table of the number of pilots for each T/P and number of HS-PDSCH codes configuration. TABLE 6Number of HS-PDSCH codes123456789101112131415T/P10142943587287101116130145160XXXX(dB)111123354758708294105117129141XXX1291828374756667585941041131231321421371522303845536168768391991061141461218243036424955616773798591154914192429343944495358636873163711151923273135394347505458 Table 6 shows an example of such a configuration table, (obtained using Equation (5)). The entries designated with an “X” may require more pilot symbols than what is available using a single code, and thereby may not be used. Optionally, for those entries, the WTRU may be configured to transmit using the maximum number of pilot symbols (i.e., 160). The network may also signal the power ratio between the WTRU-specific pilot channel and the data channel (HS-PDSCH) to the WTRU. This may be carried out, for example, via RRC signaling or other means. To assist CQI evaluation at the WTRU, one or any combination of the following power ratios may be signaled from the Node-B to the WTRU via RRC signaling: the ratio between common pilot power and WTRU specific pilot power, the ratio between WTRU specific pilot power and data power, and the ratio between common pilot power and data power. The construction of orthogonal pilot sequences for WTRU-specific pilot channels may depend on the spreading factor selected. If the spreading factor is 256, the pilot sequences shown in Table 1 for a common pilot channel may be reused. If a spreading factor of 128 is chosen, the following approach may be used to construct a set of orthogonal pilot bit sequences with bit length of 40: where 1 is mapped to binary bits 00 and −1 is mapped to binary bits 11 . The binary bits may be used for modulation symbol mapping. If the spreading factor is 4, a set of four orthogonal pilot symbols with length of 4 may be defined as follows: where each row represents one pilot pattern and 1 is mapped to binary bits 00 and −1 is mapped to binary bits 11 . Consequently, the length of the corresponding pilot bit sequence is 8. This 4-symbol long pilot may be used to construct an 8-symbol pilot, which is defined as follows: A network may also take advantage of multiple downlink transmit antennas to increase the coverage of control channels. Further, pre-coding or beamforming may be applied to the HS-SCCH in order to increase HS-DSCH coverage, reduce interference, and use less power resources for that particular control channel. Methods for the WTRU to receive, demodulate, and decode the precoded control channel information, for example, HS-SCCH, are described in the context of both pre-coded pilot and common pilot scenarios below. Although the methods are described in the context of the WTRU receiving the HS-SCCH, the methods may also apply to other channels or other technologies. In one example, the WTRU may be configured to receive the HS-SCCH using a pre-coded pilot. As a result, the amount of pilot power used by the network may be dynamically adapted and optimized. In a conventional system, the HS-SCCH may be demodulated using the common pilot and the WTRU is aware of the spreading code and pilot sequence to use for demodulation. With pre-coded pilots, a new set of pilot resources may need to be used for demodulation purposes. Methods for use in the WTRU to determine which pilot resource (i.e., channelization code, pilot sequence) to use are described herein. In a first set of methods, the WTRU is configured with a specific pilot resource, or set of resources for decoding the HS-SCCH and, optionally, the associated HS-PDSCH. In one example, the WTRU may receive the configuration via RRC signaling. Then, the WTRU may attempt to demodulate and decode the HS-SCCH using the configured pilot resource. From a resource allocation perspective, the RNC may allocate a different pilot resource to every WTRU. However, this approach may consume a large number of channelization codes and may be inefficient as only a fraction of WTRUs are expected to receive HS-DSCH transmission on any given TTI. In order to improve the efficiency of the pilot resource allocation, it may be advantageous to allocate the same pilot resource or set of pilot resources to more than one WTRU. Each pilot resource may only be used for a single WTRU at a time. The WTRU may use each pilot resource for channel estimation of a different layer. The pilot resource—layer association may be configurable or implicit, for example, based on fixed rules known by both the WTRU and Node B. In order to allow as many WTRUs as possible to be scheduled with an HS-SCCH, the same set of pilot resources may be allocated to more than one WTRU with a potentially different pilot to layer association for each WTRU. This may provide the Node B with the additional flexibility to potentially schedule more than one WTRU, for example, MU-MIMO operation, from the same pilot resource set in a single TTI provided that the actual pilot resources used for each WTRU are not the same simultaneously. Table 7 is an example of potential pilot resource allocation to multiple WTRUs. TABLE 7Pilot resource associated layerPilot resourceUE 1UE 2UE 3UE 4P0L0,L3L2LlHS-SCCHP1L1L0, HS-SCCHL3L2P2L2L1L0, HS-SCCHL3P3L3L2L1L0, HS-SCCH Each WTRU may be configured with a set of pilot resources (P0 to P3 in this example) and each pilot resource may be associated with a specific layer. In this example, the layers are L0, . . . , L3 and the HS-SCCH is further associated with L0. While each WTRU is configured with the same set of pilot resources, each resource may be associated with a different layer. The number of pilot resources and WTRUs may differ than what is shown in Table 7. For example, the pilot-resource to layer association may not be mutually exclusive. To further improve the reliability of HS-SCCH reception, the HS-SCCH may be placed on the layer that has the best signal quality. When a Node B performs association of pilot resource to the layers, the Node B may be further required to apply the pilot resource associated to HS-SCCH to the precoding weight that has the best CQI report or another type of performance measure. For example, in Table 7, WTRU 1 may assign P0 to its best layer and WTRU 4 may have P3 transmitted on its best layer. In a second set of methods, the WTRU may be configured with a set of HS-SCCH codes to monitor and there may be one or more associated pilot resources for each HS-SCCH code. The WTRU attempts to decode each configured HS-SCCH code using the associated pilot for demodulation and channel filtering. Because the number of pilot resources to be reserved by the network for control-channel demodulation purposes is of the same order as the number of HS-SCCH codes, the pilot resource used for the associated HS-PDSCH may be different than the pilot resource used for the HS-SCCH. As a result, the Node B may transmit the HS-SCCH with a different precoding weight. FIG. 18 is an example of pilot resource allocation for demodulation of HS-SCCH and HS-PDSCH. FIG. 18 includes HS-SCCH 1801 and HS-PDSCH 1802 . Each channel 1801 and 1802 includes pilot resources #0 and #1. Pilot resource #0 corresponds to P0 1804 and pilot resource #1 corresponds to P1 1803 . The pilot resource used for the associated HS-PDSCH may also be the same as the pilot resource used for the HS-SCCH. Because the HS-SCCH and associated HS-PDSCH overlap in time, the same pilot resource may not be used in adjacent TTIs. This may be achieved by allocating two sets of pilot resources for each HS-SCCH code and using them in time alternation. Provided there is an appropriate configuration, this may prevent pilot resource collision. The WTRU may be configured with two sets of pilot resources for each HS-SCCH, and the WTRU may use them in time-alternation for demodulating the HS-SCCH and the associated HS-PDSCH, if present, according to a fixed rule. FIG. 18 shows that P0 and P1 are the two pilot resources for a specific HS-SCCH code. FIG. 19 is an example of pilot resource monitoring in the presence of transmission. FIG. 19 includes HS-SCCH 1901 and HS-PDSCH 1902 . Each channel 1901 and 1902 includes pilot resources #0 and #1. Pilot resource #0 corresponds to P0 1904 and pilot resource #1 corresponds to P1 1903 . Additional rules may be used to simplify the WTRU processing. For example, when the WTRU receives an actual HS-SCCH with associated data on the HS-PDSCH, the WTRU may use the same pilot resource for channel estimation on subsequent consecutive HS-DSCH transmissions. This may simplify the WTRU channel estimation procedure as it may not need to track two different channel estimates (from different pilot resources) during subsequent transmission. Accordingly, when the WTRU decodes its H-RNTI successfully on the HS-SCCH, the WTRU may use the same pilot resource for demodulating the HS-SCCH on the next TTI. If the next TTI does not carry a transmission for that WTRU, the WTRU may revert to the predefined pilot resource schedule on the following TTI. In another method, the WTRU may be configured with a set of pilot resources for HS-SCCH monitoring and may blindly demodulate the HS-SCCH. The WTRU may attempt to decode each configured HS-SCCH with each pilot resource configured until the WTRU detects its H-RNTI or until the WTRU has exhausted the search. In another method, the WTRU may be configured to monitor a broadcast channel indicating the pilot resource to use for a particular HS-SCCH code. In a common pilot scenario, the WTRU may be configured to estimate the channel based on a set of common pilot channels. This is similar to conventional MIMO operations from Release 7 where the Node B transmits pre-coded HS-PDSCH channels and indicates an index to the pre-coding weight to the WTRU. However, here, the HS-SCCH may also be pre-coded by the Node B in order to improve the cell coverage. Because it may be preferred from the demodulation performance point-of-view that the WTRU is aware of a priori, the actual weights used for HS-SCCH pre-coding, indicating the weights on the HS-SCCH itself, may not be sufficient. In a first method, the WTRU may be configured with a set of precoding weights that may be used for HS-SCCH precoding. In one example, the WTRU may blindly determine the HS-SCCH pre-coding weights by attempting to decode the HS-SCCH with each precoding weight in the configured set. Optionally, the precoding weight indicated on the HS-SCCH for HS-PDSCH may be the same as the HS-SCCH precoding weight. The WTRU may then make correction to its channel estimate if its precoding weight estimate for HS-SCCH is wrong in the first place. In a second method, the WTRU may be configured with a set of precoding weights and one or more parameters describing a schedule for the HS-SCCH precoding weights. In each HS-SCCH subframe, the WTRU may determine the scheduled HS-SCCH precoding weights based on the configured parameters and potentially the connection frame number (CFN). The WTRU may use these weights to attempt decoding the HS-SCCH. Here, the WTRU may use different precoding weights, for example, the ones indicated in the decoded HS-SCCH, to demodulate the associated HS-PDSCH. In a third method, the WTRU may also be configured with a set of precoding weights. The WTRU may determine the actual weights used for a particular HS-SCCH subframe and HS-SCCH code based on a separate signal broadcast from the Node B. This new signal carries the precoding weight index for each configured HS-SCCH code. In a fourth method, the HS-SCCH may be precoded according to the precoding weight used in the last sub-frame of HS-PDSCH that was transmitted to this WTRU. The WTRU may be required to store the used precoding weight in the memory and use it when decoding the HS-SCCH that comes next. Here, the WTRU may be allowed to use same or different precoding weights, for example, the ones indicated in the decoded HS-SCCH, to demodulate the associated HS-PDSCH. The HS-SCCH may be precoded by using the same precoding weight carried for the associated HS-PDSCH sub-frame at an initial sub-frame of a downlink transmission or if the WTRU has been idling for an excessive period of time. Here, the first sub-frame of HS-SCCH may be decoded by blind detection of the precoding weight. Methods for control information processing may include methods to signal downlink control information for a maximum of 2 codewords or 4 codewords. For signaling downlink control information for a maximum of 2 codewords, a non-codebook-based MIMO transmission structure may not require the WTRU to know the precoder applied at the transmitter for data demodulation. On the other hand, the WTRU may need such knowledge to demodulate and decode for a codebook-based MIMO transmission. Signaling methods for both codebook-based and non-codebook-based MIMO transmissions are discussed below. As previously described, no precoding information may need to be signaled to the WTRU. Therefore, the precoding weight information field xpwipb1, xpwipb2 of the existing HS-SCCH type 3 may be reused for other purposes. For example, combined with modulation scheme and number of transport blocks information field xms1, xms2, xms3, the precoding weight information field may be used to signal rank information up to rank 4. Table 8 is a first example mapping of X pwi , X ms . TABLE 8xms,1,Modulation forModulation forNumber ofxpwipb,1,xms,2,primarysecondarytransportxpwipb,2xms,3transport blocktransport blockblocks00: rank 211116 QAM16 QAM201: rank 310: rank 400: rank 211016 QAMQPSK201: rank 310: rank 4If xccs,7 = 1 then10164 QAMIndicated byIndicated by00: rank 2xccs,7xccs,701: rank 310: rank 4Otherwise, n/an/a10016 QAMn/a100: rank 2011QPSKQPSK201: rank 310: rank 400: rank 201064 QAM64 QAM201: rank 310: rank 400: rank 200164 QAM16 QAM201: rank 310: rank 4n/a000QPSKn/a1 As one example, Table 8 shows how to use existing weight information field xpwipb and modulation scheme and number transport blocks information field xms to signal the rank information. The weight information field xpwipb may be used to signal rank information (rank 2, 3, or 4) when the number of transport blocks is two. There may be no change on part 2 of the existing HS-SCCH type 3 channel. The benefit of this example is that at the WTRU side there may be very few changes on the decoding of HS-SCCH type 3 channel. The only change may be the reinterpretation of the xpwipb field when 4-branch DL MIMO is configured. Since there are a total of 21 different combinations of modulation scheme and rank, 5 bit information may be sufficient to convey all of them. Table 9 is a second example mapping of X pwi , X ms . TABLE 9ModulationModulationfor primaryfor secondaryNumber ofxpwipb,1,xms,1, xms,2,transporttransporttransportxpwipb,2xms,3blockblockblocksrank0011116 QAM16 QAM220131040011016 QAMQPSK220131040010164 QAMQPSK220131041110164 QAMn/a11n/a10016 QAMn/a1100011QPSKQPSK220131040001064 QAM64 QAM220131040000164 QAM16 QAM22013104n/a000QPSKn/a11 As another example shown in Table 9, both xpwipb and xms fields may be used, 5-bit in total, to signal rank and modulation scheme and consequently there may be no need to use X ccs,7 to signal modulation or rank information and the channelization code-set mapping may be defined as follows. Given P (multi-) codes starting at code O, the information-field may be calculated using the unsigned binary representation of integers for the first three bits (code group indicator) of which x ccs,1 is the MSB using the following equation: The information-field may be calculated using the unsigned binary representation of integers for the last four bits (code offset indicator) of which x ccs,4 is the most significant bit (MSB) using the following equation: Compared with existing channelization code-set mapping algorithm, this method may not put any restrictions on the selection of P and O via HS-SCCH number as in the existing method and thus may increase the scheduling flexibility. In Release 7 downlink MIMO, each transport block is mapped to a single layer. With the support of up to 4 layers on the downlink, a single transport block may be carried over two layers, alone or in combination with another transport block, for example, on one or two different layers. Table 10 is a third example mapping of X pwi , X ms . TABLE 10xms,1,Modulation forModulation forNumber ofxpwipb,1,xms,2,primarysecondarytransportxpwipb,2xms,3transport blocktransport blockblocks00: rank 211116 QAM16 QAM201: rank 310: rank 400: rank 211016 QAMQPSK201: rank 310: rank 4If xccs,7 = 1 then10164 QAMIndicated byIndicated by00: rank 2xccs,7xccs,701: rank 310: rank 4Otherwise,00: rank 101: rank 200: rank 110016 QAMn/a101: rank 200: rank 2011QPSKQPSK201: rank 310: rank 400: rank 201064 QAM64 QAM201: rank 310: rank 400: rank 200164 QAM16 QAM201: rank 310: rank 400: rank 1000QPSKn/a101: rank 2 Table 10 shows an example of alternate mapping derive from Table 8. Table 11 is a fourth example mapping of X pwi , X ms . TABLE 11ModulationModulationfor primaryfor secondaryNumber ofxpwipb,1,xms,1, xms,2,transporttransporttransportxpwipb,2xms,3blockblockblocksrank0011116 QAM16 QAM220131040011016 QAMQPSK220131040010164 QAMQPSK220131041110164 QAMn/a110010020110016 QAMn/a1110200011QPSKQPSK220131040001064 QAM64 QAM220131040000164 QAM16 QAM2201310400000QPSKn/a11012 Table 11 shows an example of alternate mapping derived from Table 9. In another alternative, it may be assumed that transport blocks may only be carried with one or two layers, and when two transport blocks are transmitted, the Node-B may use three or four layers. With such restriction, a single bit of X pwi may be needed to signal the rank and the other bit may be reserved for future use. Table 12 is a fifth example mapping of X pwi , X ms . TABLE 12xms,1,Modulation forModulation forNumber ofxms,2,primarysecondarytransportxpwipb,1xms,3transport blocktransport blockblocks0: rank 311116 QAM16 QAM21: rank 40: rank 311016 QAMQPSK21: rank 4If xccs,7 = 1 then10164 QAMIndicated byIndicated by0: rank 3xccs,7xccs,71: rank 4Otherwise,0: rank 11: rank 20: rank 110016 QAMn/a11: rank 20: rank 3011QPSKQPSK21: rank 40: rank 301064 QAM64 QAM21: rank 40: rank 300164 QAM16 QAM21: rank 40: rank 3000QPSKn/a11: rank 4 Table 13 is a sixth example mapping of X pwi , X ms . TABLE 13ModulationModulationfor primaryfor secondaryNumber ofxpwipb,1,xms,1, xms,2,transporttransporttransportxpwipb,2xms,3blockblockblocksrank0011116 QAM16 QAM230140011016 QAMQPSK230140010164 QAMQPSK230141010164 QAMn/a111120010016 QAMn/a1101200011QPSKQPSK230140001064 QAM64 QAM230140000164 QAM16 QAM2301400000QPSKn/a11012 Table 13 shows another example of alternate mapping derived from Table 12, whereas in Table 9 and 11, X ccs,7 is not used for indicating the modulation format. In this example, both bits of X pwi may be needed. One of the bits may be used to differentiate the modulation scheme and number of transport blocks replacing the function of X ccs,7 . For codebook-based MIMO transmission scheme, not only may the rank information need to be signaled to the WTRU, but also the precoding weight information may need to be signaled. Additional bits may be required in order to signal precoding weight information compared to Release 7 MIMO. Therefore, the HS-SCCH type 3 may not be directly reused or extended to support 4-branch DL MIMO and a new type of HS-SCCH may be designed. Without loss of generality, the new HS-SCCH channel may be named HS-SCCH type 4. HS-SCCH type 4 may be used when the WTRU is configured in 4Tx MIMO mode. For HS-SCCH type 4 content, if one transport block is transmitted on the associated HS-PDSCH(s), the following information may be transmitted by means of the HS-SCCH type 4 physical channel. Channelization-code-set information may be transmitted by means of the HS-SCCH type 4 physical channel using 7 bits, for example, x ccs,1 , x ccs,2 , . . . , x ccs,7 . Modulation scheme and number of transport blocks information may be transmitted by means of the HS-SCCH type 4 physical channel using 3 bits, for example, x ms,1 , x ms,2 , x ms,3 . Rank information may be transmitted by means of the HS-SCCH type 4 physical channel using 2 bits, for example, x ri,1 , x ri,2 . The transmission of rank information may be optional. Precoding weight information may be transmitted by means of the HS-SCCH type 4 physical channel using 4 bits, for example, x pwipb,1 , x pwipb,2 , x pwipb,3 , x pwipb,4 . Transport-block size information may be transmitted by means of the HS-SCCH type 4 physical channel using 6 bits, for example, x tbspb,1 , x tbspb,2 , . . . , x tbspb,6 . Hybrid-ARQ process information may be transmitted by means of the HS-SCCH type 4 physical channel using 4 bits, for example, x hap,1 , x hap,2 , . . . , x hap,4 . Redundancy and constellation version may be transmitted by means of the HS-SCCH type 4 physical channel using 2 bits, for example, x rvpb,1 , x rvpb,2 . WTRU identity information may be transmitted by means of the HS-SCCH type 4 physical channel using 16 bits, for example, x wtru,1 , x wtru,2 , . . . , x wtru,16 . For HS-SCCH type 4 content, if two transport blocks are transmitted on the associated HS-PDSCHs, the following information may transmitted by means of the HS-SCCH type 4 physical channel. Channelization-code-set information may be transmitted by means of the HS-SCCH type 4 physical channel using 7 bits, for example, x ccs,1 , x ccs,2 , . . . , x ccs,7 . Modulation scheme and number of transport blocks information may be transmitted by means of the HS-SCCH type 4 physical channel using 3 bits, for example, x ms,1 , x ms,2 , x ms,3 . Rank information may be transmitted by means of the HS-SCCH type 4 physical channel using 2 bits, for example, x ri,1 , x ri,2 . Rank information may be optional. Precoding weight information may be transmitted by means of the HS-SCCH type 4 physical channel using 4 bits, for example, x pwipb,1 , x pwipb,2 , x pwipb,3 , x pwipb,4 . Transport-block size information for the primary transport block may be transmitted by means of the HS-SCCH type 4 physical channel using 6 bits, for example, x tbspb,1 , x tbspb,2 , . . . , x tbspb,6 . Transport-block size information for the secondary transport block may be transmitted by means of the HS-SCCH type 4 physical channel using 6 bits, for example, x tbssb,1 , x tbssb,2 , . . . , x tbssb,6 . Hybrid-ARQ process information may be transmitted by means of the HS-SCCH type 4 physical channel using 4 bits, for example, x hap,1 , x hap,2 , . . . , x hap,4 . Redundancy and constellation version for the primary transport block may be transmitted by means of the HS-SCCH type 4 physical channel using 2 bits, for example, x rupb,1 , x rupb,2 . Redundancy and constellation version for the secondary transport block may be transmitted by means of the HS-SCCH type 4 physical channel using 2 bits, for example, x rvsb,1 , x rvsb,2 . WTRU identity information may be transmitted by means of the HS-SCCH type 4 physical channel using 16 bits, for example, x wtru,1 , x wtru,2 , . . . , x wtru,16 . In an alternate embodiment of the content, the rank information may be derived implicitly from the precoding information; therefore the rank information becomes optional depending on the implementation. In such cases, the precoding information may consist of an index to a precoding matrix which is pre-configured with a fixed number of layers. FIG. 20 is an example of the coding chain for HS-SCCH type 4 . FIG. 20 includes a first multiplexer 2005 and a second multiplexer 2010 . Channelization code set information, modulation scheme and transport block information, rank information, and precoding weight information 2001 may be put into a first multiplexer 2005 . The inclusion of rank information may be optional. The output of the first multiplexer 2005 may then be channel coded using a first channel coding 2015 . The output of the first channel coding 2015 may then be rate matched using a first rate matching 1 2020 . The output of the first rate matching 2020 may then be masked with WTRU-specific masking 2025 . WTRU identity information may be included in the WTRU-specific masking 2025 . Transport-block size information for the primary transport block, transport-block size information for the secondary transport block, Hybrid-ARQ process information, redundancy and constellation version for the primary transport block, and redundancy and constellation version for the secondary transport block 2002 may be input to a second multiplexer 2010 . The output of the second multiplexer 2010 may receive WTRU-specific CRC attachment 2030 . WTRU identity information may be included the WTRU-specific CRC attachment 2030 . The output may then be channel coded using a second channel coding 2035 . The output of the second channel coding 2035 may then be rate matched using a second rate matching 2040 . The output of the second rate matching 2040 may then be combined with the WTRU-specific masking 2025 output for physical channel mapping 2045 . For signaling downlink control information for a maximum of 4 codewords, the Node-B may need to signal the transport block size and redundancy and constellation version for each codeword. For a non-codebook-based MIMO scheme, there may be no need to signal precoding weight information. If one transport block is transmitted on the associated HS-PDSCH(s), the following information may be transmitted by means of the HS-SCCH physical channel. Channelization-code-set information may be transmitted by means of the HS-SCCH physical channel using 7 bits, for example, x ccs,1 , x ccs,2 , . . . , x ccs,7 . Modulation scheme and number of transport blocks information may be transmitted by means of the HS-SCCH physical channel using 3 bits, for example, x ms,1 , x ms,2 , x ms,3 . Rank information may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x ri,1 , x ri,2 . Transport-block size information may be transmitted by means of the HS-SCCH physical channel using 6 bits, for example, x tbspb,1 , x tbspb,2 , . . . , x tbspb,6 . Hybrid-ARQ process information may be transmitted by means of the HS-SCCH physical channel using 4 bits, for example, x hap,1 , x hap,2 , . . . , x hap,4 . Redundancy and constellation version may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rvpb,1 , x rvpb,2 . WTRU identity information may be transmitted by means of the HS-SCCH physical channel using 16 bits, for example, x wtru,1 , x wtru,2 , . . . , x wtru,16 . If two transport blocks are transmitted on the associated HS-PDSCHs, the following information may be transmitted by means of the HS-SCCH physical channel. Channelization-code-set information may be transmitted by means of the HS-SCCH physical channel using 7 bits, for example, x ccs,1 , x ccs,2 , . . . , x ccs,7 . Modulation scheme and number of transport blocks information may be transmitted by means of the HS-SCCH physical channel using 3 bits, for example, x ms,1 , x ms,2 , x ms,3 . Rank information may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x ri,1 , x ri,2 . Transport-block size information for the primary transport block may be transmitted by means of the HS-SCCH physical channel using 6 bits, for example, x tbspb,1 , x tbspb,2 , . . . , x tbspb,6 . Transport-block size information for the secondary transport block may be transmitted by means of the HS-SCCH physical channel using 6 bits, for example, x tbssb,1 , x tbssb,2 , . . . , x tbssb,6 . Hybrid-ARQ process information may be transmitted by means of the HS-SCCH physical channel using 4 bits, for example, x hap,1 , x hap,2 , . . . , x hap,4 . Redundancy and constellation version for the primary transport block may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rvpb,1 , x rvpb,2 . Redundancy and constellation version for the secondary transport block may be transmitted by means of the HS-SCCH physical channel using 2 bits, x rvsb,1 , x rvsb,2 . WTRU identity information may be transmitted by means of the HS-SCCH physical channel using 16 bits, for example, x wtru,1 , x wtru,2 , . . . , x wtru,16 . If three transport blocks are transmitted on the associated HS-PDSCHs, the following information may be transmitted by means of the HS-SCCH physical channel. Channelization-code-set information may be transmitted by means of the HS-SCCH physical channel using 7 bits, for example, x ccs,1 , x ccs,2 , . . . , x ccs,7 . Modulation scheme and number of transport blocks information may be transmitted by means of the HS-SCCH physical channel using 3 bits, for example, x ms,1 , x ms,2 , x ms,3 . Rank information may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x ri,1 , x ri,2 . Transport-block size information for the primary transport block may be transmitted by means of the HS-SCCH physical channel using 6 bits, for example, x tbspb,1 , x tbspb,2 , . . . , x tbspb,6 . Transport-block size information for the secondary transport block may be transmitted by means of the HS-SCCH physical channel using 6 bits, for example, x tbssb,1 , x tbssb,2 , . . . , x tbssb,6 . Transport-block size information for the third transport block may be transmitted by means of the HS-SCCH physical channel using 6 bits, for example, x tbs3,1 , x tbs3,2 , . . . , x tbs3,6 . Hybrid-ARQ process information may be transmitted by means of the HS-SCCH physical channel using 4 bits, for example, x hap,1 , x hap,2 , . . . , x hap,4 . Redundancy and constellation version for the primary transport block may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rvpb,1 , x rvpb,2 . Redundancy and constellation version for the secondary transport block may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rvsb,1 , x rvsb,2 . Redundancy and constellation version for the third transport block may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rv3,1 , x rv3,2 . WTRU identity information may be transmitted by means of the HS-SCCH physical channel using 16 bits, for example, x wtru,1 , x wtru,2 , . . . , x wtru,16 . If four transport blocks are transmitted on the associated HS-PDSCHs, the following information is transmitted by means of the HS-SCCH physical channel: Channelization-code-set information may be transmitted by means of the HS-SCCH physical channel using 7 bits, for example, x ccs,1 , x ccs,2 , . . . , x ccs,7 . Modulation scheme and number of transport blocks information may be transmitted by means of the HS-SCCH physical channel using 3 bits, for example, x ms,1 , x ms,2 , x ms,3 . Rank information may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x ri,1 , x ri,2 . Transport-block size information for the primary transport block may be transmitted by means of the HS-SCCH physical channel using 6 bits, for example, x tbspb,1 , x tbspb,2 , . . . , x tbspb,6 . Transport-block size information for the secondary transport block may be transmitted by means of the HS-SCCH physical channel using 6 bits, for example, x tbssb,1 , x tbssb,2 , . . . , x tbssb,6 . Transport-block size information for the third transport block may be transmitted by means of the HS-SCCH physical channel using 6 bits, for example, x tbs3,1 , x tbs3,2 , . . . , x tbs3,6 . Hybrid-ARQ process information may be transmitted by means of the HS-SCCH physical channel using 4 bits, for example, x hap,1 , x hap,2 , . . . , x hap,4 . Redundancy and constellation version for the primary transport block may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rvpb,1 , x rvpb,2 . Redundancy and constellation version for the secondary transport block may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rvsb,1 , x rvsb,2 . Redundancy and constellation version for the third transport block may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rv3,1 , x rv3,2 . Redundancy and constellation version for the fourth transport block may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rv4,1 , x rv4,2 . WTRU identity information may be transmitted by means of the HS-SCCH physical channel using 16 bits, for example, x wtru,1 , x wtru,2 , . . . , x wtru,16 . Table 14 is an example mapping of bits X ri , X ms . TABLE 14ModulationModulationModulationModulationforforforforNumberprimarysecondarythirdfourthofxri, 1,transporttransporttransporttransporttransportxms, 1, xms, 2, xms, 3xri, 2blockblockblockblockblocks1111164 QAM64 QAM64 QAM64 QAM41101164 QAM64 QAM64 QAM16 QAM41011164 QAM64 QAM64 QAMQPSK41001164 QAM64 QAM16 QAM16 QAM40111164 QAM64 QAM16 QAMQPSK40011164 QAM64 QAMQPSKQPSK40001164 QAM16 QAM16 QAM16 QAM40101164 QAM16 QAM16 QAMQPSK40101064 QAM16 QAMQPSKQPSK41011064 QAMQPSKQPSKQPSK41111016 QAM16 QAM16 QAM16 QAM41101016 QAM16 QAM16 QAMQPSK40111016 QAM16 QAMQPSKQPSK40011016 QAMQPSKQPSKQPSK410010QPSKQPSKQPSKQPSK40001064 QAM64 QAM64 QAMn/a30100164 QAM64 QAM16 QAMn/a3101, xccs, 7 = 10164 QAM64 QAMQPSKn/a3101, xccs, 7 = 00164 QAM16 QAM16 QAMn/a31110164 QAM16 QAMQPSKn/a31100164 QAMQPSKQPSKn/a30110116 QAM16 QAM16 QAMn/a30010116 QAM16 QAMQPSKn/a31000116 QAMQPSKQPSKn/a300001QPSKQPSKQPSKn/a30100064 QAM64 QAMn/an/a20010064 QAM16 QAMn/an/a2101, xccs, 7 = 00064 QAMQPSKn/an/a21110016 QAM16 QAMn/an/a21100016 QAMQPSKn/an/a201100QPSKQPSKn/an/a2101, xccs,7 = 10064 QAMn/an/an/a11000016 QAMn/an/an/a100000QPSKn/an/an/a1 As there are 34 different combinations of modulation type and number of transport blocks illustrated in Table 14, the coding of 9 different combinations of modulation type and number of transport blocks defined in Release 7 MIMO may be used as the base line. The introduction of 2 additional bits xri, 1 and xri, 2 , may provide a total of 36 different combinations to signal the modulation type and number of transport blocks for 4-Tx MIMO. One example of using xms and xri to indicate WTRUs such control information is shown in Table 14. For rank 4 transmission, there may be no need to use x ccs,7 to indicate the modulation type. FIG. 21 is an example of a coding chain for HS-SCCH for a non-codebook based MIMO scheme with 4 transport blocks. FIG. 21 includes a first multiplexer 2105 and a second multiplexer 2110 . Channelization code set information, modulation scheme and transport block information, and rank information 2101 may be put into a first multiplexer 2105 . The output of the first multiplexer 2105 may then be channel coded using a first channel coding 2115 . The output of the first channel coding 2115 may then be rate matched using a first rate matching 2120 . The output of the first rate matching 2120 may then be masked with WTRU-specific masking 2125 . WTRU identity information may be included in the WTRU-specific masking 2125 . Transport-block size information for the primary transport block, transport-block size information for the secondary transport block, transport-block size information for the third transport block, transport-block size information for the fourth transport block, Hybrid-ARQ process information, redundancy and constellation version for the primary transport block, redundancy and constellation version for the secondary transport block, redundancy and constellation version for the third transport block, and redundancy and constellation version for the fourth transport block 2102 may be put into a second multiplexer 2110 . The output of the second multiplexer 2110 may receive WTRU-specific CRC attachment 2130 . WTRU identity information may be included the WTRU-specific CRC attachment 2130 . The output may then be channel coded using a second channel coding 2135 . The output of the second channel coding 2135 may then be rate matched using a second rate matching 2140 . The output of the second rate matching 2140 may then be combined with the WTRU-specific masking 2125 output for physical channel mapping 2145 . The indication of modulation type and number of transport blocks may use the method described in a non-codebook-based scheme, as shown in Table 14. However, the precoding weight information may need to be signaled in this case as well. If one transport block is transmitted on the associated HS-PDSCH(s) is transmitted, the following information may be transmitted by means of the HS-SCCH physical channel: Channelization-code-set information may be transmitted by means of the HS-SCCH physical channel using 7 bits, for example, x ccs,1 , x ccs,2 , . . . , x ccs,7 . Modulation scheme and number of transport blocks information may be transmitted by means of the HS-SCCH physical channel using 3 bits, for example, x ms,1 , x ms,2 , x ms,3 . Rank information may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x ri,1 , x ri,2 . The transmission of the rank information may be optional. Precoding weight information may be transmitted by means of the HS-SCCH physical channel using 4 bits, for example, x pwipb,1 , x pwipb,2 , x pwipb,3 , x pwipb,4 . Transport-block size information, may be transmitted by means of the HS-SCCH physical channel using 6 bits, for example, x tbspb,1 , x tbspb,2 , . . . , x tbspb,6 . Hybrid-ARQ process information may be transmitted by means of the HS-SCCH physical channel using 4 bits, for example, x hap,1 , x hap,2 , . . . , x hap,4 . Redundancy and constellation version may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rvpb,1 , x rvpb . WTRU identity information may be transmitted by means of the HS-SCCH physical channel using 16 bits, for example, x wtru,1 , x wtru,2 , . . . , x wtru,16 . If two transport blocks are transmitted on the associated HS-PDSCHs, the following information may be transmitted by means of the HS-SCCH physical channel: Channelization-code-set information may be transmitted by means of the HS-SCCH physical channel using 7 bits, for example, x ccs,1 , x ccs,2 , . . . , x ccs,7 . Modulation scheme and number of transport blocks information may be transmitted by means of the HS-SCCH physical channel using 3 bits, for example, x ms,1 , x ms,2 , x ms,3 . Rank information may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x ri,1 , x ri,2 . The transmission of the rank information may be optional. Precoding weight information may be transmitted by means of the HS-SCCH physical channel using 4 bits, for example, x pwipb,1 , x pwipb,2 , x pwipb,3 , x pwipb,4 . Transport-block size information, may be transmitted by means of the HS-SCCH physical channel using 6 bits, for example, x tbspb,1 , x tbspb,2 , . . . , x tbspb,6 . Hybrid-ARQ process information may be transmitted by means of the HS-SCCH physical channel using 4 bits, for example, x hap,1 , x hap,2 , . . . , x hap,4 . Redundancy and constellation version may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rvpb,1 , x rvpb . Redundancy and constellation version for the secondary transport block may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rvsb,1 , x rvsb,2 . WTRU identity information may be transmitted by means of the HS-SCCH physical channel using 16 bits, for example, x wtru,1 , x wtru,2 , . . . , x wtru,16 . If three transport blocks are transmitted on the associated HS-PDSCHs, the following information may be transmitted by means of the HS-SCCH physical channel: Channelization-code-set information may be transmitted by means of the HS-SCCH physical channel using 7 bits, for example, x ccs,1 , x ccs,2 , . . . , x ccs,7 . Modulation scheme and number of transport blocks information may be transmitted by means of the HS-SCCH physical channel using 3 bits, for example, x ms,1 , x ms,2 , x ms,3 . Rank information may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x ri,1 , x ri,2 . The transmission of the rank information may be optional. Precoding weight information may be transmitted by means of the HS-SCCH physical channel using 4 bits, for example, x pwipb,1 , x pwipb,2 , x pwipb,3 , x pwipb,4 . Transport-block size information, may be transmitted by means of the HS-SCCH physical channel using 6 bits, for example, x tbspb,1 , x tbspb,2 , . . . , x tbspb,6 . Hybrid-ARQ process information may be transmitted by means of the HS-SCCH physical channel using 4 bits, for example, x hap,1 , x hap,2 , . . . , x hap,4 . Redundancy and constellation version may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rvpb,1 , x rvpb . Redundancy and constellation version for the secondary transport block may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rvsb,1 , x rvsb,2 . Redundancy and constellation version for the third transport block may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rv3,1 , x rv3,2 . WTRU identity information may be transmitted by means of the HS-SCCH physical channel using 16 bits, for example, x wtru,1 , x wtru,2 , . . . , x wtru,16 . If four transport blocks are transmitted on the associated HS-PDSCHs, the following information may be transmitted by means of the HS-SCCH physical channel: Channelization-code-set information may be transmitted by means of the HS-SCCH physical channel using 7 bits, for example, x ccs,1 , x ccs,2 , . . . , x ccs,7 . Modulation scheme and number of transport blocks information may be transmitted by means of the HS-SCCH physical channel using 3 bits, for example, x ms,1 , x ms,2 , x ms,3 . Rank information may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x ri,1 , x ri,2 . The transmission of the rank information may be optional. Precoding weight information may be transmitted by means of the HS-SCCH physical channel using 4 bits, for example, x pwipb,1 , x pwipb,2 , x pwipb,3 , x pwipb,4 . Transport-block size information, may be transmitted by means of the HS-SCCH physical channel using 6 bits, for example, x tbspb,1 , x tbspb,2 , . . . , x tbspb,6 . Hybrid-ARQ process information may be transmitted by means of the HS-SCCH physical channel using 4 bits, for example, x hap,1 , x hap,2 , . . . , x hap,4 . Redundancy and constellation version may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rvpb,1 , x rvpb . Redundancy and constellation version for the secondary transport block may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rvsb,1 , x rvsb,2 . Redundancy and constellation version for the third transport block may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rv3,1 , x rv3,2 . Redundancy and constellation version for the fourth transport block may be transmitted by means of the HS-SCCH physical channel using 2 bits, for example, x rv4,1 , x rv4,2 . WTRU identity information may be transmitted by means of the HS-SCCH physical channel using 16 bits, for example, x wtru,1 , x wtru,2 , . . . , x wtru,16 . FIG. 22 is an example of a coding chain for HS-SCCH for a codebook based MIMO scheme with 4 transport blocks. FIG. 22 includes a first multiplexer 2205 and a second multiplexer 2210 . Channelization code set information, modulation scheme and transport block information, rank information, and precoding weight information 2201 may be put into a first multiplexer 2205 . The output of the first multiplexer 2205 may then be channel coded using a first channel coding 2215 . The output of the first channel coding 2215 may then be rate matched using a first rate matching 2220 . The output of the first rate matching 2220 may then be masked with WTRU-specific masking 2225 . WTRU identity information may be included in the WTRU-specific masking 2225 . Transport-block size information for the primary transport block, transport-block size information for the secondary transport block, transport-block size information for the third transport block, transport-block size information for the fourth transport block, Hybrid-ARQ process information, redundancy and constellation version for the primary transport block, redundancy and constellation version for the secondary transport block, redundancy and constellation version for the third transport block, and redundancy and constellation version for the fourth transport block 2202 may be put into a second multiplexer 2210 . The output of the second multiplexer 2210 may receive WTRU-specific CRC attachment 2230 . WTRU identity information may be included the WTRU-specific CRC attachment 2230 . The output may then be channel coded using a second channel coding 2235 . The output of the second channel coding 2235 may then be rate matched using a second rate matching 2240 . The output of the second rate matching 2240 may then be combined with the WTRU-specific masking 2225 output for physical channel mapping 2245 . The rank indication may also be carried out implicitly based on the precoding weight information. Therefore the rank indication field may be optional. The size of the fields is given as an example and the concepts put forth herein may also be extended to different number of bits for each field where applicable. FIG. 23 is an example of a method for determining pilot information for each data stream. FIG. 23 illustrates that a WTRU may receive a plurality of HS-SCCH resources including RRC configuration information for HSDPA, wherein the RRC configuration information includes dedicated pilot information associated with each received HS-SCCH resource 2305 . The WTRU may detect an H-RNTI associated with the WTRU in one of the plurality of HS-SCCH resources 2310 . The WTRU may then determine pilot information, based on the dedicated pilot information and the one of the plurality of HS-SCCH resources, for a high speed physically downlink shared channel (HS-PDSCH) associated with the one of the plurality of HS-SCCH resources 2315 . The dedicated pilot information may be either a channelization code or a base pilot resource index. With availability of various types of pilot channel defined, the WTRU may perform different measures to assist the network schedulers in making decision for the best transmission mode, or for the best transmission point to receiver data. These measurements may include combined CQIs from all transmission points. The measurements may be measured by the aid of the pilot channel commonly transmitted from all the involved transmission points, an individual CQI for each transmission point by exploring the cell-specific pilot channels, optimal cross-cell precoding weights, if any of the cross-site precoding schemes are configured, and rank indication information if multiflow aggregation or MU-MIMO transmission modes are to be configured. Although the WTRU may take any one or a combination of the above measurements simultaneously by monitoring the pilot channels, reporting the measurements to the network may result in overhead on the uplink feedback. Methods for reducing the overhead during WTRU reporting may include any one or combination of the methods described below. In a first embodiment, the WTRU compares the individual CQIs from each transmission point and feedback via layer 1 (L1) and determine the CQI that indicates which one cell the WTRU is associated with. In a second embodiment, the WTRU reports all of the measured CQIs via L1 signaling. The WTRU may report one type of CQI (e.g. the combined CQI) in full precision and the other types of CQI in a differential manner with less precision. In a third embodiment, the WTRU reports the CQI needed for a transmission mode using L1 signaling. The WTRU may transmit other measurements via a higher layer at a much slower update rate. In a forth embodiment, the WTRU reports signal quality at higher layer and semi-dynamically reconfigures which transmission point to use. The WTRU may report only the CQIs for that transmission point using L 1 signaling, at each configuration. Although features and elements are described above in particular combinations, one of ordinary skill in the art will appreciate that each feature or element can be used alone or in any combination with the other features and elements. In addition, the methods described herein may be implemented in a computer program, software, or firmware incorporated in a computer-readable medium for execution by a computer or processor. Examples of computer-readable media include electronic signals (transmitted over wired or wireless connections) and computer-readable storage media. Examples of computer-readable storage media include, but are not limited to, a read only memory (ROM), a random access memory (RAM), a register, cache memory, semiconductor memory devices, magnetic media such as internal hard disks and removable disks, magneto-optical media, and optical media such as CD-ROM disks, and digital versatile disks (DVDs). A processor in association with software may be used to implement a radio frequency transceiver for use in a WTRU, UE, terminal, base station, RNC, or any host computer.",en,PATENT_APPLICATION
067-664-097-398-898,US,20240384305,A1,2024-11-21,US_20240384305_A1_20241121,en,US,20240384305,A1,2024-11-21,US,18665417,2024-05-15,IDENTIFICATION AND VALIDATION OF GENOMIC SAFE-HARBOR SITES IN SKELETAL MUSCLE FOR TARGETED INTEGRATION,en,US,BOARD OF TRUSTEES OF THE UNIVERSITY OF ARKANSAS,"Little Rock, AR",US,Made Harumi Padmaswari,"Fayetteville, AR",US,1,Christopher Nelson,"Springdale, AR",C12N15/90,I,F,C12N9/22,I,L,C12N15/11,I,L,C12N15/907,I,F,C12N9/22,I,L,C12N15/11,I,L,C12N2310/20,A,L,US,20240384305,A1,2024-11-21,067-664-097-398-898,1,US,20240384305,A1,2024-11-21,067-664-097-398-898,1,UNKNOWN,"Embodiments of the present disclosure pertain to a method of expressing a nucleic acid sequence in a cell by inserting the nucleic acid sequence at a region of a creatine kinase (CKM) and/or a myoglobin (MB) gene such that the inserted nucleic acid sequence becomes expressed in the cell by a promoter of the gene. The nucleic acid sequences may be expressed in vitro or in vivo for various purposes, such as treatment or prevention of a disease and/or tissue repair or regeneration. Additional embodiments pertain to a system for expressing a nucleic acid sequence in a cell after insertion at a region of a creatine kinase (CKM) gene and/or a myoglobin (MB) gene. Further embodiments of the present disclosure pertain to a modified cell that includes an inserted nucleic acid sequence at a region of a creatine kinase (CKM) gene and/or a myoglobin (MB) gene.",en,"1 . A method of expressing a nucleic acid sequence in a cell, said method comprising: inserting the nucleic acid sequence at a region of a gene, wherein the gene comprises a creatine kinase (CKM) gene, a myoglobin (MB) gene, or combinations thereof, and wherein the inserted nucleic acid sequence becomes expressed in the cell by a promoter of the gene.","2 . The method of claim 1 , wherein the insertion occurs by a method selected from the group consisting of site-specific nucleic acid integration, site-specific recombination, RNA-guided nucleic acid integration, insertion by a gene editing system, insertion by a clustered regularly interspaced short palindromic repeats (CRISPR)/nuclease system (CRISPR system), insertion by a non-DSB (double strand break) CRISPR system with integrase or transposase combination, homology-independent targeted integration (HITI), or combinations thereof.","3 . The method of claim 1 , wherein the insertion of the nucleic acid sequence comprises: introducing the nucleic acid sequence and a clustered regularly interspaced short palindromic repeats (CRISPR)/nuclease system (CRISPR system) to the cell, wherein the CRISPR system comprises a guide RNA that directs the nuclease to the region of the gene, wherein the nuclease cuts at the region of the gene, and wherein the nucleic acid sequence is inserted at the cut region of the gene for expression.","4 . The method of claim 1 , wherein the nucleic acid sequence comprises a gene.","5 . The method of claim 1 , wherein the nucleic acid sequence lacks a promoter.","6 . The method of claim 1 , wherein the gene comprises myoglobin (MB), and wherein the region comprises a region operable to be expressed by the promoter of MB.","7 . The method of claim 1 , wherein the region is located at an intron of MB.","8 . The method of claim 7 , wherein the region comprises a sequence selected from the group consisting of GCCAGACTAGCATCTGGGA (SEQ ID NO: 1), TCCTCTTTAGAAGCCACCA (SEQ ID NO: 2), GGAAGGTATAAAAGCCCTTC (SEQ ID NO: 3), TCTTCTTCAGACTGCGCCA (SEQ ID NO: 4), AGAGCAAGTATGGGCTCACT (SEQ ID NO: 5), TCTTCTTCAGACTGCGCCAT (SEQ ID NO: 6), CTTCTTCAGACTGCGCCATG (SEQ ID NO: 7), GGTCTTCTTCAGACTGCGCCA (SEQ ID NO: 8), AAGGTATAAAAACGCCCTT (SEQ ID NO: 9), a sequence with at least 65% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 70% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 75% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 80% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 85% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 90% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 95% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 99% sequence identity with any one of SEQ ID NOS: 1-9, or combinations thereof.","9 . The method of claim 1 , wherein the gene comprises creatine kinase (CKM), and wherein the region comprises a region operable to be expressed by the promoter of CKM.","10 . The method of claim 9 , wherein the region is located at an intron of CKM.","11 . The method of claim 10 , wherein the region comprises a sequence selected from the group consisting of AGTCCCCAGCAGGATCACAT (SEQ ID NO: 10), TCTGCTCGCAGGGTCCCAA (SEQ ID NO: 11), GCAAGGCTGAGGTTCACAGG (SEQ ID NO: 12), GTGTTACCGAATGGCATGG (SEQ ID NO: 13), TTACCGAATGGCATGGTGG (SEQ ID NO: 14), GGGAAGATTGAAGATTAGC (SEQ ID NO: 15), GGTGTTACCGAATGGCATGG (SEQ ID NO: 16), TACACCGCCACCATGCCATT (SEQ ID NO: 17), GCGGTGTAGGAGACCTGAT (SEQ ID NO: 18), a sequence with at least 65% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 70% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 75% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 80% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 85% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 90% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 95% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 99% sequence identity with any one of SEQ ID NOS: 10-18, or combinations thereof.","12 . The method of claim 1 , wherein the cell comprises a human cell.","13 . The method of claim 1 , wherein the cell comprises a skeletal muscle cell.","14 . The method of claim 1 , wherein the insertion occurs in vitro.","15 . The method of claim 1 , wherein the insertion occurs in vivo in a subject.","16 . The method of claim 15 , wherein the expressed nucleic acid sequence is utilized to treat or prevent a disease in the subject.","17 . The method of claim 15 , wherein the expressed nucleic acid system is utilized for tissue repair or regeneration in the subject.","18 . The method of claim 15 , wherein the subject is a human being.","19 . A system for expressing a nucleic acid sequence in a cell, said system comprising: a nucleic acid sequence operational for insertion at a region of a gene, wherein the gene comprises a creatine kinase (CKM) gene, a myoglobin (MB) gene, or combinations thereof, and wherein the inserted nucleic acid sequence is operational to become expressed in the cell by a promoter of the gene.","20 . The system of claim 19 , wherein the system comprises a clustered regularly interspaced short palindromic repeats (CRISPR)/nuclease system (CRISPR system), wherein the CRISPR system comprises a guide RNA that directs the nuclease to the region of the gene, wherein the nuclease cuts at the region of the gene, and wherein the nucleic acid sequence is inserted at the cut region of the gene for expression.","21 . The system of claim 20 , wherein the guide RNA directs the nuclease to a region of CKM.","22 . The system of claim 21 , wherein the guide RNA comprises a sequence that directs the nuclease to a CKM region selected from the group consisting of AGTCCCCAGCAGGATCACAT (SEQ ID NO: 10), TCTGCTCGCAGGGTCCCAA (SEQ ID NO: 11), GCAAGGCTGAGGTTCACAGG (SEQ ID NO: 12), GTGTTACCGAATGGCATGG (SEQ ID NO: 13), TTACCGAATGGCATGGTGG (SEQ ID NO: 14), GGGAAGATTGAAGATTAGC (SEQ ID NO: 15), GGTGTTACCGAATGGCATGG (SEQ ID NO: 16), TACACCGCCACCATGCCATT (SEQ ID NO: 17), GCGGTGTAGGAGACCTGAT (SEQ ID NO: 18), a sequence with at least 65% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 70% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 75% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 80% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 85% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 90% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 95% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 99% sequence identity with any one of SEQ ID NOS: 10-18, or combinations thereof.","23 . The system of claim 20 , wherein the guide RNA directs the nuclease to a region of MB.","24 . The system of claim 23 , wherein the guide RNA comprises a sequence that directs the nuclease to an MB region selected from the group consisting of GCCAGACTAGCATCTGGGA (SEQ ID NO: 1), TCCTCTTTAGAAGCCACCA (SEQ ID NO: 2), GGAAGGTATAAAAGCCCTTC (SEQ ID NO: 3), TCTTCTTCAGACTGCGCCA (SEQ ID NO: 4), AGAGCAAGTATGGGCTCACT (SEQ ID NO: 5), TCTTCTTCAGACTGCGCCAT (SEQ ID NO: 6), CTTCTTCAGACTGCGCCATG (SEQ ID NO: 7), GGTCTTCTTCAGACTGCGCCA (SEQ ID NO: 8), AAGGTATAAAAACGCCCTT (SEQ ID NO: 9), a sequence with at least 65% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 70% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 75% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 80% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 85% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 90% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 95% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 99% sequence identity with any one of SEQ ID NOS: 1-9, or combinations thereof.","25 . The system of claim 19 , wherein the nucleic acid sequence comprises a gene.","26 . The system of claim 19 , wherein the nucleic acid sequence lacks a promoter.","27 . The system of claim 19 , wherein the gene comprises myoglobin (MB), and wherein the region comprises a region operable to be expressed by the promoter of MB.","28 . The system of claim 27 , wherein the region is located at an intron of MB.","29 . The system of claim 28 , wherein the region comprises a sequence selected from the group consisting of GCCAGACTAGCATCTGGGA (SEQ ID NO: 1), TCCTCTTTAGAAGCCACCA (SEQ ID NO: 2), GGAAGGTATAAAAGCCCTTC (SEQ ID NO: 3), TCTTCTTCAGACTGCGCCA (SEQ ID NO: 4), AGAGCAAGTATGGGCTCACT (SEQ ID NO: 5), TCTTCTTCAGACTGCGCCAT (SEQ ID NO: 6), CTTCTTCAGACTGCGCCATG (SEQ ID NO: 7), GGTCTTCTTCAGACTGCGCCA (SEQ ID NO: 8), AAGGTATAAAAACGCCCTT (SEQ ID NO: 9), a sequence with at least 65% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 70% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 75% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 80% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 85% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 90% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 95% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 99% sequence identity with any one of SEQ ID NOS: 1-9, or combinations thereof.","30 . The system of claim 19 , wherein the gene comprises creatine kinase (CKM), and wherein the region comprises a region operable to be expressed by the promoter of CKM.","31 . The system of claim 30 , wherein the region is located at an intron of CKM.","32 . The system of claim 31 , wherein the CKM region comprises a sequence selected from the group consisting of AGTCCCCAGCAGGATCACAT (SEQ ID NO: 10), TCTGCTCGCAGGGTCCCAA (SEQ ID NO: 11 ), GCAAGGCTGAGGTTCACAGG (SEQ ID NO: 12), GTGTTACCGAATGGCATGG (SEQ ID NO: 13), TTACCGAATGGCATGGTGG (SEQ ID NO: 14), GGGAAGATTGAAGATTAGC (SEQ ID NO: 15), GGTGTTACCGAATGGCATGG (SEQ ID NO: 16), TACACCGCCACCATGCCATT (SEQ ID NO: 17), GCGGTGTAGGAGACCTGAT (SEQ ID NO: 18), a sequence with at least 65% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 70% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 75% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 80% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 85% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 90% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 95% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 99% sequence identity with any one of SEQ ID NOS: 10-18, or combinations thereof.","33 . A modified cell comprising an inserted nucleic acid sequence at a region of a gene, wherein the gene comprises a creatine kinase (CKM) gene, a myoglobin (MB) gene, or combinations thereof, and wherein the inserted nucleic acid sequence becomes expressed in the cell by a promoter of the gene.","34 . The modified cell of claim 33 , wherein the nucleic acid sequence comprises a gene.","35 . The modified cell of claim 33 , wherein the nucleic acid sequence lacks a promoter.","36 . The modified cell of claim 33 , wherein the gene comprises myoglobin (MB), and wherein the region comprises a region operable to be expressed by the promoter of MB.","37 . The modified cell of claim 36 , wherein the region is located at an intron of MB.","38 . The modified cell of claim 37 , wherein the region comprises a sequence selected from the group consisting of GCCAGACTAGCATCTGGGA (SEQ ID NO: 1), TCCTCTTTAGAAGCCACCA (SEQ ID NO: 2), GGAAGGTATAAAAGCCCTTC (SEQ ID NO: 3 ), TCTTCTTCAGACTGCGCCA (SEQ ID NO: 4 ), AGAGCAAGTATGGGCTCACT (SEQ ID NO: 5), TCTTCTTCAGACTGCGCCAT (SEQ ID NO: 6), CTTCTTCAGACTGCGCCATG (SEQ ID NO: 7 ), GGTCTTCTTCAGACTGCGCCA (SEQ ID NO: 8), AAGGTATAAAAACGCCCTT (SEQ ID NO: 9), a sequence with at least 65% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 70% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 75% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 80% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 85% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 90% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 95% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 99% sequence identity with any one of SEQ ID NOS: 1-9, or combinations thereof.","39 . The modified cell of claim 33 , wherein the gene comprises creatine kinase (CKM), and wherein the region comprises a region operable to be expressed by the promoter of CKM.","40 . The modified cell of claim 39 , wherein the region is located at an intron of CKM.","41 . The modified cell of claim 40 , wherein the CKM region comprises a sequence selected from the group consisting of AGTCCCCAGCAGGATCACAT (SEQ ID NO: 10), TCTGCTCGCAGGGTCCCAA (SEQ ID NO: 11), GCAAGGCTGAGGTTCACAGG (SEQ ID NO: 12), GTGTTACCGAATGGCATGG (SEQ ID NO: 13), TTACCGAATGGCATGGTGG (SEQ ID NO: 14), GGGAAGATTGAAGATTAGC (SEQ ID NO: 15), GGTGTTACCGAATGGCATGG (SEQ ID NO: 16), TACACCGCCACCATGCCATT (SEQ ID NO: 17), GCGGTGTAGGAGACCTGAT (SEQ ID NO: 18), a sequence with at least 65% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 70% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 75% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 80% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 85% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 90% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 95% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 99% sequence identity with any one of SEQ ID NOS: 10-18, or combinations thereof.","42 . The modified cell of claim 33 , wherein the cell comprises a human cell.",en,"CROSS-REFERENCE TO RELATED APPLICATIONS This application claims priority to U.S. Provisional Patent Application No. 63/466,566, filed on May 15, 2023. The entirety of the aforementioned application is incorporated herein by reference. STATEMENT REGARDING FEDERALLY SPONSORED RESEARCH This invention was made with government support under R00 EB023979 awarded by the National Institutes of Health. The government has certain rights in the invention. SEQUENCE DISCLOSURE STATEMENT Pursuant to 37 C.F.R. § 1.834, Applicant has submitted a sequence listing in XML format (“Sequence Listing”). The name of the file containing the Sequence Listing is “AF42186.P031US”. The date of the creation of the Sequence Listing is Jul. 24, 2024. The size of the Sequence Listing is 17,000 bytes. Applicant hereby incorporates by reference the material in the Sequence Listing. BACKGROUND Current methods and systems for expressing nucleic acids in cells have numerous limitations. Embodiments of the present disclosure aim to address these limitations. SUMMARY In some embodiments, the present disclosure pertains to a method of expressing a nucleic acid sequence in a cell. In some embodiments, the methods of the present disclosure include a step of inserting the nucleic acid sequence at a region of a creatine kinase (CKM) gene and/or a myoglobin (MB) gene. Thereafter, the inserted nucleic acid sequence may become expressed in the cell by a promoter of the gene. In some embodiments, the nucleic acid sequences of the present disclosure may be expressed in vitro. In some embodiments, the nucleic acid sequences of the present disclosure may be expressed in vivo for various purposes, such as treatment or prevention of a disease and/or tissue repair or regeneration. Additional embodiments of the present disclosure pertain to a system for expressing a nucleic acid sequence in a cell. In some embodiments, the systems of the present disclosure include a nucleic acid sequence operational for insertion at a region of a creatine kinase (CKM) gene and/or a myoglobin (MB) gene. In some embodiments, the inserted nucleic acid sequence is operational to become expressed in the cell by a promoter of the gene. Further embodiments of the present disclosure pertain to a modified cell that includes an inserted nucleic acid sequence at a region of a creatine kinase (CKM) gene and/or a myoglobin (MB) gene. In some embodiments, the inserted nucleic acid sequence becomes expressed in the cell by a promoter of the gene. DRAWINGS FIG. 1 illustrates a method of expressing a nucleic acid in a cell in accordance with various embodiments of the present disclosure. FIG. 2 provides a schematic representation of ideal safe-harbor sites. The bar in the middle represents the optimal location of safe-harbor sites. The site should preferably be unique and located in open chromatin. Within 300 kb, the site should preferably be free from any cancer-related genes, miRNAs, or other functional small RNAs. Within 50 kb, the site should preferably be free from the 5′ end of a gene, replication origin, and ultra-conserved elements. Additionally, the site should preferably be in a region of low transcriptional activity or have no mRNA within 25 kb. FIGS. 3A-3F illustrate targeted integration of a promoter-less green fluorescent protein (GFP) at myoglobin (Mb) and creatine kinase (CKM), which leads to GFP expression in myotubes. FIG. 3A (top portion) provides an overview of transgene integration to hijack endogenous Ckm or Mb promoters. The homology-independent targeted integration (HITI)-based gene editing system is delivered via plasmid transfection, which encodes the CMV-driven S. pyogenes Cas9 protein, U6 promoter-gRNA expression cassette, and a donor plasmid containing a transgene fragment flanked by Cas9 target sites in the reverse orientation relative to genomic DNA. FIG. 3B (bottom portion) shows the schematic mechanism at the molecular level, which shows that, by integrating the transgene in the intronic region, only correct integration is expected to produce the protein, while partial or reverse integration is expected to not affect endogenous genes. The figure is not drawn to scale. FIG. 3B shows the gRNA target site map in Ckm and Mb locus. Five gRNA target sites were selected within 250 bp in the splice acceptor and 5′ UTR region of the gene. FIG. 3C shows the quantification of gRNA efficiency in C2C12 cell lines with short-read next-generation sequencing (NGS) based on indel percentage. The error bars were computed as the standard deviation obtained through bootstrap resampling technique. FIG. 3D shows validation of correct GFP integration in genomic DNA, which is shown in both 5′ and 3′ integration regions in the treated group. The schematic figure shows the forward primer in the GFP transgene overlaps with the reverse primer (Primer list). FIG. 3E shows validation of correct mRNA splicing from GFP integration by cDNA PCR, which is shown in both loci. At the transcriptome level, only 5′ integration was assessed. FIG. 3F shows fluorescent microscopy images assessing GFP expression in edited myotubes at 10 days after editing. The scale bar is 50 μm. FIGS. 4A-4E shows that targeted integration of a promoter-less hF9 gene leads to sustained expression. FIG. 4A shows a schematic of the structure of HITI insert plasmid for human F9 expression. Full-length cDNA region from exon 1 to exon 8 is cloned to replace GFP transgene in the previous construct. FIGS. 4B and 4C show genotyping of the F9 integration in C2C12 cell line using primers spanning the junction between the integration site and the transgene in genomic DNA and cDNA. FIG. 4D shows relative hF9 expression following plasmid transfection. Five biological independent samples, mean±s.e.m. All samples were processed at 10 days post-transfection including 7 days of myotubes differentiation. FIG. 4E shows time course cell culture to assess the expression of hF9 in treated C2C12 for 30 days without selection compared to the scrambled group. Two independent biological replicates, means±sem. FIGS. 5A-5F show deep sequencing results, which reveal precise and imprecise outcomes of targeted integration. FIGS. 5A-5B show deep sequencing results, which show a varied range of precise integration in 5′ and 3′ junctions. A similar pattern was observed in both 5′ and 3′ 0 junctions in both loci, vector chewback that predominantly happened on the insert side. FIG. 5C shows deep sequencing at the mRNA level, which shows a higher percentage of precise integration in three biological replicates in the treated Ckm locus. FIG. 5D shows modification frequency trace from the fusion of Ckm and GFP transgene. FIG. 5E (top) shows a schematic Tn5 tagmentation-based sequencing to quantify integration efficiency. FIG. 5E (bottom) shows the graph of a percentage of reads aligned to the Ckm gene and reference amplicon. FIG. 5F shows characterization of the fusion of Ckm and hF9 cDNA using 5′RACE with GSP reverse transcriptase primer. FIGS. 6A-6D show RNA sequencing results, which reveal significant increases in transgene expression and relative precision of integration at Ckm over Mb. FIG. 6A shows a pipeline for bulk RNA-seq experiment on Ckm- and Mb-integrated and scrambled non-integrated C2C12 myotube cells. FIG. 6B shows principal component analysis of two biological replicates of C2C12 myotubes with Ckm, Mb, or scrambled treatment. FIG. 6C shows hF9 transcript expression level measured in TPM (mean±sem). FIG. 6D shows differential expression of genes following hF9 integration in Ckm and Mb loci. DETAILED DESCRIPTION It is to be understood that both the foregoing general description and the following detailed description are illustrative and explanatory, and are not restrictive of the subject matter, as claimed. In this application, the use of the singular includes the plural, the word “a” or “an” means “at least one”, and the use of “or” means “and/or”, unless specifically stated otherwise. Furthermore, the use of the term “including”, as well as other forms, such as “includes” and “included”, is not limiting. Also, terms such as “element” or “component” encompass both elements or components comprising one unit and elements or components that include more than one unit unless specifically stated otherwise. The section headings used herein are for organizational purposes and are not to be construed as limiting the subject matter described. All documents, or portions of documents, cited in this application, including, but not limited to, patents, patent applications, articles, books, and treatises, are hereby expressly incorporated herein by reference in their entirety for any purpose. In the event that one or more of the incorporated literature and similar materials defines a term in a manner that contradicts the definition of that term in this application, this application controls. The current wave of therapies transforming genetic medicines rely on gene replacement. Efficient transgene expression is predominantly achieved through the use of adeno-associated viral (AAV) vectors to express a therapeutic gene under an exogenous promoter. While multiple studies have reported the sustainability of AAV gene therapy for years after a single administration, others have noted a decline in transgene expression, with some even observing a return to baseline levels. With the majority of transgene expression sustained episomally, research indicates that epigenetic silencing of episomal DNA and interactions between the AAV capsid and host factors significantly contribute to the reduction in expression. In line with these findings, recent studies in dogs and non-human primates revealed initially high but short-lived expression from episomal genomes, while long-term expression of the transgene was attributed to clonal expansion of cells containing integrated vectors. However, transgene integration in AAV gene replacement happens in a semi-random pattern throughout the genome. Vector integration in unpredictable positions in the genome might result in unforeseeable expression and interactions with the host genome, though no adverse events related to vector integration have been detected in humans. Thus, these limitations highlight the need to transition from random integration with viral vectors to targeted, site-specific methods such as clustered regularly interspaced short palindromic repeats (CRISPR)-Cas technology, which can overcome the problem of insertion of correct genes into random genomic sites. CRISPR-based integration methods have evolved significantly, from using the endogenous repair pathways, non-homologous end joining (NHEJ) or homology-directed repair (HDR), to engineered transposable elements. These gene editing technologies emerged as a potential avenue for long-term therapeutic transgene insertion. Despite considerable advancements in targeted gene integration methods, identifying the locus to insert replacement genes for optimal safety and efficiency remains challenging. While many gene editing approaches to target the disease locus itself, some studies indicate that the genomic structure of mutated genes may not favor entire gene replacement. Further, some exogenous promoters, such as cytomegalovirus (CMV) and human elongation factor-1 alpha (EF1alpha), are susceptible to promoter silencing. Therefore, exploring loci with high transcriptional activity emerges as an alternative. The current strategy of “hijacking” the endogenous promoter of high transcriptional activity genes or transgene overexpression remains largely organ-specific. While the results are promising, the safety and versatility of this approach have yet to undergo thorough investigation. Ensuring the safety and versatility of these sites is desirable for achieving stable expression of integrated transgenes without adversely affecting the host cell. Empirical studies have identified safe-harbor sites that support long-term transgene expression. There are three well-established sites: AAVS1, CCR5, and hROSA26, and recently explored sites such as Rogil, Rogi2, or SHS231. However, the expression from these sites is limited or tissue-specific and requires the inserted vector to include an exogenous promoter. As such, a need exists for more effective methods and systems for expressing nucleic acids in cells. Numerous embodiments of the present disclosure aim to address the aforementioned need. In some embodiments, the present disclosure pertains to a method of expressing a nucleic acid sequence in a cell. In some embodiments illustrated in FIG. 1 , the methods of the present disclosure include inserting the nucleic acid sequence at a region of a creatine kinase (CKM) gene and/or a myoglobin (MB) gene (step 10). Thereafter, the inserted nucleic acid becomes expressed in the cell (step 12). For instance, in some embodiments, the inserted nucleic acid sequence may become expressed in the cell by a promoter of a gene. In some embodiments, the nucleic acid sequences of the present disclosure may be expressed in vitro (step 14). In some embodiments, the nucleic acid sequences of the present disclosure may be expressed in vivo (step 16) for various purposes, such as treatment or prevention of a disease (step 18) and/or tissue repair or regeneration (step 20). Additional embodiments of the present disclosure pertain to a system for expressing a nucleic acid sequence in a cell. In some embodiments, the systems of the present disclosure include a nucleic acid sequence operational for insertion at a region of a creatine kinase (CKM) gene and/or a myoglobin (MB) gene. In some embodiments, the inserted nucleic acid sequence is operational to become expressed in the cell by a promoter of the gene. Further embodiments of the present disclosure pertain to a modified cell that includes an inserted nucleic acid sequence at a region of a creatine kinase (CKM) gene and/or a myoglobin (MB) gene. In some embodiments, the inserted nucleic acid sequence becomes expressed in the cell by a promoter of the gene. For the purposes of this application, CKM and MB may refer to genes from different species, such humans, mice, and other species. As set forth in more detail herein, the methods, systems and modified cells of the present disclosure can have numerous embodiments. Additionally, the methods, systems and modified cells of the present disclosure can have numerous applications. Genes The nucleic acid sequences of the present disclosure may be inserted into regions of various genes. For instance, in some embodiments, the gene is an MB gene. In some embodiments, the gene is a human version of the MB gene. In some embodiments, the gene is a non-human version of the MB gene. In some embodiments, the gene is a murine version of the MB gene. In some embodiments, the gene is a CKM gene. In some embodiments, the gene is a human version of the CKM gene. In some embodiments, the gene is a non-human version of the CKM gene. In some embodiments, the gene is a murine version of the CKM gene. The nucleic acid sequences of the present disclosure may be inserted into various regions of CKM and/or MB genes. For instance, in some embodiments, the region is positioned at an intron region of the gene, an exon region of the gene, inside of the gene, an untranslated region (UTR) of the gene, a 5′ untranslated region (UTR) of the gene, a 3′ untranslated region (UTR) of the gene, an intergenic region of the gene, a genomic safe harbor site of the gene, a promoter of the gene, a region operable to be expressed by the promoter of the gene, or combinations thereof. In some embodiments, the region is positioned at an intron region of the gene. In some embodiments, the nucleic acid sequences of the present disclosure may be inserted into a region of MB. In some embodiments, the region includes a region operable to be expressed by the promoter of MB. In some embodiments, the region is located at an intron of MB. In some embodiments, the region is located at intron 1 of MB. In some embodiments, the MB region includes a sequence that includes, without limitation, GCCAGACTAGCATCTGGGA (SEQ ID NO: 1), TCCTCTTTAGAAGCCACCA (SEQ ID NO: 2), GGAAGGTATAAAAGCCCTTC (SEQ ID NO: 3), TCTTCTTCAGACTGCGCCA (SEQ ID NO: 4), AGAGCAAGTATGGGCTCACT (SEQ ID NO: 5), TCTTCTTCAGACTGCGCCAT (SEQ ID NO: 6), CTTCTTCAGACTGCGCCATG (SEQ ID NO: 7), GGTCTTCTTCAGACTGCGCCA (SEQ ID NO: 8), AAGGTATAAAAACGCCCTT (SEQ ID NO: 9), a sequence with at least 65% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 70% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 75% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 80% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 85% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 90% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 95% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 99% sequence identity with any one of SEQ ID NOS: 1-9, or combinations thereof. In some embodiments, the nucleic acid sequences of the present disclosure may be inserted into a region of CKM. In some embodiments, the region includes a region operable to be expressed by the promoter of CKM. In some embodiments, the region is located at an intron of CKM. In some embodiments, the region is located at intron 1 of CKM. In some embodiments, the CKM region includes a sequence that includes, without limitation, AGTCCCCAGCAGGATCACAT (SEQ ID NO: 10), TCTGCTCGCAGGGTCCCAA (SEQ ID NO: 11), GCAAGGCTGAGGTTCACAGG (SEQ ID NO: 12), GTGTTACCGAATGGCATGG (SEQ ID NO: 13), TTACCGAATGGCATGGTGG (SEQ ID NO: 14), GGGAAGATTGAAGATTAGC (SEQ ID NO: 15), GGTGTTACCGAATGGCATGG (SEQ ID NO: 16), TACACCGCCACCATGCCATT (SEQ ID NO: 17), GCGGTGTAGGAGACCTGAT (SEQ ID NO: 18), a sequence with at least 65% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 70% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 75% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 80% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 85% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 90% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 95% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 99% sequence identity with any one of SEQ ID NOS: 10-18, or combinations thereof. Insertion of Nucleic Acid Sequences The present disclosure may utilize various methods to insert a nucleic acid sequence at a region of a gene. Additionally, the nucleic acid sequences of the present disclosure may be operational for insertion at a region of a gene through various methods. For instance, in some embodiments, nucleic acid sequence insertion occurs by a method that includes, without limitation, site-specific nucleic acid integration, site-specific recombination, RNA-guided nucleic acid integration, insertion by a gene editing system, insertion by a clustered regularly interspaced short palindromic repeats (CRISPR)/nuclease system (CRISPR system), insertion by non-DSB (double strand break) CRISPR system with integrase or transposase combination, homology-independent targeted integration (HITI), or combinations thereof. In some embodiments, the insertion of a nucleic acid sequence at a region of a gene occurs through the utilization of a clustered regularly interspaced short palindromic repeats (CRISPR)/nuclease system (CRISPR system). In some embodiments, the systems of the present disclosure include a CRISPR system. In some embodiments, the insertion of the nucleic acid sequence includes introducing the nucleic acid sequence and the CRISPR system to the cell, where the CRISPR system includes a guide RNA that directs the nuclease of the CRISPR system to a region of a gene, where the nuclease cuts at the region of the gene, and where the nucleic acid sequence is inserted at the cut region of the gene for expression. The CRISPR systems of the present disclosure may include various nucleases. For instance, in some embodiments, the nuclease of the CRISPR system includes, without limitation, Cas nucleases, class 2 of Cas nucleases, Cas 9, Cas Φ, CasΦ2, Cas12a, non-CRISPR enzymes, meganucleases, zinc finger nucleases, talens, or combinations thereof. In some embodiments, the nuclease includes Cas 9. The CRISPR systems of the present disclosure may also utilize various guide RNAs. For instance, in some embodiments, the guide RNAs of the CRISPR system direct the nuclease to a region of CKM. In some of such embodiments, such guide RNAs direct the nuclease to a CKM region that includes, without limitation, AGTCCCCAGCAGGATCACAT (SEQ ID NO: 10), TCTGCTCGCAGGGTCCCAA (SEQ ID NO: 11), GCAAGGCTGAGGTTCACAGG (SEQ ID NO: 12), GTGTTACCGAATGGCATGG (SEQ ID NO: 13), TTACCGAATGGCATGGTGG (SEQ ID NO: 14), GGGAAGATTGAAGATTAGC (SEQ ID NO: 15), GGTGTTACCGAATGGCATGG (SEQ ID NO: 16), TACACCGCCACCATGCCATT (SEQ ID NO: 17), GCGGTGTAGGAGACCTGAT (SEQ ID NO: 18), a sequence with at least 65% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 70% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 75% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 80% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 85% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 90% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 95% sequence identity with any one of SEQ ID NOS: 10-18, a sequence with at least 99% sequence identity with any one of SEQ ID NOS: 10-18, or combinations thereof. In some embodiments, the guide RNAs of the CRISPR system direct the nuclease to a region of MB. In some of such embodiments, such guide RNAs direct the nuclease to an MB region that includes, without limitation, GCCAGACTAGCATCTGGGA (SEQ ID NO: 1), TCCTCTTTAGAAGCCACCA (SEQ ID NO: 2), GGAAGGTATAAAAGCCCTTC (SEQ ID NO: 3), TCTTCTTCAGACTGCGCCA (SEQ ID NO: 4), AGAGCAAGTATGGGCTCACT (SEQ ID NO: 5), TCTTCTTCAGACTGCGCCAT (SEQ ID NO: 6), CTTCTTCAGACTGCGCCATG (SEQ ID NO: 7), GGTCTTCTTCAGACTGCGCCA (SEQ ID NO: 8), AAGGTATAAAAACGCCCTT (SEQ ID NO: 9), a sequence with at least 65% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 70% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 75% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 80% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 85% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 90% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 95% sequence identity with any one of SEQ ID NOS: 1-9, a sequence with at least 99% sequence identity with any one of SEQ ID NOS: 1-9, or combinations thereof. Nucleic Acid Sequences The methods of the present disclosure may insert various nucleic acid sequences at a region of a gene. Additionally, the systems of the present disclosure may include various nucleic acid sequences for insertion at a region of a gene. Moreover, the modified cells of the present disclosure may include various inserted nucleic acid sequences. For instance, in some embodiments, the nucleic acid sequence includes a gene. In some embodiments, the nucleic acid sequence expresses a therapeutic protein. In some embodiments, the therapeutic protein includes human factor 9 (hF9), human factor 8 (hF8), dystrophin, microdystrophin (DMD), and other replacements for circulating factors and neuromuscular disease. The nucleic acid sequences of the present disclosure may be in various forms. For instance, in some embodiments, the nucleic acid sequence lacks a promoter. In some embodiments, the nucleic acid sequence is inserted in the form of a double stranded DNA. In some embodiments, the nucleic acid sequence is inserted in the form of a single stranded DNA. In some embodiments, the nucleic acid sequence is inserted as part of a plasmid. Expression of Nucleic Acid Sequences in Cells The nucleic acid sequences of the present disclosure may be expressed in various cells by the methods and systems of the present disclosure. Additionally, the modified cells of the present disclosure can include various types of cells. For instance, in some embodiments, the cells include, without limitation, a skeletal muscle cell, a human cell, a mouse cell, a diseased cell, or combinations thereof. In some embodiments, the cell includes a human cell. In some embodiments, the cell includes a skeletal muscle cell. In some embodiments, the skeletal muscle cell includes a myoblast. In some embodiments, the skeletal muscle cell includes a myotube. The inserted nucleic acid sequences of the present disclosure may be expressed in cells in various manners. For instance, in some embodiments, the insertion of a nucleic acid sequence at a region of a gene results in stable expression of the nucleic acid sequence in a cell through an endogenous promoter of the gene. In some embodiments, the insertion of a nucleic acid sequence at a region of a gene results in stable expression of the nucleic acid sequence in a cell through an endogenous promoter of CKM. In some embodiments, the insertion of a nucleic acid sequence at a region of a gene results in stable expression of the nucleic acid sequence in a cell through an endogenous promoter of MB. Modes and Applications of Nucleic Acid Sequence Expression The nucleic acid sequences of the present disclosure may be inserted into gene regions in various modes and for various applications. For instance, in some embodiments, nucleic acid sequence insertion occurs in vitro. In some embodiments, the in vitro insertion occurs by a method that includes, without limitation, transfection, transduction, viral mediated transduction, lipid nanoparticle transfection, or combinations thereof. The in vitro insertion of nucleic acid sequences into cells can have numerous applications. For instance, in some embodiments, the methods and systems of the present disclosure may be utilized for in vitro expression of proteins. In some embodiments, the in vitro modified cells and the modified cells of the present disclosure could be utilized for numerous applications, such as cell therapies (e.g., muscle-directed cell therapies). In some embodiments, nucleic acid sequence insertion occurs in vivo in a subject. In some embodiments, the in vivo insertion occurs by administering the nucleic acid sequences and/or systems of the present disclosure to the subject. The nucleic acid sequences and systems of the present disclosure may be administered to various subjects. For instance, in some embodiments, the subject is a human being. In some embodiments, the subject is a non-human mammal. In some embodiments, the non-human mammal includes, without limitation, a horse, a rabbit, a mouse, a rat, a pig, a sheep, a cow, a dog, or a cat. In some embodiments, the non-human mammal is a domestic animal, such as a dog or a cat. The nucleic acid sequences and systems of the present disclosure may be administered to subjects in various manners. For instance, in some embodiments, the administration occurs by a method that includes, without limitation, intravenous administration, subcutaneous administration, transdermal administration, topical administration, intraarterial administration, intrathecal administration, intracranial administration, intraperitoneal administration, intraspinal administration, intranasal administration, intraocular administration, oral administration, intratumor administration, or combinations thereof. The in vivo insertion of nucleic acid sequences into cells can have numerous applications. For instance, in some embodiments, the expressed nucleic acid sequence is utilized to treat or prevent a disease in a subject. In some embodiments, the expressed nucleic acid sequence may be utilized for correction of a disease in a subject. In some embodiments, the expressed nucleic acid sequence may be utilized for tissue repair or regeneration in a subject. In some embodiments, the expressed nucleic acid may be utilized for reversal of age related decline. The methods, systems and modified cells of the present disclosure may be utilized to treat, prevent or correct various diseases. For instance, in some embodiments, the disease is a genetic disease or disorder. In some embodiments, the disease is a muscle disease. In some embodiments, the disease is a genetic disorder with spectrum mutations, such as hemophilia A, hemophilia B, lyosomsomal storage disorders, neuromuscular disorders, muscular dystrophies, and muscular atrophies. Applications and Advantages The methods, systems and modified cells of the present disclosure can have numerous applications and advantages. For instance, in some embodiments, the methods, systems and modified cells of the present disclosure may be utilized to improve gene therapy for genetic diseases. In some embodiments, the methods, systems, and modified cells of the present disclosure may be integrated into existing gene therapy solutions to improve their safety, efficacy, and sustainability. In some embodiments, such applications can enhance the treatment outcomes for patients with genetic discases. In some embodiments, the methods, systems, and modified cells of the present disclosure can also be used to develop new gene therapy solutions for a broad range of genetic diseases. For instance, in some embodiments, the methods, systems and modified cells of the present disclosure can be utilized to design more efficient and targeted gene therapies with better safety and efficacy profiles. In some embodiments, the methods, systems, and modified cells of the present disclosure can also be used to enhance precision medicine. For instance, in some embodiments, the methods, systems, and modified cells of the present disclosure can enhance precision medicine by enabling targeted gene therapies for specific patient populations with genetic diseases. This can help personalize treatment approaches based on the patient's genetic makeup, resulting in better treatment outcomes. In some embodiments, the methods, systems, and modified cells of the present disclosure can be used to advance regenerative medicine. For instance, in some embodiments, the methods, systems, and modified cells of the present disclosure can be used to advance regenerative medicine approaches by enabling the efficient and sustainable expression of genes that promote tissue repair and regeneration. This can potentially improve the treatment outcomes for patients with tissue injuries or degenerative diseases. Additional Embodiments Reference will now be made to more specific embodiments of the present disclosure and experimental results that provide support for such embodiments. However, Applicant notes that the disclosure below is for illustrative purposes only and is not intended to limit the scope of the claimed subject matter in any way. Example 1. Precision and Efficacy of RNA-guided DNA Integration in High-Expressing Muscle Loci Gene replacement therapies primarily rely on adeno-associated viral (AAV) vectors for transgene expression. However, episomal expression can decline over time due to vector loss or epigenetic silencing. CRISPR-based integration methods offer promise for long-term transgene insertion. While the development of transgene integration methods has made substantial progress, identifying optimal insertion loci remains challenging. Skeletal muscle is a promising tissue for gene replacement owing to low invasiveness of intramuscular injections, relative proportion of body mass, the multinucleated nature of muscle, and the potential for reduced adverse effects. Leveraging endogenous promoters in skeletal muscle, Applicant evaluated in this Example two high-expressing loci using homology-independent targeted integration (HITI) to integrate reporter or therapeutic genes in mouse myoblasts. Applicant hijacked the muscle creatine kinase (Ckm) and myoglobin (Mb) promoters by co-delivering CRISPR-Cas9 and a donor plasmid with promoter-less constructs encoding green fluorescent protein (GFP) or human Factor IX (hFIX). Additionally, Applicant deeply profiled genome and transcriptome outcomes from targeted integration and evaluated the safety of the proposed sites. As such, this Example introduces a proof-of-concept technology for achieving high-level therapeutic gene expression in skeletal muscle, with potential applications in targeted integration-based medicine and synthetic biology. In this Example, Applicant identified new potential safe-harbor sites in skeletal muscle that offer secure and stable integration, facilitating gene replacement. Choosing skeletal muscle as a gene therapy integration site can reduce procedure invasiveness and complexity. Its unique syncytial nature is less likely to create negative effects in vivo, with minimal local or systemic adverse effects associated with intramuscular injection were reported in a systematic review for AAV gene therapies. Owing to its local delivery, the exposure to circulating neutralizing antibodies, such as AAV pre-existing antibodies against the viral capsid is minimized, ensuring efficient tissue transduction. Skeletal muscle's well-vascularized nature allows secreted proteins to enter circulation and intramuscular administration of AAV vector that lead to the secretion of functional proteins has been previously demonstrated. In this Example, Applicant devised a strategy to leverage endogenous promoters in muscle cells for expressing either the reporter gene or a therapeutic gene. Applicant adapted the currently established safe-harbor characterization to evaluate two high-expressing skeletal-muscle loci. Applicant applied homology-independent targeted integration (HITI) to mediate transgene integration, introducing a reporter gene or therapeutic gene (human factor 9, hF9) to express human factor 9 protein (hFIX) from the identified loci. By delivering a promoter-less transgene, Applicant was able to detect both protein expression in the reporter gene and hFIX. Applicant deeply profiled genome and transcriptome outcomes after gene-editing outcomes using high-throughput sequencing, unidirectional sequencing, and nanopore long-read sequencing. Overall, the data support RNA-guided DNA-integration strategies as effective therapies for restoring desired gene expression in muscle and can be extended to synthetic biology applications. Example 1.1. Selected Muscle-Specific Integration Sites Have High Expression and a Predicted Favorable Safety Profile The selection of integration sites involves combining established criteria for potential safe-harbor sites with insights from prior in vivo studies that employed the highly expressing locus, albumin (ALB), to express therapeutic genes in the liver. Applicant has tailored this strategy to target integration sites within skeletal muscles. To pinpoint suitable sites, Applicant referred to a comprehensive analysis of public transcriptomics studies on skeletal muscle in humans and mice. A. M. Abdelmoez et al., American Journal of Physiology - Cell Physiology 318, C615-C626 (2020). Through their thorough examination of the compilation and the application of rigorous quality control, normalization, and annotation using official human gene names, Applicant identified the top 10 genes specifically expressed in skeletal muscle and demonstrated high expression levels in both humans and mice (Table 1). The selection of these genes is rooted in the goal of ensuring robust and consistent muscle-specific promoter expression in different models of organisms. Next, Applicant evaluated the essentiality of each gene and the safety implications associated with hijacking its endogenous promoter. TABLE 1A comparison of highly expressed genes in skeletal muscletissues between mice and humans. The data were extractedfrom A. M. Abdelmoez et al., American Journal of Physiology -Cell Physiology 318, C615-C626 (2020), which compiledskeletal muscle tissue gene expressions from the GeneExpression Omnibus public database.HumanMouseGeneRPKMSERPKMSE1ACTA17.030.277.140.142CKM6.910.245.650.603MB6.630.466.140.494MYH76.720.332.262.805MYL26.870.263.352.526DES6.110.443.351.687TNNC26.280.546.930.468TNNI6.310.431.932.739MYL35.790.243.462.2310TTN6.110.700.634.52RPKM: Reads Per Kilobase per Million mapped reads, SE: standard error. Skeletal actin (ACTA1) is on top of the list as the highest expressed gene in skeletal muscle. However, skeletal actin is the main actin isoform in skeletal muscle and it plays an essential role, alongside myosin, in facilitating muscle contraction. Further, ACTA1 disruption in newborn mice causes early demise. In light of this, Applicant propose creatine kinase (CKM) and myoglobin (MB) as genomic integration sites for skeletal muscle. While Applicant's approach does not involve knocking out the Ckm or Mb gene, these selections are based on the viability and state of skeletal muscle after a knock-out experiment. Mice lacking Ckm are viable and exhibit no changes in absolute muscle force; but, they show an inability to engage in burst activity. Mice without myoglobin are also viable with no distinct phenotype apart from depigmentation of the cardiac muscle and cardiac adaptations regarding oxygen delivery. To identify if these sites could serve as potential genomic safe-harbor sites, Applicant assessed the suitability of creatine kinase and myoglobin as safe-harbor sites in comparison to currently known safe-harbor sites. Scores were assigned based on previously established and widely accepted criteria ( FIG. 2 ; Table 2). TABLE 2A display of the assessment of currently widely used safe-harbor sites (AAVS1, CCR5, and hROSA26) and proposed safe-harbor sites. The assessment was performed based on the idealcriteria of safe-harbor sites. Each criterion was evaluatedin the UCSC genome browser track following recommendations.Criteria (FIG. 2)Location12345678AAVS1NoYesNoYesYesNoYesYesCCR5NoYesNoYesYesNoYesYeshROSA26NoYesNoYesYesNoYesYesCKM intronNoYesNoYesYesNoYesYesMB intronYesYesNoYesYesNoYesYes Table 2 indicates that Applicant's proposed sites did not meet all the criteria. However, even the most commonly used safe-harbor sites fail to achieve a 100% clearance based on these criteria. Interestingly, MB shows better results in a free oncogene region criterion compared to CKM and other sites. Example 1.2. Targeted Integration Into Ckm and Mb Lead to Expression of a Promoter-Less Reporter To experimentally assess transgene expression from proposed sites (intron 1 of CKM or MB), Applicant performed a targeted integration approach to knock in a gene construct encoding a promoter-less green fluorescence reporter protein (GFP) into mouse myoablsts (Ckm or Mb). Applicant identified the knock-in site within the intron preceding the coding sequence of the gene, ensuring that the non-edited or inaccurately edited copy maintains functional gene expression via mRNA splicing. This approach also provides flexibility in transgene selection, enabling the incorporation of specific features, such as a signaling peptide or a transgene that does not require a signaling peptide. To integrate GFP, Applicant used a CRISPR/Cas9-based genome editing strategy that uses the HITI method capitalizing the NHEJ pathway that is accessible in muscle cells ( FIG. 3A ). This approach designs Cas9 target sites in the donor DNA as reverse complements of the genomic target site to facilitate the re-cleavage of reverse-integration products, providing a means to dictate the specific directionality of the knock-in. Applicant applied this strategy to C2C12 immortalized mouse myoblast cell lines as it is the most commonly used cellular model to study murine skeletal muscle in vitro. As a proof of concept, Applicant identified five guide RNA (gRNA) sites within intron 1 of each gene, positioned distal to the enhancer and proximal to exon 2, specifically within the last 250 base pairs leading up to the noncoding portion of exon 2 ( FIG. 3B ) based on CRISPOR web tool and screened their cutting efficiency in NIH3T3 cell line. Additionally, Applicant conducted PCR-enriched next-generation sequencing (NGS) using Iseq short-read sequencing to quantify the efficiency of gRNA in the C2C12 cell line. Within the Ckm gene, g1 exhibited the highest efficiency, making it the selected gRNA for HITI-mediated GFP integration experiments. In the Mb gene, although g3 demonstrated slightly highest cutting efficiency, it targeted the non-coding exon sequence. Therefore, Applicant opted for g1 as the gRNA for the downstream integration experiment ( FIGS. 3B and 3C ). Applicant used Lipofectamine 3000 to co-transfect C2C12 muscle myoblasts with a ratio of 2:2:1 for SpCas9, guide RNA, and site-specific promoter-less insert plasmids, respectively. Three days after transfection, Applicant performed a genotyping PCR to confirm the integration. PCR primers were used spanning the intron of Ckm or Mb to the 5′ junction of the GFP insert, and from the 3′ junction of the GFP insert to the intron of Ckm or Mb. Applicant detected bands at the expected length of chimeric Ckm-GFP or Mb-GFP integration in both 5′ and 3′ end junctions at the genomic DNA and cDNA levels, but no bands were observed in the control groups ( FIGS. 3D and 3E ). In myotubes, after differentiation, Applicant observed GFP expression in the CRISPR-treated group in both loci, while the scrambled group did not exhibit GFP expression. This suggests that the GFP expression was driven by the Ckm or Mb promoter ( FIG. 3F ). Example 1.3. Targeted Integration of a Promoter-Less Human F9 Gene at Ckm and Mb Leads to Sustained Expression of hF9 Next, Applicant sought to investigate whether this strategy applies to therapeutic genes. The GFP transgene in the HITI insert plasmid was substituted with a transgene encoding human factor IX (hF9) ( FIG. 4A ). The selection of hF9 as a proof of concept for therapeutic gene integration was motivated by previous reports indicating that even with <1% of targeted integration events of hF9 under the albumin promoter, it proved adequate to attain 5-20% of FIX levels, effectively correcting bleeding in hemophilia B mice. In this approach, Applicant utilized the complete coding sequence of hF9, given the absence of signaling peptides in the Ckm or Mb gene. Following a method similar to GFP transfection, Applicant co-delivered a site-specific hF9 HITI insert plasmid with SpCas9 and Ckm or Mb gRNA into C2C12 myoblasts using Lipofectamine 3000. Post-transfection, as observed in GFP integration, Applicant verified hF9 integration exclusively within the CRISPR-mediated HITI group at both genomic DNA and cDNA levels in both sites ( FIGS. 4B and 4C ). To evaluate changes in gene expression between groups, Applicant conducted qRT-PCR using FAM probe primers spanning from exon 6 to exon 7 of hF9. The Ppia gene served as a housekeeping gene for normalizing expression levels. This selection was based on Applicant's data that showed the Ppia gene has the most consistent expression in both myoblasts and myotubes. In contrast, Gapdh has variable expression levels between the myoblast and myotube stages. Furthermore, Applicant evaluated four different reference genes from a previous study to identify the most consistent expression in both myoblast and myotubes. During the myotube stage, the relative expression of hF9 was found to be elevated in the CRISPR-mediated HITI group compared to the scrambled group, reaching up to a 3-fold increase in Ckm-mediated expression and up to a 10-fold increase in Mb-mediated expression ( FIG. 4D ). Additionally, to assess if this increased mRNA expression translates to hFIX protein expression, Applicant performed an hFIX sandwich ELISA assay on cell media after myotube differentiation. Applicant observed an increased level of protein expression in both Ckm and Mb-treated cell serum relative to a scrambled control. Furthermore, to determine whether identified sites supporting long-term stable hF9 expression, Applicant maintained the myoblast culture for 30 days. Despite Applicant's efforts to incorporate a reporter gene marker in Cas9 for cell sorting, Applicant encountered difficulties in maintaining the sorted cultured cells. This obstacle has also been noted in previously published literature, which indicates that myoblasts tend to lose their differentiation potential after single-cell cloning, and those that are successfully edited often fail to survive the stress of sorting. Since generating clonal populations was not feasible, Applicant opted to gather informative data from bulk populations by routinely passaging the cells when they reached 60-70% confluency, before spontaneously differentiating to myotubes. Following the 30-day culture, Applicant detected integration in DNA level. In the time-course analysis, hF9 gene expression was assessed at various intervals starting from 3 days post-transfection, with cell passaging every two to three days, followed by differentiation from day 23 to 30. Despite the continuous passaging of cells, this longitudinal study demonstrated generally consistent expression of hF9 throughout the observation period. The variance in fold change observed between fully differentiated myotubes in FIGS. 4D-4E could be attributed to the continuous passaging process, which involved regular splitting of the cells. Example 1.4. Multi-Pronged Targeted Next-Generation Sequencing and Long-Read Sequencing Show Relative Precision of HITI-Based Integration While Applicant has confirmed that the Ckm and Mb endogenous promoter can express the promoter-less protein, the precision of editing was unknown. Imprecise integration could negatively impact protein expression. As such, Applicant evaluated factors affecting protein expression such as the precision and the efficiency of integration. Applicant performed PCR-enriched short-read sequencing for fragments spanning the genomic insertion site or chimeric transcript to evaluate if the integrated chimeric sequence is as precise based on the mechanism of HITI or if unintentional on-target events could be detected. Genomic DNA-level analysis revealed levels of precise integration ranging from 15% to 35% in 5′ and 3′ integration junctions in both loci ( FIGS. 5A-5B ). The modified sequence predominantly resulted from NHEJ repair at gRNA cut sites with preferential removal of sequence on the inserted sequence ( FIGS. 5A-5B ). Although genomic DNA-level precision was not optimal, at the mRNA level, Applicant saw a higher average of precision integration around 80% of precise integration ( FIG. 5C ) with some modifications on the junction site of exon 1 Ckm and start codon GFP ( FIG. 5D ). Precision integration data provide a selective perspective on how effectively this method incorporates the transgene insert. However, selection of primers and PCR will bias the result. Moreover, this method does not quantify the proportion of integrated sequences due to the PCR-enrichment method that selectively amplifies the integrated sequences. To quantify the integration efficiency, Applicant conducted a Uni-Directional Targeted Sequencing methodology (UDiTaS) that is based on Tn5 transposase-assisted tagmentation short-read sequencing. This method incorporates a unique molecular identifier (UMI) to remove PCR amplification bias. Applicant quantified the number of sequences integrated with their transgene by tagging primers specific to the Ckm intron before the gRNA cut site ( FIG. 5E ). Reading from this site will capture unedited, precisely edited, and unintentionally modified alleles. In FIG. 5E , Applicant deduplicated the UMI and aligned the resulting bam files to the expected fusion of Ckm intron-GFP integration. After implementing filters to verify that the plotted reads were authentic and not sequencing artifacts, the analysis revealed that correct GFP integration represented 2.8% of the total aligned reads. This percentage was calculated by dividing the proportion of edited reads by the proportion of unedited reads that aligned with the reference amplicon from the total reads (4%). Separately, a primer specific to the GFP insert genome was used in conjunction with the same transposon-specific primer to map genome-wide GFP episome integration into the mouse genome. Following similar pipeline with UdiTaS analysis, Applicant did not detect GFP integration in other locations in the genome. The cDNA selective PCR-enriched sequencing method revealed a high percentage of precise integration. However, it may overlook large structural variants. Therefore, to get a more comprehensive picture of the precision, Applicant aimed to characterize the 5′ integration junction of the fusion between Ckm and the transgene at the transcriptome level. This approach is motivated by the expectation that intron rearrangements would be spliced out in mRNA processing. To achieve this, Applicant conducted 5′ rapid amplification of cDNA ends (RACE) using cDNA from Lipofectamine-transfected cells and amplified Ckm-hF9 fusion transcripts using a gene-specific primer (GSP) positioned at the end of the hF9 region. Long-read nanopore sequencing confirmed the addition of exon 1 of Ckm in the 5′ region of hF9. However, Applicant also observed several structural rearrangement events, including significant deletions and insertions ( FIG. 5F ). Each line in the figure represents a single alignment. Upon investigation of the 151 bp insertion, Applicant determined that it originated from the splice acceptor region incorporated from the HITI insert plasmid. This suggests the occurrence of mis-splicing events at the transcriptome level. Example 1.5. RNA Sequencing Reveals Differentially Expressed Genes After Targeted Integration To investigate whether the hF9 targeted integration into identified sites resulted in alterations in the overall transcriptome profiles, bulk RNA-seq analysis was performed. After seven days of myotube differentiation, samples exhibiting the high hF9 expression levels in Ckm and Mb based on qPCR analysis were compared with scrambled-treated cells from the same experimental batch ( FIG. 6A ). Paired-end sequencing on the DNBSEQTM Sequencing System from BGI with an average read length of 100 bp was used for sequencing two biological replicates from each treatment. Principal component analysis (PCA) was performed, followed by visualization of each sample in two dimensions using the first two principal components (PCs). PCA revealed biological variation between samples in the same treatment groups. However, Applicant observed transcriptional similarity within the Ckm-integrated and scrambled group and transcriptional variations in Mb-treated samples compared to the scrambled group ( FIG. 6B ). To assess human F9 expression in the samples, Applicant normalized the transcript expression level (TPM) and found that hF9 is highly expressed in both Ckm and Mb loci compared to the scrambled group ( FIG. 6C ). Subsequent differential gene expression analysis was performed to evaluate changes in the transcriptome post-treatment. The analysis revealed several genes that are both downregulated and upregulated in both Ckm and Mb-treated samples, such as C3 and Inmt genes. Differentially expressed genes were observed to be dispersed across various chromosomes (e.g., C3 on chromosome 17, Inmt on chromosome 6, and Mmp12 on chromosome 12), rather than being clustered within the targeted chromosome (Ckm on chromosome 7; Mb on chromosome 15), where more local contacts are expected to occur. Consistent with the PCA results, more genes were differentially expressed in Mb-treated samples compared to Ckm-treated samples ( FIG. 6D ). Additionally, downregulation of the Mb gene was observed upon editing. Applicant's current work aims to address several significant challenges associated with AAV vector-based in vivo gene therapy, including long-term efficacy, promoter or transgene silencing, and the safety of transgene integration. To enhance the sustainability of the treatment, targeted integration into the host genome has emerged as a potential solution. However, the location of where the integration must be strategic since the integrating vector with the constitutive promoter might pose risks. Despite their ability to facilitate robust and consistent transgene expression, such promoters are associated with increased risks, including an elevated likelihood of inactivation, amplified toxicity resulting from transgene overexpression, off-target transgene expression, and the potential for severe immune responses due to inadvertent transgene expression in antigen-presenting cells. Utilizing endogenous promoters for transgene expression can circumvent potential silencing mechanisms associated with exogenous promoters. In this Example, Applicant identified two highly expressed genes, Ckm and Mb, in skeletal muscle and exploited the transcriptional output of their endogenous promoters. Applicant's findings demonstrate that these loci can drive the expression of promoter-less GFP and enhance RNA expression of promoter-less hF9 in C2C12 cells upon plasmid transfection using lipofection. However, due to the limited efficiency of lipofection in myoblasts, compounded by the generally low efficiency of integration, Applicant confirmed that the efficiency of transgene integration at the genomic level is approximately 3% through the UDiTaS sequencing approach. Despite this modest level of integration, Applicant observed the expression of promoter-less GFP and increased RNA expression of hF9 compared to a control, representing the episomal vector without Cas9. Furthermore, it is noteworthy that the expression levels of Ckm and Mb in C2C12 cells were not as high as reported in skeletal muscle tissue. This disparity may contribute to the lower expression of GFP and hFIX protein observed. Additionally, regarding the hFIX protein results, the rich presence of collagen IV in skeletal muscle may facilitate the local attachment of FIX, limiting its release into circulation. Previous studies exploring the production of ectopic FIX protein in muscle have suggested the potential use of hFIX variants harboring mutations such as lysine to alanine at residue 5 (K5A) or valine to lysine at residue 10 (V10K), which exhibit reduced binding to endothelial cells while maintaining normal clotting activity, thereby enabling synthesis of hFIX in skeletal muscle. Hence, investigating the expression of this variant under a muscle endogenous promoter in muscle cells could potentially result in increased hFIX release into circulation. The general safety evaluation of the proposed sites indicates that the group treated with Mb exhibits alterations in the global transcriptome. The downregulation of the Mb gene may be attributed to the higher indel activity of the CRISPR/gRNA complex at that target site compared to the indel activity at the Ckm target sites. Despite observing minimal changes in the expression of upregulated and downregulated genes at the Ckm locus, which may suggest passing initial safety validation, a more comprehensive analysis is still warranted. C3 downregulated expression was reported in the acute stage of infection suggesting that C3 participates in inflammation and slight upregulation of Inmt was reported in skeletal muscle under spaceflight conditions. Applicant's sequencing studies also show patterns of deletions following integration induced by double-strand breaks (DSBs). These sequences demonstrate that indel-induced damage tends to occur toward the insertion site, as evidenced by the selected PCR-enriched integration sequencing. This trend is consistently observed across loci and in both 5′ and 3′ integrations. This insight is valuable for future template design efforts, emphasizing the importance of incorporating padding sequences before the coding sequence of the transgene to mitigate chewback into the desired gene before integration can occur. Applicant's approach of integrating the transgene into intronic regions offers the advantage of splicing, as indels occurring at the intronic level are typically spliced out, resulting in more precise mRNA. Despite selecting a robust splice acceptor from a highly conserved gene in skeletal muscle, based on Applicant's 5′ RACE results, Applicant observed instances of missplicing due to the failure of the splice acceptor to be properly excised. To date, there is a lack of practical guidelines available to determine the ideal length of splice acceptors or splice donors for a given gene. Thus, design of the splice acceptor allows additional optimization of gene expression. In summary, this Example presents a proof-of-concept technology for achieving high-level therapeutic gene expression in skeletal muscle. Such methods hold promise for advancing targeted integration-based medicine or synthetic biology approaches, effectively transforming skeletal muscle into a biofactory capable of producing desired therapeutic agents. Without further elaboration, it is believed that one skilled in the art can, using the description herein, utilize the present disclosure to its fullest extent. The embodiments described herein are to be construed as illustrative and not as constraining the remainder of the disclosure in any way whatsoever. While the embodiments have been shown and described, many variations and modifications thereof can be made by one skilled in the art without departing from the spirit and teachings of the invention. Accordingly, the scope of protection is not limited by the description set out above, but is only limited by the claims, including all equivalents of the subject matter of the claims. The disclosures of all patents, patent applications and publications cited herein are hereby incorporated herein by reference, to the extent that they provide procedural or other details consistent with and supplementary to those set forth herein.",en,PATENT_APPLICATION
077-095-177-067-752,US,20240388391,A1,2024-11-21,US_20240388391_A1_20241121,en,US,20240388391,A1,2024-11-21,US,18779192,2024-07-22,"SYSTEMS, METHODS, AND DEVICES FOR SECONDARY CELL ACTIVATION WITH UE-SPECIFIC REFERENCE SIGNAL",en,US,Apple Inc.,"Cupertino, CA",CN,Qiming Li,Beijing,US,1,Dawei Zhang,"Saratoga, CA",US,2,Huaning Niu,"Cupertino, CA",US,3,Jie Cui,"San Jose, CA",US,4,Manasa Raghavan,"Sunnyvale, CA",US,5,Xiang Chen,"Saratoga, CA",US,6,Yang Tang,"San Jose, CA",CN,7,Yushu Zhang,Beijing,H04L5/00,I,F,H04L5/0035,I,F,H04L5/0048,I,L,H04L5/0098,I,L,US,20240388391,A1,2024-11-21,077-095-177-067-752,1,US,20240388391,A1,2024-11-21,077-095-177-067-752,1,UNKNOWN,"Techniques discussed herein may better ensure efficient fine time tracking during a second cell (SCell) activation procedure by using a tracking reference signal (TRS) that is User Equipment (UE) specific. A primary cell (PCell) may provide a UE with a TRS configuration that includes a UE-specific reference signal (RS) with aperiodicity. In response to receiving a SCell activation command from the PCell, the UE may perform cell activation in accordance with the UE-specific RS. As the delay time for the UE-specific RS may be less than the delay time for a system synchronization block (SSB) from the SCell, use of the UE-specific RS may expedite the cell activation procedure.",en,"1 . A baseband processor, comprising: a memory configured to store instructions; and a processing circuitry coupled to the memory and, when executing the instructions, configured to: receive a secondary cell activation command; and activate, in response to the secondary cell activation command, a secondary cell with a time delay based on timing of an aperiodic user equipment (UE) specific reference signal.","2 . The baseband processor of claim 1 , wherein the aperiodic UE specific reference signal is an aperiodic tracking reference signal (ATRS).","3 . The baseband processor of claim 1 , the processing circuitry further configured to receive a reference signal configuration configuring the aperiodic UE specific reference signal.","4 . The baseband processor of claim 3 , the processing circuitry further configured to: receive the reference signal configuration from a primary cell during a radio resource control (RRC) procedure.","5 . The baseband processor of claim 1 , wherein the time delay includes a duration of time between a UE finishing processing the secondary cell activation command and a first complete available occasion of the aperiodic UE specific reference signal.","6 . The baseband processor of claim 5 , wherein the UE finishing processing the secondary cell activation command includes the UE finishing with processing a last activation command for a physical downlink shared channel (PDSCH) or physical downlink control channel (PDCCH) transmission configuration indicator (TCI); and wherein the first complete available occasion of the aperiodic UE specific reference signal has a same TCI state as the PDCCH or PDSCH TCI.","7 . The baseband processor of claim 1 , wherein the aperiodic UE specific reference signal is used to perform a fine time tracking.","8 . The baseband processor of claim 1 , wherein the aperiodic UE specific reference signal is used to perform both a fine time tracking and an automatic gain control (AGC).","9 . The baseband processor of claim 1 , wherein the time delay is further based on timing of a hybrid automatic repeat request (HARQ) message of the secondary cell activation command.","10 . The baseband processor of claim 1 , wherein the aperiodic UE specific reference signal is received at slot n+ (Time HARQ +3 ms)/(NR slot length), where n is a time when the secondary cell activation command is received, Time HARQ is a time associated with a HARQ message for the secondary cell activation command, and NR slot length is a designated slot length.","11 . The baseband processor of claim 1 , wherein the activation of the secondary cell is based on the aperiodic UE specific reference signal when a reference signal configuration indicating the aperiodic UE specific reference signal is received, and is based on a cell-specific reference signal when the reference signal configuration is not received.","12 . A User Equipment (UE) comprising: radio frequency (RF) circuitry configured to communicate with a wireless communication network; a memory device configured to store instructions; and one or more processors, connected to the RF circuitry and the memory device, and configured to execute the instructions to: receive, via the RF circuitry, a secondary cell activation command; and activate, in response to receiving the secondary cell activation command, a secondary cell based on an aperiodic tracking reference signal (ATRS).","13 . The UE of claim 12 , wherein the activation of the secondary cell is allowed with a time delay based on timing of the ATRS.","14 . The UE of claim 13 , wherein the one or more processors are configured to: perform a fine time tracking during the time delay and based on the ATRS.","15 . The UE of claim 14 , wherein the fine time tracking is performed in response to a reference trigger received as part of a media access control (MAC) command or downlink control information (DCI).","16 . The UE of claim 14 , wherein the one or more processors are configured to: receive a transmission configuration indicator (TCI) command; and perform the fine time tracking following a decoding of the TCI command.","17 . The UE of claim 13 , wherein the one or more processors are configured to: perform a fine time tracking and an automatic gain control (AGC) during the time delay and based on the ATRS.","18 . The UE of claim 13 , wherein the time delay is also based on a duration of time associated with receiving a hybrid automatic repeat request (HARQ) message associated with the secondary cell activation command.","19 . The UE of claim 12 , wherein the activation of the secondary cell is based on the ATRS when a reference signal configuration indicating the ATRS is received, and is based on a cell-specific reference signal when the reference signal configuration is not received.","20 . A method, performed by a User Equipment (UE), comprising: receiving, from a primary cell, a secondary cell activation command; and activating, in response to the secondary cell activation command, a secondary cell based on a UE-specific reference signal when a reference signal configuration indicating the UE-specific reference signal is received, and based on a cell-specific reference signal when the reference signal configuration is not received.",en,"REFERENCE TO RELATED APPLICATIONS This application is a Continuation of U.S. patent application Ser. No. 17/441,068 filed Sep. 20, 2021, which is a National Phase entry application of International Patent Application No. PCT/CN2021/071807 filed Jan. 14, 2021, entitled “SYSTEMS, METHODS AND DEVICES, FOR SECONDARY CELL ACTIVATION WITH UE-SPECIFIC REFERENCE SIGNAL, the contents of which are hereby incorporated by reference in their entirety FIELD This disclosure relates to wireless communication networks including techniques for secondary cell activation in a wireless communication network. Other aspects and techniques are also described. BACKGROUND As the number of mobile devices within wireless networks, and the demand for mobile data traffic, continue to increase, changes are made to system requirements and architectures to better address current and anticipated demands. An aspect of such networks may include carrier aggregation (CA) and radio resource management (RRM). For example, the base station of a primary cell may cause a user equipment (UE) to active communications with the base station of a secondary cell, resulting in a CA scenario for the UE involving the primary and secondary cell. BRIEF DESCRIPTION OF THE DRAWINGS The present disclosure will be readily understood and enabled by the detailed description and accompanying figures of the drawings. Like reference numerals may designate like features and structural elements. Figures and corresponding descriptions are provided as non-limiting examples of aspects, implementations, etc., of the present disclosure, and references to “an” or “one” aspect, implementation, etc., may not necessarily refer to the same aspect, implementation, etc., and may mean at least one, one or more, etc. FIG. 1 is a diagram of an example network according to one or more implementations described herein. FIG. 2 is a timeline diagram of example of secondary cell activation using a user equipment (UE) specific reference signal (RS). FIG. 3 is a flowchart of an example process for activating a secondary sell using a UE-specific RS. FIG. 4 is a sequence diagrams of an example process activating a secondary sell using a UE-specific RS. FIG. 5 is a diagram of an example of components of a device according to one or more implementations described herein. FIG. 6 is a diagram of example interfaces of baseband circuitry according to one or more implementations described herein. DETAILED DESCRIPTION The following detailed description refers to the accompanying drawings. Like reference numbers in different drawings may identify the same or similar features, elements, operations, etc. Additionally, the present disclosure is not limited to the following description as other implementations may be utilized, and structural or logical changes made, without departing from the scope of the present disclosure. Mobile communication networks may include one or more types and/or generations of wireless communication networks, such as 4th generation (4G) networks, 5th generation (5G) or new radio (NR) networks, etc. Such networks may include user equipment (UEs) and base stations. An aspect of mobile communication networks includes radio resource management (RMM), which may include configuration, assignment, allocation, management, etc., of wireless resources (e.g., channels, bandwidths, carriers, etc.) within the network. For example, carrier aggregation (CA) may include a scenario in which multiple carriers are allocated to a UE. In some scenarios, CA may involve carriers aggregated from different base stations. For example, a UE may connect to a first carrier on a first base station operating as a primary cell (PCell) for purposes of RRM, CA, etc., regarding the UE. The primary cell may perform a radio resource control (RRC) connection reconfiguration procedure that includes sending the UE connection configuration information and an activation command regarding a second carrier of the first base station or a third carrier on a second base station that may operate as a secondary cell (SCell). The UE may respond to the configuration information and command by searching for, and connecting to, the SCell, resulting in a CA scenario involving the PCell, SCell, and UE. To connect to the SCell, the UE may perform timing synchronization, which may include course timing synchronization and fine timing synchronization (also referred to herein as fine time tracking). Course timing synchronization may generate an initial estimate of a starting index or symbol for communicating with the SCell, and fine time tracking may improve the initial estimate toward an ideal or more accurate starting point regarding communications with the SCell. Because timing synchronization may be performed prior to the completion of SCell activation, the duration of time involved in SCell activation may depend, at least in part, on the time involved in fine time tracking. Further, since timing synchronization may depend on a reference signal (RS) of signal synchronization blocks (SSBs) from the SCell, SCell activation time (or activation delay) may depend on SSBs periodicity of the SCell. For example, the SCell activation delay may include a duration of time between an end of a first complete SSB burst indicated by an SSB-based RRM measurement timing configuration (SMTC) after slot n+time for a hybrid automatic repeat request (Time_HARQ)+3 ms, where n is a time when the UE receives the media access control (MAC) command including a SCell activation command, Time_HARQ is a duration of time associated with a HARQ message corresponding to the MAC command, and an NR slot length is a designated slot length for performing fine time tracking during the TRS. In another example, the SCell activation delay may be further extended by an RS time (Trs) indicated by the SMTC, provided by a measurement object, or a default duration. In yet another example, SCell activation delay may include a duration between the UE finishing the processing a last activation command for a physical downlink control channel (PDCCH) transmission configuration indicator (TCI), physical downlink shared channel (PDSCH) TCI (when applicable) and the timing of first complete available SSB corresponding to the TCI state. As such, the activation time for SCells may be long because RSs used for synchronization may depend on SSB periodicity of the SCell (e.g., 160 milliseconds) and other durations that may sometimes be involved. These durations, in turn, delay SCell activation and CA. The techniques described herein may increase SCell activation efficiency by implementing a UE-specific RS for fine time tracking instead of a cell-specific RS (e.g., via SSBs). The UE-specific RS may be configured with either a short periodicity (e.g., a shorter periodicity than the SSB periodicity of the SCell) or aperiodicity that occurs well within the SSB periodicity of the SCell. In some implementations, a UE-specific RS for fine time tracking during a SCell activation procedure may be a tracking reference signal (TRS). The network (e.g., PCell) may configure the TRS via RRC messaging (e.g., via an RRC connection configuration procedure, RRC connection reconfiguration procedure, etc.) containing a SCell addition. Later, the PCell may send the UE a media access control (MAC) control element that includes a SCell activation command. In response, the UE may perform a cell search procedure for SCell and automatic gain control (AGC) adjustment. Additionally, the UE may use the TRS to promptly perform fine timing tracking and/or AGC (e.g., well-before the reception and processing of the next SSB from the SCell) and proceed to expeditiously complete the SCell activation procedure. In some implementations, the TRS of SCell may be periodic or aperiodic. When the TRS is periodic, the delay for fine time tracking and/or ACG may be based on the TRS periodicity. When the TRS is aperiodic, the delay for fine time tracking and/or ACG may depending on network scheduling (e.g., when to trigger the TRS transmission). In some implementations, the aperiodic TRS may be transmitted shortly after the PCell transmits the SCell activation command to the UE. In some implementations, the aperiodic TRS may be triggered by the network (e.g., PCell) via downlink control information (DCI) and/or a MAC command, which may be communicated after sending the SCell activation command to the UE. When TCI is configured by the network, the TRS may be transmitted after the UE successfully decodes a TCI command from the PCell. When TCI is not configured, the TRS may be transmitted after the UE receives the MAC CE with the SCell activation command. For example, the TRS may be transmitted at slot n+ (Time_HARQ+3 ms)/(NR slot length), where n is a time when UE receives the MAC CE, Time_HARQ is a time associated with a HARQ message for the MAC CE, and new radio (NR) slot length is a designated slot length for performing fine time tracking during the TRS. In some implementations, Time_FineTiming may include a duration of time between the UE finishing with processing a last activation command for PDCCH TCI, PDSCH TCI (when applicable) and of either: 1) a first complete available SSB corresponding to the TCI state when the TRS is not configured, or a first complete available TRS occasion with the same TCI state when the TRS is configured. Alternatively, when the TRS is configured, the Time_FineTiming may be further splitter into two cases, periodic and aperiodic. When a periodic TRS is configured, Time_FineTiming may be a periodicity of the TRS. When aperiodic TRS is configured, Time_FineTiming may be a duration of time between the UE finishing with processing a last activation command for PDCCH TCI, PDSCH TCI (when applicable) and a timing of a first complete available TRS occasion with the same TCI state if TRS is configured. In some implementations, regarding RRM conditions for fine time tracking during SCell activation, Time_FirstSSB may be a duration of time from an end of a first complete SSB burst indicated by an SMTC after slot n+Time_HARQ+3 ms, when SMTC is configured on target SCell, or when TRS is configured, Time_FirstSSB may be a duration of time to an end of the first complete available TRS occasion (if configured, with the same TCI state) after n+Time_HARQ+3 ms. In cither scenario, Time_FirstSSB_MAX may be a time to an end of a first complete available TRS occasion (if configured, with the same TCI state) after slot n+Time_HARQ+3m. Alternatively, when TRS is configured, RRM conditions may be further splintered into two cases, periodic and aperiodic. When periodic TRS is configured, Time_FirstSSB may be equal to Time_TRS, and Time_FirstSSB_MAX may be equal to Time_PRS, where Time_PRS is the periodicity of PRS. When aperiodic TRS is configured, Time_FirstSSB and Time_FirstSSB_MAX are a duration of time between the UE finishing processing a SCell activation command and the timing of first complete available TRS occasion (if configured, with the same TCI state). FIG. 1 is an example network 100 according to one or more implementations described herein. Example network 100 may include UEs 110 - 1 , 110 - 2 , etc. (referred to collectively as “UEs 110 ” and individually as “UE 110 ”), a radio access network (RAN) 120 , a core network (CN) 130 , application servers 140 , external networks 150 , and satellites 160 - 1 , 160 - 2 , etc. (referred to collectively as “satellites 160 ” and individually as “satellite 160 ”). As shown, network 60 may include a non-terrestrial network (NTN) comprising one or more satellites 160 (e.g., of a global navigation satellite system (GNSS)) in communication with UEs 110 and RAN 120 . The systems and devices of example network 60 may operate in accordance with one or more communication standards, such as 2nd generation (2G), 3rd generation (3G), 4th generation (4G) (e.g., long-term evolution (LTE)), and/or 5th generation (5G) (e.g., new radio (NR)) communication standards of the 3rd generation partnership project (3GPP). Additionally, or alternatively, one or more of the systems and devices of network 60 may operate in accordance with other communication standards and protocols discussed herein, including future versions or generations of 3GPP standards (e.g., sixth generation (6G) standards, seventh generation (7G) standards, etc.), institute of electrical and electronics engineers (IEEE) standards (e.g., wireless metropolitan area network (WMAN), worldwide interoperability for microwave access (WiMAX), etc.), and more. As shown, UEs 110 may include smartphones (e.g., handheld touchscreen mobile computing devices connectable to one or more wireless communication networks). Additionally, or alternatively, UEs 110 may include other types of mobile or non-mobile computing devices capable of wireless communications, such as personal data assistants (PDAs), pagers, laptop computers, desktop computers, wireless handsets, etc. In some implementations, UEs 110 may include internet of things (IoT) devices (or IoT UEs) that may comprise a network access layer designed for low-power IoT applications utilizing short-lived UE connections. Additionally, or alternatively, an IoT UE may utilize one or more types of technologies, such as machine-to-machine (M2M) communications or machine-type communications (MTC) (e.g., to exchanging data with an MTC server or other device via a public land mobile network (PLMN)), proximity-based service (ProSe) or device-to-device (D2D) communications, sensor networks, IoT networks, and more. Depending on the scenario, an M2M or MTC exchange of data may be a machine-initiated exchange, and an IoT network may include interconnecting IoT UEs (which may include uniquely identifiable embedded computing devices within an Internet infrastructure) with short-lived connections. In some scenarios, IoT UEs may execute background applications (e.g., keep-alive messages, status updates, etc.) to facilitate the connections of the IoT network. UEs 110 may communicate and establish a connection with (e.g., be communicatively coupled) with RAN 120 , which may involve one or more wireless channels, each of which may comprise a physical communications interface/layer. In some implementations, a UE may be configured with dual connectivity (DC) as a multi-radio access technology (multi-RAT) or multi-radio dual connectivity (MR-DC), where a multiple receive and transmit (Rx/Tx) capable UE may use resources provided by different network nodes (e.g., 122 - 1 and 122 - 2 ) that may be connected via non-ideal backhaul (e.g., where one network node provides NR access and the other network node provides cither E-UTRA for LTE or NR access for 5G). In such a scenario, one network node may operate as a master node (MN) and the other as the secondary node (SN). The MN and SN may be connected via a network interface, and at least the MN may be connected to the CN 130 . Additionally, at least one of the MN or the SN may be operated with shared spectrum channel access, and functions specified for UE 110 can be used for an integrated access and backhaul mobile termination (IAB-MT). Similar for UE 61 , the IAB-MT may access the network using either one network node or using two different nodes with enhanced dual connectivity (EN-DC) architectures, new radio dual connectivity (NR-DC) architectures, or the like. As shown, UE 110 may also, or alternatively, connect to access point (AP) 116 via interface 118 , which may include an air interface enabling UE 110 to communicatively couple with AP 116 . AP 116 may comprise a wireless local area network (WLAN), WLAN node, WLAN termination point, etc. The connection 1207 may comprise a local wireless connection, such as a connection consistent with any IEEE 702.11 protocol, and AP 116 may comprise a wireless fidelity (Wi-Fi®) router or other AP. While not explicitly depicted in FIG. 1 , AP 116 may be connected to another network (e.g., the Internet) without connecting to RAN 120 or CN 130 . In some scenarios, UE 110 , RAN 120 , and AP 116 may be configured to utilize LTE-WLAN aggregation (LWA) techniques or LTE WLAN radio level integration with IPsec tunnel (LWIP) techniques. LWA may involve UE 110 in RRC_CONNECTED being configured by RAN 120 to utilize radio resources of LTE and WLAN. LWIP may involve UE 110 using WLAN radio resources (e.g., connection interface 118 ) via IPsec protocol tunneling to authenticate and encrypt packets (e.g., Internet Protocol (IP) packets) communicated via connection interface 118 . IPsec tunneling may include encapsulating the entirety of original IP packets and adding a new packet header, thereby protecting the original header of the IP packets. RAN 120 may include one or more RAN nodes, or referred to as, base stations 122 - 1 and 122 - 2 (referred to collectively as RAN nodes 122 , and individually as RAN node 122 ) that enable the connections 114 - 1 and 114 - 2 to be established between UEs 110 and RAN 120 . RAN node 122 may also, or alternatively, referred to herein as a base station. RAN nodes 122 may include network access points configured to provide radio baseband functions for data and/or voice connectivity between users and the network based on one or more of the communication technologies described herein (e.g., 2G, 3G, 4G, 5G, WiFi, etc.). As examples therefore, a RAN node may be an E-UTRAN Node B (e.g., an enhanced Node B, eNodeB, eNB, 4G base station, etc.), a next generation base station (e.g., a 5G base station, NR base station, next generation eNBs (gNB), etc.). RAN nodes 122 may include a roadside unit (RSU), a transmission reception point (TRxP or TRP), and one or more other types of ground stations (e.g., terrestrial access points). In some scenarios, RAN node 122 may be a dedicated physical device, such as a macrocell base station, and/or a low power (LP) base station for providing femtocells, picocells or other like having smaller coverage areas, smaller user capacity, or higher bandwidth compared to macrocells. As described below, in some implementations, satellites 160 may operate as bases stations (e.g., RAN nodes 122 ) with respect to UEs 110 . As such, references herein to a base station, RAN node, etc., may involve implementations where the base station, RAN node, etc., is a terrestrial network node and also to implementation where the base station, RAN node, etc., is a non-terrestrial network node (e.g., satellite 160 ). Some or all of RAN nodes may be implemented as one or more software entities running on server computers as part of a virtual network, which may be referred to as a centralized RAN (CRAN) and/or a virtual baseband unit pool (vBBUP). In these implementations, the CRAN or vBBUP may implement a RAN function split, such as a packet data convergence protocol (PDCP) split wherein radio resource control (RRC) and PDCP layers may be operated by the CRAN/vBBUP and other Layer 2 (L2) protocol entities may be operated by individual RAN nodes 122 ; a media access control (MAC)/physical (PHY) layer split wherein RRC, PDCP, radio link control (RLC), and MAC layers may be operated by the CRAN/vBBUP and the PHY layer may be operated by individual RAN nodes 122 ; or a “lower PHY” split wherein RRC, PDCP, RLC, MAC layers and upper portions of the PHY layer may be operated by the CRAN/vBBUP and lower portions of the PHY layer may be operated by individual RAN nodes 122 . This virtualized framework may allow freed-up processor cores of RAN nodes 122 to perform or execute other virtualized applications. In some implementations, an individual RAN node 122 may represent individual gNB-distributed units (DUs) connected to a gNB-control unit (CU) via individual F1 interfaces. In such implementations, the gNB-DUs may include one or more remote radio heads or radio frequency (RF) front end modules (RFEMs), and the gNB-CU may be operated by a server (not shown) located in RAN 120 or by a server pool (e.g., a group of servers configured to share resources) in a similar manner as the CRAN/vBBUP. Additionally, or alternatively, one or more of RAN nodes 122 may be next generation eNBs (i.e., gNBs) that may provide evolved universal terrestrial radio access (E-UTRA) user plane and control plane protocol terminations toward UEs 110 , and that may be connected to a 5G core network (5GC) 130 via an NG interface. Any of the RAN nodes 122 may terminate an air interface protocol and may be the first point of contact for UEs 110 . In some implementations, any of the RAN nodes 122 may fulfill various logical functions for the RAN 120 including, but not limited to, radio network controller (RNC) functions such as radio bearer management, uplink and downlink dynamic radio resource management and data packet scheduling, and mobility management. UEs 110 may be configured to communicate using orthogonal frequency-division multiplexing (OFDM) communication signals with each other or with any of the RAN nodes 122 over a multicarrier communication channel in accordance with various communication techniques, such as, but not limited to, an OFDMA communication technique (e.g., for downlink communications) or a single carrier frequency-division multiple access (SC-FDMA) communication technique (e.g., for uplink and ProSe or sidelink (SL) communications), although the scope of such implementations may not be limited in this regard. The OFDM signals may comprise a plurality of orthogonal subcarriers. In some implementations, a downlink resource grid may be used for downlink transmissions from any of the RAN nodes 122 to UEs 110 , and uplink transmissions may utilize similar techniques. The grid may be a time-frequency grid (e.g., a resource grid or time-frequency resource grid) that represents the physical resource for downlink in each slot. Such a time-frequency plane representation is a common practice for OFDM systems, which makes it intuitive for radio resource allocation. Each column and each row of the resource grid corresponds to one OFDM symbol and one OFDM subcarrier, respectively. The duration of the resource grid in the time domain corresponds to one slot in a radio frame. The smallest time-frequency unit in a resource grid is denoted as a resource element. Each resource grid comprises resource blocks, which describe the mapping of certain physical channels to resource elements. Each resource block may comprise a collection of resource elements (REs); in the frequency domain, this may represent the smallest quantity of resources that currently may be allocated. There are several different physical downlink channels that are conveyed using such resource blocks. Further, RAN nodes 122 may be configured to wirelessly communicate with UEs 110 , and/or one another, over a licensed medium (also referred to as the “licensed spectrum” and/or the “licensed band”), an unlicensed shared medium (also referred to as the “unlicensed spectrum” and/or the “unlicensed band”), or combination thereof. A licensed spectrum may include channels that operate in the frequency range of approximately 400 MHZ to approximately 3.8 GHz, whereas the unlicensed spectrum may include the 5 GHz band. A licensed spectrum may correspond to channels or frequency bands selected, reserved, regulated, etc., for certain types of wireless activity (e.g., wireless telecommunication network activity), whereas an unlicensed spectrum may correspond to one or more frequency bands that are not restricted for certain types of wireless activity. Whether a particular frequency band corresponds to a licensed medium or an unlicensed medium may depend on one or more factors, such as frequency allocations determined by a public-sector organization (e.g., a government agency, regulatory body, etc.) or frequency allocations determined by a private-sector organization involved in developing wireless communication standards and protocols, etc. To operate in the unlicensed spectrum, UEs 110 and the RAN nodes 122 may operate using licensed assisted access (LAA), eLAA, and/or feLAA mechanisms. In these implementations, UEs 110 and the RAN nodes 122 may perform one or more known medium-sensing operations or carrier-sensing operations in order to determine whether one or more channels in the unlicensed spectrum is unavailable or otherwise occupied prior to transmitting in the unlicensed spectrum. The medium/carrier sensing operations may be performed according to a listen-before-talk (LBT) protocol. The LAA mechanisms may be built upon carrier aggregation (CA) technologies of LTE-Advanced systems. In CA, each aggregated carrier is referred to as a component carrier (CC). In some cases, individual CCs may have a different bandwidth than other CCs. In time division duplex (TDD) systems, the number of CCs as well as the bandwidths of each CC may be the same for DL and UL. CA also comprises individual serving cells to provide individual CCs. The coverage of the serving cells may differ, for example, because CCs on different frequency bands will experience different pathloss. A primary service cell or PCell may provide a primary component carrier (PCC) for both UL and DL, and may handle radio resource control (RRC) and non-access stratum (NAS) related activities. The other serving cells are referred to as SCells, and each SCell may provide an individual secondary component carrier (SCC) for both UL and DL. The SCCs may be added and removed as required, while changing the PCC may require UE 110 to undergo a handover. In LAA, eLAA, and feLAA, some or all of the SCells may operate in the unlicensed spectrum (referred to as “LAA SCells”), and the LAA SCells are assisted by a PCell operating in the licensed spectrum. When a UE is configured with more than one LAA SCell, the UE may receive UL grants on the configured LAA SCells indicating different PUSCH starting positions within a same subframe. The PDSCH may carry user data and higher layer signaling to UEs 110 . The physical downlink control channel (PDCCH) may carry information about the transport format and resource allocations related to the PDSCH channel, among other things. The PDCCH may also inform UEs 110 about the transport format, resource allocation, and hybrid automatic repeat request (HARQ) information related to the uplink shared channel. Typically, downlink scheduling (e.g., assigning control and shared channel resource blocks to UE 110 - 2 within a cell) may be performed at any of the RAN nodes 122 based on channel quality information fed back from any of UEs 110 . The downlink resource assignment information may be sent on the PDCCH used for (e.g., assigned to) each of UEs 110 . The PDCCH uses control channel elements (CCEs) to convey the control information, wherein a number of CCEs (e.g., 6 or the like) may consists of a resource element groups (REGs), where a REG is defined as a physical resource block (PRB) in an OFDM symbol. Before being mapped to resource elements, the PDCCH complex-valued symbols may first be organized into quadruplets, which may then be permuted using a sub-block interleaver for rate matching, for example. Each PDCCH may be transmitted using one or more of these CCEs, where each CCE may correspond to nine sets of four physical resource elements known as REGs. Four quadrature phase shift keying (QPSK) symbols may be mapped to each REG. The PDCCH may be transmitted using one or more CCEs, depending on the size of the DCI and the channel condition. There may be four or more different PDCCH formats defined in LTE with different numbers of CCEs (e.g., aggregation level, L=1, 2, 4, 8, or 110). Some implementations may use concepts for resource allocation for control channel information that are an extension of the above-described concepts. For example, some implementations may utilize an extended (E)-PDCCH that uses PDSCH resources for control information transmission. The EPDCCH may be transmitted using one or more ECCEs. Similar to the above, each ECCE may correspond to nine sets of four physical resource elements known as an EREGs. An ECCE may have other numbers of EREGs in some situations. The RAN nodes 122 may be configured to communicate with one another via interface 123 . In implementations where the network 60 is an LTE system, interface 123 may be an X2 interface. The X2 interface may be defined between two or more RAN nodes 122 (e.g., two or more eNBs/gNBs or a combination thereof) that connect to evolved packet core (EPC) or CN 130 , or between two eNBs connecting to an EPC. In some implementations, the X2 interface may include an X2 user plane interface (X2-U) and an X2 control plane interface (X2-C). The X2-U may provide flow control mechanisms for user data packets transferred over the X2 interface and may be used to communicate information about the delivery of user data between eNBs or gNBs. For example, the X2-U may provide specific sequence number information for user data transferred from a master eNB (MeNB) to a secondary eNB (SeNB); information about successful in sequence delivery of PDCP packet data units (PDUs) to a UE 110 from an SeNB for user data; information of PDCP PDUs that were not delivered to a UE 110 ; information about a current minimum desired buffer size at the SeNB for transmitting to the UE user data; and the like. The X2-C may provide intra-LTE access mobility functionality (e.g., including context transfers from source to target eNBs, user plane transport control, etc.), load management functionality, and inter-cell interference coordination functionality. As shown, RAN 120 may be connected (e.g., communicatively coupled) to CN 130 . CN 130 may comprise a plurality of network elements 132 , which are configured to offer various data and telecommunications services to customers/subscribers (e.g., users of UEs 110 ) who are connected to the CN 130 via the RAN 120 . In some implementations, CN 130 may include an evolved packet core (EPC), a 5G CN, and/or one or more additional or alternative types of CNs. The components of the CN 130 may be implemented in one physical node or separate physical nodes including components to read and execute instructions from a machine-readable or computer-readable medium (e.g., a non-transitory machine-readable storage medium). In some implementations, network function virtualization (NFV) may be utilized to virtualize any or all the above-described network node roles or functions via executable instructions stored in one or more computer-readable storage mediums (described in further detail below). A logical instantiation of the CN 130 may be referred to as a network slice, and a logical instantiation of a portion of the CN 130 may be referred to as a network sub-slice. Network Function Virtualization (NFV) architectures and infrastructures may be used to virtualize one or more network functions, alternatively performed by proprietary hardware, onto physical resources comprising a combination of industry-standard server hardware, storage hardware, or switches. In other words, NFV systems may be used to execute virtual or reconfigurable implementations of one or more EPC components/functions. As shown, CN 130 , application servers (ASs) 140 , and external networks 150 may be connected to one another via interfaces 134 , 136 , and 138 , which may include IP network interfaces. Application servers 140 may include one or more server devices or network elements (e.g., virtual network functions (VNFs) offering applications that use IP bearer resources with CN 130 (e.g., universal mobile telecommunications system packet services (UMTS PS) domain, LTE PS data services, etc.). Application server 140 may also, or alternatively, be configured to support one or more communication services (e.g., voice over IP (VOIP sessions, push-to-talk (PTT) sessions, group communication sessions, social networking services, etc.) for UEs 110 via the CN 130 . Similarly, external networks 150 may include one or more of a variety of networks, including the Internet, thereby providing the mobile communication network and UEs 110 of the network access to a variety of additional services, information, interconnectivity, and other network features. As shown, example network 60 may include an NTN that may comprise one or more satellites 160 - 1 and 160 - 2 (collectively, “satellites 160 ”). Satellites 160 may be in communication with UEs 110 via service link or wireless interface 162 and/or RAN 120 via feeder links or wireless interfaces 164 (depicted individually as 164 - 1 and 164 ). In some implementations, satellite 160 may operate as a passive or transparent network relay node regarding communications between UE 110 and the terrestrial network (e.g., RAN 120 ). In some implementations, satellite 160 may operate as an active or regenerative network node such that satellite 160 may operate as a base station to UEs 110 (e.g., as a gNB of RAN 120 ) regarding communications between UE 110 and RAN 120 . In some implementations, satellites 160 may communicate with one another via a direct wireless interface (e.g., 166 ) or an indirect wireless interface (e.g., via RAN 120 using interfaces 164 - 1 and 164 - 2 ). Additionally, or alternatively, satellite 160 may include a GEO satellite, LEO satellite, or another type of satellite. Satellite 160 may also, or alternatively pertain to one or more satellite systems or architectures, such as a global navigation satellite system (GNSS), global positioning system (GPS), global navigation satellite system (GLONASS), BeiDou navigation satellite system (BDS), etc. In some implementations, satellites 160 may operate as bases stations (e.g., RAN nodes 122 ) with respect to UEs 110 . As such, references herein to a base station, RAN node 122 , etc., may involve implementations where the base station, RAN node 122 , etc., is a terrestrial network node and implementation, where the base station, RAN node 122 , etc., is a non-terrestrial network node (e.g., satellite 160 ). FIG. 2 is a timeline diagram of example 200 of secondary cell activation using a UE-specific RS. As shown, example 200 includes timeline 210 and corresponding SSB 220 - 1 , 220 - 2 , 220 - 3 , etc. (referred to collectively as “SSBs 220 ”). UE 110 - 1 may receive and process an activation command at 230 , which is subsequent to SSB 220 - 1 . In implementations where the network has not configured a TRS (e.g., the PCell has not provided UE 110 - 1 with a TRS configuration of the SCell), UE 110 - 1 may perform fine time tracking based on the first complete SSB 220 - 2 , of the corresponding TCI state, received by UE 110 - 1 . By contrast, in implementations where the network has configured a TRS (e.g., the PCell has provided UE 110 - 1 with a TRS configuration of the SCell), UE 110 - 1 may perform fine time tracking based on the TRS 240 from the SCell. As described herein, The TRS configuration may be periodic or aperiodic. In example, 200 , if the TRS configuration is periodic, DELTA_T 250 (e.g., Time_FineTiming) is the periodicity of the TRS configuration. By contrast, if the TRS configuration is aperiodic, DELTA_T 250 (e.g., Time_FineTiming) may be a duration of time between the UE finishing with processing a last activation command for PDCCH TCI, PDSCH TCI (when applicable) and a timing of a first complete available TRS 240 . As such, the techniques described herein enable fine time tracking to occur based on an SSB, periodic TRS, or aperiodic TRS. FIG. 3 is a flowchart of an example process 300 for activating a secondary cell using a UE-specific RS. Process 300 may be implemented by UE 110 . In some implementations, some or all of process 300 may be performed by one or more other systems or devices, including one or more of the devices of FIG. 1 . Additionally, process 300 may include one or more fewer, additional, differently ordered and/or arranged operations than those shown in FIG. 3 . Furthermore, as FIG. 3 and the corresponding description discuss an example process 300 for activating a secondary cell that may be performed by UE 110 , the scope of the techniques described herein include corresponding processes that may performed by a corresponding base station (e.g., RAN node 112 ), satellite, and/or other network device described in reference to FIG. 1 . As shown, process 300 may include receiving, from a PCell, a TRS configuration for a SCell during RRC configuration procedure (block 310 ). For example, a base station operating as a PCell for UE 110 may provide UE 110 a TRS configuration for another base station intended to operate as a SCell for UE 110 . As described herein, the TRS configuration may include a TRS during which UE 110 may synchronize (e.g., perform fine time tracking) with the SCell based on a UE-specific RS. In some implementations, UE 110 may also receive other types of configuration information, such as SMTC information, TCI information, etc., regarding the SCell. In some implementations, UE 110 may receive another MAC command or DCI message indicating a TRS configuration (e.g., a TRS transmission). In some implementations, when a TRS configuration is not provided, UE 110 may receive SMTC and/or TCI for the SCell and perform fine time tracking based on an SSB the SCell. Process 300 may also include receiving a SCell activation message and performing a cell search and ADC regarding the SCell (block 320 ). For example, UE 110 may receive, from the PCell, an MAC CE that includes an activation command for the SCell. In some implementations, the activation command may correspond to a carrier on the SCell. The carrier on the SCell may be part of a CA configuration (that may include one or more carriers of the PCell) for UE 110 . In response to the activation command, UE 110 may perform a cell search and AGC regarding the SCell. Process 300 may include determining, based on the TRS configuration, whether periodicity or aperiodicity applies to the TRS of the SCell (block 330 ). For example, UE 110 may determine, based on the TRS configuration received from the PCell, whether the SCell implements periodicity or aperiodicity regarding the TRS. UE 110 may also, or alternatively, determine based on the TRS configuration a transmission time of the TRS. Process 300 may include, when periodicity applies to the TRS, performing fine time tracking and/or AGC in accordance with the TRS periodicity (block 340 ). For example, UE 110 may perform fine time tracking and/or AGC based on the periodicity of the TRS. As described herein, when UE 110 performs fine time tracking and/or AGC based on the periodicity of the TRS, Time_FineTiming may be equal to a periodicity of the PRS. Process 300 may include, when aperiodicity applies, determining whether a TCI command is configured for the SCell (block 350 ). For example, when aperiodicity applies to the TRS, UE 110 may determine whether a TCI command has been configured for the SCell. In some implementations, TCI states may be provided to UE 110 as part of the RRC configuration procedure, the PCell may send a TCI command to UE 110 in a MAC CE (which may be the same or different MAC CE of the SCell activation command. Process 300 may also include, when TRS aperiodicity and a TCI configuration applies to the SCell activation, performing fine time tracking and/or AGC after the TCI command is decoded (block 360 ). For example, when TRS aperiodicity is used by SCell, UE 110 may determine whether a TCI command has been received, with respect to the SCell activation, and if so, perform fine time tracking during the TRS after the TCI command is decoded by UE 110 . Process 300 may include, when TRS aperiodicity applies, but a TCI command is not configured, performing fine time tracking and/or AGC during TRS after slot N+ (T_HARQ+3 ms)/(NR slot length) (block 370 ). For example, when UE 110 determines that TRS aperiodicity applies but a TCI command is not configured for the SCell activation, UE 110 may perform fine time tracking during a TRS occurring after a slot N+ (T_HARQ+3 ms)/(NR slot length). FIG. 4 is a sequence diagrams of an example process 400 activating a secondary sell using a UE-specific RS. As shown, example process 400 may involve UE 110 , base station 122 - 1 (operating as a PCell to UE 110 ), and base station 122 - 2 (operating as an SCell for UE 110 ). In some implementations, example process 400 may include one or more additional, alternative, fewer, or differently arranged operations, and/or devices, than those shown in FIG. 4 . Additionally, while the operations of FIG. 4 are depicted as being performed by UE 110 , base station 122 - 1 , and base station 122 - 2 , in some implementations, one or more of the operations may be performed by another device, or combination of devices, of a wireless communication network. As shown, base station 122 - 1 and UE 110 may engage in a RRC procedure, such as a RRC connection reconfiguration procedure (at 402 ). During the procedure, base station 122 - 1 may provide UE 110 with a TRS configuration for a SCell (e.g., base station 122 - 2 ). At some point, base station 122 - 1 may send a MAC CE to UE 110 , which may include a command for activating base station 122 - 2 as an SCell for UE 110 (at 404 ). The activation command may include a TRS configuration of base station 122 - 2 . In some implementations, the MAC CE may also include a TCI command, or another type of TCI, of base station 122 - 2 . In some implementations, the TCI command may be sent in a different MAC CE. In yet other implementations, no TCI command may be sent. In response to the activation command, UE 110 may initiate perform a cell search and AGC adjustments regarding base station 122 - 2 (at 406 ). Additionally, UE 110 may determine a TRS configuration of base station 122 - 2 (at 408 ). The TRS configuration may indicate a UE-specific RS that UE 110 may use to synchronize (e.g., perform fine time tracking) regarding base station 122 - 2 . In some implementations, this may include UE 110 determining whether the TRS of base station 122 - 2 is periodic or aperiodic. In some implementations, such as scenarios in which a TCI command is sent, UE 110 may also determine a TCI configuration of base station 122 - 2 . In some implementations, base station 122 - 1 may communicate a TRS trigger to UE 110 , which may cause or prompt UE 110 regarding the TRS (e.g., to begin performing fine time tracking during the TRS) (at 410 ). In some implementations, the TRS trigger may be part of a MAC command. In some implementations, the TRS may be part of DCI. In some implementations, the PCell may communicate the TRS trigger at another time, such as prior to, or concurrent with, UE 110 determining the TRS configuration. As shown, UE 110 may perform fine time tracking based on the TRS configuration for base station 122 - 2 . As described above, this may depend, in part, on whether the TRS is periodic or aperiodic. Also as described herein, in some implementations, the fine time tracking may also be based on a TCI configuration of base station 122 - 2 and when UE 110 processes the TCI command. UE 110 may continue by completing SCell activation regarding base station 122 - 2 , which may later result in a CA scenario involving UE 110 , base station 122 - 1 , and base station 122 - 2 . FIG. 5 is a diagram of an example of components of a device according to one or more implementations described herein. In some implementations, the device 500 can include application circuitry 502 , baseband circuitry 504 , Radio Frequency (RF) circuitry 506 , front-end module (FEM) circuitry 508 , one or more antennas 56 , and power management circuitry (PMC) 512 coupled together at least as shown. The components of the illustrated device 500 can be included in a UE or a RAN node. In some implementations, the device 500 can include fewer elements (e.g., a RAN node may not utilize application circuitry 502 , and instead include a processor/controller to process IP data received from a CN such as 5GC 130 or an Evolved Packet Core (EPC)). In some implementations, the device 500 can include additional elements such as, for example, memory/storage, display, camera, sensor (including one or more temperature sensors, such as a single temperature sensor, a plurality of temperature sensors at different locations in device 500 , etc.), or input/output (I/O) interface. In other implementations, the components described below can be included in more than one device (e.g., said circuitries can be separately included in more than one device for Cloud-RAN (C-RAN) implementations). The application circuitry 502 can include one or more application processors. For example, the application circuitry 502 can include circuitry such as, but not limited to, one or more single-core or multi-core processors. The processor(s) can include any combination of general-purpose processors and dedicated processors (e.g., graphics processors, application processors, etc.). The processors can be coupled with or can include memory/storage and can be configured to execute instructions stored in the memory/storage to enable various applications or operating systems to run on the device 500 . In some implementations, processors of application circuitry 502 can process IP data packets received from an EPC. The baseband circuitry 504 can include circuitry such as, but not limited to, one or more single-core or multi-core processors. The baseband circuitry 504 can include one or more baseband processors or control logic to process baseband signals received from a receive signal path of the RF circuitry 506 and to generate baseband signals for a transmit signal path of the RF circuitry 506 . Baseband circuitry 504 can interface with the application circuitry 502 for generation and processing of the baseband signals and for controlling operations of the RF circuitry 506 . For example, in some implementations, the baseband circuitry 504 can include a third generation (3G) baseband processor 504 A, a fourth generation (4G) baseband processor 504 B, a fifth generation (5G) baseband processor 504 C, or other baseband processor(s) 504 D for other existing generations, generations in development or to be developed in the future (e.g., second generation (2G), sixth generation (6G), etc.). The baseband circuitry 504 (e.g., one or more of baseband processors 504 A-D) can handle various radio control functions that enable communication with one or more radio networks via the RF circuitry 506 . In other implementations, some or all of the functionality of baseband processors 504 A-D can be included in modules stored in the memory 504G and executed via a Central Processing Unit (CPU) 504 E. The radio control functions can include, but are not limited to, signal modulation/demodulation, encoding/decoding, radio frequency shifting, etc. In some implementations, modulation/demodulation circuitry of the baseband circuitry 504 can include Fast-Fourier Transform (FFT), precoding, or constellation mapping/demapping functionality. In some implementations, encoding/decoding circuitry of the baseband circuitry 504 can include convolution, tail-biting convolution, turbo, Viterbi, or Low Density Parity Check (LDPC) encoder/decoder functionality. Implementations of modulation/demodulation and encoder/decoder functionality are not limited to these examples and can include other suitable functionality in other implementations. In some implementations, the baseband circuitry 504 can include one or more audio digital signal processor(s) (DSP) 504 F. The audio DSP(s) 504 F can include elements for compression/decompression and echo cancellation and can include other suitable processing elements in other implementations. Components of the baseband circuitry can be suitably combined in a single chip, a single chipset, or disposed on a same circuit board in some implementations. In some implementations, some or all of the constituent components of the baseband circuitry 504 and the application circuitry 502 can be implemented together such as, for example, on a system on a chip (SOC). In some implementations, the baseband circuitry 504 can provide for communication compatible with one or more radio technologies. For example, in some implementations, the baseband circuitry 504 can support communication with a NG-RAN, an evolved universal terrestrial radio access network (EUTRAN) or other wireless metropolitan area networks (WMAN), a wireless local area network (WLAN), a wireless personal area network (WPAN), etc. Implementations in which the baseband circuitry 504 is configured to support radio communications of more than one wireless protocol can be referred to as multi-mode baseband circuitry. RF circuitry 506 can enable communication with wireless networks using modulated electromagnetic radiation through a non-solid medium. In various implementations, the RF circuitry 506 can include switches, filters, amplifiers, etc. to facilitate the communication with the wireless network. RF circuitry 506 can include a receive signal path which can include circuitry to down-convert RF signals received from the FEM circuitry 508 and provide baseband signals to the baseband circuitry 504 . RF circuitry 506 can also include a transmit signal path which can include circuitry to up-convert baseband signals provided by the baseband circuitry 504 and provide RF output signals to the FEM circuitry 508 for transmission. In some implementations, the receive signal path of the RF circuitry 506 can include mixer circuitry 506 a , amplifier circuitry 506 b and filter circuitry 506 c . In some implementations, the transmit signal path of the RF circuitry 506 can include filter circuitry 506 c and mixer circuitry 506 a . RF circuitry 506 can also include synthesizer circuitry 506 d for synthesizing a frequency for use by the mixer circuitry 506 a of the receive signal path and the transmit signal path. In some implementations, the mixer circuitry 506 a of the receive signal path can be configured to down-convert RF signals received from the FEM circuitry 508 based on the synthesized frequency provided by synthesizer circuitry 506 d . The amplifier circuitry 506 b can be configured to amplify the down-converted signals and the filter circuitry 506 c can be a low-pass filter (LPF) or band-pass filter (BPF) configured to remove unwanted signals from the down-converted signals to generate output baseband signals. Output baseband signals can be provided to the baseband circuitry 504 for further processing. In some implementations, the output baseband signals can be zero-frequency baseband signals, although this is not a requirement. In some implementations, mixer circuitry 506 a of the receive signal path can comprise passive mixers, although the scope of the implementations is not limited in this respect. In some implementations, the mixer circuitry 506 a of the transmit signal path can be configured to up-convert input baseband signals based on the synthesized frequency provided by the synthesizer circuitry 506 d to generate RF output signals for the FEM circuitry 508 . The baseband signals can be provided by the baseband circuitry 504 and can be filtered by filter circuitry 506 c. In some implementations, the mixer circuitry 506 a of the receive signal path and the mixer circuitry 506 a of the transmit signal path can include two or more mixers and can be arranged for quadrature downconversion and upconversion, respectively. In some implementations, the mixer circuitry 506 a of the receive signal path and the mixer circuitry 506 a of the transmit signal path can include two or more mixers and can be arranged for image rejection (e.g., Hartley image rejection). In some implementations, the mixer circuitry 506 a of the receive signal path and the mixer circuitry 506 a can be arranged for direct downconversion and direct upconversion, respectively. In some implementations, the mixer circuitry 506 a of the receive signal path and the mixer circuitry 506 a of the transmit signal path can be configured for super-heterodyne operation. In some implementations, the output baseband signals and the input baseband signals can be analog baseband signals, although the scope of the implementations is not limited in this respect. In some alternate implementations, the output baseband signals and the input baseband signals can be digital baseband signals. In these alternate implementations, the RF circuitry 506 can include analog-to-digital converter (ADC) and digital-to-analog converter (DAC) circuitry and the baseband circuitry 504 can include a digital baseband interface to communicate with the RF circuitry 506 . In some dual-mode implementations, a separate radio IC circuitry can be provided for processing signals for each spectrum, although the scope of the implementations is not limited in this respect. In some implementations, the synthesizer circuitry 506 d can be a fractional-N synthesizer or a fractional N/N+1 synthesizer, although the scope of the implementations is not limited in this respect as other types of frequency synthesizers can be suitable. For example, synthesizer circuitry 506 d can be a delta-sigma synthesizer, a frequency multiplier, or a synthesizer comprising a phase-locked loop with a frequency divider. The synthesizer circuitry 506 d can be configured to synthesize an output frequency for use by the mixer circuitry 506 a of the RF circuitry 506 based on a frequency input and a divider control input. In some implementations, the synthesizer circuitry 506 d can be a fractional N/N+1 synthesizer. In some implementations, frequency input can be provided by a voltage controlled oscillator (VCO), although that is not a requirement. Divider control input can be provided by either the baseband circuitry 504 or the application circuitry 502 depending on the desired output frequency. In some implementations, a divider control input (e.g., N) can be determined from a look-up table based on a channel indicated by the application circuitry 502 . Synthesizer circuitry 506 d of the RF circuitry 506 can include a divider, a delay-locked loop (DLL), a multiplexer and a phase accumulator. In some implementations, the divider can be a dual modulus divider (DMD) and the phase accumulator can be a digital phase accumulator (DPA). In some implementations, the DMD can be configured to divide the input signal by either N or N+1 (e.g., based on a carry out) to provide a fractional division ratio. In some example implementations, the DLL can include a set of cascaded, tunable, delay elements, a phase detector, a charge pump and a D-type flip-flop. In these implementations, the delay elements can be configured to break a VCO period up into Nd equal packets of phase, where Nd is the number of delay elements in the delay line. In this way, the DLL provides negative feedback to help ensure that the total delay through the delay line is one VCO cycle. In some implementations, synthesizer circuitry 506 d can be configured to generate a carrier frequency as the output frequency, while in other implementations, the output frequency can be a multiple of the carrier frequency (e.g., twice the carrier frequency, four times the carrier frequency) and used in conjunction with quadrature generator and divider circuitry to generate multiple signals at the carrier frequency with multiple different phases with respect to each other. In some implementations, the output frequency can be a LO frequency (fLO). In some implementations, the RF circuitry 506 can include an IQ/polar converter. FEM circuitry 508 can include a receive signal path which can include circuitry configured to operate on RF signals received from one or more antennas 56 , amplify the received signals and provide the amplified versions of the received signals to the RF circuitry 506 for further processing. FEM circuitry 508 can also include a transmit signal path which can include circuitry configured to amplify signals for transmission provided by the RF circuitry 506 for transmission by one or more of the one or more antennas 56 . In various implementations, the amplification through the transmit or receive signal paths can be done solely in the RF circuitry 506 , solely in the FEM circuitry 508 , or in both the RF circuitry 506 and the FEM circuitry 508 . In some implementations, the FEM circuitry 508 can include a TX/RX switch to switch between transmit mode and receive mode operation. The FEM circuitry can include a receive signal path and a transmit signal path. The receive signal path of the FEM circuitry can include an LNA to amplify received RF signals and provide the amplified received RF signals as an output (e.g., to the RF circuitry 506 ). The transmit signal path of the FEM circuitry 508 can include a power amplifier (PA) to amplify input RF signals (e.g., provided by RF circuitry 506 ), and one or more filters to generate RF signals for subsequent transmission (e.g., by one or more of the one or more antennas 56 ). In some implementations, the PMC 512 can manage power provided to the baseband circuitry 504 . In particular, the PMC 512 can control power-source selection, voltage scaling, battery charging, or DC-to-DC conversion. The PMC 512 can often be included when the device 500 is capable of being powered by a battery, for example, when the device is included in a UE. The PMC 512 can increase the power conversion efficiency while providing desirable implementation size and heat dissipation characteristics. While FIG. 5 shows the PMC 512 coupled only with the baseband circuitry 504 . However, in other implementations, the PMC 512 may be additionally or alternatively coupled with, and perform similar power management operations for, other components such as, but not limited to, application circuitry 502 , RF circuitry 506 , or FEM circuitry 508 . In some implementations, the PMC 512 can control, or otherwise be part of, various power saving mechanisms of the device 500 . For example, if the device 500 is in an RRC_Connected state, where it is still connected to the RAN node as it expects to receive traffic shortly, then it can enter a state known as Discontinuous Reception Mode (DRX) after a period of inactivity. During this state, the device 500 can power down for brief intervals of time and thus save power. If there is no data traffic activity for an extended period of time, then the device 500 can transition off to an RRC_Idle state, where it disconnects from the network and does not perform operations such as channel quality feedback, handover, etc. The device 500 goes into a very low power state and it performs paging where again it periodically wakes up to listen to the network and then powers down again. The device 500 may not receive data in this state; in order to receive data, it can transition back to RRC_Connected state. An additional power saving mode can allow a device to be unavailable to the network for periods longer than a paging interval (ranging from seconds to a few hours). During this time, the device is totally unreachable to the network and can power down completely. Any data sent during this time incurs a large delay and it is assumed the delay is acceptable. Processors of the application circuitry 502 and processors of the baseband circuitry 504 can be used to execute elements of one or more instances of a protocol stack. For example, processors of the baseband circuitry 504 , alone or in combination, can be used execute Layer 3, Layer 2, or Layer 1 functionality, while processors of the application circuitry 502 can utilize data (e.g., packet data) received from these layers and further execute Layer 4 functionality (e.g., transmission communication protocol (TCP) and user datagram protocol (UDP) layers). As referred to herein, Layer 3 can comprise a radio resource control (RRC) layer, described in further detail below. As referred to herein, Layer 2 can comprise a medium access control (MAC) layer, a radio link control (RLC) layer, and a packet data convergence protocol (PDCP) layer, described in further detail below. As referred to herein, Layer 1 can comprise a physical (PHY) layer of a UE/RAN node, described in further detail below. FIG. 6 is a diagram of example interfaces of baseband circuitry according to one or more implementations described herein. As discussed above, the baseband circuitry 504 of FIG. 5 can comprise processors 504 A- 904 E and a memory 504G utilized by said processors. Each of the processors 504 A- 204 E can include a memory interface, 604 A- 604 E, respectively, to send/receive data to/from the memory 504G. The baseband circuitry 504 can further include one or more interfaces to communicatively couple to other circuitries/devices, such as a memory interface 612 (e.g., an interface to send/receive data to/from memory external to the baseband circuitry 504 ), an application circuitry interface 614 (e.g., an interface to send/receive data to/from the application circuitry 502 of FIG. 5 ), an RF circuitry interface 616 (e.g., an interface to send/receive data to/from RF circuitry 506 of FIG. 5 ), a wireless hardware connectivity interface 618 (e.g., an interface to send/receive data to/from Near Field Communication (NFC) components, Bluetooth® components (e.g., Bluetooth® Low Energy), Wi-Fi® components, and other communication components), and a power management interface 620 (e.g., an interface to send/receive power or control signals to/from the PMC 512 ). Examples herein can include subject matter such as a method, means for performing acts or blocks of the method, at least one machine-readable medium including executable instructions that, when performed by a machine (e.g., a processor (e.g., processor, etc.) with memory, an application-specific integrated circuit (ASIC), a field programmable gate array (FPGA), or the like) cause the machine to perform acts of the method or of an apparatus or system for concurrent communication using multiple communication technologies according to implementations and examples described. In example 1, a user Equipment (UE) device may comprise radio frequency (RF) circuitry configured to communicate with a wireless communication network; a memory device configured to store instructions; and one or more processors, connected to the RF circuitry and memory device, and configured to perform the instructions to: receive, via the RF circuitry and from a first carrier of a first base station operating as a primary cell for the UE, a tracking reference signal (TRS) configuration indicating a UE-specific reference signal (RS) corresponding to a second carrier of the first base station or a third carrier of a second base station designated to operate as a secondary cell for the UE; receive, via the RF circuitry from the primary cell, an activation command for the secondary cell; and perform, in response to the activation command, a secondary cell activation procedure regarding the second base station, the secondary cell activation procedure including fine time tracking based on the UE-specific RS. In example 2, the one or more processors is configured to determine, based on the TRS configuration, a periodicity of the UE-specific RS; and perform the fine time tracking in accordance with the periodicity of the UE-specific RS. In example 3, the one or more processors is configured to determine, based on the TRS configuration, that the UE-specific RS is an aperiodic RS; and perform the fine time tracking based on a transmission time of the activation command. In example 4, the fine time tracking is performed based on a duration of time associated with a hybrid automatic repeat request (HARQ) message associated with the activation command. In example 5, the one or more processors is configured to detect a TRS trigger based on a media access control (MAC) command or downlink control information (DCI); and perform the fine time tracking in response to the TRS trigger. In example 6, the one or more processors is configured to determine, based on the TRS configuration, that the UE-specific RS is an aperiodic RS; determine that a transmission configuration indicator (TCI) command has been received; and perform the fine time tracking following a decoding of a transmission configuration indicator (TCI) command. In example 7, the one or more processors is configured to perform the fine time tracking and automatic gain control (AGC) during transmission of the UE-specific RS. In example 8, the one or more processors is configured to receive the TRS configuration during a radio resource control (RRC) procedure involving the first base station. In example 9, the one or more processors is configured to perform the fine time tracking based on a cell-specific RS when the TRS configuration is not received. In example 10, baseband (BB) circuitry, of a User Equipment (UE) device, comprises circuitry to: receive, from a first carrier of a first base station operating as a primary cell for the UE, a tracking reference signal (TRS) configuration indicating a UE-specific reference signal (RS) corresponding to a second carrier of the first base station or a third carrier of a second base station designated to operate as a secondary cell for the UE; receive, from the primary cell, an activation command for the secondary cell; and perform, in response to the activation command, a secondary cell activation procedure regarding the second base station, the secondary cell activation procedure including fine time tracking based on the UE-specific RS. In examples 11-18, the BB circuitry of example 10 being further modified by applying one or more, or any combination, of the device features of examples 2-9 as BB circuitry features of example 10. In example 19, a method, performed by a User Equipment (UE), may comprise: receiving, from a first carrier of a first base station operating as a primary cell for the UE, a tracking reference signal (TRS) configuration indicating a UE-specific reference signal (RS) corresponding to a second carrier of the first base station or a third carrier of a second base station designated to operate as a secondary cell for the UE; receiving, from the primary cell, an activation command for the secondary cell; and performing, in response to the activation command, a secondary cell activation procedure regarding the second base station, the secondary cell activation procedure including fine time tracking based on the UE-specific RS. In examples 20-27, the method of example 19 being further modified by applying one or more, or any combination, of the features of examples 2-9 as method features of example 19. As example 28, a user Equipment (UE) device may comprise means for receiving, from a first carrier of a first base station operating as a primary cell for the UE, a tracking reference signal (TRS) configuration indicating a UE-specific reference signal (RS) corresponding to a second carrier of the first base station or a third carrier of a second base station designated to operate as a secondary cell for the UE; means for receiving, from the primary cell, an activation command for the secondary cell; and means for performing, in response to the activation command, a secondary cell activation procedure regarding the second base station, the secondary cell activation procedure including fine time tracking based on the UE-specific RS. In examples 29-36, the UE of example 28 being further modified by applying one or more, or any combination, of the features of examples 2-9 as features (e.g., means for features) of example 28. In example 37, a computer readable medium comprising instructions that when executed by a processor, cause the processor to: receive, from a first carrier of a first base station operating as a primary cell for the UE, a tracking reference signal (TRS) configuration indicating a UE-specific reference signal (RS) corresponding to a second carrier of the first base station or a third carrier of a second base station designated to operate as a secondary cell for the UE; receive, from the primary cell, an activation command for the secondary cell; and perform, in response to the activation command, a secondary cell activation procedure regarding the second base station, the secondary cell activation procedure including fine time tracking based on the UE-specific RS. In examples 38-45, the computer readable medium of example 37 being further modified by applying one or more, or any combination, of the device features of examples 2-9 as computer readable medium features of example 37. The above description of illustrated examples, implementations, aspects, etc., of the subject disclosure, including what is described in the Abstract, is not intended to be exhaustive or to limit the disclosed aspects to the precise forms disclosed. While specific examples, implementations, aspects, etc., are described herein for illustrative purposes, various modifications are possible that are considered within the scope of such examples, implementations, aspects, etc., as those skilled in the relevant art can recognize. In this regard, while the disclosed subject matter has been described in connection with various examples, implementations, aspects, etc., and corresponding Figures, where applicable, it is to be understood that other similar aspects can be used or modifications and additions can be made to the disclosed subject matter for performing the same, similar, alternative, or substitute function of the subject matter without deviating therefrom. Therefore, the disclosed subject matter should not be limited to any single example, implementation, or aspect described herein, but rather should be construed in breadth and scope in accordance with the appended claims below. In particular regard to the various functions performed by the above described components or structures (assemblies, devices, circuits, systems, etc.), the terms (including a reference to a “means”) used to describe such components are intended to correspond, unless otherwise indicated, to any component or structure which performs the specified function of the described component (e.g., that is functionally equivalent), even though not structurally equivalent to the disclosed structure which performs the function in the herein illustrated exemplary implementations. In addition, while a particular feature may have been disclosed with respect to only one of several implementations, such feature may be combined with one or more other features of the other implementations as may be desired and advantageous for any given or particular application. As used herein, the term “or” is intended to mean an inclusive “or” rather than an exclusive “or”. That is, unless specified otherwise, or clear from context, “X employs A or B” is intended to mean any of the natural inclusive permutations. That is, if X employs A; X employs B; or X employs both A and B, then “X employs A or B” is satisfied under any of the foregoing instances. In addition, the articles “a” and “an” as used in this application and the appended claims should generally be construed to mean “one or more” unless specified otherwise or clear from context to be directed to a singular form. Furthermore, to the extent that the terms “including”, “includes”, “having”, “has”, “with”, or variants thereof are used in either the detailed description and the claims, such terms are intended to be inclusive in a manner similar to the term “comprising.” Additionally, in situations wherein one or more numbered items are discussed (e.g., a “first X”, a “second X”, etc.), in general the one or more numbered items can be distinct or they can be the same, although in some situations the context may indicate that they are distinct or that they are the same. It is well understood that the use of personally identifiable information should follow privacy policies and practices that are generally recognized as meeting or exceeding industry or governmental requirements for maintaining the privacy of users. In particular, personally identifiable information data should be managed and handled so as to minimize risks of unintentional or unauthorized access or use, and the nature of authorized use should be clearly indicated to users.",en,PATENT_APPLICATION
080-421-392-297-242,US,20240388420,A1,2024-11-21,US_20240388420_A1_20241121,en,US,20240388420,A1,2024-11-21,US,18659987,2024-05-09,LOW LATENCY METADATA DECRYPTION USING HASH AND PSEUDORANDOM FUNCTIONS,en,US,"Cryptography Research, Inc.","San Jose, CA",NL,Michael Alexander Hamburg,‘s-Hertogenbosch,US,1,Evan Lawrence Erickson,"Chapel Hill, NC",NL,2,Ajay Kapoor,Eindhoven,H04L9/06,I,F,H04L9/0643,I,F,H04L9/0618,I,L,H04L9/0656,I,L,US,20240388420,A1,2024-11-21,080-421-392-297-242,1,US,20240388420,A1,2024-11-21,080-421-392-297-242,1,UNKNOWN,"Systems and techniques for cryptographically protecting data in a computer memory are disclosed. The techniques include dividing the data into a first portion and a second portion, encrypting the first portion of the data to create a first stored form of the data, encrypting the second portion of the data, and storing, in the computer memory, the first stored form of the data and a second stored form of the data. The techniques include, to encrypt the second portion, calculating a hash based on the first stored form of the data, applying a first pseudorandom function to the hash to obtain a bit sequence, and combining the bit sequence with the second portion of the data to obtain the second stored form of the data.",en,"1 . A cryptographic circuit for cryptographically protecting data in a computer memory, the cryptographic circuit to: divide the data into a first portion and a second portion; encrypt the first portion of the data to create a first stored form of the data; encrypt the second portion of the data, wherein to encrypt the second portion, the cryptographic circuit is further to: calculate, using a hash function, a hash based on the first stored form of the data; apply a first pseudorandom function to the hash to obtain a bit sequence; and combine the bit sequence with the second portion of the data to obtain a second stored form of the data; and store, in the computer memory, the first stored form of the data and the second stored form of the data.","2 . The cryptographic circuit of claim 1 , wherein the first portion is encrypted using a block-based cipher algorithm.","3 . The cryptographic circuit of claim 1 , wherein the hash function is at least one of: a universal hash function, or an almost-universal hash function.","4 . The cryptographic circuit of claim 1 , wherein at least part of the hash is used to compute a message authentication code.","5 . The cryptographic circuit of claim 1 , wherein the hash is further based on an address of the first stored form of the data.","6 . The cryptographic circuit of claim 1 , wherein the hash is further based on a key, wherein the key comprises at least one of: a key derived with a second pseudorandom function based on an address of the first stored form of the data, a global key, or a key obtained from a lookup table based on the address of the first stored form of the data.","7 . The cryptographic circuit of claim 6 , wherein the key is refreshed when the address of the first stored form of the data is cleared.","8 . The cryptographic circuit of claim 1 , wherein the second stored form of the data is further encrypted before being stored in the computer memory.","9 . A memory buffer device comprising: a cryptographic circuit for cryptographically accessing data in a computer memory, wherein the cryptographic circuit is to: load, from the computer memory, a first portion of the data and a second portion of the data; decrypt the first portion of the data to obtain a first plaintext form of the data; decrypt the second portion of the data, wherein to decrypt the second portion, the cryptographic circuit is further to: calculate, using a hash function, a hash based on the first portion of the data; apply a first pseudorandom function to the hash to obtain a bit sequence; and combine the bit sequence with the second portion of the data to obtain a second plaintext form of the data.","10 . The memory buffer device of claim 9 , wherein the cryptographic circuit is to decrypt the first portion and the second portion in parallel.","11 . The memory buffer device of claim 9 , wherein the first portion is decrypted using a block-based cipher algorithm.","12 . The memory buffer device of claim 9 , wherein the hash function is at least one of: a universal hash function, or an almost-universal hash function.","13 . The memory buffer device of claim 9 , wherein the hash is further based on an address of the first portion of the data.","14 . The memory buffer device of claim 9 , wherein the hash is further based on a key, wherein the key comprises at least one of: a key derived with a second pseudorandom function based on an address of the first portion of the data, a global key, or a key obtained from a lookup table based on the address of first portion of the data.","15 . The memory buffer device of claim 14 , wherein the key is refreshed when the address of the first portion of the data is cleared.","16 . The memory buffer device of claim 9 , wherein the second plaintext form is further decrypted.","17 . A method of accessing cryptographically protected data of a computer memory, the method comprising: receiving, on a first interface, the data, wherein the data comprises a first portion and a second portion; decrypting the first portion of the data to obtain a first plaintext form of the data; decrypting the second portion of the data, wherein decrypting the second portion comprises: calculating, using a hash function, a hash based on the first portion of the data; applying a first pseudorandom function to the hash to obtain an output; and combining the output with the second portion of the data to obtain a second plaintext form of the data; and transmitting, on a second interface, the first plaintext form of the data and the second plaintext form of the data.","18 . The method of claim 17 , wherein decrypting the first portion and decrypting the second portion are performed in parallel.","19 . The method of claim 17 , wherein the first portion is decrypted using a block-based cipher algorithm.","20 . The method of claim 17 , wherein the hash function is at least one of: a universal hash function, or an almost-universal hash function.",en,"RELATED APPLICATIONS This application claims the benefit of U.S. Provisional Patent Application No. 63/502,523, filed May 16, 2023, entitled “LOW LATENCY METADATA DECRYPTION USING HASH AND PSEUDORANDOM FUNCTIONS”, the contents of which are incorporated by reference in its entirety herein. TECHNICAL FIELD The disclosure pertains to computing applications, more specifically to systems and techniques that cryptographically protect data in a computer memory. BACKGROUND Modern computer systems generally include one or more memory devices, such as those on a memory module. The memory module may include, for example, one or more random access memory (RAM) devices or dynamic random access memory (DRAM) devices. A memory device may include memory banks made up of memory cells that a memory controller or memory client accesses through a command interface and a data interface within the memory device. The memory device may be used to store encrypted data. BRIEF DESCRIPTION OF THE DRAWINGS The present disclosure is illustrated by way of example, and not by way of limitation, in the figures of the accompanying drawings. FIG. 1 is a block diagram of a memory system with a memory module that includes a cryptographic circuit for reduced latency metadata decryption, according to at least one embodiment. FIG. 2 is a diagram illustrating examples of storing cache line data and corresponding metadata, according to at least one embodiment. FIG. 3A is a data flow diagram illustrating a method of data and metadata encryption using a cryptographic circuit, according to at least one embodiment. FIG. 3B is a data flow diagram illustrating a method of data decryption and reduced latency metadata decryption using a cryptographic circuit, according to at least one embodiment. FIG. 4 is a flow diagram of an example method of reduced latency metadata decryption using a cryptographic circuit, according to at least one embodiment. FIG. 5 is a block diagram of an example computer system operating in accordance with one or more aspects of the present disclosure. DETAILED DESCRIPTION The following description sets forth numerous specific details, such as examples of specific systems, components, methods, and so forth, in order to provide a good understanding of several embodiments of the present disclosure. It will be apparent to one skilled in the art, however, that at least some embodiments of the present disclosure may be practiced without these specific details. In other instances, well-known components or methods are not described in detail or presented in simple block diagram format to avoid obscuring the present disclosure unnecessarily. Thus, the specific details set forth are merely exemplary. Particular implementations may vary from these exemplary details and still be contemplated to be within the scope of the present disclosure. Datacenter architectures are evolving to support the workloads of emerging applications in artificial intelligence and machine learning that require a high-speed, low-latency, cache-coherent interconnect. Compute Express Link® (CXL®) is an industry-supported cache-coherent interconnect for processors, memory expansion, and accelerators. The CXL® technology defines mechanisms called Integrity and Data Encryption (IDE) for providing confidentiality, integrity, and replay protection for data transferred over a CXL® link. The CXL® IDE mechanism may secure traffic within a Trusted Execution Environment (TEE) of multiple components using one or more cryptographic algorithms. In some embodiments, an inline memory encryption (IME) module may be used to encrypt/decrypt data at rest using one or more cryptographic algorithms, such as an Advanced Encryption Standard (AES) XOR-Encrypt-XOR with Tweak and Block Ciphertext Stealing (XTS) algorithm (hereinafter AES-XTS algorithm). The AES-XTS algorithm may use a block-based cipher (e.g., AES-128, AES-256, etc.) for encryption and decryption. The AES-XTS algorithm may divide data into fixed-sized blocks (e.g., based on the size of the block cipher) and encrypt/decrypt each block separately using AES encryption and a tweakable block cipher. The tweak value may be determined from the block number and a key that is shared between encryption and decryption operations. Other encryption and authentication algorithms may be used. Storage and encryption of cache line metadata (also referred to as “metadata” herein) associated with cache line data is a desirable capability for confidential computing over, for example, a CXL interface. The metadata may, for example, contain coherency information for a Modified-Exclusive-Shared-Invalid (MESI) cache coherency protocol, TEE ownership tracking information, a message authentication code (MAC), a poison bit, device private metadata, and/or the like. In some instances, the IME algorithm AES-XTS can be used to encrypt the metadata. However, the CXL® protocol is highly sensitive to latency, and AES-XTS can incur a latency penalty when it is used to encrypt cache line metadata in addition to the corresponding cache line data. Hardware implementations of AES may include an AES engine and/or AES cores that perform a series of transformations that operate on input date to produce an output. In some instances, AES cores can take 14 cycles to decrypt encrypted cache line data and an additional 14 cycles to decrypt encrypted cache line metadata, totaling 28 cycles. For example, four 128-bit AES cores of an AES engine can decrypt 512-bit cache line data on a first pass and can decrypt 16 bits of corresponding metadata on a second pass. On the first pass, the four 128-bit AES cores can decrypt the 512-bit cache line data to output a 512-bit result using 14 cycles. Because AES is a block cipher, it requires an input that matches the block size (e.g., 128-bits). When the metadata is less than the block size, it needs to be padded (e.g., using bits of the encrypted cache line data) so it matches the block size. For example, a 16-bit metadata may be padded with 112 bits of data to obtain a 128-bit block that can be decrypted. Then on a second pass, the padded metadata may be decrypted using a 128-bit AES core to obtain a 128-bit output using 14 cycles. Thus, decrypting cache line data and metadata encrypted using AES-XTS may require 28 clock cycles. Although the AES block cipher used in XTS mode is discussed throughout, it should be seen as a non-limiting example. AES may be replaced by another block cipher (e.g., SM4) and the XTS mode may be replaced by another mode that suffers from additional latency when encrypting/decrypting data that is less than a full block size. Aspects of the present disclosure overcome these challenges and others by providing a cryptographic circuit that can decrypt encrypted metadata using a few (e.g., 1, 2, 3) additional cycles, thereby reducing the overall latency required to read protected (e.g., encrypted) data and metadata from a memory device. In some embodiments, the cryptographic circuit may use the AES-XTS algorithm to perform cryptographic operations (e.g., encryption, decryption) on cache line data and may use a hash function and a pseudorandom function to generate a bit sequence that can be combined with the cache line metadata for encryption/decryption. For example, after the cache line data has been encrypted (e.g., using AES-XTS) to obtain a ciphertext, the ciphertext may be provided as input into a hash function. In some embodiments, the hash function is a universal hash function. In some embodiments, the hash function is an almost-universal hash function. In some embodiments, the hash function receives as an additional input an address associated with the cache line data, an address associated with the metadata, and/or a key. The hash function may generate a hash based on the provided input(s). The hash may be provided as input to a pseudorandom function. In some embodiments, the pseudorandom function is a cryptographic pseudorandom function, such as AES. In some embodiments, the pseudorandom function may also receive, as input, a key. The pseudorandom function may be a block cipher, and the hash function may generate an output that matches the block size of the block cipher. The pseudorandom function may generate an output bit sequence that is combined (e.g., XOR'd) with the metadata to obtain the encrypted metadata. Both the encrypted cache line data and the encrypted metadata may be stored in a memory device. During decryption of the data, the encrypted cache line data and the encrypted metadata may be loaded from the memory device. During decryption of the encrypted cache line data (e.g., using AES-XTS which takes 14 cycles), the bit sequence required to decrypt the encrypted metadata may be calculated. To generate the bit sequence, the ciphertext (and in some embodiments, the address of the cache line data, the address of the metadata, a key, and/or another data) is (are) provided as input to the hash function. The hash function may take a few (e.g., less than 3 cycles). The resulting hash (and in some embodiments, a key) may be provided to the pseudorandom function to generate the bit sequence that, when combined (e.g., XOR'd) with the encrypted metadata, produces the original, plaintext metadata. The pseudorandom function may take about the same number of cycles as the decryption of the cache line data (e.g., 14). Thus, decrypting cache line data encrypted using AES-XTS and metadata encrypted using a combination of a hash function and a pseudorandom function may require less than 17 clock cycles. The advantages of the disclosed techniques include but are not limited to a decreased latency when decrypting encrypted cache line metadata of a memory device. System Architecture FIG. 1 is a block diagram of a memory system 100 with a memory module 102 that includes a cryptographic circuit 110 for reduced latency metadata decryption, according to at least one embodiment. In one embodiment, the memory module 102 includes a memory buffer device 104 and one or more DRAM device(s) 114 . In one embodiment, the memory buffer device 104 is coupled to one or more DRAM device(s) 114 and one or more host(s) 106 . In one embodiment, the memory buffer device 104 includes a CXL controller 108 and a memory controller 112 . The CXL controller 108 is coupled to one or more host(s) 106 . The memory controller 112 is coupled to the one or more DRAM devices 114 . In at least one embodiment, the memory buffer device 104 is implemented in a memory expansion device, such as a CXL memory expander SoC of a CXL NVM module or a CXL module. In at least one embodiment, the memory buffer device 104 includes a cryptographic circuit 110 for reduced latency decryption of metadata associated with cache lines being read from a DRAM device(s) 114 . In some embodiments, the cryptographic circuit 110 may be an in-line memory encryption (IME) block and/or used in conjunction with a separate IME block. The cryptographic circuit 110 may receive data from a host(s) 106 (e.g., via the CXL controller 108 ), may encrypt the data, and may provide the data for storage in a DRAM device(s) 114 (e.g., via the memory controller 112 ). In at least one embodiment, the received data is unencrypted (e.g., plaintext data) and is encrypted by the cryptographic circuit 110 before storage. In at least one embodiment, the received data is already encrypted (e.g., ciphertext data) and is further encrypted by the cryptographic circuit 110 before storage. The received data may include a first portion (e.g., cache line data) and a second portion (e.g., cache line metadata). In at least one embodiment, the received data is encrypted for transit (e.g., between host(s) 106 and memory module 102 ) and decrypted by the CXL controller 108 to obtain a plaintext form of the data. The plaintext form of the data may then be encrypted for storage by the cryptographic circuit 110 . In at least one embodiment, the first portion of the data is encrypted and the second portion of the data is plaintext before both being encrypted by the cryptographic circuit 110 . In at least one embodiment, the CXL controller 108 includes two interfaces, a host memory interface (e.g., CXL.mem) and a management interface (e.g., CLX.io). The host memory interface can receive, from the host(s) 106 , one or more memory access commands of a remote memory protocol, such as Compute Express Link (CXL) protocol, Gen-Z, Open Memory Interface (OMI), Open Coherent Accelerator Processor Interface (OpenCAPI), or the like. The management interface can receive, from the host(s) 106 , one or more management commands of the remote memory protocol. The cryptographic circuit 110 can include one or more cryptographic cores, such as AES-128 cores and/or AES-256 cores. In at least one embodiment, the cryptographic circuit 110 includes at least four 128-bit AES cores that can simultaneously decrypt a 512-bit cache line. In some embodiments, the cryptographic circuit 110 includes a hash function and a pseudorandom function. In at least one embodiment, the hash function is an almost-universal hash function, such as GHASH. In another embodiment, the hash function is a universal hash function. In at least one embodiment, the pseudorandom function is an AES function, such as AES-128. The cryptographic circuit 110 may receive data for storage (e.g., from a host(s) 106 , from the CXL controller 108 ) including a first portion and a second portion. In at least one embodiment, the first portion is 512 bits of cache line data and the second portion is less than 128 bits (e.g., the size of the block cipher used to encrypt the first portion of the data) of metadata. The metadata may include coherency information for a MESI cache coherency protocol, TEE ownership tracking information, a MAC, a poison bit, device private metadata, error correcting codes (ECC), and/or the like. In at least one embodiment, the first portion is encrypted using four 128-bit AES cores using the AES-XTS algorithm to obtain a ciphertext. The AES-XTS algorithm may use an encryption key and a tweak key. In some embodiments, the encryption key is the first 128 bits of a 256-bit key and the tweak key is the last 128 bits of the 256-bit key. In another embodiment, the first portion is encrypted using SM4 cores (e.g., four 128-bit cores) using the XTS block-cipher mode. The ciphertext (e.g., stored form of the data) generated by encrypting the first portion of the data may be provided as input into a hash function of the cryptographic circuit 110 . In some embodiments, the hash function of the cryptographic circuit 110 also receives as an input (e.g., in addition to the ciphertext) an address associated with the first portion of the data, an address associated with the second portion of the data, a key, and/or other security relevant information. The hash generated by the hash function based on the provided input(s) may be provided as input to a pseudorandom function of the cryptographic circuit 110 . In at least one embodiment, the pseudorandom function of the cryptographic circuit 110 also receives, as input, a key. The pseudorandom function may be a block cipher that requires an input of a fixed size (e.g., 128 bits). Because the length of the ciphertext may be greater (or smaller) than the required length of the input of the pseudorandom function, the hash function may be used to ensure the input to the pseudorandom function is the correct size. In at least one embodiment, the length of the ciphertext is the same as the length required for the input of the pseudorandom function, and the ciphertext is provided directly to the pseudorandom function without going through the hash function. In another embodiment, a pseudorandom function is used that can accept a variable-length input (e.g., TurboSHAKE) and the ciphertext is provided directly to the pseudorandom function without going through the hash function. In at least one embodiment, the pseudorandom function outputs a bit sequence that is combined (e.g., XOR'd) with the second portion of the data to encrypt the second portion. The cryptographic circuit 110 may then provide (e.g., to the memory controller 112 ) the encrypted first portion and the encrypted second portion for storage (e.g., in the DRAM device(s) 114 ). In at least one embodiment, the encrypted second portion is further encrypted before being stored. The second portion (e.g., metadata) can be stored and transferred as side-band metadata or in-line metadata, as illustrated and described below with respect to FIG. 2 . In at least one embodiment, the key used for the hash function is different than the key used for the pseudorandom function. The key used for the hash function and the key used for the pseudorandom function may both be different than the encryption key and the tweak key used during encryption of the first portion of the data. In at least one embodiment, one of the keys is derived using a pseudorandom function based on an address of data (e.g., the first portion of the data, the second portion of the data, etc.). In at least one embodiment, one of the keys is a global key that is generated when a system is initialized. In at least one embodiment, one of the keys is obtained from a lookup table based on an address of data (e.g., the first portion of the data, the second portion of the data, etc.). In at least one embodiment, all the keys are derived from a shared base key (e.g., while loading data from RAM). In at least one embodiment, a key is refreshed (e.g., modified, regenerated, updated, etc.) when the address of the first portion of the data is cleared, ensuring a unique key is used for encryption at each memory address. The cryptographic circuit 110 may receive a request (e.g., from a host(s) 106 , from the CXL controller 108 ) to load data from storage. Cryptographic circuit 110 may then retrieve (e.g., from the memory controller 112 , from a DRAM device(s) 114 ) the data, which includes, in at least one embodiment, a first portion that is 512 bits of cache line data and a second portion is less than 128 bits (e.g., the size of the block cipher used to encrypt the first portion of the data) of metadata. The cryptographic circuit 110 may then decrypt the received data. In one embodiment, the cryptographic circuit 110 decrypts the first portion of the data and the second portion of the data in parallel. For example, in at least one embodiment, the 512-bit encrypted cache line data is decrypted using four 128-bit AES cores, which takes about 14 cycles. During those 14 cycles, a hash of the encrypted cache line data is generated using the hash function of the cryptographic circuit 110 . The hash is then provided to the pseudorandom function of the cryptographic circuit 110 to generate an output bit sequence. The output bit sequence can be combined (e.g., XOR'd) with the encrypted second portion of the data to decrypt the second portion. If the second portion was further encrypted before being stored, the second portion may be decrypted using an inverse of the encryption function used for the further encryption before being combined with the output bit sequence. If the hash function and/or the pseudorandom functions were keyed using additional inputs during the encryption process, the same additional inputs are used during decryption of the first portion of the data and the second portion of the data. In at least one embodiment, the pseudorandom function of the cryptographic circuit 110 used to encrypt/decrypt the second portion of the data is an AES function (e.g., AES-128). Therefore, generating the output of the pseudorandom function during decryption of the second portion of the data may take 14 cycles. In some embodiments, a separate (e.g., fifth) AES core is used to perform the pseudorandom function while the first portion of the data is being decrypted. Generating the hash using the hash function of the cryptographic circuit 110 may take a few cycles (e.g., less than 3). Thus, decrypting the first potion of the data and the second portion of the data, when performed in parallel, may take less than 17 cycles. Using the hash function and the pseudorandom function to generate a bit sequence used to encrypt the metadata may not provide perfect security, but in some instances, it may be advantageous to have reduced latency decryption of metadata instead of the additional latency that would be required for improved security. FIG. 2 is a diagram illustrating examples of storing cache line data 204 and 210 and corresponding metadata 206 and 212 , according to at least one embodiment. In general, the metadata can be stored as side-band metadata 202 or in-line metadata 208 . In side-band metadata 202 , the metadata 206 is stored alongside cache line data 204 , and the metadata 206 can be accessible when the cache line data 204 is read from memory. In in-line metadata 208 , the metadata 212 is stored in another location than the cache line data 210 , such as in a static RAM (SRAM) or DRAM. When the cache line data 210 is read, an additional memory read would be performed to retrieve the metadata 212 . FIG. 3A is a data flow diagram illustrating a method 300 of data and metadata encryption using a cryptographic circuit, according to at least one embodiment. In at least one embodiment, method 300 is performed by the cryptographic circuit 110 . Data 302 and metadata 318 may be received to be encrypted. Data 302 may be encrypted by AES-XTS encryption 304 to obtain data ciphertext 306 . Data ciphertext 306 may then be provided to hash function 308 . In some embodiments, hash function 308 may receive as additional inputs key 310 and/or address 312 , depicted with dashed lines. Key 310 may be derived using a pseudorandom function based on the address of the data 302 or the address of the metadata 318 , a global key generated during initialization of a system and used for multiple address locations, a key obtained from a lookup table based on the address of the data 302 or the address of the metadata 318 , or the like. Address 312 may be the address of the data 302 or the address of the metadata 318 . In at least one embodiment, hash function 308 may receive another data as additional input, such as an identifier of a virtual machine that is accessing data or some other security-relevant information. The output hash generated by hash function 308 may be provided as an input to pseudorandom function 314 , which may be a cryptographic pseudorandom function. In some embodiments, only a portion of the output hash generated by hash function 308 is provided as an input to pseudorandom function 314 . In at least one embodiment, pseudorandom function 314 also receives as an input key 316 . In another embodiment, pseudorandom function 314 may receive another data as an additional input. Key 316 may be derived using a pseudorandom function based on the address of the data 302 or the address of the metadata 318 , a global key generated during initialization of a system and used for multiple address locations, a key obtained from a lookup table based on the address of the data 302 or the address of the metadata 318 , or the like. The pseudorandom function used to derive key 310 and/or key 316 can be different than the pseudorandom function 314 . The output bit sequence generated by pseudorandom function 314 may be combined ( 320 ) with metadata 318 to obtain metadata ciphertext 322 , which may then be stored as side-band metadata with data ciphertext 306 or as in-line metadata in a separate location from data ciphertext 306 . In at least one embodiment, a MAC is calculated for the encrypted data to ensure message integrity. The MAC may be calculated by the cryptographic circuit 110 or another circuit. In at least one embodiment, at least one intermediate calculation generated during execution of the hash function can be shared between generating the MAC and generating the bit sequence to encrypt the metadata. FIG. 3B is a data flow diagram illustrating a method 350 of data decryption and reduced latency metadata decryption using a cryptographic circuit, according to at least one embodiment. In at least one embodiment, method 350 is performed by the cryptographic circuit 110 . Data ciphertext 352 and metadata ciphertext 368 may be received to be decrypted. Data ciphertext 352 may be decrypted by AES-XTS decryption 354 to obtain data 356 . Simultaneous to data ciphertext 352 being decrypted, data ciphertext 352 (or a copy of data ciphertext 352 ) may be provided to hash function 358 to generate a hash. In some embodiments, hash function 358 receives as additional inputs key 360 and/or address 362 . The hash may be provided as an input to pseudorandom function 364 , which may generate an output bit sequence. In at least one embodiment, pseudorandom function 364 receives as an additional input key 366 . The output of pseudorandom function 364 may be combined ( 370 ) with metadata ciphertext 368 to obtain metadata 372 . Key 360 , address 362 , and key 366 may be the same, respectively, as key 310 , address 312 , and key 316 for a given ciphertext. FIG. 4 is a flow diagram of an example method 400 of reduced latency metadata decryption using a cryptographic circuit, according to at least one embodiment. The method 400 may be performed by processing logic that may comprise hardware (e.g., circuitry, dedicated logic, programmable logic, microcode, etc.), software (e.g., instructions run on a processing device to perform hardware simulation), or a combination thereof. For example, method 400 may be performed by one or more CPUs, graphics processing units (GPUs), parallel processing units (PPUs), dedicated accelerator circuits, and the like, or any combination thereof. In one embodiment, the method 400 is performed by the cryptographic circuit 110 of FIG. 1 . Processing logic performing method 400 may, at block 410 , receive, on a first interface, data including a first portion and a second portion. The first interface may be coupled to a storage device (e.g., DRAM), a memory controller, a message bus, or another component of a computing system. In at least one embodiment, the first portion is 512 bits long and the second portion is less than 128 bits long. At block 420 , the processing logic may decrypt the first portion of the data to obtain a first plaintext form of the data. In one embodiment, the decryption may be performed using the AES-XTS algorithm. At block 430 , the processing logic may decrypt the second portion of the data to obtain a second plaintext form of the data. As depicted in the callout box, to decrypt the second portion of the data, the processing logic may, at block 440 , calculate a hash based on the first portion of the data. The hash may be calculated using a hash function, such as a universal hash function or an almost-universal hash function. In at least one embodiment, the universal hash function receives as an additional input a key and/or an address. At block 450 , the processing logic may apply a pseudorandom function to the hash to obtain an output. The output may be a bit sequence. The output of the pseudorandom function may be the same length as the hash generated by the hash function (e.g., 128 bits). At block 460 , the processing logic may combine the output with the second portion of the data to obtain the second plaintext form of the data. At block 470 , the processing logic may transmit, on a second interface, the first plaintext form of the data and the second plaintext form of the data. The second interface may be coupled to a host device, a CXL controller, a message bus, or another component of a computing system. FIG. 5 is a block diagram of an example computer system 500 operating in accordance with one or more aspects of the present disclosure. The computer system may be connected (e.g., networked) to other computer systems in a LAN, an intranet, an extranet, or the Internet. The computer system may operate in the capacity of a server in a client-server network environment. The computer system may be a personal computer (PC), a tablet computer, a set-top box (STB), a Personal Digital Assistant (PDA), a mobile phone, a camera, a video camera, or any device capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that device. Further, while only a single computer system is illustrated, the term “computer” shall also be taken to include any collection of computers that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methods discussed herein. The exemplary computer system 500 includes a processing device 502 , a main memory 504 (e.g., read-only memory (ROM), flash memory, dynamic random access memory (DRAM) such as synchronous DRAM (SDRAM)), a static memory 506 (e.g., flash memory, static random access memory (SRAM)), and a data storage device 518 , which communicate with each other via a bus 530 . Processing device 502 (which can include processing logic 526 ) represents one or more general-purpose processing devices such as a microprocessor, central processing unit, or the like. More particularly, the processing device 502 may be a complex instruction set computing (CISC) microprocessor, reduced instruction set computing (RISC) microprocessor, very long instruction word (VLIW) microprocessor, or a processor implementing other instruction sets or processors implementing a combination of instruction sets. The processing device 502 may also be one or more special-purpose processing devices such as an application specific integrated circuit (ASIC), a field programmable gate array (FPGA), a digital signal processor (DSP), network processor, or the like. The processing device 502 may be configured to execute instructions 522 for implementing the cryptographic circuit 110 of FIG. 1 and to perform the operations discussed herein (e.g., method 300 of FIG. 3A , method 350 of FIG. 3B , and/or method 400 of FIG. 4 ). The computer system 500 may further include a network interface device 508 . The computer system 500 also may include a video display unit 510 (e.g., a liquid crystal display (LCD) or a cathode ray tube (CRT)), an alphanumeric input device 512 (e.g., a keyboard), a cursor control device 514 (e.g., a mouse), and a signal generation device 516 (e.g., a speaker). In one illustrative example, the video display unit 510 , the alphanumeric input device 512 , and the cursor control device 514 may be combined into a single component or device (e.g., an LCD touch screen). The data storage device 518 may include a computer-readable storage medium 524 on which is stored the instructions 522 embodying any one or more of the methodologies or functions described herein. The instructions 522 may also reside, completely or at least partially, within the main memory 504 and/or within the processing device 502 during execution thereof by the computer system 500 , the main memory 504 and the processing device 502 also constituting computer-readable media. In some implementations, the instructions 522 may further be transmitted or received over a network via the network interface device 508 . While the computer-readable storage medium 524 is shown in the illustrative examples to be a single medium, the term “computer-readable storage medium” should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of instructions. The term “computer-readable storage medium” shall also be taken to include any medium that is capable of storing, encoding or carrying a set of instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the present disclosure. The term “computer-readable storage medium” shall accordingly be taken to include, but not be limited to, solid-state memories, optical media, and magnetic media. Although the operations of the methods herein are shown and described in a particular order, the order of the operations of each method may be altered so that certain operations may be performed in an inverse order or so that certain operations may be performed, at least in part, concurrently with other operations. In certain implementations, instructions or sub-operations of distinct operations may be in an intermittent and/or alternating manner. It is to be understood that the above description is intended to be illustrative, and not restrictive. Many other implementations will be apparent to those of skill in the art upon reading and understanding the above description. The scope of the disclosure should, therefore, be determined with reference to the appended claims, along with the full scope of equivalents to which such claims are entitled. In the above description, numerous details are set forth. It will be apparent, however, to one skilled in the art, that the aspects of the present disclosure may be practiced without these specific details. In some instances, well-known structures and devices are shown in block diagram form, rather than in detail, in order to avoid obscuring the present disclosure. Some portions of the detailed descriptions above are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here, and generally, conceived to be a self-consistent sequence of steps leading to a desired result. The steps are those requiring physical manipulations of physical quantities. Usually, though not necessarily, these quantities take the form of electrical or magnetic signals capable of being stored, transferred, combined, compared, and otherwise manipulated. It has proven convenient at times, principally for reasons of common usage, to refer to these signals as bits, values, elements, symbols, characters, terms, numbers, or the like. It should be borne in mind, however, that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise, as apparent from the following discussion, it is appreciated that throughout the description, discussions utilizing terms such as “receiving,” “determining,” “selecting,” “storing,” “analyzing,” or the like, refer to the action and processes of a computer system, or similar electronic computing device, that manipulates and transforms data represented as physical (electronic) quantities within the computer system's registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage, transmission or display devices. The present disclosure also relates to an apparatus for performing the operations herein. This apparatus may be specially constructed for the required purposes, or it may comprise a general purpose computer selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a computer-readable storage medium, such as, but not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, and magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMS, EEPROMs, magnetic or optical cards, or any type of media suitable for storing electronic instructions, each operatively coupled to a computer system bus. The algorithms and displays presented herein are not inherently related to any particular computer or other apparatus. Various general purpose systems may be used with programs in accordance with the teachings herein, or it may prove convenient to construct more specialized apparatus to perform the required method steps. The required structure for a variety of these systems will appear as set forth in the description. In addition, aspects of the present disclosure are not described with reference to any particular programming language. It will be appreciated that a variety of programming languages may be used to implement the teachings of the present disclosure as described herein. Aspects of the present disclosure may be provided as a computer program product, or software, that may include a machine-readable medium having stored thereon instructions, which may be used to program a computer system (or other electronic devices) to perform a process according to the present disclosure. A machine-readable medium includes any mechanism for storing or transmitting information in a form readable by a machine (e.g., a computer). For example, a machine-readable (e.g., computer-readable) medium includes a machine (e.g., a computer) readable storage medium (e.g., read-only memory (“ROM”), random access memory (“RAM”), magnetic disk storage media, optical storage media, flash memory devices, etc.). The words “example” or “exemplary” are used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as “example” or “exemplary” is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the words “example” or “exemplary” is intended to present concepts in a concrete fashion. As used in this application, the term “or” is intended to mean an inclusive “or” rather than an exclusive “or”. That is, unless specified otherwise, or clear from context, “X includes A or B” is intended to mean any of the natural inclusive permutations. That is, if X includes A; X includes B; or X includes both A and B, then “X includes A or B” is satisfied under any of the foregoing instances. In addition, the articles “a” and “an” as used in this application and the appended claims should generally be construed to mean “one or more” unless specified otherwise or clear from context to be directed to a singular form. Moreover, use of the term “an implementation” or “one implementation” or “an implementation” or “one implementation” throughout is not intended to mean the same implementation unless described as such. Furthermore, the terms “first,” “second,” “third,” “fourth,” etc. as used herein are meant as labels to distinguish among different elements and may not necessarily have an ordinal meaning according to their numerical designation. Whereas many alterations and modifications of the disclosure will no doubt become apparent to a person of ordinary skill in the art after having read the foregoing description, it is to be understood that any particular implementation shown and described by way of illustration is in no way intended to be considered limiting. Therefore, references to details of various implementations are not intended to limit the scope of the claims, which in themselves recite only those features regarded as the disclosure.",en,PATENT_APPLICATION
087-838-455-817-072,US,20240385641,A1,2024-11-21,US_20240385641_A1_20241121,en,US,20240385641,A1,2024-11-21,US,18323188,2023-05-24,CLOCK PERIOD SYNTHESIS,en,US,"Groq, Inc.","Mountain View, CA",US,James David Sproch,"Monte Sereno, CA",G06F1/08,I,F,G06F1/08,I,F,US,20240385641,A1,2024-11-21,087-838-455-817-072,1,US,20240385641,A1,2024-11-21,087-838-455-817-072,1,UNKNOWN,"Clock period synthesis for fine-grain power management is provided. Methods are described for enabling clock waveform synthesis for, in some embodiments, tensor or graphical processors that enable shorter runtime latency, higher computational job throughput, more efficient power management, and a lower implementation cost than alternative clock waveform methods. This Abstract and the independent Claims are concise signifiers of embodiments of the claimed inventions. The Abstract does not limit the scope of the claimed inventions.",en,"1 . A digital circuit for clock waveform synthesis for each individual instruction or operational cycle of a processor, comprising: an instruction control unit (ICU) that supplies instructions to control the generation of clock signals; a clock pulse synthesis (CPS) controller connected to the ICUa clock pulse synthesis (CPS) controller that decodes the instructions supplied to the ICU, and generates clock waveform parameter signals; a shift register that is clocked by a high frequency clock that operates at frequency high that the nominal processor clock frequency, wherein the high frequency clock is generated by a phase-locked loop circuit on the processor; wherein the shift register is comprised of one or more toggle flip-flop registers that are initialized by using the clock waveform parameter signals supplied by the CPS controller; and a bypass multiplexer that supplies either a clock signal that is output from the shift registers, or for the phase locked loop that provides the nominal processor clock signal, the output of the multiplexer connected to a clock driver on the processor.",en,"CROSS REFERENCE TO RELATED APPLICATIONS This application claims the benefit of priority to U.S. Provisional Application No. 63/502,567, filed May 16, 2023, and entitled “POWER MANAGEMENT OF POWER REGULATOR DURING HIGH CURRENT EVENTS,” the entirety of which is expressly incorporated herein by reference. COPYRIGHT NOTICE This patent document can be exactly reproduced as it appears in the files of the United States Patent and Trademark Office, but the assignee(s) otherwise reserves all rights in any subsets of included original works of authorship in this document protected by 17 USC 102 (a) of the U.S. copyright law. SPECIFICATION—DISCLAIMERS In the following Background, Summary, and Detailed Description, paragraph headings are signifiers that do not limit the scope of an embodiment of a claimed invention (ECIN). The citation or identification of any publication signifies neither relevance nor use as prior art. A paragraph for which the font is all italicized signifies text that exists in one or more patent specifications filed by the assignee(s). A writing enclosed in double quotes (“”) signifies an exact copy of a writing that has been expressed as a work of authorship. Signifiers, such as a word or a phrase enclosed in single quotes (‘’), signify a term that as of yet has not been defined and that has no meaning to be evaluated for, or has no meaning in that specific use (for example, when the quoted term ‘module’ is first used) until defined. FIELD(S) OF TECHNOLOGY This disclosure has general significance in the field of power management in processors, in particular, the synthesis of clock waveforms for more efficient power management in high-speed processors. This information is limited to use in the searching of the prior art. BACKGROUND The operating frequency of a computer processor's system clock fundamentally impacts key performance metrics such as latency, throughput, peak power, energy required to perform a computation, and the rate of change of power supply load current. Common methods of manipulating the frequency of the clock generator, such as setting the clock frequency of a processor to a particular value during execution of an entire algorithm, may lack sufficient granularity or responsiveness to fully optimize system performance metrics. Integrated circuits, such as tensor and graphical processors, typically operate in several different modes such as high computational activity, low computational activity, and quiescent or sleep state. Overall system performance optimization requires different clock waveforms for each different mode, but dynamically changing the clock frequency has many limitations and incurs substantial implementation and operational costs, especially in the common situation where PLL techniques are incorporated in the clock generator. For example, clock frequency synthesis controllers often have coarse granularity and can provide only a relatively small number of discrete operating frequencies. The switchover mechanism must guarantee waveform integrity during all clock phases, so switching to a different frequency may take several clock cycles. Changing the PLL reference clock frequency or multiplier value may produce indeterminate waveforms for many cycles as the PLL attempts to lock in on new reference conditions. A clock waveform generator that overcomes these limitations would enable improved integrated circuit performance. SUMMARY This Summary, together with any Claims, is a brief set of signifiers for at least one ECIN (which can be a discovery, see 35 USC 100 (a); and see 35 USC 100 (j)), for use in commerce for which the Specification and Drawings satisfy 35 USC 112. In one or more ECINs disclosed herein, clock period synthesis (CPS) methods are disclosed that enable shorter runtime latency, higher computational job throughput, more efficient power management, and a lower implementation cost than existing clock waveform methods for high-speed processors. In some embodiments of the ECINs disclosed herein, the clock period is selectively increased or decreased to energy more efficiently than inserting ‘No Operation’ instructions (NOPs), and which also makes it easier to enable software support in the compiler. In some embodiments of the ECINs disclosed herein, the CPS methods can be specified by the user in a Service Level Agreement (SLA), for example, with the use specifying a clock period and waveform that minimizes power consumption, that reduces peak consumption, or that minimizes time of execution of the algorithm. In other embodiments, some CPS methods are scheduled by a compiler and enabled by the processor during execution when an upcoming power problem is anticipated. In some embodiments of the ECINs disclosed herein, the CPS circuit comprises a digital logic circuit that generates a unique clock waveform (period and duty cycle) for each individual instruction or operational cycle of a processor. The ability to provide a different clock waveform during the operation of each individual instruction cycle enables: a. faster operational performance by using the shortest period that satisfies instruction-specific timing constraints,b. a simple mechanism for limiting peak power on a per-cycle or cycle-aggregate basis,c. a simple mechanism for limiting the di/dt ramp slope for changes in load current, andd. the ability to optimize performance and yield by adjusting the duty cycle according to instruction-specific needs. In some embodiments of the ECINs disclosed herein, CPS is distinct from traditional clock generation methods that generate only one or a few operating frequencies, without the edge placement precision of CPS, and without the ability to provide a different waveform on each cycle. Advantageously, CPS uses a fully-decoded high-speed shift register instead of a traditional approach of using a counter or encoded Finite State Machine. The CPS fully-decoded shift register operates at a much higher speed than traditional approaches because there are no extra logic elements between the sequential elements. Circuits with fewer logic levels can operate at higher speeds. In some embodiments of the ECINs disclosed herein, CPS reuses the same shift register sequential elements for the clock high time and clock low time, and for edges that start or end on the rising or falling edge of the high-speed clock. Reusing the same shift register for four different purposes is more efficient than traditional clock generation approaches that may require four different circuits for these four purposes. Reusing the shift register saves chip layout area, improves yield (thus lowering manufacturing costs), and saves power. In some embodiments of the ECINs disclosed herein, CPS uses an efficient activation mechanism to provide edge-placement resolution at half of the high frequency clock period, in contrast to traditional approaches that operate at only the coarser resolution of the full high frequency clock period. Support for half-period resolution with only a tiny incremental circuit area and power cost is a significant advantage. In some embodiments of the ECINs disclosed herein, CPS can operate with a very high frequency input clock using physical design techniques that are applicable to the specific CPS microarchitecture. Some of the high-performance physical design techniques used by CPS may not be applicable to traditional approaches. Microarchitecture-specific techniques include: A. Counter-flow clock distribution network routing to maximize performance while ensuring reliable, power-efficient operation.B. Exploiting the useful-skew of the monotonically increasing latency of the clock arrival time to reduce the demands on time-critical enable signal propagation.C. Double Data Rate preload parameter delivery.D. All of the complex command processing arithmetic is performed in a low-speed clock domain, with fully decoded parameters passed into the high-speed clock domain.E. Self-synchronized shift register reload operation.F. Self-synchronized clock pulse multiplexer.G. Metastability-immune toggle flop to cross from the high-speed clock domain to the low-speed clock domain. Other high-speed implementation techniques that are not microarchitecture-specific include: a. Pre-placement of critical high-speed cells to control parasitic interconnect resistance and capacitance,b. Pre-routing of critical clock and other high-fanout signals to balance the delay relative delaysc. Replicating sequential cells and buffer drivers to minimize the circuit latencyd. Operation in a high voltage power domain for faster switching operatione. Use of extremely low threshold voltage transistorsf. Use of high-speed library cells with extra wide transistorsg. Use of high drive strength cells to operate faster and also to minimize electromigration and aging for circuits operating in the high-speed clock domain. This Summary does not completely signify any ECIN. While this Summary can signify at least one essential element of an ECIN enabled by the Specification and Figures, the Summary does not signify any limitation in the scope of any ECIN. BRIEF DESCRIPTION OF DRAWINGS The patent or application file contains at least one drawing executed in color. Copies of this patent or patent application publication with color drawing(s) will be provided by the Office upon request and payment of the necessary fee. The following Detailed Description, Figures, and Claims signify the uses of and progress enabled by one or more ECINs. All of the Figures are used only to provide knowledge and understanding and do not limit the scope of any ECIN. Such Figures are not necessarily drawn to scale. A brief list of Figures is below. FIG. 1 depicts a system for compiling programs to be executed on a tensor processor. FIGS. 2A and 2B illustrate instruction and data flow in a processor having a functional slice architecture. FIG. 3A-3C depict various clock waveforms and related energy usage. FIG. 4 depicts an abstract floor plan for a processor with circuitry for a clock period synthesis system. FIGS. 5A-5C depict deterministic relativity showing a global clock frequency compared to a virtual time across multiple devices. FIG. 6 depicts a computer system suitable for enabling embodiments of the claimed inventions. The Figures can have the same, or similar, reference signifiers in the form of labels (such as alphanumeric symbols, e.g., reference numerals), and can signify a similar or equivalent function or use. Further, reference signifiers of the same type can be distinguished by appending to the reference label a dash and a second label that distinguishes among the similar signifiers. If only the first label is used in the Specification, its use applies to any similar component having the same label irrespective of any other reference labels. In the Figures, reference signs can be omitted as is consistent with accepted engineering practice; however, a skilled person will understand that the illustrated components are understood in the context of the Figures as a whole, of the accompanying writings about such Figures, and of the embodiments of the claimed inventions. DETAILED DESCRIPTION The Figures and Detailed Description, only to provide knowledge and understanding, signify at least one ECIN. To minimize the length of the Detailed Description, while various features, structures or characteristics can be described together in a single embodiment, they also can be used in other embodiments without being written about. Variations of any of these elements, and modules, processes, machines, systems, manufactures or compositions disclosed by such embodiments and/or examples are easily used in commerce. The Figures and Detailed Description signify, implicitly or explicitly, advantages and improvements of at least one ECIN for use in commerce. In the Figures and Detailed Description, numerous specific details can be described to enable at least one ECIN. Any embodiment disclosed herein signifies a tangible form of a claimed invention. To not diminish the significance of the embodiments and/or examples in this Detailed Description, some elements that are known to a skilled person can be combined together for presentation and for illustration purposes and not be specified in detail. To not diminish the significance of these embodiments and/or examples, some well-known processes, machines, systems, manufactures or compositions are not written about in detail. However, a skilled person can use these embodiments and/or examples in commerce without these specific details or their equivalents. Thus, the Detailed Description focuses on enabling the inventive elements of any ECIN. Where this Detailed Description refers to some elements in the singular tense, more than one element can be depicted in the Figures and like elements are labeled with like numerals. FIG. 1 illustrates a system 100 for compiling programs to be executed on a tensor processor, and for generating power usage information for the compiled programs, according to an embodiment. The system 100 includes a user device 102 , a server 110 , and a processor 120 . Each of these components, and their sub-components (if any) are described in greater detail below. Although a particular configuration of components is described herein, in other embodiments the system 100 have different components and these components perform the functions of the system 100 in a different order or using a different mechanism. For example, while FIG. 1 illustrates a single server 110 , in other embodiments, compilation, assembly, and power usage functions are performed on different devices. For example, in some embodiments, at least a portion of the functions performed by the server 110 are performed by the user device 102 . The user device 102 comprises any electronic computing device, such as a personal computer, laptop, or workstation, which uses an Application Program Interface (API) 104 to construct programs to be run on the processor 120 . The server 110 receives a program specified by the user at the user device 102 , and compiles the program to generate a compiled program 114 . In some embodiments, a compiled program 114 enables a data model for predictions that processes input data and makes a prediction from the input data. Examples of predictions are category classifications made with a classifier, or predictions of time series values. In some embodiments, the prediction model describes a machine learning model that includes nodes, tensors, and weights. In one embodiment, the prediction model is specified as a TensorFlow model, the compiler 112 is a TensorFlow compiler and the processor 120 is a tensor processor. In another embodiment, the prediction model is specified as a PyTorch model, the compiler is a PyTorch compiler. In other embodiments, other machine learning specification languages and compilers are used. For example, in some embodiments, the prediction model defines nodes representing operators (e.g., arithmetic operators, matrix transformation operators, Boolean operators, etc.), tensors representing operands (e.g., values that the operators modify, such as scalar values, vector values, and matrix values, which may be represented in integer or floating-point format), and weight values that are generated and stored in the model after training. In some embodiments, where the processor 120 is a tensor processor having a functional slice architecture, the compiler 112 generates an explicit plan for how the processor will execute the program, by translating the program into a set of operations that are executed by the processor 120 , specifying when each instruction will be executed, which functional slices will perform the work, and which stream registers will hold the operands. This type of scheduling is known as “deterministic scheduling”. This explicit plan for execution includes information for explicit prediction of excessive power usage by the processor when executing the program. The assembler 116 receives compiled programs 114 , generated by the compiler 112 , and performs final compilation and linking of the scheduled instructions to generate a compiled binary. In some embodiments, the assembler 114 maps the scheduled instructions indicated in the compiled program 112 to the hardware of the server 110 , and then determines the exact component queue in which to place each instruction. The processor 120 , e.g. is a hardware device with a massive number of matrix multiplier units that accepts a compiled binary assembled by the assembler 116 , and executes the instructions included in the compiled binary. The processor 120 typically includes one or more blocks of circuity for matrix arithmetic, numerical conversion, vector computation, short-term memory, and data permutation/switching. Once such processor 120 is a tensor processor having a functional slice architecture. In some embodiments, the processor 120 comprises multiple tensor processors connected together. Example Processor FIGS. 2A and 2B illustrate instruction and data flow in a processor having a functional slice architecture, in accordance with some embodiments. One enablement of processor 200 is as an application specific integrated circuit (ASIC), and corresponds to processor 120 illustrated in FIG. 1 . The functional units of processor 200 (also referred to as “functional tiles”) are aggregated into a plurality of functional process units (hereafter referred to as “slices”) 205 , each corresponding to a particular function type in some embodiments. For example, different functional slices of the processor correspond to processing units for MEM (memory), VXM (vector execution module), MXM (matrix execution module), NIM (numerical interpretation module), and SXM (switching and permutation module). In other embodiments, each tile may include an aggregation of functional units such as a tile having both MEM and execution units by way of example. As illustrated in FIGS. 2A and 2B , each slice corresponds to a column of N functional units extending in a direction different (e.g., orthogonal) to the direction of the flow of data. The functional units of each slice can share an instruction queue (not shown) that stores instructions, and an instruction control unit (ICU) 210 that controls execution flow of the instructions. The instructions in a given instruction queue are executed only by functional units in the queue's associated slice and are not executed by another slice of the processor. In other embodiments, each functional unit has an associated ICU that controls the execution flow of the instructions. Processor 200 also includes communication lanes to carry data between the functional units of different slices. Each communication lane connects to each of the slices 205 of processor 200 . In some embodiments, a communication lane 220 that connects a row of functional units of adjacent slices is referred to as a “super-lane”, and comprises multiple data lanes, or “streams”, each configured to transport data values along a particular direction. For example, in some embodiments, each functional unit of processor 200 is connected to corresponding functional units on adjacent slices by a super-lane made up of multiple lanes. In other embodiments, processor 200 includes communication devices, such as a router, to carry data between adjacent functional units. By arranging the functional units of processor 200 into different functional slices 205 , the on-chip instruction and control flow of processor 200 is decoupled from the data flow. Since many types of data are acted upon by the same set of instructions, what is important for visualization is visualizing the flow of instructions, not the flow of data. For some embodiments, FIG. 2A illustrates the flow of instructions within the processor architecture, while FIG. 2B illustrates the flow of data within the processor architecture. As illustrated in FIGS. 2A and 2B , the instructions and control signals flow in a first direction across the functional units of processor 200 (e.g., along the length of the functional slices 205 ), while the data flows 220 flow in a second direction across the functional units of processor 200 (e.g., across the functional slices) that is non-parallel to the first direction, via the communication lanes (e.g., super-lanes) connecting the slices. In some embodiments, the functional units in the same slice execute instructions in a ‘staggered’ fashion where instructions are issued tile-by-tile within the slice over a period of N cycles. For example, the ICU for a given slice may, during a first clock cycle, issues an instruction to a first tile of the slice (e.g., the bottom tile of the slice as illustrated in FIG. 1B , closest to the ICU of the slice), which is passed to subsequent functional units of the slice over subsequent cycles. That is, each row of functional units (corresponding to functional units along a particular super-lane) of processor 200 executes the same set of instructions, albeit offset in time, relative to the functional units of an adjacent row. The functional slices of the processor are arranged such that operand data read from a memory slice is intercepted by different functional slices as the data moves across the chip, and results flow in the opposite direction where they are then written back to memory. For example, a first data flow from a first memory slice flows in a first direction (e.g., towards the right), where it is intercepted by a VXM slice that performs a vector operation on the received data. The data flow then continues to an MXM slice which performs a matrix operation on the received data. The processed data then flows in a second direction opposite from the first direction (e.g., towards the left), where it is again intercepted by a VXM slice to perform an accumulate operation, and then written back to the memory slice. In some embodiments, the functional slices of the processor are arranged such that data flow between memory and functional slices occur in both the first and second directions. For example, a second data flow originating from a second memory slice that travels in the second direction towards a second slice, where the data is intercepted and processed by a VXM slice before traveling to the second MXM slice. The results of the matrix operation performed by the second MXM slice then flow in the first direction back towards the second memory slice. In some embodiments, stream registers are located along a super-lane of the processor. The stream registers are located between functional slices of the processor to facilitate the transport of data (e.g., operands and results) along each super-lane. For example, within the memory region of the processor, stream registers are located between sets of four MEM units. The stream registers are architecturally visible to the compiler, and serve as the primary hardware structure through which the compiler has visibility into the program's execution. Each functional unit of the set contains stream circuitry configured to allow the functional unit to read or write to the stream registers in either direction of the super-lane. In some embodiments, each stream register is implemented as a collection of registers, corresponding to each stream of the super-lane, and sized based upon the basic data type used by the processor (e.g., if the TSP's basic data type is an INT8, each register may be 8-bits wide). In some embodiments, in order to support larger operands (e.g., FP16 or INT32), multiple registers are collectively treated as one operand, where the operand is transmitted over multiple streams of the super-lane. All of these functional features-superlanes of functional units, slices of instruction flow, handling of different types of integers and floating-point numbers, occurring trillions of times a second, create complicated power flows and possible disruptive power fluctuations that could negatively impact the performance of the processor. However, given the deterministic nature of executions by the processor, any disruptive power fluctuations (such as voltage droop) can be determined before execution of the program, with information (such as processor instructions, and timing for such instructions) about such fluctuations being supplied by the compiler to the processor, for the processor to use during program execution to mitigate the fluctuations. Processor Power Control In some of the ECINs disclosed here, clock period synthesis is used to achieve more efficient power management in a processor, especially tensor and graphical processors which perform billions and trillions of floating-point operations per second. A large number of such operations that are executed at the same time, or nearly at the same time, can create potentially damaging electric current flows in the processor, which can cause heat flows that are damaging, making it important to minimize changes in current flow (di/dt) during execution of a program. In some of the ECINs disclosed herein, clock period synthesis is enabled by adding additional hardware and software instructions to a processor. Clock Waveform and Power Usage FIG. 3 depicts exemplary clock waveforms, and power usage by the processor driven by the waveforms. Power consumption is a function of a processor's clock waveform, and is roughly proportional to the capacitance (C) of all of the processor's switches multiplied by the square of the processors main voltage supply (Vdd) divided by the period (P) of the clock waveform (C*Vdd*Vdd/P). A processor, for example, using a 1 nanosecond clock period will consume twice as much power as a processor using a 2 nanosecond clock period, albeit performing the algorithm approximately twice as fast. In FIG. 3 , the black waveform represents the energy used by a hypothetical processor for each respective time period. The red waveform represents the corresponding power dissipated with a 1 nS clock period. The green waveform represents the lower power for a clock with a slower 2 nS period intended to operate the circuit such that the maximum power is less than the dotted red line at 10 W. The cyan waveform represents the use of CPS to dynamically change the clock period such that the period is set to E/10 nS long so that the maximum power is less than the dotted red line at 10 W. Note that in FIG. 3A , the cyan and green waveforms are purposefully not drawn to scale to make these principles easier to illustrate. Refer now to FIG. 3B where the green waveform is drawn to scale, showing that the total runtime of the program for the green configuration is twice as long as the red waveform. The cyan waveform In FIG. 3B is also drawn to scale showing how some clock periods (i.e. when the power would have been above the red dotted line) are lengthened to lower the power, and the clock periods that would have been below the dotted red line are shortened such that the total CPS runtime is much faster than the green waveform, and may in some circumstances be faster than the red waveform as this example shows. Clock Period Synthesis—Hardware In some of the ECINs disclosed herein, the processor comprises the following four elements: a High Frequency Clock (HFC) generated by an on-chip Phase-Locked Loop (PLL) circuit where the period of the HFC is preferably shorter than the nominal period of the main clock (ChipClock) period; a waveform generator to produce the more useful ChipClock waveforms disclosed herein; a duration logic block to preload values for the waveform generator; and an instruction control unit (ICU) to provide instructions for the CPS methods disclosed herein. ChipClock waveform resolution typically is half of the HFC period, representing the smallest increment of change for the ChipClock period. The duration of half of the HFC period is called the High Frequency Clock Phase Period (HFCPP). For example, the HFC period for the TSP tensor processor from Groq, Inc., is about 27.78 picoseconds. As an example, an HFC period that is one-eighth the nominal ChipClock period enables an HFCPP that is 1/16th of the nominal ChipClock period. This HFCPP enables a clock period waveform resolution granularity of plus-or-minus 6.25%. ChipClock periods that are enabled are integer multiples of the HFCPP. The multiple does not need to be a power of two. The chip reset signal sets the ChipClock period to the Default ChipClock duration. The DefaultChipClock period can be overwritten using a configuration register. The configuration register also has a MinClock Period field which is the minimum number of HFCPP periods allowed for ChipClock, and a EnableClockChange flag that prohibits any ChipClock period changes. The default value of the MinClockPeriod minimum ChipClock period register is equal to the hardware value of the DefaultChipClock period. The DefaultChipClock period should never be set to a value less than the MinClockPeriod. The default value of the EnableClockChange flag is FALSE to prohibit clock period changes until a configuration register write operation sets the value of the flag to TRUE. After the processor has booted (restarted) and a program is running, if the EnableClockChange flag is set to TRUE, ChipClock period changes are determined exclusively by subsequent software instructions, and a configuration register write operation should not be used to change the period until after the user instructions have completed. The minimum ChipClock period is eight times the HFCPP, where the minimum ChipClock high time (the amount of time the clock is in the high state, and the duty cycle is the percentage of the clock is high) is four times the HFCPP, and the minimum low time is four times the HFCPP, forming a waveform with a 50/50 duty cycle. The minimum ChipClock period constraint implies that the HFC period should be less than or equal to one-fourth of the shortest ChipClock period that will be used. That is, the HFC frequency is at least four times the frequency of the fastest ChipClock frequency that is used. The longest possible ChipClock period is limited by either the maximum size of the Target ChipClock Period field in the instruction format which supports the use of up to 2{circumflex over ( )}9=512 HFCPP long clock periods, or by the number of shift register stages implemented in the CPS high-speed shift register, whichever is smaller. The instruction format and CPS high-speed shift register properties are described in respective sections below. Automatic Ramp From One Period to the Next Period In some of the ECINs disclosed herein, processor current flow changes (di/dt) are managed by setting the Slope, Steep, and Linear fields in a CPS instruction word to values that increase or decrease the rate of change of the current drawn by the processor per unit time. This capability, depicted in FIG. 3C , is used to control the rate of change in load current imposed on the voltage regulator during large step increases in load current, or during large release reductions in load current (when fewer instructions are being executed). When Linear=0 and Steep=0, the ChipClock period is increased or decreased by another unit of HFCPP after each time Slope ChipClock periods have been completed, until the ChipClock period equals the TargetPeriod. A larger Slope value will cause the di/dt value to be smaller. When Steep=0, Rise=1, and Run=Slope, the Ramp angle=Rise/Run. When Linear=0 and Steep=1, the ChipClock period is increased or decreased by Slope units of HFCPP after each ChipClock period has been completed, until the ChipClock period equals the TargetPeriod. A larger Slope value will cause the di/dt value to be larger. When Steep=1, Rise=Slope, and Run=1, the Ramp angle=Rise/Run. When Linear=1, Steep=0 and Slope=1, the ChipClock period is increased or decreased slowly in a way that limits the di/dt change to a small fixed value. The change in ChipClock period from one period to a new period that is one HFCPP unit larger or smaller is spread across several blocks on ChipClock periods so that the average di/dt during each block of ChipClock periods is smaller than some specified di/dt limit value. For example, if the current ChipClock period is 13 HFCPP units and the target new ChipClock period is 14 HFCPP units, the di/dt step change would equal one divided by 13, or 7.69%. Assuming a specification that di/dt must not exceed 1%, then the period transition from 13 to 14 must be spread over Ceiling (7.69)=8 blocks, where each block is 8 ChipClock periods long, and the duration of each ChipClock period in each block is either 13 or 14, and the number of ChipClock periods that are 14 in each block increases by one for each block moving from all 13 to all 14 after 8 blocks. The position of each shorter or longer period is chosen to be spread out as much as possible to minimize the local average change over any interval of ChipClock periods. Instructions for Runtime Acceleration With adequate timing information describing different timing support for different subsets of instructions, the compiler can identify sets of instruction cycles that may operate at a shorter clock period than other instruction sets. To exploit this opportunity, the hardware design process for a processor that uses CPS runtime acceleration needs to include additional timing closure activities. For example, a processor designer partitions the chip into several subsets of instructions or circuit operations. At least one of these partitions is designed to run faster than at least one other partition that runs slower. The processor designer uses Static Timing Analysis (STA) to close timing, which means that all circuit timing properties are verified to satisfy applicable timing constraints. The designer closes timing, for example, at 1.1 GHZ for the slower partition, and closes timing at 1.2 GHZ for the faster partition. The designer should give special attention when closing timing on a circuitry subset of the chip, to prevent any metastability-triggering situations. Prior to, or during execution, the ChipClock period is configured to satisfy the most stringent requirements of any instruction that is active. This may be done in the processor instruction sequence compiler prior to execution, or it may be done by a circuit or processor during runtime execution. An active instruction is either a newly dispatched instruction, or the subsequent cycles of a multi-cycle instruction that was dispatched previously. For example, the ChipClock period is set to the longest period required by any active instruction. If all active instructions are in the faster partition for certain clock cycles, then the chip will run faster than during other clock cycles when some active instructions are from the slower partition. Clock Period Synthesis Circuit Description A relatively small number of logic gates are required for CPS. FIG. 4 depicts an exemplary CPS circuit that is physically implemented as part of the PLL/Clock Control Module, and the CPS ICU uses the nearest, most appropriate ICU block. For example, for the TSP tensor processor available from Groq, given that the main vertical clock spine is located along the central axis of the processor, a preferred location of the PLL/Clock Control Module is as close as possible to the central axis at the bottom center of the chip. This location makes it desirable to locate the CPS ICU near the ICU blocks that serve VXM slices near the center of the chip. The root of a clock distribution network may be located in other configurations on other chips, and there may be more than one clock generated on any particular chip. Also depicted in FIG. 4 is a toggle flip-flop structure used by the CPS, with programmable delays to determine the high-time and low-time of ChipClock. Programmable delays are implemented as shift registers that are clocked by a high frequency clock that operates at, for example, eight times the nominal chip clock frequency. The duration of each phase of each clock cycle is determined by ‘next state’ values loaded into the high state and low state shift registers, respectively. Next state logic preloads the shift registers with new period values on respective ChipClock edges. For a given resolution, the dynamic power of the shift registers can be cut in half by using rising and falling edges to implement half-cycle resolution, where the precision of this operation depends on the degree of symmetry in the duty cycle of the high frequency reference clock. Power and implementation area can be further reduced by reusing a single shift register for both the high and low phases of the generated clock waveform. The period of the high frequency clock and the number of shift register stages required for CPS are together determined by the nominal ChipClock period, the desired waveform granularity, and the maximum desired clock period for low power operating modes. For example, with a nominal 1 nS ChipClock period, 6.25% waveform granularity, and a maximum ChipClock period that is 16 times the nominal ChipClock period, the number of shift register stages required would be as follows. The duration of HFCPP is the ChipClock period times the waveform granularity percentage, for example, 1 nS*6.25%=0.0625 nS (nanoseconds). The period of the HFC is two times the duration of HFCPP, which here equals 2*0.0625 nS=0.125 nS, so the HFC would be 8 GHZ. The number of shift register stages required is the maximum ChipClock period divided by the HFC period, or 16 nS/0.125 nS=128 DFF stages, plus a few extra DFFs to implement one HFCPP resolution. FIG. 5 depicts a global clock frequency versus virtual time for a single device. Specifically, FIG. 5A depicts a global clock and a plurality of clocks driving a plurality of devices. In FIG. 5B , deterministic relativity requires that devices (also referred to as ‘chips’) are first synchronized. Clock alignment is considered a virtual global program clock. Individual chips can vary their local program clock relative to the global program clock depending on the instructions in the various queues and the associated power requirements for such instructions. Each local program clock cycle maps to a single global program clock cycle. The local program always knows where it's at in global program time. In FIG. 5B , local programs have the flexibility to be at different cycle counts in their program execution. By way of illustration, a first chip, “Local 1 ,” transmits data at local time 2500 (global time 5000). The precise control of the clock period and duty cycle enables each chip of a larger processor device to reduce peak power by lengthening the period of clock cycles that exhibit high energy activity with the advantage that the worst-case di/dt is also reduced. Because chips that execute only a certain subset of instructions that have been characterized to pass timing with a shorter clock period as long as the activity is still below the maximum (which is likely in practice) improves the manufacturing yield and reliability with little to no impact. Shortening low-activity clock cycles allows the runtime latency to be improved. For generally high-power applications, this may compensate for the lengthening during high activity cycles, and for low-power applications, it may achieve shorter latency than would be possible by compiler optimization of the instruction sequence without CPS. Clock Period Synthesis—Software Requirements Clock Period Synthesis Instructions CPS instructions are intentionally orthogonal to other functional instructions, which means that the functional instruction sequence is scheduled by the compiler or human programmer without consideration of CPS instructions, and then CPS instructions are determined by an efficient post-processing operation based on the available instruction sequence. This orthogonality facilitates much faster program compilation than would be possible if power requirements were applied as constraints during the determination of the optimized instruction sequence. In an alternative embodiment, CPS instructions are determined in conjunction with the functional instruction sequence. CPS instructions can dispatch as often as once per ChipClock. In the absence of any CPS instructions for a job, the ChipClock period defaults to a default value at boot time. A configuration register write can be used to overwrite the HW default ChipClock period. Chip Reset sets the ChipClock period to the default value. Cumulative clock periods are aligned at data transfer times, which should be considered invariant during instruction scheduling by the compiler. The Compiler should keep a tally of the real-time duration of the instructions executed on each chip in a multi-TSP system. The real-time values should be deterministically aligned at data transfer times. The Compiler has a great deal of flexibility to optimize clock durations on each individual processor, although the longest duration required during each synchronization interval will dominate. Software control of ChipClock periods is achieved by configuring eight four CPS instruction parameter values: TargetPeriod, Slope, Steep, and Linear, MostlyHi, ExtraLong, Lengthenable, and Shortenable. All eight parameters are set in each instruction. The TargetPeriod specifies the number of HFCPP periods that will be in each ChipClock period. The Slope, Steep, and Linear fields determine how the current period changes to the TargetPeriod for each intervening clock cycle according to the descriptions in the preceding and following paragraphs. The high time duration is greater-than or equal to the low time duration if the MostlyHi bit is set true in the CPS instruction. If the ExtraLong field has a value greater than zero, then high time is increased in duration by the value of ExtraLong number of HFCPP intervals when MostlyHi is true. The low time duration is greater-than or equal to the high time duration if the MostlyHi bit is set false in the CPS instruction. If the ExtraLong field has a value greater than zero, then low time is increased in duration by the value of ExtraLong number of HFCPP intervals when MostlyHi is false. The sum of the high time plus the low time is equal to the period of the clock cycle. CPS instructions are configured to operate with an asymmetric duty cycle for circuits such as memory arrays, clock gating logic, analog circuits, etc. that require an asymmetric duty cycle for optimized operation. The Lengthenable field is set true for clock periods that are eligible to be lengthened for the purpose of aligning the timing of inter-chip deterministic events. The Shortenable field is set true for clock periods that are eligible to be shortened for the purpose of aligning the timing of inter-chip deterministic events. To control processor current flow changes, di/dt, it is desirable to spread out changes in the magnitude of current drawn by the processor. The Slope, Steep, and Linear parameters specify the size of the incremental steps taken during each ChipClock period change while transitioning from the current value of ChipClock to the TargetPeriod. CPS Instruction Word Format Bit PositionField NameField Description29:21TargetPeriodChipClock will transition to TargetPeriodaccording to the Slope, Steep, and Linearoptions below20:12SlopeClockPeriod rate of change towardTargetPeriod; If (Slope==0) ClockPeriod =TargetPeriod immediately11SteepIf (Steep == 1) then Slope is the numberof HFCPP units add/subtracted duringeach ChipClock period; If (Steep == 0)then ChipClock period is changed by oneHFCPP unit after Slope ChipClock periods10LinearIf (Linear==1) then the Slope is linearized9MostlyHiDuty Cycle biased to be high longer forodd period durations8:6ExtraLongZero to 7 extra HFCPP units added to Hior Lo phase5LengthenableIndicates clock periods eligible to belengthened for Flit Rate Synchronization4ShortenableIndicates clock periods eligible to beshortened for Flit Rate Synchronization3:0OpCodeResReserved for Future use; set to zero bydefault. The CPS instruction word format uses 9 bits (shown as Bit Positions 29 through 21 in the above example CPS Instruction Word Format table) for the TargetPeriod field, e.g., bits 21 to 29. New CPS Instructions immediately preempt previously dispatched instructions, even if the ChipClock period is not yet equal to the TargetPeriod specified in the previous instruction (i.e., the ChipClock period is still changing). Extra care is advised when the Compiler calculates the timing consequences of a preempted CPS instruction. The Linear field linearizes di/dt as the ChipClock period increases or decreases for small values of the ChipClock period. Without linearization, di/dt would be much larger for each change in ChipClock period for smaller ChipClock period durations. The pattern is a concave curve that has the functional shape of 1/x. By reducing the di/dt for smaller ChipClock periods, the di/dt is linearized, as shown in the Linearization Plot and the Linearization Table below. Linearization is activated when (Linear==1 AND Steep==0 AND Slope==0), causing the CPS FSM to emit a preselected sequence of period values from a stored table. The number of period values to linearize a transition is equal to the square of the Linearization Block (LB) size. LB size is determined by the ceiling of the ratio of the largest percentage change to or from the TargetPeriod divided by the desired maximum change size. In an ECIN where the minimum clock duration is 8 HFCC intervals, a change to this TargetPeriod value will have a maximum step size of 1/8=12.5%. If the maximum change size is 1%, then LB=the ceiling of (12.5%/1%)=13. For all other TargetPeriod values, LB=CEILING (1/(TargetPeriod−1)/MaxStepSize). Calculated LB values are shown in the table below. TargetPeriod (number of HFCPP)LinearizationBlock (LB) size8-91310121111121013914-15816-17718-21622-26527-34435-51352-1012 Operating Point Voltage and Frequency The ability to set safe and reliable operating conditions is essential for electrical systems. In one ECIN, for TSP processors available from Groq, the main operating Vdd voltage for the processor can be changed via the Board Management Controller (BMC) using a PCB microcontroller that interfaced with the voltage regulators through Serial Peripheral Interface (SPI) bus ports, and similarly the PCB clock generator frequency can be set to provide an appropriate reference clock frequency for the on-chip PLL. Changes to Vdd or the Reference Clock Frequency are made between jobs. Changing the external Reference Clock Frequency while the TSP is operating is not advised because invalid clock periods may result as the PLL tracks to lock-in on the new reference clock frequency. In the best case, if the TSP continued to operate, the latency would be indeterminate because PLL tracking has significant uncertainty, and the power would also be less predictable due to the changing clock frequency. Power levels would also be uncertain during the time it takes a Vdd level change to propagate through the voltage regulator to slew the output voltage to the new setpoint. DETAILED DESCRIPTION—TECHNOLOGY SUPPORT FROM DATA/INSTRUCTIONS TO PROCESSORS/PROGRAMS Data and Information. While ‘data’ and ‘information’ often are used interchangeably (e.g., ‘data processing’ and ‘information processing’), the term ‘datum’ (plural ‘data’) typically signifies a representation of the value of a fact (e.g., the measurement of a physical quantity such as the current in a wire, or the price of gold), or the answer to a question (e.g., “yes” or “no”), while the term ‘information’ typically signifies a set of data with structure (often signified by ‘data structure’). A data structure is used in commerce to transform an electronic device for use as a specific machine as an article of manufacture (see In re Lowry. 32 F.3d 1579 [CAFC. 1994]). Data and information are physical objects, for example binary data (a ‘bit’, usually signified with ‘0’ and ‘1’) enabled with two levels of voltage in a digital circuit or electronic component. For example, data can be enabled as an electrical, magnetic, optical or acoustical signal or state; a quantum state such as a particle spin that enables a ‘qubit’; or a physical state of an atom or molecule. All such data and information, when enabled, are stored, accessed, transferred, combined, compared, or otherwise acted upon, actions that require and dissipate energy. As used herein, the term ‘process’ signifies an artificial finite ordered set of physical actions (‘action’ also signified by ‘operation’ or ‘step’) to produce at least one result Some types of actions include transformation and transportation. An action is a technical application of one or more natural laws of science or artificial laws of technology. An action often changes the physical state of a machine, of structures of data and information, or of a composition of matter. Two or more actions can occur at about the same time, or one action can occur before or after another action, if the process produces the same result. A description of the physical actions and/or transformations that comprise a process are often signified with a set of gerund phrases (or their semantic equivalents) that are typically preceded with the signifier ‘the steps of’ (e.g., “a process comprising the steps of measuring, transforming, partitioning and then distributing . . . ”). The signifiers ‘algorithm’, ‘method’, ‘procedure’, ‘(sub)routine’, ‘protocol’, ‘recipe’, and ‘technique’ often are used interchangeably with ‘process’, and 35 U.S.C. 100 defines a “method” as one type of process that is, by statutory law, always patentable under 35 U.S.C. 101. As used herein, the term ‘thread’ signifies a subset of an entire process. A process can be partitioned into multiple threads that can be used at or about at the same time. As used herein, the term ‘rule’ signifies a process with at least one logical test (signified, e.g., by ‘IF test IS TRUE THEN DO process’). As used herein, a ‘grammar’ is a set of rules for determining the structure of information. Many forms of knowledge, learning, skills and styles are authored, structured, and enabled—objectively—as processes and/or rules—e.g., knowledge and learning as functions in knowledge programming languages. As used herein, the term ‘component’ (also signified by ‘part’, and typically signified by ‘element’ when described in a patent text or diagram) signifies a physical object that is used to enable a process in combination with other components. For example, electronic components are used in processes that affect the physical state of one or more electromagnetic or quantum particles/waves (e.g., electrons, photons) or quasiparticles (e.g., electron holes, phonons, magnetic domains) and their associated fields or signals. Electronic components have at least two connection points which are attached to conductive components, typically a conductive wire or line, or an optical fiber, with one conductive component end attached to the component and the other end attached to another component, typically as part of a circuit with current or photon flows. There are at least three types of electrical components: passive, active and electromechanical. Passive electronic components typically do not introduce energy into a circuit—such components include resistors, memristors, capacitors, magnetic inductors, crystals, Josephson junctions, transducers, sensors, antennas, waveguides, etc. Active electronic components require a source of energy and can inject energy into a circuit—such components include semiconductors (e.g., diodes, transistors, optoelectronic devices), vacuum tubes, batteries, power supplies, displays (e.g., LEDs, LCDs, lamps, CRTs, plasma displays). Electromechanical components affect current flow using mechanical forces and structures—such components include switches, relays, protection devices (e.g., fuses, circuit breakers), heat sinks, fans, cables, wires, terminals, connectors and printed circuit boards. As used herein, the term ‘netlist’ is a specification of components comprising an electric circuit, and electrical connections between the components. The programming language for the SPICE circuit simulation program is often used to specify a netlist. In the context of circuit design, the term ‘instance’ signifies each time a component is specified in a netlist. One of the most important components as goods in commerce is the integrated circuit, and its res of abstractions. As used herein, the term ‘integrated circuit’ signifies a set of connected electronic components on a small substrate (thus the use of the signifier ‘chip’) of semiconductor material, such as silicon or gallium arsenide, with components fabricated on one or more layers. Other signifiers for ‘integrated circuit’ include ‘monolithic integrated circuit’, ‘IC’, ‘chip’, ‘microchip’ and ‘System on Chip’ (‘SoC’). Examples of types of integrated circuits include gate/logic arrays, processors, memories, interface chips, power controllers, and operational amplifiers. The term ‘cell’ as used in electronic circuit design signifies a specification of one or more components, for example, a set of transistors that are connected to function as a logic gate. Cells are usually stored in a database, to be accessed by circuit designers and design processes. As used herein, the term ‘module’ signifies a tangible structure for acting on data and information. For example, the term ‘module’ can signify a process that transforms data and information, for example, a process comprising a computer program (defined below). The term ‘module’ also can signify one or more interconnected electronic components, such as digital logic devices. A process comprising a module, if specified in a programming language (defined below), such as System C or Verilog, also can be transformed into a specification for a structure of electronic components that transform data and information that produce the same result as the process. This last sentence follows from a modified Church-Turing thesis, which is simply expressed as “Whatever can be transformed by a (patentable) process and a processor, can be transformed by a (patentable) equivalent set of modules.”, as opposed to the doublethink of deleting only one of the “(patentable)”. A module is permanently structured (e.g., circuits with unalterable connections), temporarily structured (e.g., circuits or processes that are alterable with sets of data), or a combination of the two forms of structuring. Permanently structured modules can be manufactured, for example, using Application Specific Integrated Circuits (‘ASICs’) such as Arithmetic Logic Units (‘ALUs’), Programmable Logic Arrays (‘PLAs’), or Read Only Memories (‘ROMs’), all of which are typically structured during manufacturing. For example, a permanently structured module can comprise an integrated circuit. Temporarily structured modules can be manufactured, for example, using Field Programmable Gate Arrays (FPGAs—for example, sold by Xilink or Intel's Altera), Random Access Memories (RAMs) or microprocessors. For example, data and information is transformed using data as an address in RAM or ROM memory that stores output data and information. One can embed temporarily structured modules in permanently structured modules (for example, a FPGA embedded into an ASIC). Modules that are temporarily structured can be structured during multiple time periods. For example, a processor comprising one or more modules has its modules first structured by a manufacturer at a factory and then further structured by a user when used in commerce. The processor can comprise a set of one or more modules during a first time period, and then be restructured to comprise a different set of one or modules during a second time period. The decision to manufacture or implement a module in a permanently structured form, in a temporarily structured form, or in a combination of the two forms, depends on issues of commerce such as cost, time considerations, resource constraints, tariffs, maintenance needs, national intellectual property laws, and/or specific design goals [FACT]. How a module is used, its function, is mostly independent of the physical form in which it is manufactured or enabled. This last sentence also follows from the modified Church-Turing thesis. As used herein, the term ‘processor’ signifies a tangible data and information processing machine for use in commerce that physically transforms, transfers, and/or transmits data and information, using at least one process. A processor consists of one or more modules, e.g., a central processing unit (‘CPU’) module; an input/output (‘I/O’) module, a memory control module, a network control module, and/or other modules. The term ‘processor’ can also signify one or more processors, or one or more processors with multiple computational cores/CPUs, specialized processors (for example, graphics processors or signal processors), and their combinations. Where two or more processors interact, one or more of the processors can be remotely located relative to the position of the other processors. Where the term ‘processor’ is used in another context, such as a ‘chemical processor’, it will be signified and defined in that context. The processor can comprise, for example, digital logic circuitry (for example, a binary logic gate), and/or analog circuitry (for example, an operational amplifier). The processor also can use optical signal processing. DNA transformations, quantum operations, microfluidic logic processing, or a combination of technologies, such as an optoelectronic processor. For data and information structured with binary data, any processor that can transform data and information using the AND, OR and NOT logical operations (and their derivatives, such as the NAND, NOR, and XOR operations) also can transform data and information using any function of Boolean logic. A processor such as an analog processor, such as an artificial neural network, also can transform data and information. No scientific evidence exists that any of these technological processors are processing, storing and retrieving data and information, using any process or structure equivalent to the bioelectric structures and processes of the human brain. The one or more processors also can use a process in a ‘cloud computing’ or ‘timesharing’ environment, where time and resources of multiple remote computers are shared by multiple users or processors communicating with the computers. For example, a group of processors can use at least one process available at a distributed or remote system, these processors using a communications network (e.g., the Internet, or an Ethernet) and using one or more specified network interfaces (‘interface’ defined below) (e.g., an application program interface (‘API’) that signifies functions and data structures to communicate with the remote process). As used herein, the term ‘computer’ and ‘computer system’ (further defined below) includes at least one processor that, for example, performs operations on data and information such as (but not limited to) the Boolean logical operations using electronic gates that can comprise transistors, with the addition of memory (for example, memory structured with flip-flops using the NOT-AND or NOT-OR operation). Any processor that can perform the logical AND, OR and NOT operations (or their equivalent) is Turing-complete and computationally universal [FACT]. A computer can comprise a simple structure, for example, comprising an I/O module, a CPU module, and a memory that performs, for example, the process of inputting a signal, transforming the signal, and outputting the signal with no human intervention. As used herein, the term ‘programming language’ signifies a structured grammar for specifying sets of operations and data for use by modules, processors and computers. Programming languages include assembler instructions, instruction-set-architecture instructions, machine instructions, machine dependent instructions, microcode, firmware instructions, state-setting data, or either source code or object code written in any combination of one or more higher level languages, for example, the C programming language and similar general programming languages (such as Fortran, Basic, Javascript, PHP, Python, C++), knowledge programming languages (such as Lisp, Smalltalk, Prolog, or CycL), electronic structure programming languages (such as VHDL, Verilog, SPICE or SystemC), text programming languages (such as SGML, HTML, or XML), or audiovisual programming languages (such as SVG, MathML, X3D/VRML, or MIDI), and any future equivalent programming languages. As used herein, the term ‘source code’ signifies a set of instructions and data specified in text form using a programming language. A large amount of source code for use in enabling any of the claimed inventions is available on the Internet, such as from a source code library such as Github. As used herein, the term ‘program’ (also referred to as an ‘application program’) signifies one or more processes and data structures that structure a module, processor or computer to be used as a “specific machine” (see In re Alappat, 33 F3d 1526 [CAFC, 1991]). One use of a program is to structure one or more computers, for example, standalone, client or server computers, or one or more modules, or systems of one or more such computers or modules. As used herein, the term ‘computer application’ signifies a program that enables a specific use, for example, to enable text processing operations, or to encrypt a set of data. As used herein, the term ‘firmware’ signifies a type of program that typically structures a processor or a computer, where the firmware is smaller in size than a typical application program, and is typically not very accessible to or modifiable by the user of a computer. Computer programs and firmware are often specified using source code written in a programming language, such as C. Modules, circuits, processors, programs and computers can be specified at multiple levels of abstraction, for example, using the SystemC programming language, and have value as products in commerce as taxable goods under the Uniform Commercial Code (see U.C.C. Article 2, Part 1). A program is transferred into one or more memories of the computer or computer system from a data and information device or storage system. A computer system typically has a device for reading storage media that is used to transfer the program, and/or has an interface device that receives the program over a network. This transfer is discussed in the General Computer Explanation section. DETAILED DESCRIPTION—TECHNOLOGY SUPPORT GENERAL COMPUTER EXPLANATION FIG. 5 depicts a computer system suitable for enabling embodiments of the claimed inventions. In FIG. 5 , the structure of computer system 510 typically includes at least one computer 514 which communicates with peripheral devices via bus subsystem 512 . Typically, the computer includes a processor (e.g., a microprocessor, graphics processing unit, AI co-processor or digital signal processor), or its electronic processing equivalents, such as an Application Specific Integrated Circuit (‘ASIC’) or Field Programmable Gate Array (‘FPGA’). Typically, peripheral devices include a storage subsystem 524 , comprising a memory subsystem 526 and a file storage subsystem 528 , user interface input devices 522 , user interface output devices 520 , and/or a network interface subsystem 516 . The input and output devices enable direct and remote user interaction with computer system 510 . The computer system enables significant post-process activity using at least one output device and/or the network interface subsystem. The computer system can be structured as a server, a client, a workstation, a mainframe, a personal computer (PC), a tablet PC, a set-top box (STB), a personal digital assistant (PDA), a cellular telephone, a smartphone, a web appliance, a rack-mounted ‘blade’, a kiosk, a television, a game station, a network router, switch or bridge, or any data processing machine with instructions that specify actions to be taken by that machine. The term ‘server’, as used herein, refers to a computer or processor that typically performs processes for, and sends data and information to, another computer or processor. A computer system typically is structured, in part, with at least one operating system program, such as Microsoft's Windows, Sun Microsystems's Solaris, Apple Computer's MacOs and iOS, Google's Android, Linux and/or Unix. The computer system typically includes a Basic Input/Output System (BIOS) and processor firmware. The operating system, BIOS and firmware are used by the processor to structure and control any subsystems and interfaces connected to the processor. Typical processors that enable these operating systems include: the Pentium, Itanium and Xeon processors from Intel; the Opteron and Athlon processors from Advanced Micro Devices; the Graviton processor from Amazon; the POWER processor from IBM; the SPARC processor from Oracle; and the ARM processor from ARM Holdings. Any ECIN is limited neither to an electronic digital logic computer structured with programs nor to an electronically programmable device. For example, the claimed inventions can use an optical computer, a quantum computer, an analog computer, or the like. Further, where only a single computer system or a single machine is signified, the use of a singular form of such terms also can signify any structure of computer systems or machines that individually or jointly use processes. Due to the ever-changing nature of computers and networks, the description of computer system 510 depicted in FIG. 5A is intended only as an example. Many other structures of computer system 510 have more or less components than the computer system depicted in FIG. 5A . Network interface subsystem 516 provides an interface to outside networks, including an interface to communication network 518 , and is coupled via communication network (not shown) to corresponding interface devices in other computer systems or machines. Communication network can comprise many interconnected computer systems, machines and physical communication connections (signified by ‘links’). These communication links can be wireline links, optical links, wireless links (e.g., using the WiFi or Bluetooth protocols), or any other physical devices for communication of information. Communication network can be any suitable computer network, for example a wide area network such as the Internet, and/or a local-to-wide area network such as Ethernet. The communication network is wired and/or wireless, and many communication networks use encryption and decryption processes, such as is available with a virtual private network. The communication network uses one or more communications interfaces, which receive data from, and transmit data to, other systems. Embodiments of communications interfaces typically include an Ethernet card, a modem (e.g., telephone, satellite, cable, or ISDN), (asynchronous) digital subscriber line (DSL) unit, Firewire interface, USB interface, and the like. Communication algorithms (‘protocols’) can be specified using one or communication languages, such as HTTP, TCP/IP, RTP/RTSP, IPX and/or UDP. User interface input devices 522 can include an alphanumeric keyboard, a keypad, pointing devices such as a mouse, trackball, toggle switch, touchpad, stylus, a graphics tablet, an optical scanner such as a bar code reader, touchscreen electronics for a display device, audio input devices such as voice recognition systems or microphones, eye-gaze recognition, brainwave pattern recognition, optical character recognition systems, and other types of input devices. Such devices are connected by wire or wirelessly to a computer system. Typically, the term ‘input device’ signifies all possible types of devices and processes to transfer data and information into computer system 510 or onto communication network. User interface input devices typically enable a user to select objects, icons, text and the like that appear on some types of user interface output devices, for example, a display subsystem. User interface output devices 520 can include a display subsystem, a printer, a fax machine, or a non-visual communication device such as audio and haptic devices. The display subsystem can include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), an image projection device, or some other device for creating visible stimuli such as a virtual reality system. The display subsystem also can provide non-visual stimuli such as via audio output, aroma generation, or tactile/haptic output (e.g., vibrations and forces) devices. Typically, the term ‘output device’ signifies all possible types of devices and processes to transfer data and information out of computer system 510 to the user or to another machine or computer system. Such devices are connected by wire or wirelessly to a computer system. Note: some devices transfer data and information both into and out of the computer, for example, haptic devices that generate vibrations and forces on the hand of a user while also incorporating sensors to measure the location and movement of the hand. Technical applications of the sciences of ergonomics and semiotics are used to improve the efficiency of user interactions with any processes and computers disclosed herein, such as any interactions with regards to the design and manufacture of circuits that use any of the above input or output devices. Memory subsystem 526 typically includes a number of memories including a main random-access memory (‘RAM’) 530 (or other volatile storage device) for storage of instructions and data during program execution and a read only memory (‘ROM’) 532 in which fixed instructions are stored. File storage subsystem 528 provides persistent storage for program and data files, and can include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, a flash memory such as a USB drive, or removable media cartridges. If computer system 510 includes an input device that performs optical character recognition, then text and symbols printed on paper can be used as a device for storage of program and data files. The databases and modules used by some embodiments can be stored by file storage subsystem 528 . Bus subsystem 512 provides a device for transmitting data and information between the various components and subsystems of computer system 510 . Although bus subsystem 512 is depicted as a single bus, alternative embodiments of the bus subsystem can use multiple buses. For example, a main memory using RAM can communicate directly with file storage systems using Direct Memory Access (‘DMA’) systems. A memory, such as a non-transitory, processor readable data and information storage medium associated with file storage subsystem 528 , and/or with network interface subsystem 516 , and can include a data structure specifying a circuit design. The memory can be a hard disk, a floppy disk, a CD-ROM, an optical medium, removable media cartridge, or any other medium that stores computer readable data in a volatile or non-volatile form, such as text and symbols on a physical object (such as paper) that can be processed by an optical character recognition system. A program transferred in to and out of a processor from such a memory can be transformed into a physical signal that is propagated through a medium (such as a network, connector, wire, or circuit trace as an electrical pulse); or through a medium such as space or an atmosphere as an acoustic signal, or as electromagnetic radiation with wavelengths in the electromagnetic spectrum longer than infrared light). DETAILED DESCRIPTION—SEMANTIC SUPPORT The signifier ‘commercial solution’ signifies, solely for the following paragraph, a technology domain-specific (and thus non-preemptive-see Bilski): electronic structure, process for a specified machine, manufacturable circuit (and its Church-Turing equivalents), or composition of matter that applies science and/or technology for use in commerce to solve an unmet need of technology. The signifier ‘abstract’ (when used in a patent claim for any enabled embodiments disclosed herein for a new commercial solution that is a scientific use of one or more laws of nature {see Benson}, and that solves a problem of technology {see Diehr} for use in commerce—or improves upon an existing solution used in commerce {see Diehr})—is precisely defined by the inventor(s) {see MPEP 2111.01 (9th edition, Rev. 08.2017)} as follows: a) a new commercial solution is ‘abstract’ if it is not novel (e.g., it is so well known in equal prior art {see Alice} and/or the use of equivalent prior art solutions is long prevalent {see Bilski} in science, engineering or commerce), and thus unpatentable under 35 U.S.C. 102, for example, because it is ‘difficult to understand’ {see Merriam-Webster definition for ‘abstract’} how the commercial solution differs from equivalent prior art solutions; orb) a new commercial solution is ‘abstract’ if the existing prior art includes at least one analogous prior art solution {see KSR}, or the existing prior art includes at least two prior art publications that can be combined {see Alice} by a skilled person {often referred to as a ‘PHOSITA’, see MPEP 2141-2144 (9th edition, Rev. 08.2017)} to be equivalent to the new commercial solution, and is thus unpatentable under 35 U.S.C. 103, for example, because it is ‘difficult to understand’ how the new commercial solution differs from a PHOSITA-combination/-application of the existing prior art; orc) a new commercial solution is ‘abstract’ if it is not disclosed with a description that enables its praxis, either because insufficient guidance exists in the description, or because only a generic implementation is described {see Mayo} with unspecified components, parameters or functionality, so that a PHOSITA is unable to instantiate an embodiment of the new solution for use in commerce, without, for example, requiring special programming {see Katz} (or, e.g., circuit design) to be performed by the PHOSITA, and is thus unpatentable under 35 U.S.C. 112, for example, because it is ‘difficult to understand’ how to use in commerce any embodiment of the new commercial solution. DETAILED DESCRIPTION—CONCLUSION The Detailed Description signifies in isolation the individual features, structures, functions, or characteristics described herein and any combination of two or more such features, structures, functions or characteristics, to the extent that such features, structures, functions or characteristics or combinations thereof are enabled by the Detailed Description as a whole in light of the knowledge and understanding of a skilled person, irrespective of whether such features, structures, functions or characteristics, or combinations thereof, solve any problems disclosed herein, and without limitation to the scope of the Claims of the patent. When an ECIN comprises a particular feature, structure, function or characteristic, it is within the knowledge and understanding of a skilled person to use such feature, structure, function, or characteristic in connection with another ECIN whether or not explicitly described, for example, as a substitute for another feature, structure, function or characteristic. In view of the Detailed Description, a skilled person will understand that many variations of any ECIN can be enabled, such as function and structure of elements, described herein while being as useful as the ECIN. One or more elements of an ECIN can be substituted for one or more elements in another ECIN, as will be understood by a skilled person. Writings about any ECIN signify its use in commerce, thereby enabling other skilled people to similarly use this ECIN in commerce. This Detailed Description is fitly written to provide knowledge and understanding. It is neither exhaustive nor limiting of the precise structures described, but is to be accorded the widest scope consistent with the disclosed principles and features. Without limitation, any and all equivalents described, signified or Incorporated By Reference (or explicitly incorporated) in this patent application are specifically incorporated into the Detailed Description. In addition, any and all variations described, signified or incorporated with respect to any one ECIN also can be included with any other ECIN. Any such variations include both currently known variations as well as future variations, for example any element used for enablement includes a future equivalent element that provides the same function, regardless of the structure of the future equivalent element. It is intended that the domain of the set of claimed inventions and their embodiments be defined and judged by the following Claims and their equivalents. The Detailed Description includes the following Claims, with each Claim standing on its own as a separate claimed invention. Any ECIN can have more structure and features than are explicitly specified in the Claims.",en,PATENT_APPLICATION
098-843-596-649-098,US,20240384744,A1,2024-11-21,US_20240384744_A1_20241121,en,US,20240384744,A1,2024-11-21,US,18197392,2023-05-15,BREAK-AWAY BOLT,en,US,Melvin D. PLUMMER,"Los Angeles, CA",US,Niklas Seyferth,"Los Angeles, CA",US,Melvin D. PLUMMER,"Los Angeles, CA",US,1,Niklas Seyferth,"Los Angeles, CA",F16B31/06,I,F,F16B31/06,I,F,US,20240384744,A1,2024-11-21,098-843-596-649-098,1,US,20240384744,A1,2024-11-21,098-843-596-649-098,1,UNKNOWN,"A double-sided break-away bolt including a precision weakened area enabling the bolt to break into two or more sections upon predefined conditions is provided. The break-away bolt is adapted to connect a fire hydrant to a check valve with the weakened portion generally positioned at the junction between the hydrant and the valve. A force applied to the fire hydrant in a direction generally perpendicular to the break-away bolt (e.g., by a car colliding with the hydrant) may cause the break-away bolt to break at the weakened area. This in return facilitates the separation of the hydrant from the check valve allowing the check valve to close. The break-away bolt also includes a gripping element that becomes exposed upon the breaking of the bolt to facilitate the removal of the bolt from the check valve for replacement.",en,"1 . A break-away bolt comprising: an elongate member with a circumference and a first end and a second end, the elongate member including a first attachment portion towards the first end and adapted to attach the elongate member to a first item, a second attachment portion towards the second end and adapted to attach the elongate member to a second item, and a shearing location between the first and second attachment portions adapted to break upon predefined conditions; a gripping element coupled about at least a portion of the circumference between the first and second attachment portions.",2 . The break-away bolt of claim 1 wherein the predefined conditions include an external force being applied to the first item.,3 . The break-away bolt of claim 1 wherein the shearing location includes a weakened section.,4 . The break-away bolt of claim 1 wherein the shearing location is between the gripping element and the second attachment portion.,5 . The break-away bolt of claim 1 wherein the gripping element includes a hex section.,6 . The break-away bolt of claim 1 wherein the elongate member includes a stop coupled to at least a portion of the outer circumference.,7 . The break-away bolt of claim 6 wherein the stop includes a ring.,8 . The break-away bolt of claim 6 wherein the stop is between the first attachment portion and the gripping element.,9 . The break-away bolt of claim 1 wherein at least a portion of the first attachment portion includes a hollow portion.,10 . The break-away bolt of claim 8 wherein the hollow portion extends from the first end towards the shearing location.,11 . The break-away bolt of claim 1 wherein the first attachment portion and/or the second attachment portion includes outer threads.,en,"FIELD OF THE INVENTION This invention relates to bolts, including break-away bolts. BACKGROUND As is known in the art, many fire hydrants throughout the world are configured atop out check valves that automatically terminate the water supply to the hydrant upon the hydrant being severed from the valve (e.g., when an automobile collides with the hydrant). The fire hydrants are typically connected to the check valves using what is known as “break-away bolts” designed to break apart at a desire location (e.g., at the junction between the hydrant and the valve) upon a predefined force applied to the hydrant. In this way, when a force of a predefined magnitude is applied to the side of the hydrant, the break-away bolt(s) may break thereby releasing the hydrant and allowing the check valve to close. However, once a hydrant has been severed from the check valve, a portion of the severed break-away bolt(s) remain secured in the upper portion of the valve and are difficult to remove. In fact, reconfiguring the hydrant onto the check valve after breakage often requires the complete removal of the check valve from the water main system, causing water outages and increased labor costs. Accordingly, there is a need for a break-away bolt including a removal mechanism to facilitate the removal of a severed bolt from a closed check valve. There also is a need for a break-away bolt and check valve combination that facilitates the easy reconfiguration of the hydrant onto the check valve. These and other issues will be addressed herein. BRIEF DESCRIPTION OF THE DRAWINGS Various other objects, features and attendant advantages of the present invention will become fully appreciated as the same becomes better understood when considered in conjunction with the accompanying drawings, in which like reference characters designate the same or similar parts throughout the several views, and wherein: FIG. 1 shows a perspective view of a break-away bolt according to exemplary embodiments hereof; FIG. 2 shows a side view of the break-away bolt of FIG. 1 according to exemplary embodiments hereof; FIG. 3 shows a cross-sectional side view of a break-away bolt according to exemplary embodiments hereof; FIG. 4 shows a break-away bolt inserted into an opening according to exemplary embodiments hereof; FIG. 5 shows a break-away bolt used to attach a fire hydrant to a check valve according to exemplary embodiments hereof; FIG. 6 shows the fire hydrant of FIG. 5 being displaced from the check valve due to an applied force according to exemplary embodiments hereof; FIG. 7 shows a remaining portion of the break-away bolt configured with the check valve after the displacement of the fire hydrant of FIG. 6 according to exemplary embodiments hereof; FIG. 8 shows a top view of an exposed gripping element within an opening according to exemplary embodiments hereof; and FIG. 9 shows a plurality of break-away bolts configured with a check valve according to exemplary embodiments hereof. DETAILED DESCRIPTION OF EXEMPLARY EMBODIMENTS In general, the break-away bolt according to exemplary embodiments hereof provides a double-sided bolt including a precision weakened area enabling the bolt to break into two or more sections upon predefined conditions. The break-away bolt includes a first attachment portion for attaching to a first item (e.g., a first flange), a second attachment portion for attaching to a second item (e.g., a second flange), and a weakened area between the first and second attachment portions. In general, the break-away bolt is adapted to connect the first item to the second item with the weakened portion generally positioned at the junction between the first and second items. In this way, a force applied to the first item or to the second item in a direction generally perpendicular to the break-away bolt may cause the break-away bolt to break at the weakened area. This in return may facilitate the separation of the second item from the first item. As will be described in other sections, the break-away bolt also includes a gripping element configured with the first attachment portion to facilitate the removal of the first attachment portion upon breakage of the bolt. In some embodiments, the bolt's first attachment portion is designed to be received into a receiving opening with a specific arrangement of inner threads, circumferential ledges, and varying diameters to facilitate the bolt's desired performance. This will be described in other sections. For the purposes of this specification, the break-away bolt, and the receiving opening within which the break-away bolt may be attached will be described predominantly with respect to its use with attaching a fire hydrant to a break-away check valve such as described in U.S. patent application Ser. No. 16/382,124, filed Apr. 19, 2019, now U.S. Pat. No. 11,187,332, the entire contents of which are hereby fully incorporated herein by reference for all purposes. However, it is understood that the break-away bolt and/or the receiving opening may be used to attach any first item with any second item that may benefit from the functionalities of the bolt and opening. The break-away bolt will now be described in detail with reference to FIGS. 1-9 . In one exemplary embodiment hereof, as shown in FIGS. 1-2 , the break-away bolt 10 (also referred to herein as simply the bolt 10 ) includes an elongate body 12 with a first end 14 and a second end 16 . FIG. 1 shows a perspective view of the bolt 10 and FIG. 2 shows a side view of the same. The bolt's first end 14 includes a first attachment portion 18 and the bolt's second end 16 includes a second attachment portion 20 . The bolt's body 12 includes a shear location 22 generally between the first attachment portion 18 and the second attachment portion 20 . As will be described herein, the bolt 10 , upon predefined conditions, is designed to break apart at the shear location 22 into two break-apart portions 26 , 28 (see FIGS. 2 and 6 ), with a first break-apart portion 26 including the first attachment portion 18 and a second break-apart portion 28 including the second attachment portion 20 . In addition, the bolt 10 includes a gripping element 24 generally adjacent the first attachment portion 18 towards the shear location 22 for gripping the first attachment portion 18 to facilitate its removal after breakage of the bolt 10 . The bolt 10 also may include a circumferential ring 25 located between the gripping element 24 and the first attachment portion 18 and extending outward from the bolt's body 12 . The break-away bolt 10 may include additional elements as necessary for the bolt 10 to perform its functionalities. FIG. 3 shows the cross-section of the break-away bolt 10 taken from the perspective of cutlines A-A of FIG. 2 . As shown, in some embodiments, the bolt's first break-apart portion 26 may include a hollow portion 27 (e.g., a bored-out portion) that may extend from the bolt's first end 14 (e.g., its bottom end) upward to about the bolt's shear location 22 . In this way, the bolt's first break-apart portion 26 may be weakened compared to the bolt's second break-apart portion 28 . In some embodiments, the hollow portion 27 may be formed using a boring tool (e.g., a drill bit) including a sharpened conical tip resulting in the bored-out hollow portion 27 including a conical upper end, however, this may not be required. In some embodiments, the diameter of the inner bore withing section 26 is about 0.50″ leaving a wall thickness in this area of about 1/16″-⅛″ but it is understood that other diameters and wall thicknesses also may be used. In some embodiments, the diameter of the shear location 22 is about 0.56″ but it is understood that other diameters also may be used. FIG. 4 shows a side view of the break-away bolt 10 configured within a receiving opening 100 . In some embodiments, the receiving opening 100 may be located on an upper surface 102 of a break-away check valve CV to facilitate the attachment of the valve CV to a fire hydrant. In some embodiments, the receiving opening 100 includes a first opening portion 104 defined by first inner side walls 106 adapted to receive and secure the break-away bolt's first attachment portion 18 . In some embodiments, the bolt's first attachment portion 18 includes outer threads 26 and the first inner side walls 106 include corresponding inner threads 108 adapted to engage the first attachment portion's outer threads 26 . In this way, the bolt's first attachment portion 18 may be screwed into the opening's first opening portion 104 and be secured therein. In some embodiments, as shown in FIG. 4 , the receiving opening 100 includes a second opening portion 110 defined by second inner sidewalls 112 , located generally above the first opening portion 104 and adapted to receive the break-away bolt's gripping element 24 . The first opening portion 104 may include a first diameter D 1 and the second opening portion 110 may include a second diameter D 2 that, in some embodiments, may be greater than the first diameter D 1 . The first opening portion 104 may be joined with the second opening portion 110 by a circumferential ledge 114 that may preferably be generally horizontal. In some embodiments, as shown in FIG. 3 , the bolt's first attachment portion 18 may be received into (e.g., screwed into) the opening's first opening portion 104 until the circumferential ring 25 abuts the circumferential ledge 114 thereby acting as a stop. In this way, the depth of the bolt's body 12 within the receiving opening 100 may be controlled. Accordingly, it may be preferable that the height H 3 of the first opening portion 104 be equal to or greater than the length of the bolt's first attachment portion 18 . While the circumferential ring 25 is shown to extend around the entire circumference of the bolt's body 12 , it is understood that the circumferential ring 25 may extend around one or more portions of the bolt's circumference or any combination thereof. In some embodiments, the bolt's body 12 may include surface textures (e.g., may be knurled or otherwise roughened) in area between the shearing location 22 and the second attachment portion 20 to assist in the gripping of the bolt 10 for insertion into the receiving opening 100 . In some embodiments, as shown in FIG. 3 , with the bolt's first attachment portion 18 received and secured within the opening's first opening portion 104 , and the bolt's circumferential ring 25 abutted against the opening's circumferential ledge 114 , the bolt's gripping element 24 may be received into the opening's second opening portion 110 . It may be preferable that in this configuration the top of the gripping element 24 be located at or slightly below the level of the check valve's upper surface 102 . In this way, the entire height H 1 of the gripping element 24 and circumferential ring 25 combination may be received into the opening's second opening portion 110 (with height H 2 ). Accordingly, it may be preferable that H 1 ≤H 2 . This arrangement may place the shear location 22 at or slightly above the level of the check valve's upper surface 102 , and as will be described in other sections, may facilitate the shearing of the bolt 10 into a first break-apart portion 26 and a second break-apart portion 28 under predefined conditions (e.g., when a force is applied to the bolt 10 ). In some embodiments as shown in FIGS. 1-3 , the gripping element 24 extends about at least a portion of the circumference of the bolt's body 12 . In this way, at least a portion of the gripping element 24 is circumferential with respect to the bolt body 12 . In some embodiments, the gripping element 24 includes a hex section (also referred to as a hex head) that may be gripped from the side using a conventional wrench and/or from above (when the bolt 10 has broken and the second break-apart portion 28 has been removed thereby exposing the top of the gripping element 24 ) using a socket wrench. In this way, the first attachment portion 18 may be gripped from above and unscrewed from within the first opening portion 104 once the bolt 10 has been broken. It is understood that the gripping element 24 may include other types of gripping elements (such as a square head and/or any other shaped head) that may be gripped preferably from above. In some embodiments, as shown in FIG. 5 , the bolt's second attachment portion 20 may include outer threads 21 for engagement with a threaded nut 116 . The fire hydrant FH may include a through-hole 118 adapted to receive the bolt's second attachment portion 20 from the through-hole's underneath side to its top side. The length of the bolt's second attachment portion 20 may be chosen such that the bolt's second end 16 as well as a portion of the bolt's second attachment portion 20 may extend out the through-hole's 118 's top side when the fire hydrant FH is placed atop the check valve CV as shown. A threaded nut 116 may be placed on the bolt's second end 16 extending out the top side of the through-hole 118 and tightened, thereby attaching the second attachment portion 20 to the fire hydrant FH. In the arrangement shown in FIG. 5 , with the fire hydrant FH attached atop the check valve CV using the break-away bolt 10 as described herein, a force F 1 applied to the fire hydrant FH (e.g., by an automobile striking the fire hydrant FH also referred to as a hydrant strike) may cause the bolt 10 to break at the shear location 22 , thereby releasing the fire hydrant FH and the bolt's second break-apart portion 28 from the check valve CV and the bolt's first break-apart portion 26 . This is shown in FIG. 6 . Given the above, it may be preferable for the tensile strength of the bolt's shearing location 22 be chosen such that the bolt 10 may break during a hydrant strike prior to the fire hydrant FH and/or water main system becoming damaged. It is notable that the junction between the bolt's first break-apart portion 26 and the receiving opening 100 may be tight and rigid, thereby holding the first break-apart portion 26 generally stationary during the breaking of the bolt 10 . Given this, the force F 1 may cause the fire hydrant FH and to shear the bolt 10 at its shearing location 22 located just above the upper surface 102 of the check valve CV. FIG. 7 shows a side view of the bolt's first break-apart portion 26 left intact within the receiving opening 100 after the applied force F 1 caused the fire hydrant FH and the bolt's second break-apart portion 28 to be removed, and FIG. 8 shows the same from above. With the bolt's second break-away portion 28 removed, the bolt's gripping element 24 may be exposed from the top as shown in FIG. 8 . Subsequently, a socket wrench may be placed over the gripping element 24 from above and used to remove (e.g., unscrew) the first break-apart portion 26 from the receiving opening 100 . In this way, the break-away bolt 10 may be replaced for reattachment of the fire hydrant FH to the check valve CV. Given the above, it may be preferable to choose a diameter D 2 of the opening's second opening portion 110 to be wide enough to allow the socket wrench to be inserted onto the gripping element 24 between the gripping element 24 and the inner sidewalls 112 . FIG. 9 shows a plurality of break-away bolts 10 configured within receiving openings 100 in the upper surface 102 of a break-away check valve CV. It is understood that the steps described above are meant for demonstration and that additional steps may be performed, not all of the described steps may be performed, and the steps may be taken in different orders. It also is understood that the scope of the break-away bolt 10 is not limited in any way by the steps taken during its use. It also is understood that any aspect and/or element of any embodiment of the break-away bolt 10 described herein or otherwise may be combined with any other aspect and/or element of any other embodiment described herein or otherwise in any way to form additional embodiments of the bolt 10 all of which are within the scope of the bolt 10 . Where a process is described herein, those of ordinary skill in the art will appreciate that the process may operate without any user intervention. In another embodiment, the process includes some human intervention (e.g., a step is performed by or with the assistance of a human). As used herein, including in the claims, the phrase “at least some” means “one or more,” and includes the case of only one. Thus, e.g., the phrase “at least some ABCs” means “one or more ABCs”, and includes the case of only one ABC. As used herein, including in the claims, term “at least one” should be understood as meaning “one or more”, and therefore includes both embodiments that include one or multiple components. Furthermore, dependent claims that refer to independent claims that describe features with “at least one” have the same meaning, both when the feature is referred to as “the” and “the at least one”. As used in this description, the term “portion” means some or all. So, for example, “A portion of X” may include some of “X” or all of “X”. In the context of a conversation, the term “portion” means some or all of the conversation. As used herein, including in the claims, the phrase “using” means “using at least,” and is not exclusive. Thus, e.g., the phrase “using X” means “using at least X.” Unless specifically stated by use of the word “only”, the phrase “using X” does not mean “using only X.” As used herein, including in the claims, the phrase “based on” means “based in part on” or “based, at least in part, on,” and is not exclusive. Thus, e.g., the phrase “based on factor X” means “based in part on factor X” or “based, at least in part, on factor X.” Unless specifically stated by use of the word “only”, the phrase “based on X” does not mean “based only on X.” In general, as used herein, including in the claims, unless the word “only” is specifically used in a phrase, it should not be read into that phrase. As used herein, including in the claims, the phrase “distinct” means “at least partially distinct.” Unless specifically stated, distinct does not mean fully distinct. Thus, e.g., the phrase, “X is distinct from Y” means that “X is at least partially distinct from Y,” and does not mean that “X is fully distinct from Y.” Thus, as used herein, including in the claims, the phrase “X is distinct from Y” means that X differs from Y in at least some way. It should be appreciated that the words “first,” “second,” and so on, in the description and claims, are used to distinguish or identify, and not to show a serial or numerical limitation. Similarly, letter labels (e.g., “(A)”, “(B)”, “(C)”, and so on, or “(a)”, “(b)”, and so on) and/or numbers (e.g., “(i)”, “(ii)”, and so on) are used to assist in readability and to help distinguish and/or identify, and are not intended to be otherwise limiting or to impose or imply any serial or numerical limitations or orderings. Similarly, words such as “particular,” “specific,” “certain,” and “given,” in the description and claims, if used, are to distinguish or identify, and are not intended to be otherwise limiting. As used herein, including in the claims, the terms “multiple” and “plurality” mean “two or more,” and include the case of “two.” Thus, e.g., the phrase “multiple ABCs,” means “two or more ABCs,” and includes “two ABCs.” Similarly, e.g., the phrase “multiple PQRs,” means “two or more PQRs,” and includes “two PQRs.” The present invention also covers the exact terms, features, values and ranges, etc. in case these terms, features, values and ranges etc. are used in conjunction with terms such as about, around, generally, substantially, essentially, at least etc. (i.e., “about 3” or “approximately 3” shall also cover exactly 3 or “substantially constant” shall also cover exactly constant). As used herein, including in the claims, singular forms of terms are to be construed as also including the plural form and vice versa, unless the context indicates otherwise. Thus, it should be noted that as used herein, the singular forms “a,” “an,” and “the” include plural references unless the context clearly dictates otherwise. Throughout the description and claims, the terms “comprise”, “including”, “having”, and “contain” and their variations should be understood as meaning “including but not limited to”, and are not intended to exclude other components unless specifically so stated. It will be appreciated that variations to the embodiments of the invention can be made while still falling within the scope of the invention. Alternative features serving the same, equivalent or similar purpose can replace features disclosed in the specification, unless stated otherwise. Thus, unless stated otherwise, each feature disclosed represents one example of a generic series of equivalent or similar features. The present invention also covers the exact terms, features, values and ranges, etc. in case these terms, features, values and ranges etc. are used in conjunction with terms such as about, around, generally, substantially, essentially, at least etc. (i.e., “about 3” shall also cover exactly 3 or “substantially constant” shall also cover exactly constant). Use of exemplary language, such as “for instance”, “such as”, “for example” (“e.g.,”) and the like, is merely intended to better illustrate the invention and does not indicate a limitation on the scope of the invention unless specifically so claimed. While the invention has been described in connection with what is presently considered to be the most practical and preferred embodiments, it is to be understood that the invention is not to be limited to the disclosed embodiment, but on the contrary, is intended to cover various modifications and equivalent arrangements included within the spirit and scope of the appended claims.",en,PATENT_APPLICATION
107-695-733-655-076,US,20240384101,A1,2024-11-21,US_20240384101_A1_20241121,en,US,20240384101,A1,2024-11-21,US,18649499,2024-04-29,"ENGINEERED SKIN EQUIVALENT, METHOD OF MANUFACTURE THEREOF AND PRODUCTS DERIVED THEREFROM",en,US,VitroLabs Inc,"Milpitas, CA",GB,King's College London,London,US,Ingvar HELGASON,"South San Francisco, CA",GB,1,Dusko ILIC,London,C08L89/06,I,F,C12N5/071,I,L,C12N11/02,I,L,C14C3/02,I,L,C14C13/00,I,L,C08L89/06,I,F,C12N5/0625,I,L,C12N5/0629,I,L,C12N5/0698,I,L,C12N11/02,I,L,C14C3/02,I,L,C14C13/00,I,L,C12N2501/155,A,L,C12N2501/385,A,L,C12N2502/091,A,L,C12N2502/094,A,L,C12N2502/13,A,L,C12N2506/45,A,L,C12N2533/30,A,L,C12N2533/54,A,L,US,20240384101,A1,2024-11-21,107-695-733-655-076,1,US,20240384101,A1,2024-11-21,107-695-733-655-076,1,UNKNOWN,"Disclosed herein are synthetic leathers. artificial epidermal layers. artificial dermal layers, layered structures. products produced therefrom and methods of producing the same.",en,1 - 198 . (canceled),"199 . A method of forming a tanned synthetic leather, wherein the method comprises: culturing a fibroblast on a scaffold; and tanning a collagenous extracellular matrix comprising collagen secreted by the fibroblast to generate the tanned synthetic leather.","200 . The method of claim 199 , wherein the scaffold comprises a natural material, a synthetic material, or a combination thereof.","201 . The method of claim 200 , wherein the scaffold comprises the natural material.","202 . The method of claim 200 , wherein the scaffold comprises the synthetic material.","203 . The method of claim 199 , wherein the tanning comprises a vegetable tanning, a chrome tanning, an aldehyde tanning, a syntan tanning, a bacterial tanning, or any combination thereof.","204 . The method of claim 199 , wherein a thickness of the tanned synthetic leather is about 0.01 mm to about 5 mm.","205 . The method of claim 200 , wherein the fibroblast is derived from a mammal or reptile.","206 . The method of claim 205 , wherein the fibroblast is a bovine fibroblast.","207 . The method of claim 205 , further comprising forming the tanned synthetic leather into one or more of a watch strap, a belt, a packaging, a shoe, a boot, a footwear, a glove, a clothing, a luggage, a bag, a clutch, a purse, a backpack, a wallet, a saddle, a harness, a whip, or any combination thereof.","208 . A composition comprising a cell layer comprising a fibroblast in contact with a scaffold and an extracellular matrix, wherein the extracellular matrix comprises a type I collagen, a type III collagen, or a combination thereof, and wherein the composition is suitable for tanning to form a tanned synthetic leather.","209 . The composition of claim 208 , wherein the fibroblast is derived from a mammal or a reptile.","210 . The composition of claim 209 , wherein the fibroblast is a bovine fibroblast.","211 . The composition of claim 208 , wherein the composition further comprises an extracellular matrix.","212 . The composition of claim 208 , wherein the scaffold comprises a natural material, a synthetic material, or a combination thereof.","213 . The composition of claim 212 , wherein the scaffold comprises the natural material.","214 . The composition of claim 212 , wherein the scaffold comprises the synthetic material.","215 . The method of claim 208 , wherein the tanning comprises a vegetable tanning, a chrome tanning, an aldehyde tanning, a syntan tanning, a bacterial tanning, or any combination thereof.",en,"CROSS REFERENCE This application is a continuation of U.S. application Ser. No. 18/347,349, filed Jul. 5, 2023, which is a continuation of 18/092,816, filed Jan. 3, 2023, now U.S. Pat. No. 11,739,217, issued Aug. 29, 2023, which is a continuation of U.S. application Ser. No. 17/751,440, filed May 23, 2022, now U.S. Pat. No. 11,591,471, issued Feb. 28, 2023, which is a continuation of U.S. application Ser. No. 17/366,550, filed Jul. 2, 2021, now U.S. Pat. No. 11,377,559, issued Jul. 5, 2022, which is a continuation of U.S. application Ser. No. 16/892,839, filed Jun. 4, 2020, now U.S. Pat. No. 11,091,639, issued Aug. 17, 2021, which is a continuation of U.S. application Ser. No. 16/299,734, filed Mar. 12, 2019, now U.S. Pat. No. 10,711,136, issued Jul. 14, 2020, which is a continuation of U.S. application Ser. No. 15/493,083, filed Apr. 20, 2017, now U.S. Pat. No. 10,273,549, issued Apr. 30, 2019 which claims priority to U.S. Provisional Application No. 62/325,819, filed on Apr. 21, 2016, each of which are incorporated herein by reference in their entirety. GOVERNMENT SUPPORT This invention was made with the support of National Institutes of Health (NIH) Grant Number R21 ARO61583 and R01 AR051930, Medical Research Council (UK) Grant Number G0801061, Research Service of the Department of Veterans Affairs and Dystrophic Epidermolysis Bullosa Research Association. SUMMARY OF THE DISCLOSURE Disclosed herein are methods of making a synthetic leather. In some embodiments, the method can comprise forming an artificial dermal layer comprising a fibroblast. In some embodiments, the method can comprise tanning at least a portion of a dermal layer, thereby forming a synthetic leather. In some embodiments, a fibroblast can be differentiated from an induced pluripotent stem cell. In some embodiments, the method can further comprise forming an artificial epidermal layer. In some embodiments, an epidermal layer can comprise a keratinocyte. In some embodiments, the method can comprise placing an epidermal layer upon a dermal layer thereby forming a layered structure. In some embodiments, a keratinocyte can be a mammalian keratinocyte. In some embodiments, a mammal can be a non-human mammal. In some embodiments, a keratinocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a keratinocyte can express KRT14, p63, DSG3, ITGB4, LAMA5, KRT5, TAp63, Lamb3, KRT18 or a combination thereof. In some embodiments, a layered structure can further comprise a melanocyte. In some embodiments, a melanocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a melanocyte can express Sox-10, MITF-M, gp-100, DCT, TYR, MLANA or a combination thereof. In some embodiments, a synthetic leather can comprise a pigment. In some embodiments, a melanocyte can be a mammalian melanocyte. In some embodiments, a mammal can be a human. In some embodiments, a fibroblast can express CD10, CD73, CD44, CD90, type I collagen, type III collagen, prolyl-4-hydroxylase beta, or a combination thereof. In some embodiments, a fibroblast can be a mammalian fibroblast. In some embodiments, a mammal can be a non-human mammal. In some embodiments, a non-human mammal can be one of a primate, bovine, ovine, porcine, equinine, canine, feline, rodent, or lagomorph. In some embodiments, a fibroblast can be a non-mammalian fibroblast. In some embodiments, a non-mammal can be a fish, a bird or a reptile. In some embodiments, an epidermal layer can further comprise collagen. In some embodiments, an epidermal layer can be subjected to further processing. In some embodiments, a layered structure can further comprise collagen. In some embodiments, a layered structure can be subjected to further processing. In some embodiments, a dermal layer can further comprise collagen. In some embodiments, a dermal layer can be subjected to further processing. In some embodiments, processing can be selected from a group consisting of preserving, soaking, bating, pickling, depickling, thinning, retanning, lubricating, crusting, wetting, sammying, shaving, rechroming, neutralizing, dyeing, fatliquoring, filling, stripping, stuffing, whitening, fixating, setting, drying, conditioning, milling, staking, buffing, finishing, oiling, brushing, padding, impregnating, spraying, roller coating, curtain coating. polishing, plating, embossing, ironing, glazing, tumbling and any combination thereof. In some embodiments, collagen can be produced at least in part by a collagen producing cell, can be separately added, or any combination thereof. In some embodiments, a collagen producing cell can comprise an epithelial cell, a keratinocyte, a fibroblast, a comeocyte, a melanocyte, a Langerhans cell, a basal cell, or a combination thereof. In some embodiments, a collagen producing cell can comprise a epithelial cell wherein the epithelial cell can comprise a squamous cell, a cuboidal cell, a columnar cell, a basal cell, or a combination thereof. In some embodiments, a collagen producing cell can comprise a keratinocyte wherein the keratinocyte can comprise an epithelial keratinocyte, basal keratinocyte, proliferating basal keratinocyte, differentiated suprabasal keratinocyte, or a combination thereof. In some embodiments, a collagen producing cell can comprise a smooth muscle cell. In some embodiments, a synthetic leather can further comprise one or more of keratin, elastin, gelatin, proteoglycan, dermatan sulfate proteoglycan, glycosoaminoglycan, fibronectin, laminin, dermatopontin, lipid, fatty acid, carbohydrate, or a combination thereof. In some embodiments, a thickness of a dermal layer can range from about 0.02 mm to about 5 mm. In some embodiments, a thickness of a dermal layer can range from about 0.1 mm to about 0.5 mm. In some embodiments, a synthetic leather can further comprise a first dermal layer and a second dermal layer. In some embodiments, a first dermal layer can be placed upon a second dermal layer. In some embodiments, a thickness of an epidermal layer can range from about 0.01 mm to about 2 mm. In some embodiments, a thickness of an epidermal layer can range from about 0.1 mm to about 0.2 mm. In some embodiments, a synthetic leather can further comprise a first epidermal layer and a second epidermal layer. In some embodiments, a synthetic leather can further comprise a basement membrane substitute. In some embodiments, a basement membrane substitute can be between an epidermal layer and a dermal layer. In some embodiments, a basement membrane substitute can comprise a dried acellular amniotic membrane. In some embodiments, a dermal layer can be formed upon a scaffold. In some embodiments, a scaffold can be natural or synthetic. In some embodiments, a scaffold can comprise silk. In some embodiments, a scaffold can comprise chitosan. In some embodiments, a scaffold can comprise a natural tissue adhesive. In some embodiments, a natural tissue adhesive can comprise fibrin glue. In some embodiments, a scaffold can be comprised in part in a synthetic leather. In some embodiments, a dermal layer or an epidermal layer can be cultured in vitro. In some embodiments, a dermal layer can be cultured in vitro. In some embodiments, a dermal layer can be cultured with a supplement. In some embodiments, a supplement can comprise one or more of collagen, fibrin, growth factors, ascorbic acid, dextran sulphate, or carrageenan. In some embodiments, a supplement can be a natural supplement. In some embodiments, a supplement can be a synthetic supplement. In some embodiments, an induced pluripotent stem cell can be produced through the induced gene expression of Oct3, Oct4, Sox2, Klf4, c-Myc or a combination thereof in an adult somatic cell. In some embodiments, at least a portion of a leather article can be formed from the methods disclosed herein. In some embodiments, a leather article can comprise one or more of a watch strap, a belt, a packaging, a shoe, a boot, a footwear, a glove, a clothing, a luggage, a bag, a clutch, a purse, a backpack, a wallet, a saddle, a harness, a whip, an interior, an exterior, an upholstery, a book binding, a furniture, a lamp, a lamp shade, a table covering, a wall covering, a floor covering, a ceiling covering, a car interior, a car exterior, a boat interior, a boat exterior, an airplane interior, a yacht interior, a yacht exterior, a pillow case, a sheet, a duvet cover, jewelry, an accessory, a pair of glasses, a pair of sun glasses, or a consumer electronic. In some embodiments, a leather article can be a watch strap. In some embodiments, a leather article can be a belt. In some embodiments, a leather article can be a bag. At least about 2% of cells comprised in a synthetic leather can be differentiated from an induced pluripotent stem cell. At least about 10% of cells comprised in synthetic leather can be differentiated from an induced pluripotent stem cell. At least about 50% of cells comprised in a synthetic leather can be differentiated from an induced pluripotent stem cell. Disclosed herein are methods of making a synthetic leather. In some embodiments, the method can comprise placing an artificial epidermal layer upon an artificial dermal layer thereby forming a layered structure. In some embodiments, an epidermal layer can comprise a keratinocyte and a dermal layer can comprise a fibroblast. In some embodiments, the method can comprise tanning at least a portion of a layered structure, thereby forming a synthetic leather. In some embodiments, a fibroblast or a keratinocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a keratinocyte can be a mammalian keratinocyte. In some embodiments, a mammal can be a non-human mammal. In some embodiments, a keratinocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a keratinocyte can express KRT14, p63, DSG3, ITGB4, LAMA5, KRT5, TAp63, Lamb3, KRT18 or a combination thereof. In some embodiments, a layered structure can further comprise a melanocyte. In some embodiments, a melanocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a melanocyte can express Sox-10, MITF-M, gp-100, DCT, TYR, MLANA or a combination thereof. In some embodiments, a synthetic leather can comprise a pigment. In some embodiments, a melanocyte can be a mammalian melanocyte. In some embodiments, a mammal can be a human. In some embodiments, a fibroblast can be differentiated from an induced pluripotent stem cell. In some embodiments. a fibroblast can express CD10, CD73, CD44, CD90, type I collagen, type III collagen, prolyl-4-hydroxylase beta, or a combination thereof. In some embodiments, a fibroblast can be a mammalian fibroblast. In some embodiments, a mammal can be a non-human mammal. In some embodiments, a non-human mammal can be one of a primate, bovine, ovine, porcine, equinine, canine, feline, rodent, or lagomorph. In some embodiments, a fibroblast can be a non-mammalian fibroblast. In some embodiments, a non-mammal can be a fish, a bird or a reptile. In some embodiments, an epidermal layer can further comprise collagen. In some embodiments, an epidermal layer can be subjected to further processing. In some embodiments, a layered structure can further comprise collagen. In some embodiments, a layered structure can be subjected to further processing. In some embodiments, a dermal layer can further comprise collagen. In some embodiments, a dermal layer can be subjected to further processing. In some embodiments, processing can be selected from a group consisting of preserving, soaking, bating, pickling, depickling, thinning, retanning, lubricating, crusting, wetting, sammying, shaving, rechroming, neutralizing, dyeing, fatliquoring, filling, stripping, stuffing, whitening, fixating, setting, drying, conditioning, milling, staking, buffing, finishing, oiling, brushing, padding, impregnating, spraying, roller coating, curtain coating, polishing, plating, embossing, ironing, glazing, tumbling and any combination thereof. In some embodiments, collagen can be produced at least in part by a collagen producing cell, can be separately added, or any combination thereof. In some embodiments, a collagen producing cell can comprise an epithelial cell, a keratinocyte, a fibroblast, a comeocyte, a melanocyte, a Langerhans cell, a basal cell, or a combination thereof. In some embodiments, a collagen producing cell can comprise a epithelial cell wherein the epithelial cell can comprise a squamous cell, a cuboidal cell, a columnar cell, a basal cell, or a combination thereof. In some embodiments, a collagen producing cell can comprise a keratinocyte wherein the keratinocyte can comprise an epithelial keratinocyte, basal keratinocyte, proliferating basal keratinocyte, differentiated suprabasal keratinocyte, or a combination thereof. In some embodiments, a collagen producing cell can comprise a smooth muscle cell. In some embodiments, a synthetic leather can further comprise one or more of keratin, elastin, gelatin, proteoglycan, dermatan sulfate proteoglycan, glycosoaminoglycan, fibronectin, laminin, dermatopontin, lipid, fatty acid, carbohydrate, or a combination thereof. In some embodiments, a thickness of a dermal layer can range from about 0.02 mm to about 5 mm. In some embodiments, a thickness of a dermal layer can range from about 0.1 mm to about 0.5 mm. In some embodiments, a synthetic leather can further comprise a first dermal layer and a second dermal layer. In some embodiments, a first dermal layer can be placed upon a second dermal layer. In some embodiments, a thickness of an epidermal layer can range from about 0.01 mm to about 2 mm. In some embodiments, a thickness of an epidermal layer can range from about 0.1 mm to about 0.2 mm. In some embodiments, a synthetic leather can further comprise a first epidermal layer and a second epidermal layer. In some embodiments, a synthetic leather can further comprise a basement membrane substitute. In some embodiments, a basement membrane substitute can be between an epidermal layer and a dermal layer. In some embodiments, a basement membrane substitute can comprise a dried acellular amniotic membrane. In some embodiments, a dermal layer can be formed upon a scaffold. In some embodiments, a scaffold can be natural or synthetic. In some embodiments, a scaffold can comprise silk. In some embodiments, a scaffold can comprise chitosan. In some embodiments, a scaffold can comprise a natural tissue adhesive. In some embodiments, a natural tissue adhesive can comprise fibrin glue. In some embodiments, a scaffold can be comprised in part in a synthetic leather. In some embodiments, a dermal layer or an epidermal layer can be cultured in vitro. In some embodiments, a dermal layer can be cultured in vitro. In some embodiments, a dermal layer can be cultured with a supplement. In some embodiments, a supplement can comprise one or more of collagen, fibrin, growth factors, ascorbic acid, dextran sulphate, or carrageenan. In some embodiments, a supplement can be a natural supplement. In some embodiments, a supplement can be a synthetic supplement. In some embodiments, an induced pluripotent stem cell can be produced through the induced gene expression of Oct3, Oct4, Sox2, Klf4, c-Myc or a combination thereof in an adult somatic cell. In some embodiments, at least a portion of a leather article can be formed from the methods disclosed herein. In some embodiments, a leather article can comprise one or more of a watch strap, a belt, a packaging, a shoe, a boot, a footwear, a glove, a clothing, a luggage, a bag, a clutch, a purse, a backpack, a wallet, a saddle, a harness, a whip, an interior, an exterior, an upholstery, a book binding, a furniture, a lamp, a lamp shade, a table covering, a wall covering, a floor covering, a ceiling covering, a car interior, a car exterior, a boat interior, a boat exterior, an airplane interior, a yacht interior, a yacht exterior, a pillow case, a sheet, a duvet cover, jewelry, an accessory, a pair of glasses, a pair of sun glasses, or a consumer electronic. In some embodiments, a leather article can be a watch strap. In some embodiments, a leather article can be a belt. In some embodiments, a leather article can be a bag. At least about 2% of cells comprised in a synthetic leather can be differentiated from an induced pluripotent stem cell. At least about 10% of cells comprised in synthetic leather can be differentiated from an induced pluripotent stem cell. At least about 50% of cells comprised in a synthetic leather can be differentiated from an induced pluripotent stem cell. Disclosed herein are methods of making a synthetic leather. In some embodiments, the method can comprise placing an artificial epidermal layer upon an artificial dermal layer thereby forming a layered structure. In some embodiments, an epidermal layer can comprise a keratinocyte and a dermal layer can comprise a fibroblast. In some embodiments, the method can comprise removing at least a portion of an epidermal layer from a layered structure to form a removed product. In some embodiments, the method can comprise tanning at least a portion of a removed product, thereby forming a synthetic leather. In some embodiments, a fibroblast or a keratinocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a keratinocyte can be a mammalian keratinocyte. In some embodiments, a mammal can be a non-human mammal. In some embodiments, a keratinocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a keratinocyte can express KRT14, p63, DSG3, ITGB4, LAMA5, KRT5, TAp63, Lamb3, KRT18 or a combination thereof. In some embodiments, a layered structure can further comprise a melanocyte. In some embodiments, a melanocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a melanocyte can express Sox-10, MITF-M, gp-100, DCT, TYR, MLANA or a combination thereof. In some embodiments, a synthetic leather can comprise a pigment. In some embodiments, a melanocyte can be a mammalian melanocyte. In some embodiments, a mammal can be a human. In some embodiments, a fibroblast can be differentiated from an induced pluripotent stem cell. In some embodiments, a fibroblast can express CD10, CD73, CD44, CD90, type I collagen, type III collagen, prolyl-4-hydroxylase beta, or a combination thereof. In some embodiments, a removed product can further comprise collagen. In some embodiments, a removed product can be subjected to further processing. In some embodiments, a fibroblast can be a mammalian fibroblast. In some embodiments, a mammal can be a non-human mammal. In some embodiments, a non-human mammal can be one of a primate, bovine, ovine, porcine, equinine, canine, feline, rodent, or lagomorph. In some embodiments, a fibroblast can be a non-mammalian fibroblast. In some embodiments, a non-mammal can be a fish, a bird or a reptile. In some embodiments, an epidermal layer can further comprise collagen. In some embodiments, an epidermal layer can be subjected to further processing. In some embodiments, a layered structure can further comprise collagen. In some embodiments, a layered structure can be subjected to further processing. In some embodiments, a dermal layer can further comprise collagen. In some embodiments, a dermal layer can be subjected to further processing. In some embodiments, processing can be selected from a group consisting of preserving, soaking, bating, pickling, depickling, thinning, retanning, lubricating, crusting, wetting, sammying, shaving, rechroming, neutralizing, dyeing, fatliquoring, filling, stripping, stuffing, whitening, fixating, setting, drying, conditioning, milling, staking, buffing, finishing, oiling, brushing, padding, impregnating, spraying, roller coating, curtain coating, polishing, plating, embossing, ironing, glazing, tumbling and any combination thereof. In some embodiments, collagen can be produced at least in part by a collagen producing cell, can be separately added, or any combination thereof. In some embodiments, a collagen producing cell can comprise an epithelial cell, a keratinocyte, a fibroblast, a comeocyte, a melanocyte, a Langerhans cell, a basal cell, or a combination thereof. In some embodiments, a collagen producing cell can comprise a epithelial cell wherein the epithelial cell can comprise a squamous cell, a cuboidal cell, a columnar cell, a basal cell, or a combination thereof. In some embodiments, a collagen producing cell can comprise a keratinocyte wherein the keratinocyte can comprise an epithelial keratinocyte, basal keratinocyte, proliferating basal keratinocyte, differentiated suprabasal keratinocyte, or a combination thereof. In some embodiments, a collagen producing cell can comprise a smooth muscle cell. In some embodiments, a synthetic leather can further comprise one or more of keratin, elastin, gelatin, proteoglycan, dermatan sulfate proteoglycan, glycosoaminoglycan, fibronectin, laminin, dermatopontin, lipid, fatty acid, carbohydrate, or a combination thereof. In some embodiments, a thickness of a dermal layer can range from about 0.02 mm to about 5 mm. In some embodiments, a thickness of a dermal layer can range from about 0.1 mm to about 0.5 mm. In some embodiments, a synthetic leather can further comprise a first dermal layer and a second dermal layer. In some embodiments, a first dermal layer can be placed upon a second dermal layer. In some embodiments, a thickness of an epidermal layer can range from about 0.01 mm to about 2 mm. In some embodiments, a thickness of an epidermal layer can range from about 0.1 mm to about 0.2 mm. In some embodiments, a synthetic leather can further comprise a first epidermal layer and a second epidermal layer. In some embodiments, a synthetic leather can further comprise a basement membrane substitute. In some embodiments, a basement membrane substitute can be between an epidermal layer and a dermal layer. In some embodiments, a basement membrane substitute can comprise a dried acellular amniotic membrane. In some embodiments, a dermal layer can be formed upon a scaffold. In some embodiments, a scaffold can be natural or synthetic. In some embodiments, a scaffold can comprise silk. In some embodiments, a scaffold can comprise chitosan. In some embodiments, a scaffold can comprise a natural tissue adhesive. In some embodiments, a natural tissue adhesive can comprise fibrin glue. In some embodiments, a scaffold can be comprised in part in a synthetic leather. In some embodiments, a dermal layer or an epidermal layer can be cultured in vitro. In some embodiments, a dermal layer can be cultured in vitro. In some embodiments, a dermal layer can be cultured with a supplement. In some embodiments, a supplement can comprise one or more of collagen, fibrin, growth factors, ascorbic acid, dextran sulphate, or carrageenan. In some embodiments, a supplement can be a natural supplement. In some embodiments, a supplement can be a synthetic supplement. In some embodiments, an induced pluripotent stem cell can be produced through the induced gene expression of Oct3, Oct4, Sox2, Klf4, c-Myc or a combination thereof in an adult somatic cell. In some embodiments, at least a portion of a leather article can be formed from the methods disclosed herein. In some embodiments, a leather article can comprise one or more of a watch strap, a belt, a packaging, a shoe, a boot, a footwear, a glove, a clothing, a luggage, a bag, a clutch, a purse, a backpack, a wallet, a saddle, a harness, a whip, an interior, an exterior, an upholstery, a book binding, a furniture, a lamp, a lamp shade, a table covering, a wall covering, a floor covering, a ceiling covering, a car interior, a car exterior, a boat interior, a boat exterior, an airplane interior, a yacht interior, a yacht exterior, a pillow case, a sheet, a duvet cover, jewelry, an accessory, a pair of glasses, a pair of sun glasses, or a consumer electronic. In some embodiments, a leather article can be a watch strap. In some embodiments, a leather article can be a belt. In some embodiments, a leather article can be a bag. At least about 2% of cells comprised in a synthetic leather can be differentiated from an induced pluripotent stem cell. At least about 10% of cells comprised in synthetic leather can be differentiated from an induced pluripotent stem cell. At least about 50% of cells comprised in a synthetic leather can be differentiated from an induced pluripotent stem cell. Disclosed herein are tanned synthetic leathers. In some embodiments, prior to tanning a tanned synthetic leather can comprise an artificial dermal layer comprising fibroblast. In some embodiments, a fibroblast can be differentiated from an induced pluripotent stem cell. In some embodiments, prior to tanning, a tanned synthetic leather can further comprise an artificial epidermal layer. In some embodiments, an epidermal layer can further comprise a keratinocyte. In some embodiments, an epidermal layer can be upon a dermal layer thereby forming a layered structure. In some embodiments, a keratinocyte can be a mammalian keratinocyte. In some embodiments, a mammal can be a non-human mammal. In some embodiments, a keratinocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a keratinocyte can express KRT14, p63, DSG3, ITGB4, LAMA5, KRT5, TAp63, Lamb3, KRT18 or a combination thereof. In some embodiments, a layered structure can further comprise a melanocyte. In some embodiments, a melanocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a melanocyte can express Sox-10, MITF-M, gp-100, DCT, TYR, MLANA or a combination thereof. In some embodiments, a synthetic leather can further comprise a pigment. In some embodiments, a melanocyte can be a mammalian melanocyte. In some embodiments, a mammal can be a non-human mammal. In some embodiments, a fibroblast can express CD10, CD73, CD44, CD90, type I collagen, type III collagen, prolyl-4-hydroxylase beta, or a combination thereof. In some embodiments, a fibroblast can be a mammalian fibroblast. In some embodiments, a mammal can be a non-human mammal. In some embodiments, a non-human mammal can be one of a primate, bovine, ovine, porcine, equinine, canine, feline, rodent, or lagomorph. In some embodiments, a fibroblast can be a non-mammalian fibroblast. In some embodiments, a non-mammal can be a fish, a bird or a reptile. In some embodiments, an epidermal layer cam further comprises collagen. In some embodiments, a layered structure can further comprise collagen. In some embodiments, a dermal layer can further comprise collagen. In some embodiments, collagen can be produced at least in part by a collagen producing cell, can be separately added, or any combination thereof. In some embodiments, a collagen producing cell can comprise an epithelial cell, a keratinocyte, a fibroblast, a comeocyte, a melanocyte, a Langerhans cell, a basal cell, or a combination thereof. In some embodiments, a collagen producing cell can comprise an epithelial cell wherein an epithelial cell can comprise a squamous cell, a cuboidal cell, a columnar cell, a basal cell, or a combination thereof. In some embodiments, a collagen producing cell can comprises a keratinocyte wherein a keratinocyte comprises epithelial keratinocyte, basal keratinocyte, proliferating basal keratinocyte, differentiated suprabasal keratinocyte, or a combination thereof. In some embodiments, a collagen producing cells can comprise a smooth muscle cell. In some embodiments, a synthetic leather can further comprise one or more of keratin, elastin, gelatin, proteoglycan, dermatan sulfate proteoglycan, glycosoaminoglycan, fibronectin, laminin, dermatopontin, lipid, fatty acid, carbohydrate, or a combination thereof. In some embodiments, a thickness of a dermal layer can range from about 0.02 mm to about 5 mm. In some embodiments, a thickness of a dermal layer can range from about 0.1 mm to about 0.5 mm. In some embodiments, a synthetic leather can further comprise a first dermal layer and a second dermal layer. In some embodiments, a first dermal layer can be upon a second dermal layer. In some embodiments, a thickness of an epidermal layer can range from about 0.01 mm to about 2 mm. In some embodiments, a thickness of an epidermal layer can range from about 0.1 mm to about 0.2 mm. In some embodiments, a synthetic leather can further comprise a first epidermal layer and a second epidermal layer. In some embodiments, a synthetic leather can comprise a basement membrane substitute. In some embodiments, a basement membrane substitute can be between an epidermal layer and an dermal layer. In some embodiments, a basement membrane substitute can comprise a dried acellular amniotic membrane. In some embodiments, a dermal layer can be formed upon a scaffold. In some embodiments, a scaffold can be natural or synthetic. In some embodiments, a scaffold can comprise silk. In some embodiments, a scaffold can comprise chitosan. In some embodiments, a scaffold can comprise a natural tissue adhesive. In some embodiments, a natural tissue adhesive can comprise fibrin glue. In some embodiments, a scaffold can be comprised in part in a tanned synthetic leather. In some embodiments, a dermal layer or an epidermal layer can be cultured in vitro. In some embodiments, a dermal layer can be cultured in vitro. In some embodiments, a tanned synthetic leather can be comprised in one or more of a watch strap, a belt, a packaging, a shoe, a boot, a footwear, a glove, a clothing, a luggage, a bag, a clutch, a purse, a backpack, a wallet, a saddle, a harness, a whip, an interior, an exterior, an upholstery, a book binding, a furniture, a lamp, a lamp shade, a table covering, a wall covering, a floor covering, a ceiling covering, a car interior, a car exterior, a boat interior, a boat exterior, an airplane interior, a yacht interior, a yacht exterior, a pillow case, a sheet, a duvet cover, jewelry, an accessory, a pair of glasses, a pair of sun glasses, or a consumer electronic. In some embodiments, a tanned synthetic leather can be comprised in a watch strap. In some embodiments, a tanned synthetic leather can be comprised in a belt. In some embodiments, a tanned synthetic leather can be comprised in a bag. At least about 2% of cells comprised in a tanned synthetic leather can be differentiated from an induced pluripotent stem cell. At least about 10% of cells comprised in tanned synthetic leather can be differentiated from an induced pluripotent stem cell. At least about 50% of cells comprised in a tanned synthetic leather can be differentiated from an induced pluripotent stem cell. Disclosed herein are tanned synthetic leathers. In some embodiments, prior to tanning a tanned synthetic leather can comprise a layered structure. In some embodiments, a layered structure can comprise an artificial dermal layer. In some embodiments, a dermal layer can comprise a fibroblast. In some embodiments, a layered structure can comprise an artificial epidermal layer. In some embodiments an epidermal layer can comprise a keratinocyte. In some embodiments, a fibroblast or a keratinocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a fibroblast can be differentiated from an induced pluripotent stem cell. In some embodiments, a keratinocyte can be a mammalian keratinocyte. In some embodiments, a mammal can be a non-human mammal. In some embodiments, a keratinocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a keratinocyte can express KRT14, p63, DSG3, ITGB4, LAMA5, KRT5, TAp63, Lamb3, KRT18 or a combination thereof. In some embodiments, a layered structure can further comprise a melanocyte. In some embodiments, a melanocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a melanocyte can express Sox-10, MITF-M, gp-100, DCT, TYR, MLANA or a combination thereof. In some embodiments, a synthetic leather can further comprise a pigment. In some embodiments, a melanocyte can be a mammalian melanocyte. In some embodiments, a mammal can be a non-human mammal. In some embodiments, a fibroblast can express CD10, CD73, CD44, CD90, type I collagen, type III collagen, prolyl-4-hydroxylase beta, or a combination thereof. In some embodiments, a fibroblast can be a mammalian fibroblast. In some embodiments, a mammal can be a non-human mammal. In some embodiments, a non-human mammal can be one of a primate, bovine, ovine, porcine, equinine, canine, feline, rodent, or lagomorph. In some embodiments, a fibroblast can be a non-mammalian fibroblast. In some embodiments, a non-mammal can be a fish, a bird or a reptile. In some embodiments, an epidermal layer cam further comprises collagen. In some embodiments, a layered structure can further comprise collagen. In some embodiments, a dermal layer can further comprise collagen. In some embodiments, collagen can be produced at least in part by a collagen producing cell, can be separately added, or any combination thereof. In some embodiments, a collagen producing cell can comprise an epithelial cell, a keratinocyte, a fibroblast, a comeocyte, a melanocyte, a Langerhans cell, a basal cell, or a combination thereof. In some embodiments, a collagen producing cell can comprise an epithelial cell wherein an epithelial cell can comprise a squamous cell, a cuboidal cell, a columnar cell, a basal cell, or a combination thereof. In some embodiments, a collagen producing cell can comprises a keratinocyte wherein a keratinocyte comprises epithelial keratinocyte, basal keratinocyte, proliferating basal keratinocyte, differentiated suprabasal keratinocyte, or a combination thereof. In some embodiments, a collagen producing cells can comprise a smooth muscle cell. In some embodiments, a synthetic leather can further comprise one or more of keratin, elastin, gelatin, proteoglycan, dermatan sulfate proteoglycan, glycosoaminoglycan, fibronectin, laminin, dermatopontin, lipid, fatty acid, carbohydrate, or a combination thereof. In some embodiments, a thickness of a dermal layer can range from about 0.02 mm to about 5 mm. In some embodiments, a thickness of a dermal layer can range from about 0.1 mm to about 0.5 mm. In some embodiments, a synthetic leather can further comprise a first dermal layer and a second dermal layer. In some embodiments, a first dermal layer can be upon a second dermal layer. In some embodiments, a thickness of an epidermal layer can range from about 0.01 mm to about 2 mm. In some embodiments, a thickness of an epidermal layer can range from about 0.1 mm to about 0.2 mm. In some embodiments, a synthetic leather can further comprise a first epidermal layer and a second epidermal layer. In some embodiments, a synthetic leather can comprise a basement membrane substitute. In some embodiments, a basement membrane substitute can be between an epidermal layer and a dermal layer. In some embodiments, a basement membrane substitute can comprise a dried acellular amniotic membrane. In some embodiments, a dermal layer can be formed upon a scaffold. In some embodiments, a scaffold can be natural or synthetic. In some embodiments, a scaffold can comprise silk. In some embodiments, a scaffold can comprise chitosan. In some embodiments, a scaffold can comprise a natural tissue adhesive. In some embodiments, a natural tissue adhesive can comprise fibrin glue. In some embodiments, a scaffold can be comprised in part in a tanned synthetic leather. In some embodiments, a dermal layer or an epidermal layer can be cultured in vitro. In some embodiments, a dermal layer can be cultured in vitro. In some embodiments, a tanned synthetic leather can be comprised in one or more of a watch strap, a belt, a packaging, a shoe, a boot, a footwear, a glove, a clothing, a luggage, a bag, a clutch, a purse, a backpack, a wallet, a saddle, a harness, a whip, an interior, an exterior, an upholstery, a book binding, a furniture, a lamp, a lamp shade, a table covering, a wall covering, a floor covering, a ceiling covering, a car interior, a car exterior, a boat interior, a boat exterior, an airplane interior, a yacht interior, a yacht exterior, a pillow case, a sheet, a duvet cover, jewelry, an accessory, a pair of glasses, a pair of sun glasses, or a consumer electronic. In some embodiments, a tanned synthetic leather can be comprised in a watch strap. In some embodiments, a tanned synthetic leather can be comprised in a belt. In some embodiments, a tanned synthetic leather can be comprised in a bag. At least about 2% of cells comprised in a tanned synthetic leather can be differentiated from an induced pluripotent stem cell. At least about 10% of cells comprised in tanned synthetic leather can be differentiated from an induced pluripotent stem cell. At least about 50% of cells comprised in a tanned synthetic leather can be differentiated from an induced pluripotent stem cell. Disclosed herein are tanned synthetic leathers. In some embodiments, prior to tanning a tanned synthetic leather can comprise a removed product comprising a layered structure. In some embodiments, a layered structure can comprise an artificial dermal layer. In some embodiments, a dermal layer can comprise a fibroblast. In some embodiments, a layered structure can comprise an artificial epidermal layer. In some embodiments, an epidermal layer can comprise a keratinocyte. In some embodiments, a portion of an epidermal layer can be removed. In some embodiments, a fibroblast or a keratinocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a removed product can further comprise collagen. In some embodiments, a fibroblast can be differentiated from an induced pluripotent stem cell. In some embodiments, a keratinocyte can be a mammalian keratinocyte. In some embodiments, a mammal can be a non-human mammal. In some embodiments, a keratinocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a keratinocyte can express KRT14, p63, DSG3, ITGB4, LAMA5, KRT5, TAp63, Lamb3, KRT18 or a combination thereof. In some embodiments, a layered structure can further comprise a melanocyte. In some embodiments, a melanocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a melanocyte can express Sox-10, MITF-M, gp-100, DCT, TYR, MLANA or a combination thereof. In some embodiments, a synthetic leather can further comprise a pigment. In some embodiments, a melanocyte can be a mammalian melanocyte. In some embodiments, a mammal can be a non-human mammal. In some embodiments, a fibroblast can express CD10, CD73, CD44, CD90, type I collagen, type III collagen, prolyl-4-hydroxylase beta, or a combination thereof. In some embodiments, a fibroblast can be a mammalian fibroblast. In some embodiments, a mammal can be a non-human mammal. In some embodiments, a non-human mammal can be one of a primate, bovine, ovine, porcine, equinine, canine, feline, rodent, or lagomorph. In some embodiments, a fibroblast can be a non-mammalian fibroblast. In some embodiments, a non-mammal can be a fish, a bird or a reptile. In some embodiments, an epidermal layer cam further comprises collagen. In some embodiments, a layered structure can further comprise collagen. In some embodiments, a dermal layer can further comprise collagen. In some embodiments, collagen can be produced at least in part by a collagen producing cell, can be separately added, or any combination thereof. In some embodiments, a collagen producing cell can comprise an epithelial cell, a keratinocyte, a fibroblast, a comeocyte, a melanocyte, a Langerhans cell, a basal cell, or a combination thereof. In some embodiments, a collagen producing cell can comprise an epithelial cell wherein an epithelial cell can comprise a squamous cell, a cuboidal cell, a columnar cell, a basal cell, or a combination thereof. In some embodiments, a collagen producing cell can comprises a keratinocyte wherein a keratinocyte comprises epithelial keratinocyte, basal keratinocyte, proliferating basal keratinocyte, differentiated suprabasal keratinocyte, or a combination thereof. In some embodiments, a collagen producing cells can comprise a smooth muscle cell. In some embodiments, a synthetic leather can further comprise one or more of keratin. elastin, gelatin, proteoglycan, dermatan sulfate proteoglycan, glycosoaminoglycan, fibronectin, laminin, dermatopontin, lipid, fatty acid, carbohydrate, or a combination thereof. In some embodiments, a thickness of a dermal layer can range from about 0.02 mm to about 5 mm. In some embodiments, a thickness of a dermal layer can range from about 0.1 mm to about 0.5 mm. In some embodiments, a synthetic leather can further comprise a first dermal layer and a second dermal layer. In some embodiments, a first dermal layer can be upon a second dermal layer. In some embodiments, a thickness of an epidermal layer can range from about. 0.01 mm to about 2 mm. In some embodiments, a thickness of an epidermal layer can range from about 0.1 mm to about 0.2 mm. In some embodiments, a synthetic leather can further comprise a first epidermal layer and a second epidermal layer. In some embodiments, a synthetic leather can comprise a basement membrane substitute. In some embodiments, a basement membrane substitute can be between an epidermal layer and an dermal layer. In some embodiments, a basement membrane substitute can comprise a dried acellular amniotic membrane. In some embodiments, a dermal layer can be formed upon a scaffold. In some embodiments, a scaffold can be natural or synthetic. In some embodiments, a scaffold can comprise silk. In some embodiments, a scaffold can comprise chitosan. In some embodiments, a scaffold can comprise a natural tissue adhesive. In some embodiments, a natural tissue adhesive can comprise fibrin glue. In some embodiments, a scaffold can be comprised in part in a tanned synthetic leather. In some embodiments, a dermal layer or an epidermal layer can be cultured in vitro. In some embodiments, a dermal layer can be cultured in vitro. In some embodiments, a tanned synthetic leather can be comprised in one or more of a watch strap, a belt, a packaging, a shoe, a boot, a footwear, a glove, a clothing, a luggage, a bag, a clutch, a purse, a backpack, a wallet, a saddle, a harness, a whip, an interior, an exterior, an upholstery, a book binding, a furniture, a lamp, a lamp shade, a table covering, a wall covering, a floor covering, a ceiling covering, a car interior, a car exterior, a boat interior, a boat exterior, an airplane interior, a yacht interior, a yacht exterior, a pillow case, a sheet, a duvet cover, jewelry, an accessory, a pair of glasses, a pair of sun glasses, or a consumer electronic. In some embodiments, a tanned synthetic leather can be comprised in a watch strap. In some embodiments, a tanned synthetic leather can be comprised in a belt. In some embodiments, a tanned synthetic leather can be comprised in a bag. At least about 2% of cells comprised in a tanned synthetic leather can be differentiated from an induced pluripotent stem cell. At least about 10% of cells comprised in tanned synthetic leather can be differentiated from an induced pluripotent stem cell. At least about 50% of cells comprised in a tanned synthetic leather can be differentiated from an induced pluripotent stem cell. Disclosed herein are artificial epidermal layers. In some embodiments, an artificial epidermal layer can comprise a hair follicle cell and a melanocyte. In some embodiments, an artificial epidermal layer can comprise a hair follicle cell. In some embodiments, an artificial epidermal layer can comprise a melanocyte. In some embodiments, a melanocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a hair follicle cell can comprise a dermal papilla cell, an outer root sheath cell or a combination thereof. In some embodiments, a melanocyte can be a mammalian melanocyte. In some embodiments, an epidermal layer can further comprise a keratinocyte. In some embodiments, a keratinocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a keratinocyte can express KRT14, p63, DSG3, ITGB4, LAMA5, KRT5, TAp63, Lamb3, KRT18 or a combination thereof. In some embodiments, a keratinocyte can be a mammalian keratinocyte. In some embodiments, a mammal can be a non-human mammal. In some embodiments, a mammal can be a human mammal. In some embodiments, a non-human mammal can be one of a primate, bovine, ovine, porcine, equinine, canine, feline, rodent, or lagomorph. In some embodiments, a fibroblast can be a non-mammalian fibroblast. In some embodiments, a non-mammal can be a fish, a bird or a reptile. In some embodiments, a melanocyte can express Sox-10, MITF-M, gp-100, DCT, TYR, MLANA or a combination thereof. In some embodiments, an epidermal layer can comprise a hair follicle. At least about 2% of cells comprised in an artificial epidermal layer can be differentiated from an induced pluripotent stem cell. At least about 10% of cells comprised in an artificial epidermal layer can be differentiated from an induced pluripotent stem cell. At least about 50% of cells comprised in an artificial epidermal layer can be differentiated from an induced pluripotent stem cell. Disclosed herein are layered structures. In some embodiments, a layered structure can comprise an artificial epidermal layer. In some embodiments, an epidermal layer can comprise a hair follicle cell. In some embodiments, a layered structure can comprise an artificial dermal layer. In some embodiments, a dermal layer can comprise a fibroblast. In some embodiments, a fibroblast can be differentiated from an induced pluripotent stem cell. In some embodiments, a fibroblast can express CD10, CD73, CD44, CD90, type I collagen, type III collagen, prolyl-4-hydroxylase beta, or a combination thereof. In some embodiments, an epidermal layer can further comprise a keratinocyte. In some embodiments, a keratinocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a keratinocyte can express KRT14, p63, DSG3, ITGB4, LAMA5, KRT5, TAp63, Lamb3, KRT18 or a combination thereof. In some embodiments, a hair follicle cell can be a dermal papilla cell, outer root sheath cell or a combination thereof. In some embodiments, an epidermal layer can further comprise a melanocyte. In some embodiments, a melanocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a melanocyte can express Sox-10, MITF-M, gp-100, DCT, TYR, MLANA or a combination thereof. In some embodiments, a layered structure can be pigmented. In some embodiments, an epidermal layer can be stratified. In some embodiments, a layered structure can comprise a basement membrane substitute. In some embodiments, a basement membrane substitute can be between an epidermal layer and a dermal layer. In some embodiments, a basement membrane substitute can comprise a dried acellular amniotic membrane. In some embodiments, a layered structure can further comprise a scaffold. In some embodiments, a scaffold can be natural or synthetic. In some embodiments, a scaffold can comprise silk. In some embodiments, a scaffold can comprise chitosan. In some embodiments, a dermal layer can be upon a scaffold. In some embodiments, a layered structure can comprise one or more components selected from a group consisting of keratin, elastin, gelatin, proteoglycan, dermatan sulfate proteoglycan, glycosoaminoglycan, fibronectin, laminin, dermatopontin, lipid, fatty acid, carbohydrate, or a combination thereof. In some embodiments, a layered structure can further comprise two or more dermal layers. In some embodiments, a layered structure can further comprise a hair follicle. In some embodiments, a layered structure can further comprise a fur. At least about 2% of cells comprised in a layered structure can be differentiated from an induced pluripotent stem cell. At least about 10% of cells comprised in a layered structure can be differentiated from an induced pluripotent stem cell. At least about 50% of cells comprised in layered structure can be differentiated from an induced pluripotent stem cell. Disclosed herein are methods for making a layered structure. In some embodiments, the method can comprise placing an artificial epidermal layer comprising a hair follicle cell upon an artificial dermal layer comprising a cell differentiated from an induced pluripotent stem cell thereby forming a layered structure. In some embodiments, a cell differentiated from an induced pluripotent stem cell can be a fibroblast, melanocyte, keratinocyte or a combination thereof. In some embodiments, a cell differentiated from an induced pluripotent stem cell can be a fibroblast. In some embodiments, a fibroblast can express CD10, CD73, CD44, CD90, type I collagen, type III collagen, prolyl-4-hydroxylase beta, or a combination thereof. In some embodiments, an epidermal layer can further comprise a keratinocyte. In some embodiments, a keratinocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a keratinocyte can express KRT14, p63, DSG3, ITGB4, LAMA5, KRT5, TAp63, Lamb3, KRT18 or a combination thereof. In some embodiments, a hair follicle cell can comprise a dermal papilla cell, an outer root sheath cell or a combination thereof. In some embodiments, an epidermal layer can further comprise a melanocyte. In some embodiments, a melanocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a melanocyte can express Sox-10, MITF-M, gp-100, DCT, TYR, MLANA or a combination thereof. In some embodiments, a dermal layer can be cultured with a supplement. In some embodiments, a supplement can comprise collagen, fibrin, growth factors, ascorbic acid, dextran sulphate, carrageenan or a combination thereof. In some embodiments, a supplement can be a natural supplement. In some embodiments, a supplement can be a synthetic supplement. In some embodiments, a dermal layer can be cultured upon a scaffold. In some embodiments, a scaffold can be natural or synthetic. In some embodiments, a scaffold can comprise silk. In some embodiments, a scaffold can comprise chitosan. In some embodiments, a dermal layer can be placed upon a scaffold. In some embodiments, an epidermal layer can be stratified. In some embodiments, a dermal layer can be cultured upon a second dermal layer. In some embodiments, a dermal layer can be cultured in vivo. In some embodiments, a dermal layer may not be cultured upon a collagen matrix. In some embodiments, a thickness of a dermal layer can range from about 0.02 mm to about 5 mm. In some embodiments, a thickness of a dermal layer can range from about 0.1 mm to about 0.5 mm. In some embodiments, a thickness of an epidermal layer can range from about 0.01 mm to about 2 mm. In some embodiments, a thickness of an epidermal layer can range from about 0.1 mm to about 0.2 mm. At least about 2% of cells comprised in a layered structure can be differentiated from an induced pluripotent stem cell. At least about 10% of cells comprised in a layered structure can be differentiated from an induced pluripotent stem cell. At least about 50% of cells comprised in layered structure can be differentiated from an induced pluripotent stem cell. Disclosed herein are layered structures. In some embodiments, a layered structure can comprise an artificial epidermal layer comprising a hair follicle cell and a keratinocyte or a melanocyte; an artificial dermal layer comprising a fibroblast, wherein a fibroblast, a keratinocyte or a melanocyte can be differentiated from an induced pluripotent stem cell, wherein a melanocyte expresses Sox-10, MITF-M, gp-100, DCT, TYR, MLANA or a combination thereof, wherein a fibroblast expresses CD10, CD73, CD44, CD90, type I collagen, type III collagen, prolyl-4-hydroxylase beta, or a combination thereof, wherein a keratinocyte expresses KRT14, p63, DSG3, ITGB4, LAMA5, KRT5, TAp63, Lamb3, KRT18 or a combination thereof. In some embodiments, a layered structure can comprise an artificial epidermal layer. In some embodiments, an epidermal layer can comprise a hair follicle cell. In some embodiments, an epidermal layer can comprise a keratinocyte or a melanocyte. In some embodiments, a layered structure can comprise an artificial dermal layer. In some embodiment, a dermal layer can comprise a fibroblast. In some embodiments, a fibroblast, a keratinocyte or a melanocyte can be differentiated from an induced pluripotent stem cell. In some embodiments, a melanocyte can express Sox-10, MITF-M, gp-100, DCT, TYR, MLANA or a combination thereof. In some embodiments, a fibroblast can express CD10, CD73, CD44, CD90, type I collagen, type III collagen, prolyl-4-hydroxylase beta, or a combination thereof. In some embodiments, a keratinocyte can express KRT14, p63, DSG3, ITGB4, LAMA5, KRT5, TAp63, Lamb3, KRT18 or a combination thereof. In some embodiments, a thickness of a dermal layer can range from about 0.02 mm to about 5 mm. In some embodiments, a thickness of a dermal layer can range from about 0.1 mm to about 0.5 mm. In some embodiments, a thickness of an epidermal layer can range from about 0.01 mm to about 2 mm. In some embodiments, a thickness of an epidermal layer can range from about 0.1 mm to about 0.2 mm. At least about 2% of cells comprised in a layered structure can be differentiated from an induced pluripotent stem cell. At least about 10% of cells comprised in a layered structure can be differentiated from an induced pluripotent stem cell. At least about 50% of cells comprised in layered structure can be differentiated from an induced pluripotent stem cell. Disclosed herein are artificial epidermal layer. An artificial epidermal layer can comprise a stratum corneum. An artificial epidermal layer can comprise a stratum granulosum. An artificial epidermal layer can comprise a stratum spinosum. An artificial epidermal layer can comprise a stratum basale. In some embodiments a stratum corneum, a stratum granulosum, a stratum spinosum, or a stratum basale can be organized as depicted in FIG. 6A , or FIG. 8A . A thickness of a stratum corneum can range from about 0.01 mm to about 0.05 mm. A thickness of a stratum granulosum can range from about 0.01 mm to about 0.15 mm. A thickness of a stratum spinosum can range from about 0.01 mm to about 0.15 mm. A thickness of said stratum basale can range from about 0.01 mm to about 0.15 mm. A thickness of a stratum corneum can range from about 4% to about 20% of an artificial epidermal layer. A thickness of a stratum granulosum can range from about 4% to about 60% of a artificial epidermal layer. A thickness of a stratum spinosum can range from about 4% to about 40% of a artificial epidermal layer. A thickness of a stratum basale can range from about 4% to about 40% of an artificial epidermal layer. At least about 2% of cells comprised in an artificial epidermal layer can be differentiated from an induced pluripotent stem cell. At least about 10% of cells comprised in an artificial epidermal layer can be differentiated from an induced pluripotent stem cell. At least about 50% of cells comprised in a artificial epidermal layer can be differentiated from an induced pluripotent stem cell. INCORPORATION BY REFERENCE All publications, patents, and patent applications mentioned in this specification are herein incorporated by reference to the same extent as if each individual publication, patent, or patent application was specifically and individually indicated to be incorporated by reference. BRIEF DESCRIPTION OF THE DRAWINGS The novel features described herein are set forth with particularity in the appended claims. A better understanding of the features and advantages of the features described herein will be obtained by reference to the following detailed description that sets forth illustrative examples, in which the principles of the features described herein are utilized, and the accompanying drawings of which: FIG. 1 illustrates a synthetic leather production schematic. FIGS. 2A-2F illustrate a layered structure. FIG. 2A depicts a layered structure comprising an epidermal layer and a dermal layer on a scaffold. FIG. 2B depicts a layered structure comprising an epidermal layer, a basement membrane substitute and a dermal layer on a scaffold. FIG. 2C depicts a layered structure comprising an epidermal layer and multiple dermal layers on a scaffold. FIG. 2D depicts a layered structure comprising an epidermal layer, a basement membrane substitute and multiple dermal layers on a scaffold. FIG. 2E depicts a layered structure comprising an epidermal layer, a basement membrane substitute and multiple dermal layers. FIG. 2F depicts a layered structure comprising an epidermal layer and multiple dermal layers. FIG. 3 illustrates a layered structure development. FIGS. 4A-4C illustrate a comparative analysis of leather ( FIG. 4A ), native skin ( FIG. 4B ) and epidermal equivalent ( FIG. 4C ). FIGS. 5A-5C illustrate a comparative analysis of stratum corneum of native skin and epidermal equivalent. FIG. 5A depicts an epidermal surface image. FIG. 5B depicts a corneo-desmosome image. FIG. 5C depicts a CDSN/Hoechst image. FIGS. 6A-6E illustrate a comparative analysis of stratum granulosum of native skin and epidermal equivalent. FIG. 6A depicts a Loricrin (LOR) staining. FIG. 6B depicts an epidermal Ca ++ gradient captured on transmission electron microscopy as electron-dense precipitates. FIG. 6C depicts an assessment of permeability barrier integrity by lanthanum perfusion. FIG. 6D illustrates that tight junction protein 1/zonula occludens-1 (TJP1/ZO-1) anchors tight junction strand proteins, which can be fibril-like structures within the lipid bilayer, to the actin cytoskeleton. FIG. 6E illustrates that Filaggrin (FLG) monomers, tandemly clustered into a large, 350 kDa protein precursor known as profilaggrin, are present in the keratohyalin granules in cells of the SG. FIGS. 7A-7C illustrate a Lipid bilayer formation in native skin and epidermal equivalent assessed with TEM. FIG. 7A depicts normal lipid secretion at the border of SC and SG. FIG. 7B depicts lamellar bodies in the SG. FIG. 7C depicts normal lipid bilayer (LB) morphology of native skin. FIGS. 8A-8C illustrate a comparative analysis of markers of suprabasal layers of native skin and epidermal equivalent, including Keratin 10 (KRT10; FIG. 8A ), keratin 1 (KRT1; FIG. 8B ), desmocollin 1 (DCL1; FIG. 8C ), markers of suprabasal layers. FIG. 8D depicts Desmosomes in both native skin in vivo and epidermal equivalents generated in vitro. FIGS. 9A-9C illustrate a comparative analysis of stratum basale of native skin and epidermal equivalent. MKI67 ( FIG. 9A ), a marker of proliferation, keratin 14 (KRT14; FIG. 9B ), and transcription factor TP63 ( FIG. 9C ) show typical basal layer distribution in both native skin in vivo (left side of panel) and epidermal equivalents generated in vitro. FIG. 9D depicts hemi-desmosomes in both native skin in vivo and epidermal equivalents generated in vitro. FIGS. 10A-10F illustrate a comparative analysis of extracellular matrix components of basement membrane. FIG. 10A depicts Integrin β1 expression. FIG. 10B depicts fibronectin expression. FIG. 10C depicts collagen IV expression. FIG. 10D depicts collagen VI expression. FIG. 10E depicts collagen VII expression. FIG. 10F depicts Laminin 5 expression. FIGS. 11A-11I illustrate a structural analysis of full-thickness skin equivalent (FSE). FIGS. 11A and 11B depict cross sections of FSE displays distinct cellular layers of epidermis under 2600× magnification ( FIG. 11A ) and 5200× magnification ( FIG. 11B ). FIG. 11C depicts a surface of an FSE at 900× magnification. FIGS. 11D -IF depict longitudinal sections of dermal scaffold with residing dermal fibroblasts and rich extracellular matrix at 91× magnification ( FIG. 11D ), 162× magnification ( FIG. 11E ) and 405× magnification ( FIG. 11F ). FIGS. 11G-11I depict dermal scaffolds with residing dermal fibroblasts and rich extracellular matrix at 80× magnification ( FIG. 11G ), 695× magnification ( FIG. 11H ) and 2700× magnification ( FIG. 11I ). FIGS. 12A-12R illustrate a time-course of engineering dermal equivalent. FIGS. 12A-12I depict day 2 after seeding dermal fibroblasts onto scaffold at 36× magnification ( FIG. 12A ), 695× magnification ( FIG. 12B ), 1470× magnification ( FIG. 12C ), 7750× magnification ( FIG. 12D ), 2320× magnification ( FIG. 12E ), 2420× magnification ( FIG. 12F ), 6560× magnification ( FIG. 12G ), 17000× magnification ( FIG. 12H ) and 22000× magnification ( FIG. 12I ), FIGS. 12J-12R depict day 7 after seeding dermal fibroblasts onto scaffold at 64× magnification ( FIG. 12J ), 100× magnification ( FIG. 12K ), 364× magnification ( FIG. 12L ), 82× magnification ( FIG. 12M ), 253× magnification ( FIG. 12N ), 3940× magnification ( FIG. 120 ), 5550× magnification ( FIG. 12P ), 9440× magnification ( FIG. 12Q ) and 21680 magnification ( FIG. 12R ). DETAILED DESCRIPTION OF THE DISCLOSURE Several aspects are described below with reference to example applications for illustration. It should be understood that numerous specific details, relationships, and methods are set forth to provide a full understanding of the features described herein. One having ordinary skill in the relevant art, however, will readily recognize that the features described herein can be practiced without one or more of the specific details or with other methods. The features described herein are not limited by the illustrated ordering of acts or events, as some acts can occur in different orders and/or concurrently with other acts or events, unless otherwise specifically indicated. Furthermore, not all illustrated acts or events are required to implement a methodology in accordance with the features described herein. The terminology used herein is for the purpose of describing particular cases only and is not intended to be limiting. As used herein, the singular forms “a”, “an” and “the” are intended to include the plural forms as well, unless the context clearly indicates otherwise. Furthermore, to the extent that the terms “including”, “includes”, “having”, “has”, “with”, or variants thereof are used in either the detailed description and/or the claims, such terms are intended to be inclusive in a manner similar to the term “comprising”. In this disclosure the term “about” or “approximately” can mean a range of up to 10% of a given value. In this disclosure the term “substantially” refers to something that can be done to a great extent or degree. As used herein, the term “pluripotent stem cell” can refer to any precursor cell that has the ability to form any adult cell. As used herein, the term “embryonic stem cells” or “ES cells” or “ESC” can refer to precursor cells that have the ability to form any adult cell. As used herein, the term “induced pluripotent stem cells” or “iPS cells” or “iPSCs” can refer to a type of pluripotent stem cell artificially derived from a non-pluripotent cell (e.g., an adult somatic cell). Induced pluripotent stem cells can be identical to embryonic stem cells in the ability to form any adult cell but are not derived from an embryo. In some cases, IPSC cells disclosed herein can be IPSC cells. As used herein, the term “synthetic leather” can mean that the skin equivalents described herein can serve as a skin equivalent for any mammal or non-mammal. Embodiments can be practiced with human and non-human mammals, such as non-human primates and members of the bovine, ovine, porcine, equinine, canine and feline species as well as rodents such as mice, rats and guinea pigs, members of the lagomorph family including rabbit: and non-mammals such as fish including shark and stingray, birds including ostrich and reptiles including lizards, snakes and crocodiles. The particular mammalian synthetic leather which will be formed can be dependent on the source of the cells used in embodiments described herein, e.g., Keratinocytes and fibroblasts, e.g., when bovine keratinocytes and fibroblasts are used to form a skin equivalent, a bovine synthetic leather can be formed. Overview Disclosed herein are synthetic leathers, artificial epidermal layers, artificial dermal layers, layered structures, products made thereof and methods of producing the same. In certain cases, disclosed herein are synthetic leathers. In some cases, a synthetic leather comprises one or a plurality of layers. In some cases, one or a plurality of layers comprises cells, wherein said cells are cultured in vitro. In some cases, the methods described herein provide high-throughput methods that reliably, accurately, and reproducibly scale up to commercial levels the production of synthetic leather. Advantages of the synthetic leather, engineered epidermal equivalent, engineered full thickness skin equivalent and methods of making the same disclosed herein include, but are not limited to, production of customized tissues in a reproducible, high throughput and easily scalable fashion with appealing appearance, texture, thickness, and durability. As used herein, full thickness skin equivalent can comprise at least one dermal layer and at least one epidermal layer. As used herein, full thickness skin equivalent and full skin equivalent can be used interchangeably. A synthetic leather disclosed herein can comprise a layer of artificial dermal layer comprising a fibroblast and an artificial epidermal layer comprising a keratinocyte. The dermal layer and the epidermal layer can form a layered structure. A synthetic leather can comprise one or more layered structure. The synthetic leather can be tanned and further processed. The cells forming the synthetic layer can be differentiated from stem cells, e.g., induced pluripotent stem cells (iPSC). The dermal layer can be placed on a scaffold, such as silk, to achieve natural leather thickness and texture. Also disclosed herein are methods of making a synthetic leather. The method can comprise forming a layered structure comprising an artificial dermal layer, and an artificial epidermal layer, and tanning the layered structure. The methods can also comprise further processing the artificial dermal layers and epidermal layers, e.g., to achieve natural leather thickness and texture. Synthetic Leather A synthetic leather can comprise one or more cell layers. For example, a synthetic leather can comprise one or more of: a dermal layer, an epidermal layer, and a basement membrane or a basement membrane substitute. A synthetic leather can further comprise hypodermis, scale, scute, osteoderm, or a combination thereof. In some cases, a synthetic layer comprises a full thickness skin equivalent. Such full thickness skin equivalent can comprise any one or combination of the layers disclosed herein. A portion of one or more cell layers in a synthetic leather can be removed, e.g., by shaving. In some cases, a synthetic leather can be tanned. The tanning can be performed after formation of one or more of the cell layers or layered structures. The tanning can be performed after at least a portion of a cell layer can be removed from a synthetic leather. In some cases, a synthetic leather can be further processed. In some cases, a synthetic leather can comprise a hair follicle cell and a melanocyte. The hair follicle cell and/or the melanocyte can be differentiated from a stem cell (e.g., an iPSC). In some embodiments, a tanned synthetic leather can comprise a layered structure. A layered structure can comprise an artificial dermal layer comprising a fibroblast. A layered structure can also comprise an artificial epidermal layer comprising a keratinocyte. In some cases, a layered structure can comprise an artificial dermal layer comprising a fibroblast and an artificial epidermal layer comprising a keratinocyte. In some cases, a fibroblast or a keratinocyte can be differentiated from an induced pluripotent stem cell. In some cases, a tanned synthetic leather can comprise at least part of a dermal layer. In some cases, a tanned synthetic leather does not comprise a dermal layer. In some cases, a dermal layer can be removed. Dermal Layer A synthetic leather can comprise a dermal layer (e.g., an artificial dermal layer). A dermal layer can be an engineered dermis equivalent, e.g., an artificial dermal layer formed in vitro. A dermal layer can comprise cells of connective tissue. For example, a dermal layer can comprise fibroblasts. Fibroblasts in the dermal layer can express one or more markers including, but not limited to, cluster of differentiation 10 (CD10), cluster of differentiation 73 (CD73), cluster of differentiation 44 (CD44), cluster of differentiation 90 (CD90), type I collagen, type III collagen, and prolyl-4-hydroxylase beta fibroblasts. In some cases, a dermal layer also comprises other types of cells, such as immune cells, macrophages, adipocytes, or a combination thereof. A dermal layer can further comprise matrix components in addition to cells. Examples of matrix components include but are not limited to any one or more of collagen, elastin, and extrafibrillar matrix, an extracellular gel-like substance primarily composed of glycosaminoglycans (e.g., hyaluronan), proteoglycans, and glycoproteins. A dermal layer can comprise a matrix support. A matrix support can be a scaffold. The matrix support can comprise contracted collagen gels. Alternatives to a pure collagen matrix can be a polyglygolic acid mesh, e.g., as described in Hansbrough, et al., J. Burn Care Rehabil., 15:346-53 (1994), or collagen and glycosaminoglycan matrix covered with a silastic membrane (C-GAG), e.g., as described in Burke, et al., Ann. Surg., 194:413-420 (1981) or various biopolymers, e.g. chitosan as described in Kellouche, et al., Biochem Biophys Res Commun., 363:472-478 (2007). In some cases, the matrix can be seeded with fibroblasts, e.g., to give rise to organotypic models. Naturally derived dermis, from allogenic cadaver skin can also be used with keratinocyte sheets. A variation of this technique can use lyophilized devitalized dermis from cadaver skin to support the keratinocyte sheets. The thickness of leather units may be reported in millimeters, ounces, or irons. (One ounce equals 1/64 in. or 0.0156 in. or 0.396 mm. One iron equals 1/48 in, or 0.0208 in. or 0.53 mm.) The thickness of a dermal layer can be engineered to fit the function or use of a synthetic leather. A dermal layer can have a thickness from about 0.01 mm to about 50 mm. For example, a dermal layer can have a thickness from about 0.01 mm to about 10 mm, from about 0.01 mm to about 8 mm, from about 0.01 to about 5 mm, from about 0.02 to about 5 mm, from about 0.05 to about 5 mm, from about 0.1 to about 5 mm, from about 0.1 to about 2 mm, from about 0.1 to about 1 mm, from about 0.1 to about 0.8 mm, or from about 0.1 to about 0.5 mm. For example, a dermal layer can have a thickness from about 0.02 mm to 5 mm. For example, a dermal layer can have a thickness from about 0.1 mm to 0.5 mm. For example, a dermal layer can have a thickness from about 0.2 mm to 0.5 mm. In some cases, the thickness of a dermal layer can be at least 0.001 mm, 0.01 mm, 0.02 mm, 0.04 mm, 0.08 mm, 0.1 mm, 0.2 mm, 0.4 mm, 0.8 mm, 1 mm, 2 mm, 4 mm, 8 mm, or 10 mm. In some cases, the thickness of a dermal layer can be at most 50 mm, 40 mm, 20 mm, 10 mm, 8 mm, 4 mm, 2 mm, 1 mm, 0.8 mm, 0.4 mm, 0.2 mm, 0.1 mm, 0.08 mm, 0.04 mm, 0.02 mm, or 0.01 mm. In some embodiments, a dermal layer can have a thickness of at least about 50 mm. The length of a dermal layer can be engineered to fit the function or use of a synthetic leather. A dermal layer can have a length from about 0.01 mm to about 50 m. For example, a dermal layer can have a length from about 0.01 mm to about 10 mm, from about 0.01 mm to about 8 mm, from about 0.01 to about 5 mm, from about 0.02 to about 5 mm, from about 0.05 to about 5 mm, from about 0.1 to about 5 mm, from about 0.1 to about 2 mm, from about 0.1 to about 1 mm, from about 0.1 to about 0.8 mm, or from about 0.1 to about 0.5 mm. For example, a dermal layer can have a length from about 0.02 mm to 5 mm. For example, a dermal layer can have a length from about 0.1 mm to 0.5 mm. For example, a dermal layer can have a length from about 0.2 mm to 0.5 mm. In some cases, the length of a dermal layer can be at least 0.001 mm, 0.01 mm, 0.02 mm, 0.04 mm, 0.08 mm, 0.1 mm, 0.2 mm, 0.4 mm, 0.8 mm, 1 mm, 2 mm, 4 mm, 8 mm, or 10 mm. In some cases, the length of a dermal layer can be at most 50 mm, 40 mm, 20 mm, 10 mm, 8 mm, 4 mm, 2 mm, 1 mm, 0.8 mm, 0.4 mm, 0.2 mm, 0.1 mm, 0.08 mm, 0.04 mm, 0.02 mm, or 0.01 mm. In some embodiments, a dermal layer can have a length of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 700, 1000 mm. In some embodiments, a dermal layer can have a length of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700 cm. In some embodiments, a dermal layer can have a length of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400 m. The width of a dermal layer can be engineered to fit the function or use of a synthetic leather. A dermal layer can have a width from about 0.01 mm to about 50 m. For example, a dermal layer can have a width from about 0.01 mm to about 10 mm, from about 0.01 mm to about 8 mm, from about 0.01 to about 5 mm, from about 0.02 to about 5 mm, from about 0.05 to about 5 mm, from about 0.1 to about 5 mm, from about 0.1 to about 2 mm, from about 0.1 to about 1 mm, from about 0.1 to about 0.8 mm, or from about 0.1 to about 0.5 mm. For example, a dermal layer can have a width from about 0.02 mm to 5 mm. For example, a dermal layer can have a width from about 0.1 mm to 0.5 mm. For example, a dermal layer can have a width from about 0.2 mm to 0.5 mm. In some cases, the width of a dermal layer can be at least 0.001 mm, 0.01 mm, 0.02 mm, 0.04 mm, 0.08 mm, 0.1 mm, 0.2 mm, 0.4 mm, 0.8 mm, 1 mm, 2 mm, 4 mm, 8 mm, or 10 mm. In some cases, the width of a dermal layer can be at most 50 mm, 40 mm, 20 mm, 10 mm, 8 mm, 4 mm, 2 mm, 1 mm, 0.8 mm, 0.4 mm, 0.2 mm, 0.1 mm, 0.08 mm, 0.04 mm, 0.02 mm, or 0.01 mm. In some embodiments, a dermal layer can have a width of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 700, 1000 mm. In some embodiments, a dermal layer can have a width of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700 cm. In some embodiments, a dermal layer can have a width of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400 m. A synthetic leather can comprise one or more dermal layers. For example, a synthetic leather can have at least 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 18, 20, 40, 60, 80, or 100 dermal layers. When a synthetic leather comprises more than one dermal layer, a dermal layer can be placed upon another dermal layer. For example, a synthetic leather can comprise two dermal layers, e.g., a first dermal layer and a second dermal layer. The first dermal layer can be placed upon the second dermal layer. A dermal layer can be stratified, e.g., having a plurality of sublayers. The sublayers can have different compositions, e.g., different concentrations of the fibers. The sublayers of a dermal layer can have different thicknesses and densities. For example, a dermal layer can have a papillary dermal layer, a reticular dermal layer, or any combination thereof. A papillary dermal layer can comprise loose areolar connective tissue and/or loosely arranged fibers, e.g., collagen fibers. A reticular dermal layer can comprise dense irregular connective tissue, including collagen fibers and dermal clastic fibers. A dermal layer can comprise a free collagen matrix or lattice, which can be contractile in all directions, and homogeneous. Fibroblasts, and where appropriate other types of cells of the dermis, can be distributed in a continuous collagen gel. The dermis equivalent can comprise at least one matrix of collagen type I in which the fibroblasts are distributed. It can also contain other extracellular matrix constituents. Extracellular matrix constituent can include collagens, e.g., collagen IV, laminins, entactin, fibronectin, proteoglycans. glycosaminoglycans or hyaluronic acid. A dermal layer can contain collagen type IV and laminin, entactin. or a combination thereof. The concentrations of these various constituents can be adjusted. For example, the concentration of laminin can be from about 1% to about 15% of the final volume. For example, the concentration of collagen IV can be from about 0.3% to about 4.5% of the final volume. For example, the concentration of entactin can be from about 0.05% to about 1% of the final volume. The collagen used can be collagen of bovine origin, from rat tail or from fish, or any other source of natural collagen or collagen produced by genetic engineering which allows contraction in the presence of fibroblasts. In some embodiments, collagen can be from an unnatural source. The matrix can be a gel of collagen which may not taut, obtained by contraction both horizontally and vertically, which does not impose a preferential organization of the fibroblasts. Such a matrix, also termed “free”, may not adhere to the support and the volumes thereof can be modified without limit, conferring on it a varying thickness and diameter. The thickness of the dermis equivalent can be at least 0.05 cm, and in some cases, approximately from 0.05 to 2cm. The thickness can also be increased without harming the advantageous properties of the skin equivalent or synthetic leather. In some cases, the thickness can be from about 3 mm to about 20 cm or more. Epidermal Layer A synthetic leather can comprise an epidermal layer (e.g., an artificial epidermal layer). An epidermal layer can be an engineered epidermis equivalent, e.g., an artificial epidermal layer formed in vitro. An epidermal layer can comprise one or more types of cells, including keratinocytes, melanocytes, Langerhans cells. Merkel cells, and inflammatory cells. For example, an epidermal layer can comprise keratinocytes. Keratinocytes in an epidermal layer can include epithelial keratinocytes, basal keratinocytes, proliferating basal keratinocytes, differentiated suprabasal keratinocytes, or any combination thereof. In some cases, an epidermal layer comprises at least basal keratinocytes, e.g., keratinocytes which are not differentiated. An epidermal layer can further comprise partially differentiated keratinocytes as well as fully differentiated keratinocytes. In one or more epidermal layers in a synthetic leather there can be a transition from undifferentiated basal keratinocytes to fully differentiated keratinocytes as one progresses from the dermal-epidermal junction where the basal keratinocytes are localized. Basal keratinocytes can express hemidesmosomes, which serve to help secure the epidermal and dermal layers together. Basal keratinocytes can also serve to regenerate the skin. An epidermal layer in a synthetic leather herein can have basal keratinocytes that serve these functions. Thus, a synthetic leather comprising such basal keratinocytes can be capable of regeneration. Other distinctions between basal keratinocytes and differentiated keratinocytes in one or more epidermal layers in a synthetic leather can be that both E-and P-cadherin's are present in epidermal keratinocytes along the basal membrane zone (BMZ), but keratinocytes which are differentiated and located away from the BMZ only express E-cadherin. The basal keratinocytes of an epidermal layer can be aligned in a layer in direct contact with the dermal layer, serving as the boundary between the differentiated keratinocytes and the fibroblasts. In alternative cases, there are gaps between the basal keratinocytes and the dermal layer. Still further, there can be gaps between the basal keratinocytes and other basal keratinocytes, leaving gaps between the differentiated keratinocytes and the dermal layer. In these latter cases where there are gaps between the basal or differentiated keratinocytes and the dermal layer, the dermal and epidermal layers are not uniformly in contact with one another but are adjacent to each other. They are adjacent in that there can be generally fluid, but substantially no other intervening materials such as layers of cells, collagen, matrices or other supports between the dermal and epidermal layers. Keratinocytes in an epidermal layer can express one or more markers. Such markers include, but are not limited to, Keratin 14 (KRT14), tumor protein p63 (p63), Desmoglein 3 (DSG3), Integrin, beta 4 (ITGB4), Laminin, alpha 5 (LAMA5), Keratin 5 (KRT5), an isoform of tumor protein p63 (e.g., TAp63), Laminin, beta 3 (LAMB3), and Keratin 18 (KRT18). The thickness of an epidermal layer can be engineered to fit the function or use of the synthetic leather. An epidermal layer can have a thickness from about 0.001 mm to about 10 mm. For example, an epidermal layer can have a thickness from about 0.005 mm to about 10 mm, from about 0.005 mm to about 5 mm, from about 0.005 mm to about 2 mm, from about 0.01 mm to about 10 mm, from about 0.01 mm to about 5 mm, from about 0.01 mm to about 2 mm, from about 0.01 mm to about 1, from about 0.01 mm to about 0.8 mm, from about 0.01 mm to about 0.4 mm, from about 0.01 mm to about 0.2 mm, from about 0.01 mm to about 0.1 mm, from about 0.05 mm to about 0.4 mm, from about 0.05 mm to about 0.2 mm, from about 0.05 mm to about 0.1 mm, from about 0.1 mm to about 0.4 mm, from about 0.1 mm to about 0.2 mm, from about 0.08 mm to about 1 mm, or from about 0.05 mm to about 1.5 mm. For example, an epidermal layer can have a thickness from about 0.01 mm to about 2 mm. For example, an epidermal layer can have a thickness from about 0.1 mm to about 0.22 mm. In some cases, the thickness of an epidermal layer can be at least 0.001 mm, 0.01 mm, 0.02 mm, 0.04 mm, 0.08 mm, 0.1 mm, 0.2 mm, 0.4 mm, 0.8 mm, 1 mm, 2 mm, 4 mm, 8 mm, or 10 mm. In some cases, the thickness of the dermal layer can be at most 50 mm, 40 mm, 20 mm, 10 mm, 8 mm, 4 mm, 2 mm, 1 mm, 0.8 mm, 0.4 mm, 0.2 mm, 0.1 mm, 0.08 mm, 0.04 mm, 0.02 mm, or 0.01 mm. In some cases, thickness values described herein can be the thickness of an epidermal layer and a basement membrane substitute. The length of an epidermal layer can be engineered to fit the function or use of a synthetic leather. An epidermal layer can have a length from about 0.01 mm to about 50 m. For example, an epidermal layer can have a length from about 0.01 mm to about 10 mm, from about 0.01 mm to about 8 mm, from about 0.01 to about 5 mm, from about 0.02 to about 5 mm, from about 0.05 to about 5 mm, from about 0.1 to about 5 mm, from about 0.1 to about 2 mm, from about 0.1 to about 1 mm, from about 0.1 to about 0.8 mm, or from about 0.1 to about 0.5 mm. For example, an epidermal layer can have a length from about 0.02 mm to 5 mm. For example, an epidermal layer can have a length from about 0.1 mm to 0.5 mm. For example, an epidermal layer can have a length from about 0.2 mm to 0.5 mm. In some cases, the length of an epidermal layer can be at least 0.001 mm, 0.01 mm, 0.02 mm, 0.04 mm, 0.08 mm, 0.1 mm, 0.2 mm, 0.4 mm, 0.8 mm, 1 mm, 2 mm, 4 mm, 8 mm, or 10 mm. In some cases, the length of an epidermal layer can be at most 50 mm, 40 mm, 20 mm, 10 mm, 8 mm, 4 mm, 2 mm, 1 mm, 0.8 mm, 0.4 mm, 0.2 mm, 0.1 mm, 0.08 mm, 0.04 mm, 0.02 mm, or 0.01 mm. In some embodiments, an epidermal layer can have a length of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 700, 1000 mm. In some embodiments, an epidermal layer can have a length of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700 cm. In some embodiments, an epidermal layer can have a length of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400 m. The width of an epidermal layer can be engineered to fit the function or use of a synthetic leather. An epidermal layer can have a width from about 0.01 mm to about 50 m. For example, an epidermal layer can have a width from about 0.01 mm to about 10 mm, from about 0.01 mm to about 8 mm, from about 0.01 to about 5 mm, from about 0.02 to about 5 mm, from about 0.05 to about 5 mm, from about 0.1 to about 5 mm, from about 0.1 to about 2 mm, from about 0.1 to about 1 mm, from about 0.1 to about 0.8 mm, or from about 0.1 to about 0.5 mm. For example, an epidermal layer can have a width from about 0.02 mm to 5 mm. For example, an epidermal layer can have a width from about 0.1 mm to 0.5 mm. For example, an epidermal layer can have a width from about 0.2 mm to 0.5 mm. In some cases, the width of an epidermal layer can be at least 0.001 mm, 0.01 mm, 0.02 mm, 0.04 mm, 0.08 mm, 0.1 mm, 0.2 mm, 0.4 mm, 0.8 mm, 1 mm, 2 mm, 4 mm, 8 mm, or 10 mm. In some cases, the width of an epidermal layer can be at most 50 mm, 40 mm, 20 mm, 10 mm, 8 mm, 4 mm, 2 mm, 1 mm, 0.8 mm, 0.4 mm, 0.2 mm, 0.1 mm, 0.08 mm, 0.04 mm, 0.02 mm, or 0.01 mm. In some embodiments, an epidermal layer can have a width of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 700, 1000 mm. In some embodiments, an epidermal layer can have a width of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700 cm. In some embodiments, an epidermal layer can have a width of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400 m. A synthetic leather can comprise one or more epidermal layers. For example, a synthetic leather can have at least about 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 18, 20, 40, 60, 80, or 100 epidermal layers. When a synthetic leather comprises more than one epidermal layer, one epidermal layer can be placed upon another epidermal layer. For example, a synthetic leather can comprise two epidermal layers, e.g., a first epidermal layer and a second epidermal layer. The first epidermal layer can be placed upon the second epidermal layer. An epidermal layer can be stratified, e.g., having a plurality of sublayers. The sublayers can have different cell compositions, e.g., different types of keratinocytes. The sublayers of an epidermal layer can have different thicknesses and/or densities. For example, an epidermal layer can have one or more of cornified layer (stratum corneum), clear/translucent layer (stratum lucidum), granular layer (stratum granulosum), spinous layer (stratum spinosum), basal/germinal layer (stratum basale/germinativum), or any combination thereof. In some cases, an epidermal layer comprises functional epidermal permeability barrier (e.g., organized lipid bilayers in stratum corneum). In some cases, a stratum corneum, stratum lucidum, stratum granulosum, stratum spinosum, or stratum basale/germinativum, can have a thickness of about 0.0001 mm to about 5 mm. In some cases, a stratum corneum, stratum lucidum, stratum granulosum, stratum spinosum, or stratum basale/germinativum, can have a thickness of at least about 0.001 mm, 0.01 mm, 0.02 mm, 0.04 mm, 0.08 mm, 0.1 mm, 0.15 mm, 0.2 mm, 0.4 mm, 0.8 mm, 1 mm, 2 mm, 4 mm, 8 mm, or 10 mm. In some cases, a stratum corneum, stratum lucidum, stratum granulosum, stratum spinosum, or stratum basale/germinativum, can have a thickness of at most about 50 mm, 40 mm, 20 mm, 10 mm, 8 mm, 4 mm, 2 mm, 1 mm, 0.8 mm, 0.4 mm, 0.2 mm, 0.15 mm, 0.1 mm, 0.08 mm, 0.04 mm, 0.02 mm, or 0.01 mm. An epidermal layer can further comprise cells producing pigments, e.g., melanin. Such pigment-producing cells can be melanocytes. Melanocytes in the epidermal layer can express one or more markers. Such markers can include, but are not limited to, SRY-box containing gene 10 (Sox-10), Microphthalmia-associated transcription factor (MITF-M), premelanosome protein (gp-100), Dopachrome tautomerase (DCT), Tyrosinase (TYR), and Melan-A (MLANA). Cells in Synthetic Leather A synthetic leather can comprise cells in the dermal layer and epidermal layer disclosed herein. In some cases, a synthetic leather also comprises hair follicle cells, endothelial cells, dermal papilla cells, immune system cells (such as lymphocytes, dendritic cells, macrophages or Langerhans cells), adipocytes, nerve cells, and a mixture thereof. One or more cells in a synthetic leather can be genetically engineered cells. The term “genetically engineered” can refer to a man-made alteration to the nucleic acid content of a cell. Therefore, genetically engineered cells can include cells containing an insertion, deletion, and/or substitution of one or more nucleotides in the genome of a cell as well as alterations including the introduction of self-replicating extrachromosomal nucleic acids inserted into the cell. Genetically engineered cells also include those in which transcription of one or more genes has been altered, e.g., increased or reduced. In some cases, a synthetic leather has at least one of the components of native skin such as melanocytes, hair follicles, sweat glands and nerve endings. In certain cases, a synthetic leather can be distinguished from normal native skin by its lack of at least one of these components. In some cases, displaying abnormal phenotypes or having at least one cell with an altered genotype, a synthetic leather can include all of these components. In some case, additional components can be added to a synthetic leather. Such additional components can include myoepithelial cells, duct cells, secretory cells, alveolar cells, langerhans cells, Merkel cells, adhesions, mammary glands, or any mixture thereof. In some cases, a synthetic leather comprises one or more of: neural cells, connective tissue (including bone, cartilage, cells differentiating into bone forming cells and chondrocytes, and lymph tissues), epithelial cells (including endothelial cells that form linings in cavities and vessels or channels, exocrine secretory epithelial cells, epithelial absorptive cells, keratinizing epithelial cells, and extracellular matrix secretion cells), and undifferentiated cells (such as embryonic cells, stem cells, and other precursor cells). A synthetic leather can comprise hair follicles. A hair follicle can comprise one or more structures, including papilla, matrix, root sheath, bulge, infundibulum, the arrector pili muscles, the sebaceous glands, and the apocrine sweat glands. A hair follicle can comprise one or more hair follicle cells, including dermal papilla cell, outer root sheath cell, or any combination thereof. In some cases, a hair follicle can be in an epidermal layer. In some cases, a hair follicle can be in a dermal layer. A hair follicles cell can be differentiated from a progenitor, e.g., a stem cell such as an iPSC. In some embodiments, at least about 1%, 2%, 3%, 4%, 5%, 6%, 7%, 8%, 9%, 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%, 100% of hair follicle cells can be differentiated from induced pluripotent stem cells. In some embodiments, a synthetic leather can be devoid of hair, blood vessels, sebaceous glands, hair follicle, oil glands, nerve, or a combination thereof In some cases, a synthetic leather can comprise hairs, e.g., in one or more layered structures. For example, a synthetic leather can comprise fur. The hairs (e.g., fur) can be natural, synthetic, or a combination thereof. The hairs (e.g., fur) can be grown from cells in the synthetic leather or added to synthetic leather from an exogenous source. In other cases, a synthetic leather may not have any hairs. Stem Cells One or more cells in a synthetic leather can be differentiated from progenitor cells, such as stem cells. For example, fibroblasts in a synthetic leather can be differentiated from stem cells. For example, keratinocytes in a synthetic leather can be differentiated from stem cells. For example, melanocytes in a synthetic leather can be differentiated from stem cells. In some embodiments, at least about 1%, 2%, 3%, 4%, 5%, 6%, 7%, 8%, 9%, 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%, 100% of cells disclosed herein can be differentiated from stem cells. In some embodiments, at least about 1%, 2%, 3%, 4%, 5%, 6%, 7%, 8%, 9%, 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%, 100% of fibroblasts can be differentiated from induced pluripotent stem cells. In some embodiments, at least about 1%, 2%, 3%, 4%, 5%, 6%, 7%, 8%, 9%, 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%, 100% of keratinocytes can be differentiated from induced pluripotent stem cells. In some embodiments, at least about 1%, 2%, 3%, 4%, 5%, 6%, 7%, 8%, 9%, 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%, 100% of melanocytes cells can be differentiated from induced pluripotent stem cells. Stem cells can be embryonic stem cells (ESCs), adult stem cells (i.e., somatic stem cells) or induced pluripotent stem cells (iPSCs). In some embodiments, a stem cell can be totipotent, pluripotent or multipotent for example adult stem cells and cord blood stem cells). Embryonic stem cells can be derived from fertilized embryos that are less than one week old. Induced pluripotent stem cells can be obtained through the induced expression of one or more of Oct3, Oct4, Sox2, Klf4, and c-Myc genes in any somatic cell (e.g., adult somatic cell) such as fibroblast. In some cases, one or more other genes can also be induced for reprograming a somatic cell to an induced pluripotent stem cell. Examples of such genes include NANOG, UTF1, LIN28, SALL4, NR5A2, TBX3, ESSRB, DPPA4, SV40LT, REM2, MDM2, and cyclin D1. Various delivery methods can be used to modulate the expression of genes to reprogram a somatic cell to an iPSC. Exemplary delivery methods include naked DNA delivery, adenovirus, electrical delivery, chemical delivery, mechanical delivery, polymer-based systems, microinjection, retroviruses (e.g., MMLV-derived retroviruses), and lentiviruses (e.g., excisable lentiviruses). In some cases, induced pluripotent stem cells can be obtained according to the protocol as described by Takahashi et al., Cell. 2007 Nov. 30; 131(5):861-72 (2007), or by Yu et al., Science 318, 1917-1920 (2007) (2007). In some case, somatic cells (e.g., adult somatic cells) are transfected with viral vectors, such as retroviral vectors, which comprise Oct3, Oct4, Sox2, Klf4, and c-Myc genes. In some cases. Sendai viruses are used as a delivery system, e.g., Sendai viruses produced by ID Pharma Co., Ltd., Japan. Sources of Cells A synthetic leather can comprise cells derived from animals of one or more species. For example, the cells in a synthetic leather can be derived from mammals, birds, reptiles, amphibian, fish, invertebrates, or any combination thereof. A synthetic leather can comprise cells derived from mammals, e.g., mammalian cells, or non-mammals. A mammal can be a non-human mammal. A non-human mammal can be antelope, bear, beaver, bison, boar, camel, caribou, cat, cattle, deer, dog, elephant, elk, fox, giraffe, goat, hare, horse, ibex, kangaroo, lion, llama, lynx, mink, moose, oxen, peccary, pig, rabbit, rhino, seal, sheep, squirrel, tiger, whale, wolf, yak, or zebra. In some cases, a mammal can be primate, bovine, ovine, porcine, equinine, canine, feline, rodent, or lagomorph. A non-mammal can be a fish, a bird or a reptile. In some cases, a mammal can be a human. In some embodiments a human can be a celebrity. As used herein, the term “celebrity” can be defined as a person that has come into the community attention by way of notoriety or general fame of previous activities. A “celebrity” can be associated with industries including but not limited to professional and amateur sports, entertainment, music, motion picture, business, print and electronic media, politics, and the like. A synthetic leather can comprise cells derived from other species. In some cases, the cells are derived from birds, such as chicken, duck, emu, goose, grouse, ostrich, pheasant, pigeon, quail, or turkey. In some cases, the cells are derived from reptiles such as turtle, snake, crocodile, or alligator. In some cases, the cells are derived from amphibians such as frog, toad, salamander, or newt. In some cases, the cells are derived from fish, such as anchovy, bass, catfish, carp, cod, eel, flounder, fugu, grouper, haddock, halibut, herring, mackerel, mahi-mahi, manta ray, marlin, orange roughy, perch, pike, pollock, salmon, sardine, shark, snapper, sole, stingray, swordfish, tilapia, trout, tuna, or walleye. In some cases, all cells in a synthetic leather are derived from the same species. For example, all cells in a synthetic leather can be bovine cells. In other cases, a synthetic leather comprises cells derived from multiple species. For example, a synthetic leather can comprise bovine cells and alligator cells. In some cases, a synthetic leather comprises cells derived from at least 2, 3, 4, 5, 6, 7, 8, or 10 species. Progenitors of the cells in a synthetic leather can also be derived from the sources described herein. For example, stem cells (e.g., iPSCs), somatic cells (e.g., to be reprogramed to iPSCs), primary cells used in synthetic cells, dermal layer cells, epidermal layer cells, or any cells in the synthetic and their progenitors thereof can be derived from the sources described herein. Any cell can be a live cell or a dead cell. When multiple cells are present, a cell may be a live cell, may be a dead cell, or any combination thereof. Layered Structure A synthetic leather can comprise one or more layered structures. A layered structure can be formed by placing a first type of layer upon a second type of layer. The first type of layer and the second type of layer can be the same or different. In some cases, a layered structure can be formed by placing an epidermal layer upon a dermal layer. For example, a layered structure can be formed by placing an epidermal layer upon a dermal layer, with a basement membrane substitute in between. A layered structure can comprise two or more layers. In some cases, a layered structure comprises at least 2, 3, 4, 5 6, 7, 8, 9, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100, 500, or 1000 layers. In some cases, a layered structure comprises at least 2, 3, 4, 5 6, 7, 8, 9, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100, 500, or 1000 first type of layers, and at least 2, 3, 4, 5 6, 7, 8, 9, 10, 15, 20, 30, 40, or 50 second type of layers. For example, a layered structure can comprise at least 2, 3, 4, 5 6, 7, 8, 9, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100, 500, or 1000 dermal layers, and at least 2, 3, 4, 5 6, 7, 8, 9, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100, 500, or 1000 layers of epidermal layers. A layered structure can comprise one or more types of cells described herein. For example, a layered structure can comprise cells in a dermal layer, such as fibroblasts, cells in an epidermal layer, such as keratinocytes, or any combination thereof. In some cases, a layered structure further comprises cells other than fibroblasts and keratinocytes. For example, a layered structure can comprise melanocytes. A layered structure can have a thickness from about 0.001 mm to about 100 mm. For example, a layered structure can have a thickness from about 0.005 mm to about 50 mm, from about 0.005 to about 10, from about 0.01 mm to about 10 mm, from about 0.02 to about 5 mm, from about 0.05 to about 5 mm, from about 0.1 to about 5 mm, from about 0.1 to about 2 mm, from about 0.1 to about 1 mm, or from about 0.1 to about 0.5 mm. In some cases, the thickness of a layered structure can be at least 0.001 mm, 0.01 mm, 0.02 mm, 0.04 mm, 0.08 mm, 0.1 mm, 0.2 mm, 0.4 mm, 0.8 mm, 1 mm, 2 mm, 4 mm, 8 mm, 10 mm, 20 mm, 40 mm, 60 mm, 80 mm, or 100 mm. In some cases, the thickness of a layered structure can be at most 100 mm, 50 mm, 40 mm, 20 mm, 10 mm, 8 mm, 4 mm, 2 mm, 1 mm, 0.8 mm, 0.4 mm, 0.2 mm, 0.1 mm, 0.08 mm, 0.04 mm, 0.02 mm, or 0.01 mm. In some embodiments, a layered structure can have a thickness of at least about 100, 200, 300, 400, 500, 600, 700, 800 mm. The length of a layered structure can be engineered to fit the function or use of a synthetic leather. A layered structure can have a length from about 0.01 mm to about 50 m. For example, a layered structure can have a length from about 0.01 mm to about 10 mm, from about 0.01 mm to about 8 mm, from about 0.01 to about 5 mm, from about 0.02 to about 5 mm, from about 0.05 to about 5 mm, from about 0.1 to about 5 mm, from about 0.1 to about 2 mm, from about 0.1 to about 1 mm, from about 0.1 to about 0.8 mm, or from about 0.1 to about 0.5 mm. For example, a layered structure can have a length from about 0.02 mm to 5 mm. For example, a layered structure can have a length from about 0.1 mm to 0.5 mm. For example, a layered structure can have a length from about 0.2 mm to 0.5 mm. In some cases, the length of a layered structure can be at least 0.001 mm, 0.01 mm, 0.02 mm, 0.04 mm, 0.08 mm, 0.1 mm, 0.2 mm, 0.4 mm, 0.8 mm, 1 mm, 2 mm, 4 mm, 8 mm, or 10 mm. In some cases, the length of a layered structure can be at most 50 mm, 40 mm, 20 mm, 10 mm, 8 mm, 4 mm, 2 mm, 1 mm, 0.8 mm, 0.4 mm, 0.2 mm, 0.1 mm, 0.08 mm, 0.04 mm, 0.02 mm, or 0.01 mm. In some embodiments, a layered structure can have a length of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 700, 1000 mm. In some embodiments, a layered structure can have a length of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700 cm. In some embodiments, a layered structure can have a length of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400 m. The width of a layered structure can be engineered to fit the function or use of a synthetic leather. A layered structure can have a width from about 0.01 mm to about 50 m. For example, a layered structure can have a width from about 0.01 mm to about 10 mm, from about 0.01 mm to about 8 mm, from about 0.01 to about 5 mm, from about 0.02 to about 5 mm, from about 0.05 to about 5 mm, from about 0.1 to about 5 mm, from about 0.1 to about 2 mm, from about 0.1 to about 1 mm, from about 0.1 to about 0.8 mm, or from about 0.1 to about 0.5 mm. For example, a layered structure can have a width from about 0.02 mm to 5 mm. For example, a layered structure can have a width from about 0.1 mm to 0.5 mm. For example, a layered structure can have a width from about 0.2 mm to 0.5 mm. In some cases, the width of a layered structure can be at least 0.001 mm, 0.01 mm, 0.02 mm, 0.04 mm, 0.08 mm, 0.1 mm, 0.2 mm, 0.4 mm, 0.8 mm, 1 mm, 2 mm, 4 mm, 8 mm, or 10 mm. In some cases, the width of a layered structure can be at most 50 mm, 40 mm, 20 mm, 10 mm, 8 mm, 4 mm, 2 mm, 1 mm, 0.8 mm, 0.4 mm, 0.2 mm, 0.1 mm, 0.08 mm, 0.04 mm, 0.02 mm, or 0.01 mm. In some embodiments, a layered structure can have a width of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 700, 1000 mm. In some embodiments, a layered structure can have a width of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700 cm. In some embodiments, a layered structure can have a width of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400 m. A layered structure can comprise fibroblasts and keratinocytes at any ratio of at least about 50:1, 40:1, 30:1, 29:1, 28:1, 27:1, 26:1, 25:1, 24:1, 23:1, 22:1, 21:1, 20:1, 19:1, 18:1, 17:1, 16:1, 15:1, 14:1, 13:1, 12:1. 11:1, 10:1, 9:1, 8:1, 7:1, 6:1, 5:1, 4:1, 3:1, 2:1, 1:1, 1:2, 1:10, or 1:100. In some cases, the ratio of fibroblasts to keratinocytes can be from about 20:1 to about 3:1, from about 20:1 to about 4:1, from about 20:1 to about 5:1, from about 20:1 to about 10:1, or from about 20:1 to about 15:1. A layered structure can comprise fibroblasts and melanocytes at any ratio of at least about 50:1, 40:1, 30:1, 29:1, 28:1, 27:1, 26:1, 25:1, 24:1, 23:1, 22:1, 21:1, 20:1, 19:1, 18:1, 17:1, 16:1, 15:1, 14:1, 13:1, 12:1, 11:1, 10:1, 9:1, 8:1, 7:1, 6:1, 5:1, 4:1, 3:1, 2:1, 1:1, 1:2, 1:10, or 1:100. In some cases, the ratio of fibroblasts to melanocyte can be from about 20:1 to about 3:1, from about 20:1 to about 4:1, from about 20:1 to about 5:1, from about 20:1 to about 10:1, or from about 20:1 to about 15:1. A layered structure can comprise keratinocytes and melanocytes at any ratio of at least about 50:1, 40:1, 30:1, 29:1, 28:1, 27:1, 26:1, 25:1, 24:1, 23:1, 22:1, 21:1, 20:1, 19:1, 18:1, 17:1, 16:1, 15:1, 14:1, 13:1, 12:1, 11:1, 10:1, 9:1, 8:1, 7:1, 6:1, 5:1, 4:1, 3:1, 2:1, 1:1, 1:2, 1:10, or 1:100. In some cases, the ratio of keratinocytes to melanocyte can be from about 20:1 to about 3:1, from about 20:1 to about 4:1, from about 20:1 to about 5:1, from about 20:1 to about 10:1, or from about 20:1 to about 15:1. One type of cells in a layered structure can comprise at most 99%, 95%, 90%, 85%, 80%, 75%, 70%, 65%, 60%, 55%, 50%, 45%, 40%, 35%, 30%, 25%, 20%, 10%, 5%, or 1% of the total cell population in the layered structure. One type of cells in a layered structure can comprise about at least 1%, 5%, 10%, 20%, 30%, 40%, 50%, 60%, 65%, 70%, 75%, 80%, 85%, 90%, or 95% of the total cell population in the layered structure. For example, fibroblasts in a layered structure can comprise about at least 5%, 10%, 20%, 30%, 40%, 50%, 60%, 65%, 70%, 75%, 80%, 85%, 90%, or 95% of the total cell population in the layered structure. Synthetic Leather A synthetic leather can be formed by one or more layered structures. For example, a synthetic leather can be formed by at least 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, or 100 layered structures. A synthetic leather can be of various thickness. For example, a synthetic leather can have a thickness resembling to a natural leather. In some cases, a synthetic leather can have a thickness from about 0.001 mm to about 100 mm. For example, a layered structure can have a thickness from about 0.005 mm to about 50 mm, from about 0.005 to about 10, from about 0.01 mm to about 10 mm, from about 0.1 to about 5 mm, from about 0.5 mm to about 5 mm, from about 0.5 mm to about 3 mm, from about 0.8 mm to about 3 mm, from about 0.8 mm to about 2 mm, from about 0.8 mm to about 1.8 mm, from about 0.8 mm to about 1.6 mm, from about 0.9 mm to about 1.4 mm, from about 1 mm to about 1.5 mm, from about 1 mm to about 1.4 mm, or from about 1 mm to about 1.3 mm. In some cases, the thickness of a synthetic leather can be at least 0.001 mm, 0.01 mm, 0.02 mm, 0.04 mm, 0.08 mm, 0.1 mm, 0.2 mm, 0.4 mm, 0.8 mm, 1 mm, 2 mm, 4 mm, 8 mm, 10 mm, 20 mm, 40 mm, 60 mm, 80 mm, or 100 mm. In some cases, the thickness of a synthetic leather can be at most 100 mm, 50 mm, 40 mm, 20 mm, 10 mm, 8 mm, 4 mm, 2 mm, 1 mm, 0.8 mm, 0.4 mm, 0.2 mm, 0.1 mm, 0.08 mm, 0.04 mm, 0.02 mm, or 0.01 mm. In some cases, the thickness of a synthetic leather can be about 1.2 mm. A synthetic leather can have a length from about 0.01 mm to about 50 m. For example, a synthetic leather can have a length from about 0.01 mm to about 10 mm, from about 0.01 mm to about 8 mm, from about 0.01 to about 5 mm, from about 0.02 to about 5 mm, from about 0.05 to about 5 mm, from about 0.1 to about 5 mm, from about 0.1 to about 2 mm, from about 0.1 to about 1 mm, from about 0.1 to about 0.8 mm, or from about 0.1 to about 0.5 mm. For example, a synthetic leather can have a length from about 0.02 mm to 5 mm. For example, a synthetic leather can have a length from about 0.1 mm to 0.5 mm. For example, a synthetic leather can have a length from about 0.2 mm to 0.5 mm. In some cases, the length of a synthetic leather can be at least 0.001 mm, 0.01 mm, 0.02 mm, 0.04 mm, 0.08 mm, 0.1 mm, 0.2 mm, 0.4 mm, 0.8 mm, 1 mm, 2 mm, 4 mm, 8 mm, or 10 mm. In some cases, the length of a synthetic leather can be at most 50 mm, 40 mm, 20 mm, 10 mm, 8 mm, 4 mm, 2 mm, 1 mm, 0.8 mm, 0.4 mm, 0.2 mm, 0.1 mm, 0.08 mm, 0.04 mm, 0.02 mm, or 0.01 mm. In some embodiments, a synthetic leather can have a length of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 700, 1000 mm. In some embodiments, a synthetic leather can have a length of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700 cm. In some embodiments, a synthetic leather can have a length of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400 m. A synthetic leather can have a width from about 0.01 mm to about 50 m. For example, a synthetic leather can have a width from about 0.01 mm to about 10 mm, from about 0.01 mm to about 8 mm, from about 0.01 to about 5 mm, from about 0.02 to about 5 mm, from about 0.05 to about 5 mm, from about 0.1 to about 5 mm. from about 0.1 to about 2 mm, from about 0.1 to about 1 mm, from about 0.1 to about 0.8 mm, or from about 0.1 to about 0.5 mm. For example, a synthetic leather can have a width from about 0.02 mm to 5 mm. For example, a synthetic leather can have a width from about 0.1 mm to 0.5 mm. For example, a synthetic leather can have a width from about 0.2 mm to 0.5 mm. In some cases, the width of a synthetic leather can be at least 0.001 mm, 0.01 mm, 0.02 mm, 0.04 mm, 0.08 mm, 0.1 mm, 0.2 mm, 0.4 mm, 0.8 mm, 1 mm, 2 mm, 4 mm, 8 mm, or 10 mm. In some cases, the width of a synthetic leather can be at most 50 mm, 40 mm, 20 mm, 10 mm, 8 mm, 4 mm, 2 mm, 1 mm, 0.8 mm, 0.4 mm, 0.2 mm, 0.1 mm, 0.08 mm, 0.04 mm, 0.02 mm, or 0.01 mm. In some embodiments, a synthetic leather can have a width of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 700, 1000 mm. In some embodiments, a synthetic leather can have a width of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700 cm. In some embodiments, a synthetic leather can have a width of at least about 50, 60, 70, 80, 90, 100, 200, 300, 400 m. Basement Membrane Substitute A synthetic leather can further comprise a basement membrane substitute. A basement membrane substitute can be between two cell layers, e.g., between a dermal layer and an epidermal layer. A basement membrane substitute can be a dermo-epidermal junction similar to that which exists in vivo, from a structural point of view and/or from a biochemical point of view. From the biochemical point of view, a basement membrane substitute can comprise components of the basal membrane, of the lamina densa, of the lamina lucida and of the sub-basal zone, such as, collagen IV, collagen VII, laminin 5, entactin fibronectin, or any combination thereof. A basement membrane substitute in a synthetic leather can be urinary basement membrane (UBM), liver basement membrane (LBM), amnion, chorion, allograft pericardium, allograft acellular dermis, amniotic membrane. Wharton's jelly, or any combination thereof. For example, a basement membrane substitute can be a dried acellular amniotic membrane. In certain cases, a basement membrane substitute can be a polymer, e.g., a nanopolymer. For example, a basement membrane substitute can be nano-fibrous poly hydroxy butyrate-cohydroxyvalerate (PHBV), as described by Bye et al., Journal of Biomaterials and Tissue Engineering Vol. 4, 1-7, 2014. Scaffold A cell layer (e.g, a dermal layer), a layered structure, or a synthetic leather can be placed on a scaffold. A scaffold can provide certain firmness (e.g., resistance to tearing), elasticity, or both. In some cases, a part of or the entire scaffold can be comprised in the synthetic leather. In other cases, a scaffold may not be comprised in the synthetic leather. After assisting the formation of a layer in a synthetic leather, a scaffold can be removed from the final synthetic leather product. In certain cases, a scaffold comprised in a synthetic leather can be degraded after a period of time. A scaffold described herein can comprise a trabecular pattern. A scaffold can be made of natural materials, synthetic materials, or combination thereof. Examples of scaffolds include a scaffold formed using a net made of a bioabsorbable synthetic polymer, a scaffold formed by attaching a nylon net to a silicon film, a scaffold having a two-layered structure of a collagen sponge and a silicon sheet, a scaffold formed using an atelo collagen sponge made into a sheet, a scaffold formed by matching collagen sponges having different pore sizes, and acellular dermal matrices (ADM) formed using fibrin glue or allogeneic skin that has been made cell-free. A scaffold can comprise natural substances such as collagen (e.g., collagen matrix), natural adhesive (e.g., fibrin glue, cold glues, animal glue, blood albumen glue, casein glue, or vegetable glues such as starch and dextrin glues). In some cases, a scaffold comprises silk. For example, a scaffold can be made of silk. In some embodiments, a scaffold can comprise, silk fibroin, cellulose, cotton, acetate, acrylic, latex fibers, linen, nylon, rayon, velvet, modacrylic, olefin polyester, saran, vinyon, wool, jute, hemp, bamboo, flax or a combination thereof. In some embodiments, a scaffold can comprise fibers. In some embodiments, the fibers can be fibers of silk, cotton, wool, linen, cellulose extracted in particular from wood, vegetables or algae, polyamide, modified cellulose (rayon, viscose, acetate, especially rayon acetate), poly-p-phenyleneterephthalamide, acrylic fibers, for example those of polymethyl methacrylate or of poly-2-hydroxyethyl methacrylate, fibers of polyolefin for example fibers of polyethylene or polypropylene, glass, silica, aramid, carbon, for example in the form of graphite, poly(tetrafluoroethylene), insoluble collagen, polyesters, polyvinyl chloride or polyvinylidene chloride, polyvinyl alcohol, polyacrylonitrile, chitosan, polyurethane, poly(urethane-urea) or polyethylene phthalate, and fibers formed from a blend of polymers such as those mentioned above, such as polyamide/polyester fibers or any combination thereof. A scaffold can comprise polymers. A polymer can be a biopolymer. A biopolymer can include but is not limited to chitin, chitosan, elastin, collagen, keratin or polyhydroxyalkanoate. The polymers can be biodegradable, biostable, or combinations thereof. The polymer in a scaffold can be natural polymers. Exemplary natural polymers include polysaccharides such as alginate, cellulose, dextran, pullane, polyhyaluronic acid, chitin, poly(3-hydroxyalkanoate), poly(3-hydroxyoctanoate) or poly(3-hydroxyfatty acid). In some cases, a scaffold also comprises chemical derivatives of the natural polymers. Such chemical derivatives can include substitutions and/or additions of chemical groups such as alkyl, alkylene, hydroxylations, oxidations, as well as other modifications familiar to those skilled in the art. The natural polymers can also be selected from proteins such as collagen, zein, casein, gelatin, gluten, and serum albumen. The polymer in a scaffold can be biodegradable synthetic polymers, including poly alpha-hydroxy acids such as poly L-lactic acid (PLA), polyglycolic acid (PGA) or copolymers thereof (e.g., poly D,L-lactic co-glycolic acid (PLGA)), and hyaluronic acid. A scaffold can be bioabsorbable. A bioabsorbable scaffold can be a non-cytotoxic structure or substance that can be capable of containing or supporting living cells and holding them in a desired configuration for a period of time. The term “bioabsorbable” can refer to any material the body can break down into non-toxic by-products that are excreted from the body or metabolized therein. Exemplary bioabsorbable materials for a scaffold include, poly(lactic acid), poly(glycolic acid), poly(trimethylene carbonate), poly(dimethyltrimethylene carbonate), poly(amino acids)s, tyrosine-derived poly(carbonates)s, poly(carbonates)s, poly(caprolactone), poly(para-dioxanone), poly(esters)s, poly(ester-amides)s, poly(anhydrides)s, poly(ortho esters)s, collagen, gelatin, serum albumin, proteins, polysaccharides, mucopolysaccharides, carbohydrates, glycosaminoglycans, poly(ethylene glycols)s, poly(propylene glycols)s, poly(acrylate esters)s, poly(methacrylate esters)s, poly(vinyl alcohol), hyaluronic acid, chondroitin sulfate, heparin, dermatan sulfate, versican, copolymers, blends and mixtures of polymers, and oligomers containing bioabsorbable linkages. A scaffold can be of various thicknesses. For example, a scaffold can have a thickness that can be suitable for forming a cell layer. For example, a scaffold can have a thickness from about 0.1 mm to about 10 mm, such as from about 0.1 mm to about 5 mm, from about 0.1 mm to about 4 mm, from about 0.1 mm to about 3 mm, from about 0.1 mm to about 2 mm, to about 0.1 mm to about 1 mm, from about 0.2 mm to about 1 mm, from about 0.3 mm to about 1 mm, from about 0.4 mm to about 1 mm, from about 0.5 mm to about 1 mm, from 0.3 mm to about 1.5 mm, from about 0.4 mm to about 1.2 mm, from about 0.6 mm to about 1.2 mm, or from about 0.7 mm to about 1.5 mm. For example, a scaffold can have a thickness from about 0.5mm to 1 mm. In some cases, a scaffold can be at least 0.1 mm, 0.2 mm, 0.3 mm, 0.4 mm, 0.5 mm, 0.8 mm, 1 mm, 2 mm, 3 mm, 4 mm, or 5 mm thick. In some cases, a scaffold can be at most 0.5 mm, 0.8 mm, 1 mm, 2 mm, 3 mm, 4 mm, 5 mm, 6 mm, 7 mm, 8 mm, 9 mm, or 10 mm thick. In some embodiments, a scaffold can have a length and/or a width of a cell layer to be placed and/or grown upon a scaffold. In some embodiments, a scaffold can have a length and/or a width of a cell layer described herein. A scaffold can have a surface area on a face of a synthetic leather. For example, a scaffold can have a surface area of from about 0.1 mm 2 to about 100 mm 2 , from about 0.1 mm 2 to about 95 mm 2 , from about 0.1 mm 2 to about 90 mm 2 , from about 0.1 mm 2 to about 85 mm 2 , from about 0.1 mm 2 to about 80 mm 2 , from about 0.1 mm 2 to about 75 mm 2 , from about 0.1 mm 2 to about 70 mm 2 , from about 0.1 mm 2 to about 65 mm 2 , from about 0.1 mm 2 to about 60 mm 2 , from about 0.1 mm 2 to about 55 mm 2 , from about 0.1 mm 2 to about 50 mm 2 . from about 0.1 mm 2 to about 45 mm 2 , from about 0.1 mm 2 to about 40 mm 2 , from about 0.1 mm 2 to about 35 mm 2 , from about 0.1 mm 2 to about 30 mm 2 , from about 0.1 mm 2 to about 25 mm 2 , from about 0.1 mm 2 to about 20 mm 2 , from about 0.1 mm 2 to about 15 mm 2 , from about 0.1 mm 2 to about 10 mm 2 , from about 0.1 mm 2 to about 5 mm 2 , or from about 0.1 mm 2 to about 1 mm2. In some cases, a scaffold can have a surface area of from about 0.1 cm 2 to about 100 cm 2 , from about 0.1 cm 2 to about 95 cm 2 , from about 0.1 cm 2 to about 90 cm 2 , from about 0.1 cm 2 to about 85 cm 2 , from about 0.1 cm 2 to about 80 cm 2 , from about 0.1 cm 2 to about 75 cm 2 , from about 0.1 cm 2 to about 70 cm 2 , from about 0.1 cm 2 to about 65 cm 2 , from about 0.1 cm 2 to about 60 cm 2 , from about 0.1 cm 2 to about 55 cm 2 , from about 0.1 cm 2 to about 50 cm 2 , from about 0.1 cm 2 to about 45 cm 2 , from about 0.1 cm 2 to about 40 cm 2 , from about 0.1 cm 2 to about 35 cm 2 , from about 0.1 cm 2 to about 30 cm 2 , from about 0.1 cm 2 to about 25 cm 2 , from about 0.1 cm 2 to about 20 cm 2 , from about 0.1 cm 2 to about 15 cm 2 , from about 0.1 cm 2 to about 10 cm 2 , from about 0.1 cm 2 to about 5 cm 2 , or from about 0.1 cm 2 to about 1 cm 2 . In some cases, a scaffold can have a surface area of from about 0.1 m 2 to about 100 m 2 , from about 0.1 m 2 to about 95 m 2 , from about 0.1 m 2 to about 90 m 2 , from about 0.1 m 2 to about 85 m 2 , from about 0.1 m 2 to about 80 m 2 , from about 0.1 m 2 to about 75 m 2 , from about 0.1 m 2 to about 70 m 2 . from about 0.1 m 2 to about 65 m 2 , from about 0.1 m 2 to about 60 m 2 , from about 0.1 m 2 to about 55 m 2 , from about 0.1 m 2 to about 50 m 2 , from about 0.1 m 2 to about 45 m 2 , from about 0.1 m 2 to about 40 m 2 , from about 0.1 m 2 to about 35 m 2 , from about 0.1 m 2 to about 30 m 2 , from about 0.1 m 2 to about 25 m 2 , from about 0.1 m 2 to about 20 m 2 , from about 0.1 m 2 to about 15 m 2 , from about 0.1 m 2 to about 10 m 2 , from about 0.1 m 2 to about 5 m 2 , or from about 0.1 m 2 to about 1 m 2 . In some cases, a scaffold can have a surface area of at least about 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29. 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 75, or 100 mm 2 . In some cases, a scaffold can have a surface area of at least about 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 75, or 100 cm 2 . In some cases, a scaffold can have a surface area of at least about 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 75, or 100 m 2 . Alternatively, a cell layer may not form on a scaffold. For example, a dermal layer may not form on a scaffold (e.g., collagen matrix). In certain cases, a synthetic leather does not comprise a scaffold. Pigments A synthetic leather can comprise one or more pigments. One, or more layer structures of the synthetic leather can be pigmented. A pigment in a synthetic leather can be a natural pigment produced in cells forming the synthetic leather. For example, a pigment can be melanin, including eumelanin (e.g., brown eumelanin and black eumelanin), pheomelanin, neuromelanin, or any combination thereof. A pigment in a synthetic leather can be an exogenous pigment, such as a leather pigment dye. Collagen A synthetic leather can comprise collagen. Collagen can refer to any member of a family of at least 28 distinct collagen types. Collagens can be characterized by a repeating triplet of amino acids, -(Gly-X-Y)n-, so that approximately one-third of the amino acid residues are in collagen are glycine. X can be proline and Y can be hydroxyproline. Thus, the structure of collagen can have twined triple units of peptide chains of differing lengths. A synthetic leather can comprise collagen from one or more species. In some cases, a synthetic leather comprises collagen from different animals. Different animals can produce different amino acid compositions of the collagen, which can result in different properties (and differences in the resulting leather). Collagen fiber monomers can be produced from alpha-chains of about 1050 amino acids long, so that the triple helix takes the form of a rod of about 300 nm long, with a diameter of about 1.5 nm. A synthetic leather can comprise one or more types of collagen. Collagen comprised in a synthetic leather can include fibrillary collagens, non-fibrillar collagens, or a combination thereof. Fibrillary collagens include type I, type II, type III, type V, and type XI collagens. Non-fibrillar collagens include fibril associated collagens with interrupted triple helices (e.g., type IX, type XII, type XIV, type XVI, and type XIX), short chain collagens (e.g., type VIII and type X), basement membrane collagens (type IV), Multiplexin (Multiple Triple Helix domains with Interruptions) (e.g., Type XV and type XVIII), MACIT collagens (Membrane Associated Collagens with Interrupted Triple Helices) (e.g., Type XIII and type XVII). Collagen can be comprised in one or more parts of a synthetic leather. For example, collagens can be comprised in one or more dermal layers, one or more epidermal layers, or combination thereof, in a synthetic leather. For example, collagens can be comprised in one or more layered structures in a synthetic leather. In some cases, when part of the synthetic leather can be removed during process, collagen can also be comprised in the removed product. Collagen in a synthetic leather can be from one or more sources. For example, the collagen can be produced by collagen producing cells in the synthetic leather. For example, the collagen can be separately added to the leather. In some cases, a synthetic leather comprises collagen produced by collagen producing cells and collagens separately added. At least part of the collagen in a synthetic leather can be produced by collagen producing cells. Such collagen producing cells can be comprised in the synthetic leather. Exemplary collagen producing cells include epithelial cells, fibroblasts, keratinocytes, comeocytes, melanocytes, Langerhans cells, basal cells, smooth muscle cells, or a combination thereof. The epithelial cells can include squamous cells, cuboidal cells. columnar cells, basal cells, or a combination thereof. The fibroblasts can include dermal fibroblasts. The keratinocytes can include epithelial keratinocytes, basal keratinocytes, proliferating basal keratinocytes, differentiated suprabasal keratinocytes, or a combination thereof. Collagen in a synthetic leather can be produced by one or more types of collagen-producing cells. Additives A synthetic leather can further comprise one or more additives. Such additives can enhance the commercial appeal (e.g., appearance, color, or odor). Exemplary additives include minerals, fiber, fatty acids, and amino acids, proteins. An additive can be an odorant. Additives can include one or more of: matrix proteins, proteoglycans, antioxidants, perfluorocarbons, and growth factors. A growth factor can be a protein, a polypeptide, or a complex of polypeptides, including cytokines (e.g., that are produced by a cell, and which can affect itself and/or a variety of other neighboring or distant cells). Growth factors can affect the growth and/or differentiation of specific types of cells, either developmentally or in response to a multitude of physiological or environmental stimuli. Some, but not all, growth factors are hormones. Exemplary growth factors include insulin, insulin-like growth factor (IGF), nerve growth factor (NGF), vascular endothelial growth factor (VEGF), keratinocyte growth factor (KGF), fibroblast growth factors (FGFs), including basic FGF (bFGF), platelet-derived growth factors (PDGFs), including PDGF-AA and PDGF-AB, hepatocyte growth factor (HGF), transforming growth factor alpha (TGF-a), transforming growth factor beta (TGF-β), including TGFpi and TGFP3, epidermal growth factor (EGF), granulocyte-macrophage colony-stimulating factor (GM-CSF), granulocyte colony-stimulating factor (G-CSF), interleukin-6 (IL-6). IL-8, and the like. Other polypeptides or molecules (e.g., healing agents; enzymes such as matrix-degrading enzymes and matrix-degrading enzyme inhibitors (e.g., TIMPs), antibiotics, and antimycotics) can also be added to a synthetic leather. Additives can also include preservatives known to the art. Exemplary preservatives include antimicrobial preservatives such as calcium propionate, sodium nitrate, sodium nitrite, sulfites (e.g., sulfur dioxide, sodium bisulfate, potassium hydrogen sulfite, etc.), disodium ethylenediammetetraacetic acid (EDTA), antioxidant such as butylated hydroxyanisole (BHA) and butylated hydroxytoluene (BHT). In certain cases, a synthetic leather can comprise an extracellular matrix or connective tissue. For example, a synthetic leather can further comprise collagen, keratin, elastin, gelatin, proteoglycan, dermatan sulfate proteoglycan, glycosoaminoglycan, fibronectin, laminin, dermatopontin, lipid, fatty acid, carbohydrate, and a combination thereof. Pattern of Synthetic Leather A synthetic leather can be patterned. For example, the synthetic leather may be patterned after a skin pattern of an animal selected from antelope, bear, beaver, bison, boar, camel, caribou, cat, cattle, deer, dog, elephant, elk, fox, giraffe, goat, hare, horse, ibex, kangaroo, lion, llama, lynx, mink, moose, oxen, peccary, pig, rabbit, seal, sheep, squirrel, tiger, whale, wolf, yak, zebra, turtle, snake, crocodile, alligator, dinosaur, frog, toad, salamander, newt, chicken, duck, emu, goose, grouse, ostrich, pheasant, pigeon, quail, turkey, anchovy, bass, catfish, carp, cod, eel, flounder, fugu, grouper, haddock, halibut, herring, mackerel, mahi mahi, manta ray, marlin, orange roughy, perch, pike, pollock, salmon, sardine, shark, snapper, sole, stingray, swordfish, tilapia, trout, tuna, walleye, and a combination thereof. The pattern can be a skin pattern of a fantasy animal selected from dragon, unicorn, griffin, siren, phoenix, sphinx, Cyclops, satyr, Medusa, Pegasus, Cerberus, Typhoeus, gorgon, Charybdis, empusa, chimera, Minotaur, Cetus, hydra, centaur, fairy, mermaid, Loch Ness monster, Sasquatch, thunderbird, yeti, chupacabra, and a combination thereof. A synthetic leather can be made to resemble traditional animal skin, hide, or leather products and design parameters (e.g., cell types, additives, size, shape). In some cases, a synthetic leather comprises a cell layer characterized by a composition that can be substantially similar to traditional animal skin, hide, or leather products. For example, such layer can be characterized by a composition that can be substantially about 60% to 80% aqueous fluid, about 14%-35% protein, about 1%-25% fat. In some cases, keratinocytes of the cell layer are aligned. For example, the keratinocytes can be aligned by application of an electrical field. For example, keratinocytes can be aligned by application of a mechanical stimulus, such as cyclical stretching and relaxing the substratum. In some cases, aligned (e.g., electro-oriented and mechano-oriented) keratinocytes have substantially the same orientation with regard to each other as can be found in many animal skin tissues. Leather Articles A synthetic leather herein can be at least a portion of a leather article. For example, a synthetic leather can be used as substitute of natural leather in a leather article. Exemplary leather articles include a watch strap, belt, suspender, packaging, shoe, boot, footwear, glove, clothing (e.g., tops, bottoms, and outerwear), luggage, bag (e.g., a handbag with or without shoulder strap), clutch, purse, coin purse, billfold, key pouch, credit card case, pen case, backpack, cases, wallet, saddle, harness, whip, travel goods (e.g., a trunk, suitcase, travel bag, beauty case, or a toilet kit), rucksacks, portfolio, document bag, briefcase, attaché case, pet article (e.g., a leash or collar), hunting and fishing article (e.g., a gun case, cutlery case, or a holster for firm arms), a stationary article (e.g., a writing pad, book cover, camera case, spectacle case, cigarette case, cigar case, jewel case, or a mobile phone holster), a sport article (e.g., a ball such as basketball, soccer ball, or a football), a building interior, a building exterior, an upholstery, a book binding, a furniture, a lamp, a lamp shade, a table covering, a wall covering, a floor covering, a ceiling covering, a car interior, a car exterior, a boat interior, a boat exterior, an airplane interior, a yacht interior, a yacht exterior, a pillow case, a sheet, a duvet cover, jewelry, an accessory, a pair of glasses, a pair of sun glasses, or a consumer electronic. For example, a leather article can be a watch wrap. For example, a leather article can be a belt. For example, a leather article can be a bag. Skin Graft A synthetic leather or portions thereof can also be used as a skin graft, e.g., an allograft or xenograft for transplanting to a subject. For example, the synthetic leather, dermal layer, epidermal layer and/or a layered structure can be a source of skin graft for allotransplant or xenotransplant. In some cases, the synthetic leather, dermal layer, epidermal layer and/or a layered structure can be produced with cells genetically modified to reduce immune rejection in the recipient of the graft. Methods Also disclosed herein are methods of making a synthetic leather. The methods can comprise forming an artificial dermal layer, forming an artificial epidermal layer, or a combination thereof. The methods can further comprise tanning at least of a portion of the artificial dermal layer and/or artificial epidermal layer. The cells in a synthetic leather, e.g., those in the dermal layer and/or the epidermal layer can be differentiated from stem cells (e.g., iPSCs). The methods herein can further comprise differentiating stem cells (e.g., iPSCs) into cells in the synthetic leather, e.g., cells in the dermal layer and/or the epidermal layer. In certain cases, the methods comprise placing a first cell layer (e.g., an epidermal layer) upon a second cell layer (e.g., a dermal layer) thereby forming a layered structure, and tanning at least a portion of the layered structure. In some cases, the methods can further comprise removing at least a portion of the first cell layer (e.g., an epidermal layer). Forming Cell Layers A cell layer can be formed by preparing a plurality of multicellular bodies comprising one or more type of cells and arranging such multicellular bodies to form a cell layer. For example, a cell layer can be formed by adjacently arranging a plurality of multicellular bodies, wherein the multicellular bodies are fused to form a planar layer. Forming a cell layer may need a scaffold. A cell layer can be formed by arranging a plurality of multicellular bodies on a scaffold. For example, the forming step can comprise arranging or placing multicellular bodies on a support substrate that allows the multicellular bodies to fuse to form a layer (e.g., a substantially planar layer). In some cases, the multicellular bodies or the layers are arranged horizontally and/or vertically adjacent to one another. Alternatively, forming a cell layer may not need a scaffold. Cell layers can be formed by embedding cells in a medium or gel. In some cases, dermal layers can be formed using fibroblasts embedded in a collagen I or fibrin gel. Other types of media can also be used. For example, a medium can promote fibroblast to secret sufficient amount of extracellular matrix to enable extended maintenance of epidermis without the need for collagen gels. Forming Multicellular Bodies There are various ways to make multicellular bodies having the characteristics described herein. In some cases, a multicellular body can be fabricated from a cell paste containing a plurality of cells, e.g., with a desired cell density and viscosity. In further cases, the cell paste can be shaped into a desired shape and a multicellular body formed through maturation (e.g., incubation). In some cases, an elongate multicellular body can be produced by shaping a cell paste including a plurality of cells into an elongate shape (e.g., a cylinder). In further cases, the cell paste can be incubated in a controlled environment to allow the cells to adhere and/or cohere to one another to form the elongate multicellular body. For example, a multicellular body can be produced by shaping a cell paste including a plurality of living cells in a device that holds the cell paste in a three-dimensional shape. In some cases, the cell paste can be incubated in a controlled environment while it can be held in the three-dimensional shape for a sufficient time to produce a body that has sufficient cohesion to support itself on a flat surface, as described herein. A cell paste can be provided by: (A) mixing cells or cell aggregates (of one or more cell types) and a cell culture medium (e.g., in a pre-determined ratio) to result in a cell suspension, and (B) compacting the cellular suspension to produce a cell paste with a desired cell density and viscosity. Compacting can be achieved by a number of methods, such as by concentrating a particular cell suspension that resulted from cell culture to achieve the desired cell concentration (density), viscosity, and consistency required for the cell paste. In some cases, a relatively dilute cell suspension from cell culture can be centrifuged for a determined time to achieve a cell concentration in the pellet that allows shaping in a mold. Tangential flow filtration (“TFF”) is another suitable method of concentrating or compacting the cells. In some cases, compounds are combined with the cell suspension to lend the extrusion properties required. Suitable compounds include, collagen, hydrogels, Matrigel, nanofibers, self-assembling nanofibers, gelatin, and fibrinogen. One or more ECM components (or derivatives of ECM components) can also be included by, resuspending the cell pellet in one or more physiologically acceptable buffers containing the ECM components (or derivatives of ECM components) and the resulting cell suspension centrifuged again to form the cell paste. Various methods can be used to shape the cell paste. For example, in a particular embodiment, the cell paste can be manually molded or pressed (e.g., after concentration/compaction) to achieve a desired shape. By way of a further example, the cell paste can be taken up (e.g., aspirated) into a preformed instrument, such as a micropipette (e.g., a capillary pipette), that shapes the cell paste to conform to an interior surface of the instrument. The cross-sectional shape of the micropipette (e.g., capillary pipette) can be alternatively circular, square, rectangular, triangular, or other non-circular cross-sectional shape. In some embodiments, the cell paste can be shaped by depositing it into a preformed mold, such as a plastic mold, metal mold, or a gel mold. In some embodiments, centrifugal casting or continuous casting can be used to shape the cell paste. The cell paste can be further matured. In some cases, the cell paste can be incubated at about 37° C. for a time period (which can be cell-type dependent) to foster adherence and/or coherence. Alternatively or in addition. the cell paste can be held in the presence of cell culture medium containing factors and/or ions to foster adherence and/or coherence. Arranging Multicellular Bodies on a Support Substrate to Form Layers Multicellular bodies can be arranged on a support substrate to produce a desired three-dimensional structure (e.g., a substantially planar layer). For example, multicellular bodies can be manually placed in contact with one another, deposited in place by extrusion from a pipette, nozzle, or needle, or positioned in contact by an automated machine such as a biofabricator. A support substrate can be permeable to fluids, gasses, and nutrients and allows cell culture media to contact all surfaces of the multicellular bodies and/or layers during arrangement and subsequent fusion. In some cases, a support substrate can be made from natural biomaterials such as collagen, fibronectin, laminin, and other extracellular matrices. In some cases, a support substrate can be made from synthetic biomaterials such as hydroxyapatite, alginate, agarose, polyglycolic acid, polylactic acid, and their copolymers. In some cases, a support substrate can be solid, semisolid, or a combination of solid and semisolid support elements. In some cases, a support substrate can be planar to facilitate production of planar layers. In some cases, a support substrate can be raised or elevated above a non-permeable surface, such as a portion of a cell culture environment (e.g., a Petri dish, a cell culture flask, etc.) or a bioreactor. A permeable, elevated support substrate can contribute to prevention of premature cell death, contributes to enhancement of cell growth. and facilitates fusion of multicellular bodies to form layers. Once assembly of a layer is complete, a tissue culture medium can be poured over the top of the construct. In some cases, the tissue culture medium enters the spaces between the multicellular bodies to support the cells in the multicellular bodies. The multicellular bodies in the three-dimensional construct can be allowed to fuse to one another to produce a layer (e.g., a substantially planar) for use in formation of the synthetic leather. The terms “fuse.” “fused” or “fusion.” can mean that the cells of contiguous multicellular bodies become adhered and/or cohered to one another, either directly through interactions between cell surface proteins, or indirectly through interactions of the cells with ECM components or derivatives of ECM components. A fused layer can be completely fused and that multicellular bodies have become substantially contiguous. Alternatively, a fused layer can be substantially fused or partially fused, and the cells of the multicellular bodies have become adhered and/or cohered to the extent necessary to allow moving and manipulating the layer intact. Multicellular bodies can fuse to form a layer in a cell culture environment (e.g., a Petri dish, cell culture flask. or bioreactor). In some cases, the multicellular bodies fuse to form a layer in an environment with conditions suitable to facilitate growth of the cell types included in the multicellular bodies. In some cases, fusing takes place over about 15, 20, 25, 30, 35, 40, 45, 50, 55, and 60 minutes, and increments therein. In other cases, fusing takes place over about 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, and 48 hours, and increments therein. In yet other cases, fusing takes place over about 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, and 14 days, and increments therein. In further cases, fusing takes place over about 2 hours to about 24 hours. Factors relevant to the fusing time can include cell types, cell type ratios, culture conditions, and the presence of additives such as growth factors. Once fusion of a layer is complete, the layer and the support substrate can be separated. In other cases, the layer and the support substrate are separated when fusion of a layer is substantially complete or partially complete, but the cells of the layer are adhered and/or cohered to one another to the extent necessary to allow moving, manipulating, and stacking the layer without breaking it apart. The layer and the support substrate can be separated via standard procedures for melting, dissolving, or degrading the support substrate. In some cases, the support substrate can be dissolved, for example, by temperature change, light, or other stimuli that do not adversely affect the layer. In certain cases, the support substrate can be made of a flexible material and peeled away from the layer. The separated layer can be transferred to a bioreactor for further maturation. In some cases, the separated layer matures and further fuses after incorporation into an engineered animal skin, hide, or leather product. Alternatively, the layer and the support substrate may not be separated. The support substrate degrades or biodegrades prior to packaging, freezing, sale or consumption of the assembled engineered animal skin, hide, or leather product. A cell layer can be formed over a period of time. In some cases, a cell layer, e.g., an epidermal layer or a dermal layer, can be formed within 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 25, 30, 40, 50, 60, 120, 300 days. In some cases, a dermal layer can be formed in about 1 to 15 days, e.g., 5 to 10 days, or 10 to 12 days. In some cases, a dermal layer can be formed about 5 to 25 days, e.g., 14 to 15 days. The present disclosure provides methods for making synthetic leather improved barrier function. In some cases, the methods comprise providing keratinocytes and a culture media comprising ascorbic acid and linoleic acid; and culturing the keratinocytes under conditions such that a synthetic leather having improved barrier function can be formed. In some cases, the culture conditions include culture at about 50 to 95% humidity, e.g., about 75% humidity. In some cases, the ascorbic acid can be provided at concentration of from about 10 to 100 micrograms/ml. In still further cases, linoleic acid can be provided at a concentration of from about 5 to 80 micromolar. The present disclosure is not limited to synthetic leather formed from a particular source of keratinocytes. Indeed, the synthetic leather can be formed from a variety of primary and immortal keratinocytes, including, but not limited to Near-Diploid Immortalized Keratinocytes (NIKS) cells. In still further cases, the keratinocytes express exogenous wild-type or variant Kruppel-like factor (GKLF). In still further cases, the keratinocytes are derived from two different sources. In other cases, the synthetic leather has a surface electrical capacitance of from about 40 to about 240 pF. In some preferred cases, the skin equivalent has a surface electrical capacitance of from about 80 to about 120 pF. In other preferred cases. the content of ceramides 5, 6, and 7 in the skin equivalent can be from about 20 to about 50% of total ceramide content. In still other preferred cases, the content of ceramide 2 in the skin equivalent can be from about 10to about 40% of total ceramide content. In still further cases, the present disclosure provides the skin equivalent made by the method just described. Arranging Layers to Form a Layered Structure Multiple cell layers can be arranged to form a layer structure, thus producing synthetic leathers described herein. In some cases, dermal layers and epidermal layers are formed separately and assembled by placing the epidermal layers atop of the dermal layers (e.g., when both an epidermal layer and a dermal layer are fully formed). In some cases, an epidermal layer can be grown atop a dermal layer. In certain cases, a basement membrane or basement membrane substitute can be placed between a dermal layer and an epidermal layer. For example, the cell layers can be manually placed in contact with one another or deposited in place by an automated, computer-aided machine such as a biofabricator, according to a computer script. Before assembling multiple cell layers, one or more quality control steps can be performed. For example, Trans Epithelial Electrical Resistance (TEER) can be performed on epidermis before placement on dermis (e.g., 0 day), followed by histology analysis (e.g., minimum 3˜5 days). Using methods provided herein, the risk of improperly formed layered structure or full thickness skin equivalents can be low. Multiple cell layers can be assembled in various ways. In some cases, an epidermal layer and a dermal layer (with or without a basement membrane substitute) are placed on a scaffold (e.g., silk), e.g., to achieve thickness and tensile strength of natural leather. In some cases, an epidermal layer and multiple dermal layers (with or without a basement membrane substitute) are assembled without using a scaffold. Such assembly can achieve thickness and tensile strength that resemble natural leather. In some cases, an epidermal layer and multiple dermal layers (with or without a basement membrane substitute) are placed on a scaffold (e.g., silk) achieve thickness and tensile strength that resemble natural leather. In some embodiments, chemical, mechanical, performance, strength, durability, moisture, dimensional tests or a combination thereof can be performed on one or more multiple cell layer, synthetic leathers, artificial epidermal layers, artificial dermal layers, layered structures, products produced therefrom. In some embodiments, a chemical, mechanical, performance, strength, durability, moisture, dimensional tests or a combination thereof can be performed using a non-standard test. In some embodiments, a chemical, mechanical, performance, strength, durability, moisture, dimensional tests or a combination thereof can be performed using a standard test. In some embodiments, a test can be performed as instructed and/or adopted and/or ratified and/or developed by the International Standards Organization (ISO), European standards body (CEN), ASTM International or by the International Union of Leather Technicians and Chemists (IULTCS). In some embodiments, a test in any one of Table 1-Table 11 or any variation thereof can be performed using any one or more corresponding method or any variation thereof. TABLE 1IULTCS - CHEMICAL TEST METHODSIU No.Method nameIUC 1General commentsIUC 2SamplingIUC 3Preparation of test material by grindingIUC 4Determination of substances (fats and other soluble) soluble in Dichloromethane.IUC 5Determination of volatile matterIUC 6Determination of water-soluble matter, water soluble inorganic matter and water-solubleorganic matterIUC 7Determination of sulphated total ash and sulphated water insoluble ashIUC 8Determination of chromic oxideIUC 9Determination of water-soluble magnesium saltsIUC 10Determination of nitrogen and hide substanceIUC 11Determination of pH and difference figureIUC 13Determination of zirconiumIUC 15Determination of phosphorusIUC 16Determination of aluminiumIUC 17Determination of hydroxyproline in materials containing collagenIUC 18Photometric Determination of chromium (VI) using 1,5-DiphenylcarbazideIUC 19Determination of formaldehyde content of leatherIUC 20Method for the detection of certain AZO colourants in dyed leatherIUC 21Method for the detection of certain AZO colourants in dyestuff mixturesIUC 22Determination of aluminium oxide content of aluminium tanning agentsIUC 23Determination of the pH of aqueous solutions of aluminium tanning agentsIUC 24Determination of basicity of aluminium tanning agents.IUC 25Determination of pentachlorophenol content TABLE 2IULTCS - PHYSICAL TEST METHODSIU No.Method nameIUP 1General remarksIUP 2SamplingIUP 3ConditioningIUP 4Measurement of thicknessIUP 5Measurement of apparent densityIUP 6Measurement of tensile strength and percentage elongationIUP 7Measurement of static absorption of waterIUP 8Measurement of tear load - Double edge tearIUP 9Measurement of distension and strength of grain by the Ball Burst TestIUP 10Water resistance of flexible leatherIUP 11Measurement of water resistance of heavy leatherIUP 12Measurement of resistance to grain cracking and the grain crack indexIUP 13Measurement of two-dimensional extensionIUP 14Measurement of waterproofness of gloving leathersIUP 15Measurement of water vapour permeabilityIUP 16Measurement of shrinkage temperature up to 100° C.IUP 17Assessment of the resistance of air dry insole leathers to heatIUP 18Resistance of air dry lining leathers to heatIUP 19Resistance of air dry upper leather to heatIUP 20Measurement of flex resistance by flexometer methodIUP 21Measurement of set in lastingIUP 22Assessment of scuff damage by use of the viewing boxIUP 23Measurement of scuff damageIUP 24Measurement of surface shrinkage by immersion in boiling waterIUP 26Measurement of resistance to abrasion of heavy leatherIUP 28Measurement of the resistance to bending of heavy leatherIUP 29Measurement of cold crack temperature of surface coatingsIUP 30Measurement of water vapour absorption and desorption (See IUP 42)IUP 32Measurement of areaIUP 35Measurement of dry heat resistance of leatherIUP 36Measurement of leather softnessDraft IUP 37Measurement of water repellancy of garment leatherIUP 38Measurement of heat resistance of patent leatherIUP 39Measurement of flex resistance by the vamp flex methodIUP 40Measurement of tear load - Single edge tearIUP 41Measurement of surface coating thicknessIUP 42Measurement of water vapour absorptionIUP 43Measurement of extension setIUP 44Measurement of stitch tear resistanceDraft IUP 45Measurement of water penetration pressureDraft IUP 46Measurement of fogging characteristicsDraft IUP 47Measurement of resistance to horizontal spread of flameDraft IUP 48Measurement of abrasion resistance of upholstery leatherDevelopmentMeasurement of bagginess (IUP 49)DevelopmentMeasurement of soiling (IUP 50)DevelopmentMeasurement of Surface Friction (IUP 51)DevelopmentMeasurement of Compressibility (IUP 52) TABLE 3IULTCS - FASTNESS TEST METHODSIU No.Method nameIUF 105Numbering code for fastness testsIUF 120Principles of colour fastness testingIUF 131Grey scale for assessing change in colourIUF 132Grey scale for assessing stainingIUF 151Preparation of Standard Storable Chrome leatherIUF 201Approx. determination of solubility of leather dyesIUF 202Fastness to acid of dye solutionsIUF 203Stability to acid of dye solutionsIUF 205Stability to hardness of dye solutionsIUF 401Fastness to daylightIUF 402Fastness to light (Xenon arc)IUF 420Fastness to water spottingIUF 421Fastness to waterIUF 423Fastness to washingIUF 424Fastness to formaldehydeIUF 426Fastness to perspirationIUF 434Fastness to dry-cleaning of small samplesIUF 435Fastness to machine washingIUF 441Fastness in respect to staining raw crepe rubberIUF 442Fastness in respect of staining plasticised PVCIUF 450Fastness to and fro rubbingIUF 454Fastness to buffing of dyed leatherIUF 458Fastness to ironingIUF 470Adhesion of finishIUF 412Change of colour with accelerated ageing TABLE 4ASTM's Leather Standards-Apparel Test TitleDesignationTest TitleD1913-00(2015)Standard Test Method for Resistance to Wetting of Garment-Type Leathers (Spray Test)D2096-11Standard Test Method for Colorfastness and Transfer of Color in the Washing of LeatherD2821-14Standard Test Method for Measuring the Relative Stiffness of Leather by Means of aTorsional Wire ApparatusD5053-03(2015)Standard Test Method for Colorfastness of Crocking of LeatherD5552-10(2015)Standard Test Method for Resistance of Colored Leather to BleedingD6012-03(2013)Standard Test Method for Determination of Resistance of Leather to (Bleeding) Color StainTransferD6013 - 00(2010)Standard Test Method for Determination of Area Stability of Leather to LaunderingD6014 - 00(2015)Standard Test Method for Determination of Dynamic Water Absorption of Leather Surfaces TABLE 5ASTM's Leather Standards-Chemical AnalysisDesignationTest TitleD2617-12Standard Test Method for Total Ash in LeatherD2807-93(2015)Standard Test Method for Chromic Oxide in Leather (Perchloric Acid Oxidation)D2810-13Standard Test Method for pH of LeatherD2868-10(2015)Standard Test Method for Nitrogen Content (Kjeldahl) and Hide Substance Content ofLeather, Wet Blue and Wet WhiteD3495-10(2015)Standard Test Method for Hexane Extraction of LeatherD3790- 79(2012)Standard Test Method for Volatile Matter (Moisture) of Leather by Oven DryingD3897-91(2012)Standard Practice for Calculation of Basicity of Chrome Tanning LiquorsD3898-93(2015)Standard Test Method for Chromic Oxide in Basic Chromium Tanning LiquorsD3913-03(2015)Standard Test Method for Acidity in Basic Chromium Tanning LiquorsD4653-87(2015)Standard Test Method for Total Chlorides in LeatherD4654-87(2015)Standard Test Method for Sulfate Basicity in LeatherD4655-95(2012)Standard Test Methods for Sulfates in Leather (Total, Neutral, and Combined Acid)D4906-95(2012)Standard Test Method for Total Solids and Ash Content in Leather Finishing MaterialsD4907-10(2015)Standard Test Method for Nitrocellulose in Finish on LeatherD5356-10(2015)Standard Test Method for pH of Chrome Tanning SolutionsD6016-06(2012)Standard Test Method for Determination of Nitrogen, Water Extractable in LeatherD6017-97(2015)Standard Test Method for Determination of Magnesium Sulfate (Epsom Salt) in LeatherD6018-96(2012)Standard Test Method for Determining the Presence of Lead Salts in LeatherD6019-15Test Method for Determination of Chromic Oxide in Basic Chromium Tanning Liquors(Ammonium Persulfate Oxidation) TABLE 6ASTM's Leather Standards-Fats and OilsDesignationTest TitleD5346-93(2009)Standard Test Method for Determination of the Pour Point of Petroleum Oil Used in Fatliquorsand Softening CompoundsD5347-95(2012)Standard Test Method for Determination of the Ash Content of Fats and OilsD5348-95(2012)Standard Test Method for Determination of the Moisture Content of Sulfonated and SulfatedOils by Distillation with XyleneD5349-95(2012)Standard Test Method for Determination of the Moisture and Volatile Content of Sulfonatedand Sulfated Oils by Hot-Plate MethodD5350-95(2012)Standard Test Method for Determination of Organically Combined Sulfuric Anhydride byTitration, Test Method AD5351-93(2009)Standard Test Method for Determination of Organically Combined Sulfuric Anhydride byExtraction Titration, Test Method BD5352-95(2012)Standard Test Method for Determination of Organically Combined Sulfuric Anhydride Ash-Gravimetric, Test Method CD5353-95(2012)Standard Test Method for Determination of Total Desulfated Fatty MatterD5354-95(2012)Standard Test Method for Determination of Total Active Ingredients in Sulfonated andSulfated OilsD5355-95(2012)Standard Test Method for Specific Gravity of Oils and Liquid FatsD5439-95(2012)Standard Test Method for Determination of Sediment in MoellonD5440-93(2009)Standard Test Method for Determining the Melting Point of Fats and OilsD5551-95(2012)Standard Test Method for Determination of the Cloud Point of OilD5553-95(2012)Standard Test Method for Determination of the Unsaponifiable Nonvolatile Matter in SulfatedOilsD5554-15Standard Test Method for Determination of the Iodine Value of Fats and OilsD5555-95(2011)Standard Test Method for Determination of Free Fatty Acids Contained in Animal, Marine, andVegetable Fats and Oils Used in Fat Liquors and Stuffing CompoundsD5556-95(2011)Standard Test Method for Determination of the Moisture and Other Volatile Matter Containedin Fats and Oils Used in Fat Liquors and Softening CompoundsD5557-95(2011)Standard Test Method for Determination of Insoluble Impurities Contained in Fats and OilsUsed in Fat Liquors and Stuffing CompoundsD5558-95(2011)Standard Test Method for Determination of the Saponification Value of Fats and OilsD5559-95(2011)Standard Test Method for Determination of Acidity as Free Fatty Acids/Acid Number in theAbsence of Ammonium or Triethanolamine Soaps in Sulfonated and Sulfated OilsD5560-95(2011)Standard Test Method for Determination of Neutral Fatty Matter Contained in Fats and OilsD5562-95(2011)Standard Test Method for Determination of the Acidity as Free Fatty Acids/Acid Number inthe Presence of Ammonium or Triethanolamine SoapsD5564-95(2011)Standard Test Method for Determination of the Total Ammonia Contained in Sulfonated orSulfated OilsD5565-95(2011)Standard Test Method for Determination of the Solidification Point of Fatty Acids Contained inAnimal, Marine, and Vegetable Fats and OilsD5566-95(2011)Standard Test Method for Determination of Inorganic Salt Content of Sulfated and SulfonatedOils TABLE 7ASTM's Leather Standards-FootwearDesignationTest TitleD2098-13Standard Test Method for Dynamic Water Resistance of Shoe Upper Leather by the DowCorning Leather TesterD2099-14Standard Test Method for Dynamic Water Resistance of Shoe Upper Leather by the MaeserWater Penetration TesterD2210-13Standard Test Method for Grain Crack and Extension of Leather by the Mullen TestD2322-14Standard Test Method for Resistance of Shoe Upper Leather to Artificial PerspirationD2346-13Standard Test Method for Apparent Density of LeatherD2941-13Standard Test Method for Measuring Break Pattern of Leather (Break Scale)D6015-14Standard Test Method for Static Water Absorption of LeatherD7340-07(2012)e1Standard Practice for Thermal Conductivity of Leather TABLE 8ASTM's Leather Standards-Physical PropertiesDesignationTest TitleD1516-05(2010)Standard Test Method for Width of LeatherD1610-01(2013)Standard Practice for Conditioning Leather and Leather Products for TestingD1813-13Standard Test Method for Measuring Thickness of Leather Test SpecimensD1814-70(2015)Standard Test Method for Measuring Thickness of Leather UnitsD1815-00(2015)Standard Test Method for Water Absorption (Static) of Vegetable Tanned LeatherD2207-00(2015)Standard Test Method for Bursting Strength of Leather by the Ball MethodD2209-00(2015)Standard Test Method for Tensile Strength of LeatherD2211-00(2015)Standard Test Method for Elongation of LeatherD2212-00(2015)Standard Test Method for Slit Tear Resistance of LeatherD2347-00(2015)Standard Test Method for Measuring Area of Leather Test SpecimensD2813-03(2013)Standard Practice for Sampling Leather for Physical and Chemical TestsD4704-13Standard Test Method for Tearing Strength, Tongue Tear of LeatherD4705-13Standard Test Method for Stitch Tear Strength of Leather, Double HoleD5052-00(2010)Standard Test Method for Permeability of Leather to Water VaporD6076-08(2013)Standard Test Method for Shrinkage Temperature of LeatherD6182-00(2015)Standard Test Method for Flexibility and Adhesion of Finish on LeatherD6183-00(2015)Standard Test Method for Tackiness of Finish on LeatherD7255 -14Standard Test Method for Abrasion Resistance of Leather (Rotary Platform, AbraserMethod) TABLE 9ASTM's Leather Standards-UpholsteryDesignationTest TitleD1912-00(2010)Standard Test Method for Cold-Crack Resistance of Upholstery LeatherD2097-03(2010)Standard Test Method for Flex Testing of Finish on Upholstery LeatherD2208-00(2010)Standard Test Method for Breaking Strength of Leather by the Grab MethodD6077-10Standard Test Method for Trapezoid Tearing Strength of LeatherD6116-00(2010)Standard Test Method for BlockingD7912-14Standard Test Method for Resistance of Finish to Heat Aging (Finish Stability) TABLE 10ASTM's Leather Standards-Vegetable LeatherDesignationTitleD1611-12Standard Test Method for Corrosion Produced by Leather in Contact with MetalD2213-00(2010)Standard Test Method for Compressibility of LeatherD2875-00(2010)Standard Test Method for Insoluble Ash of Vegetable-Tanned LeatherD2876-00(2010)Standard Test Method for Water-Soluble Matter of Vegetable-Tanned LeatherD4786-00(2010)Standard Test Method for Stitch Tear Strength, Single HoleD4831-00(2010)Standard Test Method for Buckle Tear Strength of LeatherD4899-99(2009)Standard Practice for Analysis of Vegetable Tanning Materials-GeneralD4900-99(2009)Standard Test Method for Lignosulfonates (Sulfite Cellulose) in Tanning ExtractsD4901-99(2009)Standard Practice for Preparation of Solution of Liquid Vegetable Tannin ExtractsD4902-99(2009)Standard Test Method for Evaporation and Drying of Analytical SolutionsD4903-99(2009)Standard Test Method for Total Solids and Water in Vegetable Tanning Material ExtractsD4904-99(2009)Standard Practice for Preparation of Solution of Liquid Vegetable Tannin ExtractsD4905-99(2009)Standard Practice for Preparation of Solution of Solid, Pasty and Powdered VegetableTannin ExtractsD6020-00(2010)Standard Practice for Calculation of (Non-Mineral) Combined Tanning Agents and Degreeof TannageD6075-13Standard Test Method for Cracking Resistance of LeatherD6401-99(2009)Standard Test Method for Determining Non-Tannins and Tannin in Extracts of VegetableTanning MaterialsD6402-99(2014)Standard Test Method for Determining Soluble Solids and Insolubles in Extracts ofVegetable Tanning MaterialsD6403-99(2014)Standard Test Method for Determining Moisture in Raw and Spent MaterialsD6404-99(2014)Standard Practice for Sampling Vegetable Materials Containing TanninD6405-99(2014)Standard Practice for Extraction of Tannins from Raw and Spent MaterialsD6406-99(2014)Standard Test Method for Analysis of Sugar in Vegetable Tanning MaterialsD6407-99(2014)Standard Test Method for Analysis of Iron and Copper in Vegetable Tanning MaterialsD6408-99(2014)Standard Test Method for Analysis of Tannery LiquorsD6409-99(2014)Standard Practice for Color Tests with Sheepskin SkiverD6410-99(2014)Standard Test Method for Determining Acidity of Vegetable Tanning Liquors TABLE 11ASTM's Leather Standards-Wet BlueDesignationTitleD4576-08(2013)Standard Test Method for Mold Growth Resistance of Wet BlueD6656-14bStandard Test Method for Determination of Chromic Oxide in Wet Blue (Perchloric AcidOxidation)D6657-14ae1Standard Test Method for pH of Wet BlueD6658-08(2013)Standard Test Method for Volatile Matter (Moisture) of Wet Blue by Oven DryingD6659-10(2015)Standard Practice for Sampling and Preparation of Wet Blue for Physical and Chemical TestsD6714-01(2015)Standard Test Method for Chromic Oxide in Ashed Wet Blue (Perchloric Acid Oxidation)D6715-13Standard Practice for Sampling and Preparation of Fresh or Salt-Preserved (Cured) Hides andSkins for Chemical and Physical TestsD6716-08(2013)Standard Test Method for Total Ash in Wet Blue or Wet WhiteD7476-08(2013)Standard Test Method for Brine Saturation Value of Cured (Salt-Preserved) Hides and SkinsD7477-08(2013)Standard Test Method for Determining the Area Stability of Wet Blue Submersed in BoilingWaterD7584-10(2015)Standard Test Method for Evaluating the Resistance of the Surface of Wet Blue to the Growthof Fungi in an Environmental ChamberD7674-14aStandard Test Method for Hexane/Petroleum Ether Extract in Wet Blue and Wet WhiteD7816-12Standard Test Method for Enumeration of Halophilic and Proteolytic Bacteria in RacewayBrine, Brine-Cured Hides and SkinsD7817-12Standard Test Method for Enumeration of Yeast and Mold in Raceway Brine, Brine-CuredHides and SkinsD7818-12Standard Test Method for Enumeration of Proteolytic Bacteria in Fresh (Uncured) Hides andSkinsD7819-12Standard Test Method for Enumeration of Yeast and Mold on Fresh (Uncured) Hides andSkins In some embodiments, leather products can have physical properties similar to real leather. In some embodiments, a synthetic leather disclosed herein or a leather product made therefrom can tensile strength as measured by ASTM D-2209-95 of at least about 20, 30, 40, 50, 60, 70, 80, 90, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1050, 1100, 1150, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000 lbs/in 2 . In some embodiments, a synthetic leather disclosed herein or a leather product made therefrom can tensile strength as measured by ASTM D-2209-95 of less than about 5000, 4000, 3000, 2000, 1900, 1800, 1700, 1600, 1500, 1400, 1300, 1200, 1100, 1000, 950, 900, 850, 800, 750, 700, 650, 600, 550, 500, 450, 400, 350, 300, 250, 200, 100, 90, 80, 70, 60, 50, 40, 30, 20 lbs/in 2 . In some embodiments, a synthetic leather disclosed herein or a leather product made therefrom can have a slit of at least about 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100 lbs as measured by ASTM-D2212-94. In some embodiments, a synthetic leather disclosed herein or a leather product made therefrom can have a slit of less than about 200, 150, 100, 95, 90, 85, 80, 75, 70, 65, 60, 55, 50, 45, 40, 35, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1 lbs as measured by ASTM-D2212-94. In some embodiments, a synthetic leather disclosed herein or a leather product made therefrom can have a stitch of at least about 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100 when measured in accordance with ASTM-D4705-93. In some embodiments, a synthetic leather disclosed herein or a leather product made therefrom can have a stitch of less than about 200, 150, 100, 95, 90, 85, 80, 75, 70, 65, 60, 55, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1. In some embodiments, a synthetic leather disclosed herein or a leather product made therefrom can have the slit and stitch values are at least about 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100 lbs., when measured in accordance with their respective tests. In some embodiments, a synthetic leather disclosed herein, or a leather product made therefrom can have a Bally flex of at least about 5000, 6000, 7000, 8000, 9000, 10000, 15000, 20000, 25000, 30000, 35000, 40000, 45000, 50,000, 55000, 60000, 65000, 70000, 80000 as measured by ASTM D6182. Multiple cell layers can be assembled to form a synthetic leather (e.g., a full thickness skin equivalent). A synthetic leather can comprise a top part, a middle part and a bottom part. The top part can comprise an epidermal layer. For example, the top part can be a single layer epidermal layer. The middle part can comprise a basement membrane substitute. In some cases, the middle part does not have a basement membrane substitute. For example, the middle part can have a layer of negligible thickness. The bottom part can have one or more dermal layers. In some cases, the bottom part has a single dermal layer placed on a scaffold (e.g., silk). In some cases, the bottom part has multiple dermal layers (e.g., up to 5 layers) without any scaffold. In some cases, the bottom part has multiple dermal layers stacked atop each other and placed on a scaffold (e.g., silk). Adhesiveness between epidermal and dermal layers can be strong enough to resist layer splitting. In some cases, the cells layers can be assembled by adhering on to a scaffold. Natural or synthetic adhesives can be used for the assembly. A natural adhesive can be fibrin glue, cold glues, animal glue (e.g., bone glue, fish glue, hide glue, hoof glue, rabbit skin glue, meat glue), blood albumen glue, casein glue, vegetable glues (e.g., starch, dextrin glues, Canada balsam, pine rosin based glue, cocconia, gum Arabic, postage stamp gum, latex, library paste, methyl cellulose, mucilage, resorcinol resin, or urea-formaldehyde resin), or any combination thereof. A synthetic adhesive can be Acrylonitrile, Cyanoacrylate (e.g., n-buthyl-2-cyanoacrylate glue), Acrylic, Resorcinol glue, Epoxy resins, Epoxy putty, Ethylene-vinyl acetate, Phenol formaldehyde resin, Polyamide, Polyester resins, Polyethylene, Polypropylene, Polysulfides, Polyurethane, Polyvinyl acetate (including white glue (e.g. Elmer's Glue) and yellow carpenter's glue (Aliphatic resin), Polyvinyl alcohol, Polyvinyl chloride (PVC), Polyvinyl chloride emulsion (PVCE), Polyvinylpyrrolidone Rubber cement, Silicones, and Styrene acrylic copolymer. For example, the assembly can be performed using fibrin glue. For example, the assembly can be performed using n-buthyl-2-cyanoacrylate glue. In some cases, cell layers (e.g., substantially planar layers) are stacked to form a synthetic leather. A cell layer can have an orientation defined by the placement, pattern, or orientation of multicellular bodies. In some cases, each layer can be stacked with a particular orientation relative to the support substrate and/or one or more other layers. For example, one or more layers can be stacked with an orientation that includes rotation relative to the support substrate and/or the layer below, wherein the rotation can be between 0.1 and 180 degrees, for example, about 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, and 180 degrees, or increments therein. In other cases, all layers are oriented substantially similarly. Once stacking of the layers is complete, the layers in the three-dimensional construct can be allowed to fuse to one another to produce a synthetic leather. In some cases, the layers fuse in a cell culture environment (e.g., a Petri dish, cell culture flask, bioreactor, etc.). In some cases, the fusing take place over about 15, 20, 25, 30, 35, 40, 45, 50, 55, and 60 minutes, and increments therein. In other cases, fusing takes place over between 1 and 48 hours, e.g., over about 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, and 48 hours, and increments therein. For example, fusing can take place over about 2 hours to about 24 hours. Culturing Condition The cells and cell layers can be cultured in various cell culture conditions. The cells or cell layers can be cultured in vitro. For example, a dermal layer and/or an epidermal layer can be cultured in vitro. Alternatively, the cells or cell layers can be cultured in vivo. For example, a dermal layer and/or an epidermal layer can be cultured in vivo. The cells and cell layers can be cultured with one or more supplements. The one or more supplements can be natural supplements, synthetic supplements, or a combination thereof. In some cases, a supplement can be an additive. In some cases, one or more of the supplements induce production and assembly of extracellular matrix from iPSC-derived fibroblasts, thus enhancing natural look of the synthetic leather. Exemplary supplements can include ECM components such as collagen and fibrin, growth factors, small molecules such as ascorbic acid or the like, macromolecules such as dextran sulphate, carrageenan, or the like. The cell layers can be cultured with certain air humidity. For example, the cell layers (e.g., dermal layers or epidermal layers) can be cultured at from about 20% to about 100% humidity. For example, the humidity can be from about 40% to about 100%, from about 50% to about 95%, from about 45% to about 90%, from about 55% to about 95%, from about 60% to about 90%, from about 70% to about 80%, from about 71% to about 79%, from about 72% to about 78%, from about 73% to about 77%, from about 74% to about 76%, from about 60% to about 70%, from about 65% to about 75%, from about 70% to about 80%, from about 75% to about 85%, from about 80% to about 90%, from about 85% to about 95%, or from about 90% to about 100%, from about 40% to about 60%, from about 45% to about 55%, from about 46% to about 54%, from about 47% to about 53%, from about 48% to about 52%, from about 48% to about 53%, from about 49% to about 54%, or from about 47% to about 51%. Leather Processing Tanning Methods herein can comprise tanning at least a portion of a synthetic leather, e.g., at least a portion of a dermal layer and/or an epidermal layer in the synthetic leather. Tanning can make a synthetic leather resemble a natural leather, which can be a durable and flexible material created by the tanning of animal rawhide and skin, often cattle hide. Tanning herein can refer to the process of treating the skins of animals to produce leather. Tanning can be performed various ways, including vegetable tanning (e.g., using tannin), chrome tanning (chromium salts including chromium sulfate), aldehyde tanning (using glutaraldehyde or oxazolidine compounds), syntans (synthetic tannins, using aromatic polymers), bacterial dyeing, and the like. Tanning can be performed to convert proteins in the hide/skin into a stable material that will not putrefy. while allowing the material to remain flexible. Chromium can be used as tanning material. The pH of the cell layer or layered structure can be adjusted (e.g., lowered; e.g, to pH about 2.8-3.2) to enhance the tanning; following tanning the pH can be raised (“basification” to a slightly higher level, e.g., pH about 3.8-4.2). Tanning can be performed on cell layers, e.g., dermal layers and epidermal layers. Tanning can also be performed on layered structures, e.g., layered structures comprising at least a dermal layer and at least an epidermal layer. In certain cases, tanning can also be performed on a synthesized leather. For example, tanning can be performed after forming cell layers, e.g., dermal layers or epidermal layers. For example, tanning can be performed after forming layered structures. Tanning can be performed by modify the extracellular matrix (ECM) material. Tanning can be performed by modifying collagen in the ECM. The tanning can be performed using a tanning agent, e.g., chromium (III) sulfate ([Cr(H 2 O) 6 ]2(SO 4 ) 3 ). Chromium (III) sulfate can dissolve to give the hexaaquachromium (III) cation, [Cr(H 2 O) 6 ] 51 3+ , which at higher pH undergoes processes called olation to give polychromium (III) compounds that are active in tanning, being the cross-linking of the collagen subunits. Some ligands include the sulfate anion, the collagen's carboxyl groups, amine groups from the side chains of the amino acids, as well as masking agents. Masking agents can be carboxylic acids, such as acetic acid, used to suppress formation of polychromium (III) chains. Masking agents can allow the tanner to further increase the pH to increase collagen's reactivity without inhibiting the penetration of the chromium (III) complexes. Tanning can increase the spacing between protein chains in collagen (e.g., from 10 to 17 Å), consistent with cross-linking by polychromium species, of the sort arising from olation and oxolation. The chromium can be cross-linked to the collagen. Chromium-tanned leather can contain between about 4% and 5% of chromium. This efficiency can be characterized by its increased hydrothermal stability of the leather, and its resistance to shrinkage in heated water. Other tanning agents can be used to tan the layered body and modify the collagen. Tanning can also be performed using other minerals. In some cases, tanning can be performed using agent based on alum, zirconium, titanium, iron salts, or a combination thereof, Further Processing Cell layers, layered structures, and synthetic leathers made herein can be further processed after tanning. In some cases, methods provided herein further comprise one or more leather processing steps (e.g., those used in traditional leather formation). Examples of processing steps include: preserving, soaking, liming, unhairing, fleshing, splitting, deliming, reliming, bating, degreasing, frizing, bleaching, colouring, pickling, depickling, tanning, re-tanning (e.g., if color is lost during processing), thinning, retanning, lubricating, crusting, wetting, sammying, shaving, rechroming, neutralizing, dyeing, fatliquoring, filling, stripping, stuffing, whitening, fixating, setting, drying, conditioning, milling (e.g., dry milling), staking, buffing, finishing, oiling, brushing, padding, impregnating, spraying, roller coating, curtain coating, polishing, plating, embossing, ironing, glazing, and tumbling. The synthetic leather can be shaped by, for example, controlling the number, size, and arrangement of the multicellular bodies and/or the layers used to construct the animal skin, hide, or leather. In other cases, the animal skin, hide, or leather can be shaped by, for example, cutting, pressing, molding, or stamping. The shape the synthetic leather can be made to resemble a traditional animal skin, hide, or leather product. Methods herein can comprise removing a portion of a synthetic leather produced herein. In some cases, the method comprises removing at least a portion of epidermal layer to form a removed product. For example, the removing can be shaving. Pigmentation Methods herein can comprise pigmenting the synthetic leather. In some cases, pigmentation can be performed by introducing pigments producing cells (e.g., melanocytes) in the synthetic leather. In some cases, the synthetic leather comprises functional live melanocytes. The melanocytes can have a similar location to that in the human skin. In some cases, melanin can be constitutively produced by melanocytes. In some cases, melanin can be transferred to keratinocytes. In some cases, melanocytes are produced upon stimulation, e.g., UV radiation or by propigmenting active agents, such as alpha melanocyte stimulating hormone (aMSH). endothelin 1 (ET1), stem cell factor (SCF), prostaglandins E2 and F2α (PGE2, PGF2α), basic fibroblast growth factor (bFGF) or nerve growth factor (NGF). Differentiation of Progenitor Cells to Cells in a Synthetic Leather Cells in epidermal layers, such as keratinocytes and melanocytes, as well as cells in dermal layers, such as fibroblasts can be derived, e.g., differentiated, from progenitor cells, such as iPSCs. In other case, primary cells or cultured cells derived from primary cells can be used to form cell layers to make synthetic leather. Various methods of differencing iPSCs to cells in a synthetic leather, e.g., keratinocytes, melanocytes, or fibroblasts can be used. In some cases, differentiation of iPSCs to keratinocytes and building 3D epidermis from the iPSC-derived keratinocytes can be performed using method described by Petrova et al., 3D In vitro model of a functional epidermal permeability barrier from human embryonic stem cells and induced pluripotent stem cells. Stem Cell Reports. 2014 Apr. 24; 2(5):675-89. In other cases, cell layers can be formed using primary cells. For example, building 3D epidermis from primary keratinocytes can be performed using the method described in Sun R et al., Lowered humidity produces human epidermal equivalents with enhanced barrier properties. Tissue Eng Part C Methods. 2015 January; 21(1):15-22. In some cases, the methods described herein provide high-throughput methods that reliably, accurately, and reproducibly scale up to commercial levels the production of synthetic leather. Advantages of the synthetic leather, engineered epidermal equivalent, engineered full thickness skin equivalent and methods of making the same disclosed herein include, but are not limited to, production of customized tissues in a reproducible, high throughput and easily scalable fashion with appealing appearance, texture, thickness, and durability. In some embodiments, the methods described herein can produce increase yields of one or more of an epidermal layer, dermal layer, layered structure or synthetic leather. In some embodiments, increase yields can be at least about 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.5, 2, 2.5, 3, 3.5, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, or about 15 times yield compared to a comparable method. In some embodiments, the methods disclosed herein can reduce the cost of the manufacture of synthetic leathers, artificial epidermal layers, artificial dermal layers, layered structures, and products produced therefrom. In some embodiments, the methods disclosed herein can produce uniform thickness synthetic leathers, artificial epidermal layers, artificial dermal layers, layered structures, and products produced therefrom. In some embodiments, the synthetic leathers, artificial epidermal layers, artificial dermal layers, layered structures, and products produced therefrom can have a substantially uniform thickness, length and/or width. In some embodiments, cells in any one or more of the epidermal layers, dermal layers, layered structures can be homogeneously distributed. In some embodiments, cells in any one or more of the epidermal layers, dermal layers, layered structures can be heterogeneously distributed. Comparative Analysis of Epidermal Equivalent FIG. 4A-4C . Illustrates a Comparative analysis of fine leather, native skin and epidermal equivalent. FIG. 4A illustrates FESEM of longitudinal sections of native skin and fine leather. FIG. 4A show distinct morphological structures of epidermis (e) and dermis (d). Tanning permanently altered the structure of the skin. Borders between the individual cells in epidermis became indistinguishable. Removing moisture caused collagen bundles in dermis to become more compact and durable. Magnification: 1000×. FIG. 4B depicts FESEM images. In one instance. FIG. 4B depicts that both surface of fine leather and surface of epidermal equivalents, have a similar smooth appearance indicating that they will likely induce a comparable tactile (touch) experience. Magnification: 2000×. FIG. 4C depicts FESEM of longitudinal sections of fine leather and epidermal equivalent. Before the tanning takes place, similar to epidermis of native skin FIG. 4A , individual cell layers are distinguishable in epidermal equivalent. As collagen in the dermis can be responsible largely for tensile strength of the skin, collagen bundles can give thickness and durability to leather (inset), but may not give sensory experience, which may entirely rely on outer layers of epidermis. FIG. 5A-5C illustrate a comparative analysis of stratum corneum (SC: cornified layer) of native skin and epidermal equivalent. FIG. 5A depicts FESEM images showing the surface of epidermal equivalents appear smoother than the surface of native skin, which may be due to the controlled environment of cell culture. FIG. 5 B illustrates using TEM corneodesmosomes (arrows), the principal “mechanical” junctions of the SC, could be detected as electron denser areas in epidermal equivalents. FIG. 5C illustrates that corneodesmosin (CDSN) can be synthesized and excreted into the extracellular spaces by cells in SG, shortly before onset of cornification. CDSN can embed within the intercellular portions of the SG desmosomes occupied by cadherins and in such a way can form corneodesmosomes. Arrows point to similarly aligned dotty accumulations of CDSN in SC of native skin and epidermal equivalent, likely representing corenodesmosomes. SC, stratum corneum; SG, stratum granulosum; SS, stratum spinosum. FIG. 6A-6E illustrate a comparative analysis of stratum granulosum (SG; granular layer) of native skin and epidermal equivalent. FIG. 6A illustrates that Loricrin (LOR) staining. LOR is a major protein component of the cornified cell envelope and can be expressed in the granular layer of keratinizing epithelia. A similar LOR expression pattern (dark brown pigment on H&E-stained tissue sections pointed by arrows) in SG was detected in both epidermis of native skin (left side of the panel) and epidermal equivalent (right side of the panel), d, dermis; H&E, hematoxylin & eosin: SB, stratum basale; SC, stratum corneum; SG, stratum granulosum; SS, stratum spinosum; *, Transwell filter membrane. In FIG. 6B , epidermal Ca ++ gradient can be captured on transmission electron microscopy as electron-dense precipitates. Ca ++ deposits were present in SG and absent from SC in both native skin in vivo (left side of panel) and epidermal equivalents generated in vitro (right side of the panel). SG, stratum granulosum; SS, stratum spinosum. In FIG. 6C , permeability barrier integrity was assessed by lanthanum perfusion. Lanthanum was visualized as electron-dense deposits in the extracellular spaces of the viable SG, demonstrating that lanthanum and, by extension, water and other small ions can pass between keratinocytes in this stratum. In contrast, lanthanum cannot penetrate further into the SC because a functioning lipid barrier is blocking its movement upward. Epidermal equivalent generated in vitro, demonstrated equally functional permeability barrier as native skin. SG, stratum granulosum; SS, stratum spinosum. FIG. 6D illustrates that tight junction protein 1/zonula occludens-1 (TJP1/ZO-1) anchors tight junction strand proteins, which are fibril-like structures within the lipid bilayer, to the actin cytoskeleton. Arrows point to similarly aligned bright green cell membrane-associated accumulations of TJP1/ZO-1 in SG of native skin in vivo (left side of panel) and epidermal equivalents generated in vitro (right side of the panel). SC, stratum corneum; SG, stratum granulosum; SS, stratum spinosum. FIG. 6E illustrates that Filaggrin (FLG) monomers, tandemly clustered into a large, 350 kDa protein precursor known as profilaggrin, are present in the keratohyalin granules in cells of the SG. Arrows point to similarly aligned bright red granule and cell membrane-associated accumulations of FLG in SG of native skin in vivo (left side of panel) and epidermal equivalents, generated in vitro (right side of the panel). SC, stratum corneum; SG, stratum granulosum; SS, stratum spinosum. FIG. 7A-7C illustrate lipid bilayer formation in native skin and epidermal equivalents assessed with TEM. In FIG. 7A white arrows point to normal lipid secretion at the border of SC and SG in both native skin in vivo (left side of panel) and epidermal equivalents generated in vitro (right side of the panel). In FIG. 7B , lamellar bodies (white arrowheads) are seen in the SG of both native skin in vivo (left side of panel) and epidermal equivalents generated in vitro (right side of the panel). FIG. 7C depicts normal lipid bilayer (LB) morphology of native skin in vivo (left side of panel). Lipid bilayers in epidermal equivalents, generated in vitro (right side of the panel), had a similar appearance. SC, stratum corneum. FIG. 8A-8D illustrate comparative analysis of markers of suprabasal layers of native skin and epidermal equivalent. FIG. 8A , FIG. 8B and FIG. 8C Keratin 10 (KRT10), keratin 1 (KRT1), desmocollin 1 (DCL1), markers of suprabasal layers, stratum spinosum (SS) and stratum granulosum (SG), have a similar expression pattern in native skin in vivo (left side of panel) and epidermal equivalents, generated in vitro (right side of the panel) as demonstrated by immunohistochemistry (KRT10: dark brown pigment on H&E-stained tissue sections pointed by arrows) and immunofluorescence (red cytoplasmic/KRT1/and red cell membrane/DCL1/ staining indicated by arrows), d, dermis; H&E, hematoxylin & eosin; SB, stratum basale; SC, stratum corneum; SG, stratum granulosum; SS, stratum spinosum; *, Transwell filter membrane. In FIG. 8D Desmosomes (arrows) are clearly defined in both native skin in vivo and epidermal equivalents generated in vitro. SS, stratum spinosum. FIG. 9A-9D illustrate a comparative analysis of stratum basale (SB; basal layer) of native skin and epidermal equivalent. With regard to FIG. 9A , FIG. 9B , FIG. 9C , and FIG. 9D , MK167, a marker of proliferation, keratin 14 (KRT14), and transcription factor TP63 show typical basal layer distribution in both native skin in vivo (left side of panel) and epidermal equivalents generated in vitro (right side of the panel), as demonstrated by immunohistochemistry (MK167; dark brown pigment on H&E-stained tissue sections pointed by arrows) and immunofluorescence (green cytoplasmic/KRT14/and white nuclear/TP63/staining indicated by arrows). Hemidesmosomes (arrows) are clearly defined in both native skin in vivo and epidermal equivalents generated in vitro. BM, basement membrane; Cy, cytoplasm; d, dermis; H&E, hematoxylin & eosin; SB, stratum basale; SS, stratum spinosum; TM, Transwell filter membrane. FIG. 10A-10F illustrate comparative analysis of extracellular matrix components of basement membrane. Basement membrane (BM) can be formed from condensed networks of extracellular matrix (ECM) proteins, which can provide an essential structural scaffold on dermal-epidermal junction. Integrin β1 regulates multiple epithelial cell functions by connecting cells with the ECM and it can be crucial for maintenance of BM at dermal-epidermal junction. In FIG. 10A , integrin β1 show typical basal layer distribution in both native skin in vivo (left side of panel) and epidermal equivalents generated in vitro (right side of the panel) as indicated by arrows (cell membrane-associated staining) and arrowheads (on the tip of cells protruding through the holes on Transwell membrane). BM, basement membrane; d, dermis; SB, stratum basale; SS, stratum spinosum; SG, stratum granulosum; TM, Transwell membrane. Fibronectin can play a role in cellular adhesion. FIG. 10B illustrates that in native skin in vivo (left side of panel) fibronectin can be expressed mostly in dermis and relatively little can be detected in the BM area. Similarly, in epidermal equivalents generated in vitro (right side of the panel), fibronectin can be detected on the tip of cells protruding through the holes on Transwell membrane (red arrowheads), d, dermis; SB, stratum basale; SS, stratum spinosum; SG, stratum granulosum; TM, Transwell membrane. The mechanical support provided by the BM can be determined by the collagen IV or, in the case of epidermal equivalents, scaffold. In FIG. 10C . As indicated with arrowheads, collagen IV expression has a similar patchy pattern in native skin in vivo (left side of panel) as epidermal equivalents generated in vitro (right side of the panel). BM, basement membrane; d, dermis; SB, stratum basale; SS, stratum spinosum; SG, stratum granulosum; TM, Transwell membrane. Collagen VI can play a role in cellular adhesion and can be associated with fibronectin. In native skin in vivo (left side of panel), collagen VI can be expressed mostly in dermis and relatively little can be detected in the BM area as in FIG. 10D . Similarly, in epidermal equivalents generated in vitro (right side of the panel), collagen VI can be detected on the tip of cells protruding through the holes in the Transwell membrane (arrowheads). FIG. 10D , BM, basement membrane; d, dermis; SB, stratum basale; SS, stratum spinosum; SG, stratum granulosum; TM, Transwell membrane. Collagen VII can anchor basement membrane for collagen I and III fibrils in dermis. In FIG. 10E , as pointed with arrowheads, collagen VII expression has a similar patchy pattern in native skin in vivo (left side of panel) as epidermal equivalents generated in vitro (right side of the panel). BM, basement membrane; d, dermis; SB, stratum basale; SS, stratum spinosum; SG, stratum granulosum; TM, Transwell membrane. Laminin 5 (chain composition α3β3γ2) can be a major component of anchoring filaments and can be essential for the initial assembly of the BM in vivo. In FIG. 10F as indicated with arrowheads. Laminin 5 expression has a similar pattern in native skin in vivo (left side of panel) as epidermal equivalents generated in vitro (right side of the panel). In addition to its extra abundance in epidermal equivalents, traces of laminin 5 can be seen on cell membrane of basal layer cells (arrows). BM, basement membrane; d, dermis: SB, stratum basale; SS, stratum spinosum; SG, stratum granulosum; TM, Transwell membrane. FIGS. 11A-11I illustrate a structural analysis of full-thickness skin equivalent (FSE). FIG. 11A and 11B depict cross sections of FSE displays distinct cellular layers of epidermis under 2600× magnification ( FIG. 11A ) and 5200× magnification ( FIG. 11B ). FIG. 11C depicts a surface of an FSE at 900× magnification having a similar smooth appearance as fine leather, indicating that an FSE can induce a comparable tactile (touch) experience. FIGS. 11D-1F depict longitudinal sections of dermal scaffold with residing dermal fibroblasts and rich extracellular matrix at 91× magnification ( FIG. 11D ) 162× magnification ( FIG. 11E ) and 405× magnification ( FIG. 11F ). FIGS. 11G-11I depict dermal scaffolds with residing dermal fibroblasts and rich extracellular matrix at 80× magnification ( FIG. 11G ), 695× magnification ( FIG. 11H ) and 2700× magnification ( FIG. 11I ). FIGS. 12A-12R illustrate a time-course of engineering dermal equivalent. FIGS. 12A-12I depict day 2 after seeding dermal fibroblasts onto scaffold at 36× magnification ( FIG. 12A ), 695× magnification ( FIG. 12B ), 1470× magnification ( FIG. 12C ), 7750× magnification ( FIG. 12D ), 2320× magnification ( FIG. 12E ), 2420× magnification ( FIG. 12F ), 6560× magnification ( FIG. 12G ), 17000× magnification ( FIG. 12H ) and 22000× magnification ( FIG. 12I ). Cells can begin migrating into hollow structures of a scaffold and secreting extracellular matrix. FIGS. 12J-12R depict day 7 after seeding dermal fibroblasts onto scaffold at 64× magnification ( FIG. 12J ), 100× magnification ( FIG. 12K ), 364× magnification ( FIG. 12L ), 82× magnification ( FIG. 12M ), 253× magnification ( FIG. 12N ), 3940× magnification ( FIG. 12O ), 5550× magnification ( FIG. 12P ), 9440× magnification ( FIG. 12Q ) and 21680 magnification ( FIG. 12R ). Longitudinal ( FIGS. 12J-12L ) and transversal sections ( FIGS. 12M-12R ) can show denser cells and richer extracellular matrix, with some areas having almost complete obstruction of the hollow structure of the scaffold ( FIGS. 12M-12P ). EXAMPLES Example 1. Differentiation of iPSCs to Keratinocytes To induce differentiation, undifferentiated iPSCs are transferred into a 20% O 2 atmosphere environment and treated with mTESR1 or other pluripotent stem cell basal media supplemented with 1 mM ATRA (Sigma-Aldrich) and 25 ng/ml BMP4 (R&D) for 7 days (Induction). To select for cells with early acquisition of ectodermal fate, the cells are harvested and replated onto freshly prepared 3D HDF ECM or other type of ECM at a density of 5˜10×10 3 cells per cm 2 and grown in Dulbecco's modified Eagle's medium/Ham F12 (3:1: Life Technologies) or Keratinocyte media supplemented with scrum substitute such as human platelets lyste and with 1 mM ATRA and 25 ng/ml BMP4 for a further 7 days (Selection). To enrich for putative epidermal progenitors, rapid adhesion to type IV collagen-coated dishes can be used, and the rapidly adhering cells are cultured in defined keratinocyte-SFM or other keratinocyte medium supplemented with 1 mM ATRA for 7 days (Enrichment). After that, the cells are cultured in EpiLife medium (Life Technologies) or other keratinocyte medium for a further 7 days (Expansion) before final harvest and analysis. Example 2. Differentiation of Induced Pluripotent Stem Cells Into a Keratinocyte Lineage Coating Tissue Culture Dishes With Geltrex and Col The procedure may be performed in a biological safety cabinet using aseptic techniques. Similar to Matrigel. Geltrex matrix solidifies rapidly at room temperature (RT). Aliquot each new batch of the matrix upon arrival and use pre-chilled pipet tips, racks and tubes while working with the reagent. 50, 100 and 200 μL aliquots are made and stored at −80° C. Use Geltrex at 1:100 dilutions. The coating procedure below can be described for a 60 mm tissue culture dish. If a larger dish is to be used. adjust the volume of the coating solution accordingly. 1. Remove a 50 μL aliquot of Geltrex from the −80° C. freezer and place it on ice in the biological safety cabinet. 2. Add 5 mL of cold sterile DMEM/F12 to a 15 mL conical tube. 3. Use a 1 mL glass pipet, take 1 mL cold DMEM/F12 from the 15 ml conical tube prepared in step 2, and add to the frozen Geltrex. Gently pipet up and down to thaw and dissolve Geltrex. Transfer the dissolved Geltrex to the rest of DMEM/F12 in the 15 mL conical tube prepared in step 2. Pipet to mix diluted Geltrex. 4. Add 50 μL of 3 mg/mL ColI stock solution into diluted Geltrex from step 3. Pipet to mix diluted Geletrex with ColI. Add 4 mL of coating solution into 60 mm dish. Tap or swirl the plate to ensure that the entire surface is coated. 5. Incubate the dish with Geltrex/ColI coating solution at 37° C. in the tissue culture incubator for at least 1 h. 6. Once the coating is complete, leave the coating solution in the dish and proceed with the plating of iPSCs as described in the next subsection. Alternatively, aspirate the coating solution and add 2 mL of fresh DMEM/F12 into the coated dish to prevent it from drying before plating the cells. Plating iPSCs for Differentiation Prepare one 60 mm tissue culture dish of feeder-free iPSCs grown to ˜70% of confluency. Examine cells under a microscope to confirm the absence of contamination and the maintenance of their undifferentiated phenotype. If the cells are stressed or dying, they start to differentiate, presenting themselves as “cobblestone” areas with larger polymorphic cells, and should not be used for the differentiation toward keratinocytes. For iPSC differentiation toward keratinocytes, a 1:8 split ratio of iPSCs. 1. Prewarm N2B27 medium and Dispase in the 37° C., water bath. 2. Using the microscope, confirm that the colonies are ready for passaging. Gently aspirate medium from the dish. Add 2 mL of 1×PBS, swirl the plate to wash the cells, and gently aspirate PBS. 3. Add 1 mL of Dispase and return the plate to the 37° C. tissue culture incubator for 3-5 min. 4. While the cells are being incubated with Dispase, gently aspirate the Geltrex/ColI coating solution (or DMEM/F12) from step 6 in the Geltrex/ColI coating procedure and add 4 mL of complete N2B27 medium into the coated dish. 5. After 3˜5 min incubating with Dispase, confirm that the cells are ready to be picked by looking for rolled or folded edges around the colonies. 6. Transfer the plate to the biological safety cabinet and carefully aspirate Dispase. After the treatment with Dispase, the colonies are very loosely attached to the surface of the dish and may peel off if too much force is used. 7. Gently add 2 mL of plain DMEM/F12. Aspirate off the medium and repeat the wash 3 times. 8. Add 2 mL of complete N2B27 into the dish and gently scrape the colonies off the plate. Transfer the cells from the dish into a 15 mL conical tube and add 6 mL of complete N2B27 bring the total volume of cell suspension to 8 mL. 9. Gently mix the cell suspension to break large clumps of cells. Transfer 1 mL of the cell suspension to the coated dish prepared in step 3 of the current subsection. Discard or replate the leftover cells using the conditions established for a given laboratory. 10. Transfer the newly plated cells to the incubator and gently shake the plate back and forth and side to side to distribute the cells evenly. Incubate the cells overnight in the 37° C. tissue culture incubator. Differentiation of iPSCs With RA and BMP4 The differentiation and subculturing of iPSC-derived keratinocytes are to be performed in a biological safety cabinet using aseptic techniques. Examine the new plate the day after passaging to confirm the successful attachment of iPSCs. If iPSCs start forming colonies, proceed with the differentiation protocol below. 1. Prewarm complete DKSFM (with antibiotics and DKSFM supplement) in the 37° C. water bath. 2. Add 5 mL of prewarmed DKSFM from the previous step to a 15 ml conical tube, add 5 μL of 1 mM RA to achieve 1 μM final working concentration and 5 μL of 25 μg/mL BMP4 to achieve 25 ng/ml final working concentration, mix well. 3. Aspirate off N2B27 medium from the dish with plated iPSCs, wash once with 4 mL of 1×PBS, and add 4 mL of DKSFM containing 1 μM RA and 25 ng/mL BMP4 from the step above. This is day 1 of differentiation procedure. 4. Transfer the cells to the incubator and incubate for 48 h. 5. Replace the medium with fresh DKSFM containing 1 μM RA and 25 ng/mL BMP4 after 48 h of incubation. Transfer the cell to the incubator for another 48 h. 6. After the second round of 48 h induction (day 4 of differentiation), replace the medium with complete DKSFM without RA and BMP4. Incubate cells in the incubator for 10 days in complete DKSFM, changing medium every other day. 7. On day 14 of differentiation, prepare complete CnT-07 medium by adding antibiotics and provided supplements, pre-warm the medium. By this day, the majority of the cells in the outgrown iPSC colony start exhibiting an epithelial-like phenotype. 8. Aspirate off DKSFM from differentiated cells and replace with 4 mL of complete CnT-07. Incubate the cells in the tissue culture incubator for another 10 days, changing complete CnT-07 every other day. Rapid Attachment and Culturing of iPSC-Derived Keratinocytes On day 24 of differentiation, many cells that migrate away from the outgrown iPSC colony exhibit a keratinocyte-like phenotype, and start expressing p63, a master regulator required for the commitment of the ectoderm to a keratinocyte fate, and Krt14. By this day, the 60 mm dish used for iPSC differentiation is fully confluent and need to be passaged. To enrich for iPSC-derive keratinocytes during passaging, the of the differentiated iPSC culture is rapidly attached to ColI/ColIV coated plates. Up to four 100 mm ColI/ColIV-coated tissue culture dishes are used to perform the rapid attachment procedure from one 60 mm dish containing differentiated iPSCs. If only one 100 mm dish is to be used, plate one fourth of the differentiated iPSC culture for the rapid attachment procedure. Coating Plates With ColI and ColIV The procedure may be performed in the biological safety cabinet using aseptic techniques. 1. Reconstitute ColIV powder to a concentration of 2 mg/mL in sterile 0.25% Glacial acetic acid. Dissolve for several hours at 2˜8° C. occasionally swirling. Make aliquots and store them at −20° C. 2. Thaw the aliquot of ColIV stock solution (2 mg/mL) very slowly by placing the vial in an ice bucket and keeping it at 4° C. for several hours. 3. Resuspend ColIV stock solution in the appropriate volume (5 mL per each 100 mm dish) of sterile 0.25% Glacial acetic acid to a final working concentration of 7 μg/mL. Add an appropriate volume of ColI stock solution to achieve a final working ColI concentration of 30 μg/mL. Coat the plates by using 5 mL of working solution to cover a 100 mm dish. Incubate the plates at room temperature in the biological safety cabinet for 1 hour. 4. Aspirate the liquid from the coated plates, rinse the dishes once with 5 mL of sterile 1×PBS and once with 5 mL of ddH20. 5. Air-dry the washed dishes in the biological safety cabinet. Use plates directly or seal them with Parafilm and store at 4° C. for up to 6 months. To use a previously stored ColIV-coated plate, allow the plate to warm up at room temperature in the biological safety cabinet for at least 1 hour prior to plating cells. Rapid Attachment of iPSC-Derived Keratinocytes 1. On day 24 of differentiation, prewarm complete CnT-07. Accutase, and ColI/ColIV-coated dish(es). 2. Wash the cells with 1×PBS, add 2 mL of Accutase and incubate in the tissue culture incubator for 5 min. Confirm under the microscope that cells start detaching. 3. Add 3 mL of complete Cnt-07, pipet up and down to dislodge the cells and collect the cell suspension into a 15 mL conical tube. Spin the cells down at 260×g for 5 min and aspirate the supernatant. Resuspend the pellet in 10 mL of complete Cnt-07 medium, repeat the spin at 260×g for 5 min, and aspirate the supernatant. 4. Resuspend the pellet in 4 mL of complete CnT-07, pipet up and down to break cell clumps into single cells. 5. Add 9 mL of complete CnT-07 medium into each ColI/ColIV-coated dish and transfer 1 mL of cell suspension from step 4 above into each ColI/ColIV-coated dish. Allow the cells to attach to the coated dish at room temperature for 15-30 min. 6. Carefully aspirate the medium with the floating cells (these are undifferentiated or partially differentiated iPSCs). Do not disturb the attached cells (these are iPSC derived Krt14 positive cells). Add 10 mL of fresh complete CnT-07 medium into the plate with the attached cells. Let the cells expand in the 37° C. tissue culture incubator. changing the medium every other day. Passage cells as needed with Accutase in CnT-07 or EpiLife (with EDGS supplement) on ColI-coated dishes. After passage 2 or 3 and following the rapid attachment step, the culture should consist of ˜90% of Krt14 positive cells exhibiting a keratinocyte-like phenotype. The keratinocyte-like phenotype of the obtained culture can be verified by standard immunflorescence analyses for Krt14 expression and by the ability to reconstitute a normal stratified epidermis in organotypic cultures. Example 3. Preparing Epidermal Layer From Primary Keratinocytes Primary keratinocytes are isolated from a single neonatal foreskin and grown in 0.07 mM Ca 2+ 154CF medium (Life Technologies) supplemented with man keratinocyte growth supplement. A suspension of first-passage keratinocytes (2.21×10 5 /cm 2 insert) is seeded on Cellstart CTS (Life Technologies) (or other ECM substrate) coated PET, 0.4-mm inserts (EMD Millipore) in CnT-07 media (CELLnTEC) or CnT-Prime media (CELLnTEC) according to manufacturer's protocol. Day 3 (D3) after seeding, the media are switched to CnT-02-3D (CELLnTEC) or CnT-3D Barrier (CELLnTEC). On day 4, the HEEs are air exposed by feeding the bottom of the insert with CnT-02-3D or CnT-3D Barrier. From day 4 onward. HEEs are fed daily with CnT-02-3D or CnT-3D Barrier until harvested. HEEs are grown in a humid (at 100% RH) or dry incubator (at 50% RH) at 37° C. and 5% CO 2 . A dial hydrometer (Fisher Scientific) is used to measure incubator humidity. Low incubator humidity is maintained by removal of water pan. To control for possible changes in osmolarity, media are refreshed daily. Significant changes in osmolarity are not detected using this protocol, as measured by a Micro Osmometer (Precision Systems). Twelve-well inserts are used for transepithelial electrical resistance (TEER) measurements, light microscopy, and electron microscopy, while six-well inserts are used for transepidermal water loss (TEWL) measurements and immunoblotting. Example 4. Culturing an Epidermal Layer Keratinocytes are seeded at a density of 2.0-2.5×10 5 cells/cm 2 of polyethylene terephthalate (PET) membrane with 0.4 μm pore inserts (EMD Millipore: Cat. No.: MCHT12H48) in CnT-07 media (CELLnTEC) or CnT-Prime media (CELLnTEC). Day 3 (D3) after seeding, the media are switched to CnT-02-3D (CELLnTEC) or CnT-3D Barrier (CELLnTEC). On day 4, the cells air exposed by feeding the bottom of the insert with CnT-02-3D CnT-3D Barrier. From Day 4 onward, the epidermal layer is fed daily with CnT-02-3D or CnT-3D Barrier until harvested at Day 14. Example 5. Preparing Support Substrate To prepare a 2% agarose solution. 2 g of Ultrapure Low Melting Point (LMP) agarose is dissolved in 100 mL of ultrapure water/buffer solution (1:1, v/v). The buffer solution may be optionally PBS (Dulbecco's phosphate buffered saline 1×) or HBSS (Hanks' balanced salt solution 1×). The agarose solution may be placed in a beaker containing warm water (over 80° C.) and held on the hot plate until the agarose dissolves completely. The agarose solution remains liquid as long as the temperature is above 36° C. Below 36° C., a phase transition occurs, the viscosity increases, and finally the agarose forms a gel. To prepare agarose support substrate. 10 mL of liquid 2% agarose (temperature >40° C.) may be deposited in a 10 cm diameter Petri dish and evenly spread to form a uniform layer. Agarose is allowed for form a gel at 4° C. in a refrigerator. Example 6. Producing a Synthetic Leather Comprising Fibroblasts, Keratinocytes, and Melanocytes The outline of the protocol can be as follow: a) bringing fibroblasts and a solution of collagen into contact, then incubating for a sufficient period of time to obtain a contracted collagen matrix in which the fibroblasts are distributed, constituting a dermis equivalent, b) seeding, with a mixture of keratinocytes and melanocytes. the dermis equivalent obtained in a), and immersion culture in a liquid medium, c) immersion of the entire culture (keratinocytes and melanocytes seeded on the dermis equivalent) obtained in b), and continuation of the culture at the air-liquid interface until a pluristratified epidermis equivalent containing melanocytes, on a dermis equivalent containing fibroblasts in a collagen matrix, constituting a skin equivalent, is obtained. Step a) can be carried out with collagen type I, in particular of bovine origin, or a mixture of collagens I and III (approximately 30% relative to the final volume of the lattice) in homogeneous suspension. Advantageously, other constituents are added thereto, such as laminin (in particular, from 1% to 15% relative to the final volume), collagen IV (in particular, from 0.3% to 4.5% relative to the final volume) and/or entactin (in particular, from 0.05% to 1% relative to the final volume) so as to obtain a homogeneous suspension. The fibroblasts are obtained from skin. They are cultured in a suitable medium, and then suspended before mixing with the suspension of collagen and growth factors. The mixture is incubated for I to 6 days, preferably for 4 or 5 days, at a temperature of approximately 37° C. generally from 36° C. to 37.5° C. Advantageously, the mixture is incubated on a support which does not allow adhesion thereof, in particular which prevents adhesion of the mixture to the edges of the support: such a support may in particular be obtained by prior treatment of its surface, for example by coating said surface with bovine albumin or serum. A collagen gel which is contracted freely in several directions, while discharging the nutritive medium, and in which the fibroblasts are embedded, is thus obtained. In order to carry out step b), use can be made of keratinocytes originating from skin, preferably from adult skin. The keratinocytes are amplified before seeding according to the technique of Rheinwald and Green (Cell, vol. 6. 331-344. 1975) by culture on a feeder support constituted of 3T3 fibroblasts in a suitable medium known to those skilled in the art, in the presence of growth factors, in particular of amino acids, serum, cholera toxin, insulin, triiodothyronine and pH buffer solution. In particular, such a culture medium may especially contain at least one mitogenic growth factor for keratinocytes (for example, epidermal growth factor (EGF) and/or keratinocyte growth factor (KGF), in particular KGF), insulin, hydrocortisone and, optionally, an antibiotic (for example: gentamycin, amphotericin B). The melanocytes can be melanocytes originating from young or adult animal skin. They are amplified by culture in a suitable medium, in the absence of phorbol ester, composed of a base medium such as DMEM/F12 or MCDB153 and supplemented with melanocyte-specific growth factors (such as, for example, bFGF, SCF, ET-1, ET3 or αMSH), and in particular in M2 medium (Promocell) or in other media such as M254 (Cascades Biologics™). Cell suspensions of melanocytes and of keratinocytes are prepared from these cultures and mixed so as to obtain mixed keratinocyte/melanocyte suspensions. The melanocyte/keratinocyte ratio may be from 1:10 to 2:1 and is generally approximately 1:1. This mixed suspension is deposited on the dermis equivalent. The dermis equivalent is advantageously attached to a support via a biological material such as collagen. The melanocyte/keratinocyte suspension is deposited in a ring or any equivalent means for maintaining it on a delimited surface part. A liquid nutritive medium is added in such a way as to cover the mixture of cells. This medium contains growth factors known to those skilled in the art, in particular EGF and/or KGF. The medium will be replaced regularly and the culture continued as an immersion, generally for a period of from 2 to 10 days, in particular from 5 to 8 days, and approximately 7 days. The medium contains KGF starting from the 2nd day of immersion, and ideally starting from the 4th day of immersion. The skins are subsequently, in a manner known per se, immersed so as to obtain differentiation of the keratinocytes and formation of a stratified epidermis equivalent. This step c) corresponding to the culture as an immersion at the air-liquid interface is continued until a differentiated structure is obtained, in general approximately 7 days. However, step c) may be continued for a longer period of time, for example for approximately 28 days, while at the same time conserving a skin equivalent having the advantageous characteristics specified in the above text. The nutritive culture medium will be refreshed regularly. The skin equivalent is subsequently removed so as to perform required tests. Example 7. Induction of Follicle Formation in Cultured Skin Specimens Expanded DP cells are mixed with cultured ORS cells, washed, and carefully resuspended in 20 ml of sterile phosphate buffered saline (PBS, Sigma) at suitable cell densities. Cultured DP and ORS cells used in each experiment are obtained from different donors, because the different duration of culture for DP and ORS cells do not allow preparation of the two cell types from the same donor. The cell suspension is slowly injected into the dermis of cultured skin pieces 1 day after establishing the culture. Example 8. Culturing Hair Follicle Cell Populations Hair follicles are obtained from the occipital region. Dermal papilla (DP) cells are prepared and cultured as described in Randall et al., A comparison of the culture and growth of dermal papilla cells from hair follicles from non-balding and balding (androgenetic alopecia) scalp. Br J Dermatol 1996: 134: 437-444). Briefly the DP of the hair follicles is isolated under a dissecting microscope and transferred individually to a 24-well tissue culture plate (Sarstedt). Cell culture is performed in DMEM, supplemented with 15% FCS (Sigma). After initiation of cell proliferation, cells are cultured to confluency and expanded for two passages. For isolation of outer root sheath (ORS) cells, the middle part of the hair follicles, containing the bulge region, is excised and subjected to mild trypsinization. At least cells of 10 hair follicles are used for each culture. The obtained cells are washed twice in RPMI-1640 medium (Sigma) and subjected to cell culture in standard keratinocyte medium (Epilife, Sigma). Cells are harvested after 1 week of culture. Example 9. Tanning Full Thickness Skin Equivalents Full thickness skin equivalents are tanned by chrome tanning. The first step is ice and sulfuric acid treatment. This opens up the tissue so it can receive the chromium. The chromium is then added along with magnesium oxide. The process brings the pH level of the full thickness skin equivalents down to around 3. After chromium has worked through the full thickness skin equivalents the tanning liquor is then introduced which brings the pH level up to around 4. This is followed by a warm water bath and then roll pressing to remove excessive liquid. The final stage is then to apply a surface treatment if necessary and then dry the full thickness skin equivalents while stretched out and then re-press when done. Example 10. X-tan Tanning Protocol Full thinkness skin equivalents can be tanned using an X-tan procedure. Prior to tanning, a full thinkness skin equivalent was limed, which comprises the steps of soaking the skin equivalent, adding a substrate, adjusting the pH, and washing. The skin equivalent will then be de-limed by washing the skin equivalent, adding a pre-deliming buffer, deliming the skin equivalent, and washing. The skin equivalent was then tanned by wetting back, adding a tanning substrate, adjusting the pH to a pH conducive for the tanning, performing cycles of fixation and fixation, and fat liquoring to obtain the tanned skin equivalent. Example 11. Full Thickness Skin Equivalents A type I collagen matrix (containing 0.5×10 6 iPSC derived fibroblasts) can be deposited onto polyethylene terephthalate membranes (BD Biosciences) and allowed to polymerize. After incubation of the polymerized matrix for about 7 days. 1×10 6 iPSC-derived keratinocytes and 0.1×10 6 iPSC-derived melanocytes can be seeded onto the matrix, and incubated for a further 7 days. The composite culture can be raised to the air-liquid interface and fed from below to induce epidermal differentiation. Full thickness skin equivalents can be harvested about 14 days later and either snap frozen in LN2 or embedded in wax. For melanin quantification Example 12. Immunostaining of Frozen Section Fixation: Tissues can be fixed in 3.8% paraformaldehyde/phosphate-buffered saline (PBS), pH 7.2-7.6 for 30 minutes. The samples can be washed three times 5 minutes in PBS. The tissue samples can be infiltrated with a series of sterile sucrose gradients (10% sucrose overnight. 15% sucrose for 6-8 hours. 30% sucrose overnight and finally in 30% sucrose mixed 1:1 with optimal cutting temperature (OCT) compound overnight) rotating on 4° C. The samples can be embedded in OCT and frozen in liquid nitrogen vapor. The cryo-blocks can be stored at −80° C. Cutting sections: The day before cutting, the cryo-blocks can be transferred to −20° C. overnight. Sections (10 μm) can be prepared using a standard cryostat. The sections can be kept at −20° C. until processing. Processing: Control incubations can be included. Preimmune sera or isotype-matched nonimmune antibodies can be used instead of the primary antibodies. The sections can be submerged in either 90% cold acetone for 10 minutes or 0.2% triton X-100/PBS for 5 minutes to expose antigens. The samples can be washed three times 5 minutes in PBS. Nonspecific antibody reactivity was blocked by submerging the sections into 5% BSA with 0.1% triton X-100 for 1 hr. The sections can be then incubated overnight at 4° C. with a mixture of the two antibodies: i) 2.5 μg/ml of ChromPure donkey whole IgG (for purpose of blocking; all secondary antibodies can be made in donkey): ii) 1 μg/ml of appropriate primary antibody. The sections can be rinsed three times 5 minutes in PBS. The sections can be incubated for 30 to 60 min at room temperature with the appropriate species-specific secondary antibody, made in donkey, and conjugated to cither red or green fluorophore. The sections can be washed three times 5 minutes in PBS. The sections can be then incubated for 10 minutes with 10 μg/ml Hoechst 33342 at room temperature. The sections can be washed 3 times 5 minutes with PBS. Visualization: The samples can be mounted with Vectashield medium (Vector) and the samples can be visualized with epifluorescence microscope (Zeiss), equipped with appropriate filters. Example 13. Immunostaining of Paraffin Embedded Sections Fixation: Tissues can be fixed in 3.8% paraformaldehyde/phosphate-buffered saline (PBS), pH 7.2-7.6 for 30 minutes. The samples can be washed three times 5 minutes in PBS. The tissue samples can be dehydrated in ascending ethanol series (50%, 70%, 2×100%; 20 min each) and clearing agent (xylene, 2×20 min). The samples can be perfused with paraffin wax at 65 C 2×1 hour and embedded in paraffin blocks. The paraffin blocks can be stored at room temperature until further use. Cutting sections: The tissue was sectioned at 5 μm thickness using a standard microtome. The sections can be kept at room temperature untill processing. Processing: Control incubations can be included. Preimmune sera or isotype-matched nonimmune antibodies can be used instead of the primary antibodies. The sections can be re-hydrated in ascending series xylene/ethanol series 2× xylene. 2×100% ethanol, and 1×70% and 50%; 10 min each. The sections can be then briefly rinsed with tap water. The sections can be then stained with hematoxylin for 5 minutes. The sections can be then washed with dH 2 O until solution was clear. The sections can be stained with 0.5% Eosin for 10 minutes. The sections can be then rinsed briefly in tap water. Nonspecific antibody reactivity was blocked by submerging the sections into 5% BSA for 1 hr. The sections can be then incubated for overnight at 4° C. with a mixture of the two antibodies: i) 2.5 μg/ml of ChromPure donkey whole IgG (for purpose of blocking: all secondary antibodies are made in donkey): ii) 1 μg/ml of appropriate primary antibody. The sections can be rinsed three times 5 minutes in PBS. The sections can be incubated for 30 min at room temperature with the appropriate species-specific secondary antibody, made in donkey, and conjugated to horseradish peroxidase (HRP). The sections can be washed three times 5 minutes in PBS. Visualization: The samples can be incubated with 3, 3′-diaminobenzidine (DAB) substrate kit (VectorLaboratories) according to manufacturer's protocol. DAB yield a brown stain. If nickel chloride is added to the substrate solution, a gray-black stain can result. The samples can be dehydrated in ascending ethanol series (50%, 70%. 2×100%; 10 min each) and clearing agent (xylene. 2×10 min). The samples can be mounted in mounting medium and visualized with phase contrast microscope (Zeiss) equipped with digital camera. Example 14. Field Emission Scanning Electron Microscopy (FESEM) Fixation: Samples can be fixed for 24 hours at 4° C. with 4% paraformaldehyde and 2% glutaraldehyde in 0.1M Sodium Cacodylate Buffer (pH7.4) and placed in 0.1 M sodium cacodylate buffer and maintained at 4° C. prior to further processing. Processing: The samples can be post-fixed for 1 hour with 1% aqueous OsO 4 . After dehydration in an ascending ethanol series (50%, 70%, 2×100%; 10 min each) samples can be critical point dried with liquid CO 2 in a Tousimis Autosamdri-815B apparatus, mounted with double-sided copper tape onto 15 mm aluminum mounts, and sputter-coated with 40 Å of gold-palladium using a Denton DeskII Sputter Coater. Visualization: Cross sections of duplicate samples can be mounted onto low profile 45/90 degree SEM mounts for analysis of internal morphology. Visualization can be performed with a Zeiss Sigma FESEM (Carl Zeiss Microscopy. Thornwood. NY) operated at 2-3 kV, using inLens Secondary Electron (SE) detection, as well as mixed signal InLens/SE2 (75/25%) detection at working distance 3-5 mm. Images can be captured in TIFF using store resolution 2048×536 and a line averaging noise reduction algorithm. Processing: Previously dried samples (i.e., leather) can be cut to size and sputter-coated with 40 Å of gold-palladium using a Denton DeskII Sputter Coater. Visualization: Cross sections of duplicate samples can be mounted onto low profile 45/90 degree SEM mounts for analysis of internal morphology. Visualization was performed with a Zeiss Sigma FESEM (Carl Zeiss Microscopy. Thornwood. NY) operated at 2-3 kV, using inLens Secondary Electron (SE) detection, as well as mixed signal InLens/SE2 (75/25%) detection at working distance 3-5 mm. Images can be captured in TIFF using store resolution 2048×1536 and a line averaging noise reduction algorithm. Example 15. Transmission Electron Microscopy (TEM) Fixation: Samples can be fixed 30 minutes at 4° C. in 2% glutaraldehyde and 2% paraformaldehyde with 0.06% calcium chloride in 0.1 M sodium cacodylate buffer, pH 7.4. The samples can be then placed in 0.1 M sodium cacodylate buffer and maintained at 4° C. prior to further processing. Processing: The samples can be then washed and placed in either 0.2% ruthenium tetroxide (for visualization of lipid bilayers) or 1.5% osmium tetroxide with 1.5% potassium ferrocyanide, in 0.1 M sodium cacodylate, pH 7.4, at room temperature in the dark for 45 minutes. After rinsing in buffer, the samples can be dehydrated in a graded ethanol series (50%, 70%, 2×100%; 10 min each), and subsequently embedded in a low-viscosity Epoxy resin. Visualization: Semi-thin sections can be stained with 1% toluidine blue with 1% azure II in 1% borax solutions and viewed under phase contrast microscope (Zeiss). Ultrathin sections can be collected and stained with water-saturated 3% uranyl acetate and/or contrasted in 2.5% lead citrate on uncoated nickel grids. Ultrathin sections can be viewed with a Zeiss 10 A electron microscope operated at 60 kV. Images can be captured in TIFF. Ion Capture Cytochemistry (Ca++ Gradient): Fixation: For ultrastructural Ca ++ localization, the samples can be fixed in 2% paraformaldehyde, 2% glutaraldehyde, 0.09 M potassium oxalate, containing 0.04 M sucrose. Samples can be subsequently fixed overnight at 4° C., Processing: The samples can be post-fixed in 1% osmium tetroxide containing 2% potassium pyroantimonate, pH 7.4 for 2 hrs at 4° C. in the dark. Tissue samples then can be washed in alkalinized water (pH 10) and transferred to ethanol solutions (50%, 70%, 2×100%; 10 min each) for dehydration and embedding in a low-viscosity. Epoxy resin. Visualization: Ultrathin sections can be collected and stained with water-saturated 3% uranyl acetate and/or contrasted in 2.5% lead citrate on uncoated nickel grids. Ultrathin sections can be viewed with a Zeiss 10 A electron microscope operated at 60 kV. Images can be captured in TIFF. Lanthanum Perfusion: Fixation: The perfusion pathway was assessed in all subjects by immersion of samples in 4% lanthanum nitrate in 0.05 M Tris buffer containing 2% glutaraldehyde, 1% paraformaldehyde, pH 7.4, for 1 hour at room temperature. Processing: The samples can be washed and placed in 1.5% osmium tetroxide with 1.5% potassium ferrocyanide, in 0.1 M sodium cacodylate, pH 7.4, at room temperature in the dark for 45 minutes. After rinsing in cacodylate buffer, the samples can be dehydrated in a graded ethanol series (50%, 70%, 2×100%; 10 minutes each), and subsequently embedded in a low-viscosity, Epoxy resin. Visualization: Ultrathin sections can be collected and stained with water-saturated 3% uranyl acetate and/or contrasted in 2.5% lead citrate on uncoated nickel grids. Ultrathin sections can be viewed with a Zeiss 10 A electron microscope operated at 60 kV. Images can be captured in TIFF. While some embodiments have been shown and described herein, such embodiments are provided by way of example only. Numerous variations, changes, and substitutions will now occur to those skilled in the art without departing from the disclosure provided herein. It should be understood that various alternatives to the embodiments described herein can be employed.",en,PATENT_APPLICATION
111-870-256-612-54X,US,20240385428,A1,2024-11-21,US_20240385428_A1_20241121,en,US,20240385428,A1,2024-11-21,US,18319247,2023-05-17,LOW F NUMBER REFRACTIVE TELESCOPE WITH DYNAMIC ALTITUDE COMPENSATION,en,US,BAE Systems Information and Electronic Systems Integration Inc.,"Nashua, NH",US,Michael J. Powers,"Amherst, NH",US,1,Matthew W. Grabowski,"Nashua, NH",G02B23/16,I,F,G02B7/02,I,L,G02B27/62,I,L,G05D23/19,I,L,G02B23/16,I,F,G02B7/021,I,L,G02B27/62,I,L,G05D23/1931,I,L,US,20240385428,A1,2024-11-21,111-870-256-612-54X,1,US,20240385428,A1,2024-11-21,111-870-256-612-54X,1,UNKNOWN,"A system and method are disclosed for a low F-number precision variable-focus telescope that includes a telescope housing containing an optical system. There is a first temperature sensing device to detect a temperature of the telescope housing, a second temperature sensing device to detect an ambient temperature around the telescope housing, and a pressure sensing device to detect ambient pressure around the telescope housing. A controller is in operative communication with the first temperature-sensing device, the second temperature sensing device, and the pressure sensing device. The control regulates the heater to maintain the telescope at a desired temperature to achieve diffraction limited performance in response to signals from the first temperature-sensing device, the second temperature sensing device, and the pressure sensing device.",en,"1 . A low F-number precision variable-focus telescope comprising: a telescope housing comprising an interior and an exterior, wherein the telescope housing interior contains an optical element, wherein the optical element in the telescope housing is associated with an F-number, and wherein the F-number is less than or equal to 2; a heater coupled directly or indirectly to the telescope housing; a first temperature sensing device to detect a temperature of the telescope housing; a pressure sensing device to detect pressure proximate the telescope housing; a controller in operative communication with the first temperature-sensing device and the pressure sensing device; wherein the controller regulates the heater to maintain the telescope housing at a desired temperature to maintain diffraction limited performance in response to signals from the first temperature-sensing device and the pressure sensing device.","2 . The low F-number precision variable-focus telescope of claim 1 , further comprising: a second temperature sensing device to detect an temperature proximate the telescope housing; wherein the controller is in operative communication with the first temperature-sensing device, the second temperature sensing device, and the pressure sensing device and the controller regulates the heater to maintain the telescope housing at a desired temperature to achieve diffraction limited performance in response to signals from the first temperature-sensing device, the second temperature sensing device, and the pressure sensing device.","3 . The low F-number precision variable-focus telescope of claim 2 , further comprising: a summation device in operative communication with the first temperature sensing device, the second temperature sensing device, and the pressure sensing device, wherein the summation device is in operative communication with the controller; and wherein the controller includes a proportional/integral controller (PID controller) comprising an input and output; and a linear power supply comprising an input and output, wherein the input of the linear power supply receives the output signal from the PID controller; wherein the linear power supply output regulates the voltage across the heater and controls the power applied to the heater to maintain the telescope at the desired temperature.","4 . The low F-number precision variable-focus telescope of claim 1 , further comprising: a heat spreader comprising a first side and a second side, wherein the second side of the heat spreader is coupled to at least a portion of the exterior of the telescope housing; and wherein the heater is coupled directly to the heat spreader.","5 . The low F-number precision variable-focus telescope of claim 4 , further comprising: a gap pad disposed between the exterior of the telescope housing and the heat spreader.","6 . The low F-number precision variable-focus telescope of claim 1 , wherein the telescope housing comprises a material having a coefficient of thermal expansion (CTE) value that is greater than 9.","7 . The low F-number precision variable-focus telescope of claim 6 , wherein the CTE value is greater than 16.","8 . The low F-number precision variable-focus telescope of claim 1 , wherein the telescope housing comprises aluminum.","9 . The low F-number precision variable-focus telescope of claim 1 , wherein the telescope housing comprises a material having a thermal conductivity value, at 0° C., that is greater than 100 W/m K.","10 . The low F-number precision variable-focus telescope of claim 1 , wherein the heater is an electric heater that comprises a polyimide foil.","11 . A method comprising: sensing, with a first temperature sensing device, a temperature of a telescope housing, wherein the telescope housing comprises an interior and an exterior, wherein the telescope housing interior contains an optical element, wherein the optical element in the telescope housing is associated with an F-number, and wherein the F-number is less than or equal to 2 ; sensing, with a pressure sensing device, apressure proximate the telescope housing; and regulating, with a controller that is in operative communication with the first temperature sensing device and the pressure sensing device, a heater that is coupled directly or indirectly to the telescope housing to maintain diffraction limited performance of the optical element in response to signals from the first temperature-sensing device and the pressure sensing device.","12 . The method of claim 11 , further comprising: sensing, with a second temperature sensing device, temperature proximate the telescope housing, wherein the controller is in operative communication with the first temperature-sensing device, the second temperature sensing device, and the pressure sensing device and the controller; regulating the heater to maintain the telescope housing at a desired temperature to achieve diffraction limited performance in response to signals from the first temperature-sensing device, the second temperature sensing device, and the pressure sensing device.","13 . The method of claim 12 , further comprising: summing, in a summation device, the signals from the first temperature-sensing device, the second temperature sensing device, and the pressure sensing device, wherein the summation device is in operative communication with the controller; transmitting an output from the summation device to the controller, wherein the controller includes a proportional/integral controller (PID controller) comprising an input and output; receiving the output from the PID controller into a linear power supply comprising an input and output; and regulating, via the linear power supply, a voltage across the heater to control power applied to the heater to maintain the telescope housing at the desired temperature.","14 . The method of claim 11 , further comprising: observing an optical prescription of the telescope to obtain a linear performance thereof; adjusting a coarse adjustment of the telescope to a desired focus; adjusting a setpoint temperature for fine adjustment of the telescope; modifying the temperature of the telescope until diffraction-limited performance is achieved, wherein modifying the temperature is accomplished by using feedback from the first temperature sensing device and feedback from the second temperature sensing device.","15 . The method of claim 14 , wherein modifying the temperature is further accomplished by using feedback from an ambient pressure sensing device.","16 . A computer program product including least one non-transitory computer readable storage medium on a moving platform in operative communication with a computer processing unit (CPU) in an optical system having a housing, a first temperature sensing device to sense temperature of the housing, a second temperature sensing device to sense ambient temperature around the housing, and a pressure sensing device to sense ambient pressure around the housing, and a heater coupled directly or indirectly to the housing, the storage medium having instructions stored thereon that, when executed by the CPU, implement a process to maintain the housing at a desired temperature to achieve diffraction limited performance in response to signals from the first temperature sensing device, the second temperature sensing device, and the pressure sensing device, the instructions comprising: determine the housing temperature with the first temperature sensing device; determine temperature proximate the housing with the second temperature sensing device; determine pressure proximate the housing with the pressure sensing device; and maintain the housing temperature at the desired temperature in response to feedback from the first temperature sensing device, the second temperature sensing device, and the pressure sensing device.","17 . The computer program product of claim 16 , wherein the instructions further comprise: sum, in a summation device, signals from the first temperature-sensing device, the second temperature sensing device, and the pressure sensing device, wherein the summation device is in operative communication with the controller; transmit an output from the summation device to the controller, wherein the controller includes a proportional/integral controller (PID controller) comprising an input and output; receive the output from the PID controller into a linear power supply comprising an input and output; and regulate, via the linear power supply, a voltage across the heater to control power applied to the heater to maintain the telescope at the desired temperature.","18 . The computer program product of claim 17 , wherein the instructions further comprise: observe an optical prescription of the telescope to obtain a linear performance thereof; adjust a coarse adjustment of the telescope to a desired focus; adjust a setpoint temperature for fine adjustment of the telescope; and modify the temperature of the telescope until diffraction-limited performance is achieved, wherein modifying the temperature is accomplished by using feedback from the first temperature sensing device and feedback from the second temperature sensing device.",en,"TECHNICAL FIELD The present disclosure relates generally to refractive telescopes. BACKGROUND ART Traditional telescopes rely on the use of multiple lenses to achieve the desired magnification and clarity. However, variations in temperature can cause these lenses or the housing that retains the lenses to expand or contract, resulting in changes in the focal length of the telescope and thereby degrading the image quality. Athermalization is the process of designing an optical system to minimize the effects of temperature changes on its performance. Refractive telescopes can be designed to be athermal (i.e., insensitive to temperature) by carefully selecting the optical glass and telescope housing materials so that their temperature dependent properties self-cancel as the telescope temperature changes. These properties include the change in refractive index with temperature (dn/dT) and the coefficient of thermal expansion (CTE) of the housing and lens materials. The “f-number” (sometimes referred to as F-number or f#) is the ratio of focal length to aperture diameter. Here, a “low” f-Number is one considered to be a f-number value of 2 or less. Compact telescope designs with low f-number are challenging for an athermalization technique since they can become highly sensitive to lot-to-lot variations in the change in refractive index with temperature (i.e., dn/dT) and the coefficient of thermal expansion (i.e., the CTE) material properties. This variation can require material characterization of every lot during manufacture. One method to counteract refractive telescopes sensitivity to ambient temperature is through the addition of a heater, which reduces the range of telescope temperatures by always keeping the telescope hot or at least warm (at about 50° C.) when exposed to ambient temperatures. Stated otherwise, in the current state of the art, refractive telescopes with low f-numbers can be heated or maintained at about 50° C. so they are maintained as constant, albeit hot, temperature. The use of a heater maintains the telescope at this static temperature. Low f-number refractive telescopes are also sensitive to ambient air pressures and altitudes since the air refractive index varies from approximately n=1.0003 at sea level approaching a value of n=1 at high altitudes and space vacuum applications. This complicates ground testing and operations due to focus differences between ground and operational altitudes of the system. The current methods for compensating for these effects include attempting to athermalize the telescope by balancing the dn/dT and CTE effects of the glass and housing materials. To do this for a Low F-number refractive telescope, a designer often has to match sets of material properties and geometries of optics and housings that are complex and expensive. This is easier to match with High F-number (i.e., f-number greater than 2) telescopes, but the matching comes at the expense of larger size and mass telescopes (i.e., increases size, weight, power and cost considerations). Further, they use CTE matched housing materials with the glass or lens material, which are typically heavy & poor thermal conductivity (e.g. Invar, Titanium) or expensive (e.g. Beryllium alloys). Additionally, they may include mechanical motion (e.g. piezo flexure or motor driven) focus adjustment techniques to adjust spacing between lenses, which are complex and take up space and weight. SUMMARY OF THE INVENTION Prior systems or prior telescopes that would use a heater to maintain the telescope at a constant hot temperature have the drawbacks discussed above. For example, there is still temperature sensitivity because the body or housing of the telescope may not be one uniform temperature due to thermal gradients from the front to the back of the telescope even though it is being heated. Given the drawbacks of the current state of the art for refractive variable focus telescopes, especially low F-number refractive telescopes, a need continues to exist for a refractive variable focus telescope that is optimized for sensitivity to temperature for heating compensation of altitude defocus. The present disclosure address this need, amongst other needs, and provides a low F-number refractive variable focus telescope with dynamic altitude compensation by accounting for ambient air altitude (or pressure) and optionally accounting for ambient air temperature. According to one exemplary aspect of an embodiment of the present disclosure, a preexisting or a legacy heater that is on an afocal telescope can be utilized with an improved system architecture or computer program product that re-optimizes the entire design to take advantage of the fact that the heater is present. This exemplary system of the present disclosure utilizes software to control the heater to maintain the temperature at a desired operating temperature. This enables the system of the present disclosure to use a software controller that knows the ambient pressure and altitude (from sea level to space) and knows the ambient environmental temperature. The altitude/pressure and temperature are used to calculate what temperature the telescope shall be maintained to compensate for those temperatures and altitude/pressures. In addition to dynamically controlling the temperature of the housing of the telescope, an embodiment of the present disclosure utilizes a housing of the telescope of the present disclosure that includes a material with an increased coefficient of thermal expansion. Recall, previous designs of afocal telescopes used housing materials, such as titanium, that balanced the change in optical behaviors of the glass. However, titanium has a coefficient of thermal expansion that is fairly close to the coefficient of thermal expansion of the glass forming the triplet pair of optics. In order to dynamically control the temperature with software to adjust the focus of the telescope across the temperature range that is desired, the system of the present disclosure is able to provide greater control of the focus relative to temperature. Stated otherwise, the amount of focus for every degree of temperature that is shifted can be controlled. In order to accomplish this, the present disclosure utilizes a specific material with higher coefficient of thermal expansion than the previous usage of titanium. One such material that should suffice is aluminum. This higher CTE provides, for a given temperature change, the ability to precisely change the focus more than what was previously able to be accomplished with a titanium housing. Stated otherwise, by heating or controlling the temperature of the housing the physical housing expands and contracts to alter the spacing distance between the first pair of optics and the second pair of triplet optics. The embodiments disclosed herein should achieve diffraction limited performance (≤±¼ waves of defocus) over a wide temperature and altitude environment by compensating the high sensitivity to ambient pressure by importing at least some of or all of the following features: a housing material with a high coefficient of thermal expansion (CTE) for high focus sensitivity to temperature, which results in a wide dynamic range of defocus compensation (e.g. Aluminum); a housing material with a high thermal conductivity, which results in low thermal gradients when heater power is applied allowing for accurate temperature measurement and low gradients when parts of the telescope are exposed to different ambient temperatures (e.g. Aluminum); a pair of triplet lenses (i.e, three lenses defining a first set of lenses from the pair and three lenses defining a second set of lenses from the pair) with low intra-pair sensitivity to temperature, which results in keeping higher order optical aberrations (non-focus) under control over wide temperature ranges; a telescope temperature sensor is used to monitor the telescope body temperature for closed loop control of the heater power; an ambient pressure sensor that is used as feedback for telescope temperature setpoint control to compensate for defocus from sea level to >100 kft or more (i.e., space); and an ambient temperature sensor that is used as feedback for telescope setpoint control for corrections related to ambient temperatures. Some embodiments of the present disclosure enable provide a compact 50 mm aperture, 5× magnification, afocal SWIR telescope. Some of these embodiments provide a faster thermal response over prior systems which allow the system of the present disclosure to achieve full performance over a larger temperature and altitude operational envelope. Some embodiments provide for rapid dynamic focusing adjustment, which enables: significantly expanded performance operating envelope (altitude and temperature) for the system; significant reduction in testing time and calibration required of supply base and factory, providing cost and schedule improvements; yield improvements by reducing tight dependence of custom precision housing dimensions with matched sets of optics; and the elimination of time constraints on system operation for telescope warmup periods which speeds factory testing and operational timelines in the field. In one exemplary aspect, an embodiment of the present disclosure may provide a method comprising: sensing, with a first temperature sensing device, a temperature of a telescope housing, wherein the telescope housing comprises an interior and an exterior, wherein the telescope housing interior contains an optical element, wherein optical element in the telescope housing is associated with an F-number, and wherein the F-number is less than or equal to 2; sensing, with a pressure sensing device, an ambient pressure around the telescope housing; and regulating, with a controller that is in operative communication with the first temperature sensing device and the pressure sensing device, a heater that is coupled directly or indirectly to the telescope housing to achieve diffraction limited performance of the optical element in response to signals from the first temperature-sensing device and the pressure sensing device. In another exemplary aspect, an embodiment of the present disclosure may provide a low F-number precision variable-focus telescope comprising: a telescope housing comprising an interior and an exterior, wherein the telescope housing interior contains an optical element, wherein optical element in the telescope housing is associated with an F-number, and wherein the F-number is less than or equal to 2; a heater coupled directly or indirectly to the telescope housing; a first temperature sensing device to detect a temperature of the telescope housing; a pressure sensing device to detect ambient pressure around the telescope housing; a controller in operative communication with the first temperature-sensing device and the pressure sensing device; wherein the controller regulates the heater to maintain the telescope at a desired temperature to achieve diffraction limited performance in response to signals from the first temperature-sensing device and the pressure sensing device. BRIEF DESCRIPTION OF THE DRAWINGS Sample embodiments of the present disclosure are set forth in the following description, are shown in the drawings and are particularly and distinctly pointed out and set forth in the appended claims. FIG. 1 ( FIG. 1 ) is a side elevation view of an exemplary low F-number telescope according to one embodiment of the present disclosure. FIG. 2 ( FIG. 2 ) is a longitudinal cross section view of the exemplary low F-number telescope taken along line 2 - 2 in FIG. 1 . FIG. 3 ( FIG. 3 ) is an exemplary diagrammatic chart representative of the exemplary low F-number telescope according to one embodiment of the present disclosure. FIG. 4 ( FIG. 4 ) is a flow chart depicting an exemplary method or process according to one embodiment of the present disclosure. FIG. 5 ( FIG. 5 ) is a graph depicting defocus difference versus lens F-number at various altitudes. FIG. 6A ( FIG. 6A ) is a graph depicting the performance of defocus over time for a telescope of the present disclosure and a conventional telescope along an exemplary flight path. FIG. 6B ( FIG. 6B ) is a graph depicting the altitude versus pressure for the exemplary flight path shown in FIG. 6A . Similar numbers refer to similar parts throughout the drawings. DETAILED DESCRIPTION FIG. 1 and FIG. 2 depict a telescope with a f-number that is 2 or less. Thus, as used herein the term “low f-number” refers to an optical system or assembly with a f-number less than 2. In one embodiment, a low f-number telescope is a variable-focus telescope 10 . Telescope 10 includes a housing 12 and one or more optical lenses 14 . In one embodiment, the optical lenses 14 include a pair of triplet lens 14 A, 14 B. By way of example, the telescope housing 12 may be comprised of aluminum. The material selected to form the housing 12 of the telescope 10 should also have a high thermal conductivity. Thus, aluminum may be a desirable material to utilize when constructing the housing 12 because it has a high level of thermal conductivity as well as a high CTE. The high level of thermal conductivity reduces the likelihood of thermal gradient across the body of housing 12 of the telescope 10 . A heat spreader 16 is attached or coupled to the telescope housing 12 either directly or indirectly. In one embodiment, the heat spreader 16 surrounds at least a portion of the telescope housing 12 . Additionally, the heat spreader 16 may be contiguous with the periphery of the telescope housing 12 . The heat spreader 16 may be one piece in one embodiment. In another embodiment, the heat spreader 16 may be machined in at least two parts. The heat spreader 16 parts may be connected or coupled by any suitable method, including but not limited to, screws, adhesives, and welds. By way of example, the heat spreader 16 may be comprised of aluminum. In one embodiment, the heat spreader 16 is wrapped around the outside of the central barrel of housing 12 of the telescope 16 . In one embodiment, the heat spreader 16 may be a foil-based heater (including heater 18 or heater elements) that is conformal to the outer surface of the central barrel of the housing in between the pair of triplet optics 14 A, 14 B. Given the placement of the heat, if the material has a low level of thermal conductivity, then the central portion of the telescope would be warmer and the outer ends where the triplet optics are located would be cooler. Thus forming the housing 12 from a material with high thermal conductivity results in low gradients to establish temperature uniformity across the entirety of the body forming the telescope while allowing the heater to be utilized only in the central portion thereof. Another material that would be useful for forming the housing of the telescope with a relatively high CTE and a relatively high thermal conductivity would be copper. In one embodiment, at least one heater 18 is attached to the heat spreader 16 to regulate temperature of the precision variable-focus telescope 10 . The heater 18 can be spread along sections of the heat spreader 16 and in patterns such as strips or rows to allow for effective heating. In one embodiment, the heater 18 is an electric-resistance heater that is contiguous with the heat spreader 16 via the foil or film. The electric-resistance heater 18 has a resistance element to evenly heat the heat spreader 16 . By way of example, the electric-resistance heater 18 may be comprised of polyimide foil. In one embodiment, the electric-resistance heater 18 is attached to the heat spreader 16 by pressure-sensitive adhesive. The composition of the adhesive should not interfere with the heating of the heat spreader 16 . Other attachment mechanisms of the film heater 18 to the heat spreader 16 include screws, pins and posts. In another embodiment, the heat spreader 16 may be eliminated and the heater 18 may simply connected to or wrapped around the housing 12 of telescope 10 . For example, when the housing 12 of telescope 10 is manufactured with a sufficiently high CTE and a sufficiently high thermal conductivity, as in greater detail described herein, the heat spreader may be eliminated and the heater 18 is connected directly to or wrapped around in direct contact with housing 12 of telescope. FIG. 2 depicts a cross-sectional view of the low f-number precision variable-focus telescope 10 that has a telescope housing 12 and optics 14 . In one embodiment, a gap pad may be sandwiched between the telescope housing 12 and the heat spreader 16 . The gap pad fills in the empty space between the heat spreader 16 and telescope housing 12 to evenly heat the telescope housing 10 . In various embodiments, the gap pad may be contiguous with the telescope housing 10 and the heat spreader 16 . By way of example, the gap pad may be comprised of a material with a low thermal impedance to transfer heat from the heat spreader 16 to the telescope housing 10 . FIGS. 1-3 depict that at least one temperature-sensing device 20 is attached or coupled to the heat spreader 16 either directly or indirectly. Device 20 may be considered as a first temperature sensing device 20 . The temperature-sensing device 20 may also be mounted elsewhere on the telescope. The temperature-sensing device 20 measures the temperature of the telescope 10 . By way of example, the temperature-sensing device 20 may comprise a thermistor. In one example the temperature-sensing device 20 is located away from the heater 18 . In another example, there are multiple temperature-sensing devices 20 . According to one embodiment a temperature calibration table may be used such as a temperature of the telescope 10 is or any location thereof is established by knowing the temperature at the temperature-sensing device 20 . The first temperature sensing device 20 is operatively coupled with the heater (see also FIG. 3 ). The first temperature sensing device 20 has an output that is processed in a manner so as to control the heater. Software, protocols, instructions or other logic is in operative communication with the temperature sensor to maintain the telescope 10 at the desired temperature. One particular temperature that would be useful for maintaining the telescope in a desired operating temperature range would be within plus or minus 5% of 50 degrees Celsius. The software controls the amount of power provided to the heater to regulate the desired temperature. Control of the heater power is accomplished through a closed loop control. A closed loop control refers to a temperature sensor on the telescope that measure the temperature of the telescope. That temperature or data is provided to a PID control loop (proportional integrator derivative control loop) that generates feedback to regulate the heater power to the heater on the telescope to always maintain the temperature sensor at a desired set point. Thus, the loop between the temperature sensor and the heater power is closed. The system or telescope 10 also includes a second temperature sensing device 22 that is adjacent the telescope 10 and measures ambient temperature of the volume of air or space near or proximate the exterior of telescope 10 . In one embodiment, the second temperature sensing device 22 is composed of a single sensor. In another particular embodiment, the system of the present disclosure utilizes sensors as part of the second temperature sensing device 22 to detect ambient temperatures near the telescope. Specifically, this embodiment may utilize three separate ambient temperature sensors which collectively define the second temperature sensing device 22 , wherein a first ambient temperature sensor detects ambient temperature near the front end of the telescope 10 , an intermediate ambient temperature sensor that detects the ambient temperature near the middle of the telescope 10 , and a third ambient temperature sensor that detect the ambient temperature near a second end of the telescope 10 . The ambient temperature sensors that may collectively define the second temperature sensing device 22 detect the ambient temperature to identify three different temperature zones around the telescope 10 . The ambient temperatures may be averaged across their respective values, or they may be used independently in the processing to control heater 18 as discussed herein. A pressure sensor 24 is adjacent the telescope 10 and measures ambient pressure of the volume of air or space near the exterior of telescope 10 . The pressure sensor 24 could alternatively be an altimeter inasmuch as the pressure is mostly directly related to altitude. FIGS. 3 depicts one exemplary embodiment of a precision variable-focus telescope heating mechanism control loop 30 . The first temperature-sensing device 20 attached to the heat spreader 16 or another portion of the housing 12 of telescope 10 measures the temperature of the telescope 10 . The second temperature sensing device 22 is adjacent the telescope 10 and measures ambient temperature of the volume of air or space near the exterior of telescope 10 . The pressure sensor 24 is adjacent the telescope 10 and measures ambient pressure of the volume of air or space near the exterior of telescope 10 . The temperature feedback 32 from the first temperature sensing device 20 is an input to temperature digitization electronics that converts the temperature feedback or signal to a reading that can be processed. Temperature feedback 32 is transmitted to a summation block 34 . The temperature feedback 36 from the second temperature sensing device 22 is an input to temperature digitization electronics that converts the temperature feedback 36 or signal to a reading that can be processed. Temperature feedback 36 is transmitted to a temperature setpoint algorithm or temperature setpoint logic 38 . The ambient pressure feedback 40 from the pressure sensing device 24 is an input to temperature digitization electronics that converts the pressure feedback or signal to a reading that can be processed. Pressure feedback 40 is transmitted to the temperature setpoint algorithm or temperature setpoint logic 38 . Temperature setpoint logic 38 determines what temperature that the telescope 10 or telescope housing 12 should operate to optimize the diffraction limited performance (≤±¼ waves of defocus) over a wide temperature and altitude environment. This allows the telescope 10 having a low f-number to compensate for the high sensitivity to ambient pressure and ambient temperature. Regarding the set point logic 38 , one exemplary embodiment has only ambient temperature input (from second temperature sensing device 22 ), whereas another embodiment has both ambient pressure input (from pressure device 24 ) and ambient temperature sensor input. Thus, in some embodiments, the ambient pressure sensor input is optional to setpoint logic 38 . In some embodiments, the ambient temperature sensor input is optional to setpoint logic 38 , such that it only uses the ambient pressure. Further, other embodiments enable setpoint logic 38 to use a multitude of pressure and temperature sensor inputs if there are large gradients around the telescope. For example if the ambient temperatures around each end of the telescope are different, it may be advantageous to have readings of both zones to calculate optimal setpoint temperature for focus of the telescope 10 . Setpoint logic 38 may include a look-up table to determine the best setpoint temperature for focus of the telescope based on ambient pressure and temperature readings. Setpoint logic may perform a method to use the look-up table to determine the best setpoint temperature for focus of the telescope based on ambient pressure and temperature readings. Setpoint logic 38 may interpolate between points in the look-up table. The setpoints in the look-up table temperature setpoints may be determined by calibration testing over ambient temperatures and pressures of the telescope or a representative telescope while measuring focus of the telescope. In an alternative configuration, rather than the look-up table, the setpoint logic 38 may utilize an equation used with coefficients similarly determined by fitting calibration test data to a formula. For example, “Setpoint temperature=A*(Ambient Temperature)+B*(Ambient Pressure)+C” where the equation and A,B,C are coefficients determined by the telescope design. A, B, C values would be based on best fit of equation to the calibration data. The output 42 of setpoint logic 38 , which is the desired telescope temperature, is transmitted to summation block 34 . Summation block 34 receives, as an input to the summation block 34 , the output 42 of setpoint logic 38 . Also, summation block 34 receives, as an input to the summation block 34 , the temperature feedback 32 from first temperature sensing device 20 . The output of the summation block 34 is an input to the telescope temperature input control or controller 44 . Controller 44 may be coupled to a sample rate count, which outputs the reading from both temperature sensors 20 , 20 as a 12-bit telescope temperature, respectively. The 12-bit telescope temperature may be an input to a 1—a filter unit that applies filter gain. The filtered telescope temperatures may be an input to the heater controller 44 . In one example, the controller 44 is a Proportional/Integral controller (PID controller) implemented within a controller such as a Field Programmable Gate Array (FPGA). A temperature calibration table is used in one embodiment to provide a more precise temperature for the telescope 10 . The output of the PID controller 44 feeds the heater input to a switching power supply. By way of example, the switching power supply may be a “buck” (step-down), “boost,” “buck-boost,” “isolated,” or “non-isolated” switching power supply. This switching power supply regulates the voltage across the heater 18 , which may be an electric film heater that applies the heat energy to the heat spreader 16 thereby, maintaining the telescope 10 at a desired temperature to achieve diffraction-limited performance. In one embodiment, the temperature set-point for the control loop 30 is “user-settable” through a digital interface. In another example, the output of the PID controller 44 feeds the heater input to a linear power supply of the heater 18 . This linear power supply regulates the voltage across the heater 18 , which may be an electric film heater, and controls the power applied by the electric film heater, thereby maintaining the telescope 10 at a desired temperature to achieve diffraction-limited performance. FIG. 3 depicts the heater power into the heater 18 that surrounds the telescope 10 . There is another temperature sensor (telescope middle housing) that detects the temperature of the telescope in the middle thereof. This temperature sensor (i.e., device 20 ) measures what the temperature of the middle of the telescope is at a certain time. That temperature signal is sent to a summation block. The data from the middle temperature sensor is summed in the summation block with the set point temperature that the system wants to hold the telescope. The summation block generates an error signal if the measured temperature does not equal the set point temperature. Stated otherwise, if the temperature sensor data equals the set point temperature then there is no error output from the summation block. The error signal, when present, enters the PID control loop, which is a proportional controller. The proportional controller is generating a signal to the heater to tell the heater how much power is needed to warm the telescope to the desired level of the set point temperature. This process loops and repeats until there is no more error signal. For example, if the set point temperature is below the registered temperature detected from the temperature sensor at the middle of the housing, then instructions will be sent by the PID controller to turn off the power so the telescope cools down. Alternatively, if the temperature detected by the middle temperature housing is below the set point temperature, then the PID controller will increase power to the heater in order to warm the telescope. This, there is a feedback loop such that this process is continuous to continuously warm or cool the telescope to maintain the telescope at the desired set point temperature. The system of the present disclosure is an improvement over prior telescopes that used static or fixed temperatures to maintain the telescopes at a preset temperature. In these previous systems or device, there was no feedback loop that allowed responsive changes to be made to the heater based on the temperature of the telescope or based on altitude. This was limiting because it only permitted the telescope to be focused at a certain range of altitudes. By adding the feedback loop and the PID controllers of the present disclosure, the system utilizing the telescope with this improved technique allows a greater range of altitudes and temperatures to be utilized while maintaining the low f-number telescope to be usable at those broader ranges of altitudes and pressures. The set point calculator also obtains pressure data from a pressure sensor and calculates the pressure as a function of temperature or in relation to temperature. By measuring pressure in conjunction with the temperature, the set point calculator can determine the best focus for the telescope given the temperature and pressure parameters. This can be calculated via a look up table or a pre-calibrated measurements or alternatively can be adaptively learned through artificial intelligence. It should be noted that the control loop can take many different forms and the PID of controller 44 disclosed herein is not required in every embodiment of controller 44 . However, a PID has been found to be reliable and efficient for the purposes disclosed herein. Thus, the PID can be one portion of the control loop, but PID is not required so long as the control loop still accounts for temperature and pressure to dynamically power the heater element around or on the telescope in order to maintain the defocus of the telescope within the desired target range. FIG. 4 depicts a method for focusing the optical system in a low F-number precision variable-focus telescope according to one embodiment, shown generally at 400 . This method 400 in this example includes designing the optical prescription to get a linear performance, which is shown generally at 402 . The method also includes adjusting the coarse adjustment of the telescope optical system to a desired focus during assembly by aligning and spacing the lenses, which is shown generally at 404 . The method further comprises characterizing the optimal telescope temperature for best focus using a heater circuit over a defined temperature and altitude range, which is shown generally at 406 . The method may further comprise adjusting heater driver set point temperature for fine adjustment of the telescope optical system focus, which is shown generally at 408 . The method includes maintaining the heater driver set point temperature for diffraction-limited performance over a wide temperature environment and wide altitude (from sea level to space) environment, which is shown generally at 410 . In one embodiment, initial (coarse) adjustment of the desired focus utilizes a traditional method of adjusting and setting of lens optics within the telescope housing 10 while the telescope is heated to an initial value. The method further includes adjusting the heater driver set point temperature for fine adjustment of the telescope optical system, which is shown generally at 406 . In one embodiment, fine (precision) adjustment of focus is accomplished by modifying the temperature of the telescope until diffraction-limited performance is achieved. The precision adjustment of focus of modifying the temperature of the telescope until diffraction-limited performance is accomplished by using feedback 32 from the first temperature sensing device 20 , feedback 36 from the ambient second temperature sensing device 22 , and optionally feedback 40 from the ambient pressure sensing device 24 . In various embodiments, the range of temperature used to adjust the focus is greater than the maximum environmental temperature of the air surrounding the telescope 10 . The constant heat flow into the telescope 10 eliminates the need to use cooling to maintain temperature. In one embodiment, the telescope 10 of the present disclosure is an afocal telescope in which a collimated beam enters the first end of the telescope at the first pair of triplet lenses that focus the light to a focal point. From the focal point, the light expands to the second pair of triplet lenses. The light ten transmits through the second pair of triplet lenses to exit the second end of the afocal telescope in a smaller diameter collimated beam. The focal length is fairly short compared to the diameter of the optic lenses. This results in the low F-number. As depicted in FIG. 5 , as the f-number decreases for a given telescope, the telescope becomes more sensitive to the refractive index of air or the medium. FIG. 5 plots the difference in defocus between sea level and different altitude examples versus the lens F-number. Line 50 A represents the defocus difference between sea level for a conventional telescope located in space or extremely high altitudes (>100,000 Feet). Line 50 D represents the defocus difference from sea level for a conventional telescope located at 10,000 feet. Lines 50 B and 50 C represent two intermediate altitudes located between sea level and outer space. As shown by the lines 50 A- 50 D, even at low altitudes, for low f-number lenses, the defocused difference is high (i.e., above the known defocus limited threshold limit of 0.25. The graph of FIG. 5 also indicates that the defocus limit of 0.25 (shown by dashed line 52 ) is the general accepted rule of thumb for sharp focus. As will be shown below in FIG. 6A , the telescope 10 of the present disclosure is able to achieve operation below the defocus limit of 0.25 (wherein the +/− range of 0.25 is shown as defocus range 62 ) based on the structural configurations detailed in FIGS. 1-3 and the operations detailed in FIGS. 3-4 . FIG. 6A and FIG. 6B are graphs indicative of a defocus calculation based on the telescope 10 utilizing second temperature sensing device 22 and the pressure sensing device 24 . The defocus calculations model the residual defocus of the system. FIG. 6B depicts a flight profile of a platform (which may be manned or unmanned) that changes altitudes over time. For example, as altitude increases pressure decreases. For example, at time zero the altitude is at its lowest point ant the pressure is at its highest point. As altitude increases over time the pressure decreases. FIG. 6A identifies the calculated the amount of defocus the telescope 10 has as the temperature in the telescope is controlled utilizing both the first temperature sensing device 20 , the second temperature sensing device 22 , and the pressure sensing device 24 . FIG. 6A depicts that the target defocus p-v waves should be in defocus range 62 that is +/− 0.25 waves. The results graphed in FIG. 6A the dynamic feedback loop controlled temperature system for the telescope 10 yield desirable results to maintain the telescope within the target defocus range 62 better that previous static systems. For example at time zero, the heater 18 is not yet turned on and all systems are relatively defocused. However, as time increases the dynamic feedback loop controlled temperature system for the telescope 10 rapidly brings the telescope 10 within to the defocused target range by heating the telescope as evidenced by line 60 B being located within range 62 . Then, once the heater 18 warms up the telescope 10 , the telescope 10 is better focused. In the old systems or conventional telescopes where the temperature was a static value, there is never a focused value of the telescope when the platform carrying the system is on the ground. This is shown by the line 60 A of the old static telescope leveling at point to a value of about 1.25 on the defocus vertical scale. Line 60 B indicates that the temperature on the telescope 10 is being set to its desired temperature prior to the platform leaving the ground based on the ambient temperatures and pressures being sent to the controller 44 . As shown in this exemplary graph, the platform carrying the telescope takes off or starts its flight path at about 45 minutes. The temperature of the telescope 10 is constantly changing as sensed by the first and second temperature sensing devices 20 , 22 and pressure sensing device 24 . However, the dynamically controlled system allow the telescope 10 to maintain and remain within the target defocus band range 62 throughout the flight pattern as evidenced by line 60 B. This is shown in distinction to the old static systems, represented by line 60 A, which may leave the defocused target range when the altitude and pressures vary. For example, which respect to the old static design, at time T 120 , the platform begins to decrease its altitude which thereby increases pressure. The old design exits the target defocus range 62 or band between about time T 135 to about time T 195 (wherein the time is represented by the capital letter T preceding the time on the X-axis of the graph in FIG. 6A-6B ). However, over this same timeframe from T 135 to T 195 , the new system of telescope 10 is able to maintain the defocus within the target band range 62 as altitude and pressure of the flight path change, as evidenced by line 60 B in range 62 from time T 135 to time T 195 . Although the present disclosure has described the telescope housing 12 as being formed from aluminum in one embodiment, other embodiments can use different materials to form housing 12 . However, these other embodiments should select a material that has a sufficiently high CTE value. For example, as stated previously, it was determined that titanium, which has a CTE value of about 9 was too low for the desired purpose of a low f-number (i.e., f-number less than or equal to 2) precision variable-focus telescope. Thus, other materials that should be used to form the telescope housing 12 should have a CTE value greater than 9. One other embodiment determined that copper, which has a CTE value of about 16 is sufficient to improve performance of a titanium housing. Thus, other materials that could be used to form the telescope housing 12 should have a CTE value greater than 16. Some exemplary other materials that have a CTE value greater than 9 which might be utilized to form housing 12 , according to other embodiments of the present disclosure, are identified below in Table 1. TABLE 1Linear Coefficientof Thermal ExpansionMaterial(10 −6 m/(m ° C.))Steel10.8-12.5Scandium10.2Terbium10.3Yttrium10.6Cast Iron Gray10.8Cement, Portland11Promethium11Inconel11.5-12.6Holmium11.2Hastelloy C11.3Iron, forged11.3Sandstone11.6Terne11.6Palladium11.8Beryllium12Cobalt12Iron, pure12.0Thorium12Lanthanum12.1Erbium12.2Samarium12.7Nickel13.0Bismuth13-13.5Concrete13-14Thulium13.3Uranium13.4Monel metal13.5Gold14.2Steel Stainless Austenitic (310)14.4Constantan15.2-18.8Gold - platinum15.2Gold - copper15.5Steel Stainless Austenitic (316)16.0Copper16-16.7Vinyl Ester16-22Cupronickel 30% (constantan)16.2Phosphor bronze16.7Plaster17Bronze17.5-18Steel Stainless Austenitic (304)17.3Copper, Beryllium 2517.8Gunmetal18Brass18-19Manganin18.1German silver18.4Silver19-19.7Speculum metal19.3Fluorspar, CaF 219.5Kapton20Tin20-23Barium20.6Aluminum21-24Polycarbonate - glass fiber-reinforced21.5Bakelite, bleached22Manganese22Calcium22.3Strontium22.5Duralumin23Nylon, glass fiber reinforced23Magnalium23.8Polyester - glass fiber-reinforced25Solder lead-tin, 50%-50%25Magnesium25-26.9Magnesium alloy AZ31B26Ytterbium26.3Antimonial lead (hard lead)26.5Lead29Thallium29.9Cadmium30Wood, across (perpendicular) to grain30Zinc30-35ABS -glass fiber-reinforced31Polypropylene - glass fiber-reinforced32Indium33Europium35Epoxy - glass fiber reinforced36Polyphenylene - glass fiber-reinforced36Tellurium36.9Selenium37Acetal - glass fiber-reinforced39Plastics40-120Rock salt40.4Benzocyclobutene42Epoxy, cast resins & compounds, unfilled45-65Lithium46Plutonium47-54 In addition to sufficiently high CTE value discussed above, the material used for housing 12 should also have a sufficiently high thermal conductivity value. For example, as stated previously, it was determined that titanium, which has a thermal conductivity value, at 0° C., of about 22.4 W/m K was too low for the desired purpose of a low F-number (i.e., F-number less than or equal to 2) precision variable-focus telescope. Thus, other materials that should be used to form the telescope housing 12 should have a thermal conductivity value greater than that of titanium. One other embodiment determined that copper, which has a thermal conductivity value, at 0° C., of about 401 W/m K is sufficient to improve performance of a titanium housing. Thus, other materials that could be used to form the telescope housing 12 should have a thermal conductivity value greater than about 100. Some exemplary other materials that have a thermal conductivity value greater than about 100 which might be utilized to form housing 12 , according to other embodiments of the present disclosure, are identified below in Table 2. TABLE 2ThermalTemperatureConductivity- t -- k -Metal, Metallic Element or Alloy(° C.)(W/m K)Aluminum−73237Aluminum0236Aluminum127240Aluminum327232Aluminum527220Aluminum - Duralumin (94-96% Al,201643-5% Cu, trace Mg)Aluminum - Silumin (87% Al, 13% Si)20164Aluminum alloy 3003, rolled0-25190Aluminum alloy 2014. annealed0-25190Aluminum alloy 3600-25150Beryllium−73301Beryllium0218Beryllium127161Beryllium327126Beryllium527107Cadmium−7399.3Cadmium097.5Cadmium12794.7Chromium−73111Chromium094.8Chromium12787.3Chromium32780.5Chromium52771.3Chromium72765.3Chromium92762.4Cobalt−73122Cobalt0104Cobalt12784.8Copper−73413Copper0401Copper127392Copper327383Copper527371Copper727357Copper927342Copper, electrolytic (ETP)0-25390Copper - Admiralty Brass20111Copper - Aluminum Bronze (95% Cu,20835% Al)Copper - Bronze (75% Cu, 25% Sn)2026Copper - Brass (Yellow Brass) (70% Cu,2011130% Zn)Copper - Cartridge brass (UNS C26000)20120Copper - Constantan (60% Cu, 40% Ni)2022.7Copper - German Silver (62% Cu, 15%2024.9Ni, 22% Zn)Copper - Phosphor bronze (10% Sn,2050UNS C52400)Copper - Red Brass (85% Cu, 9% Sn,20616% Zn)Gold−73327Gold0318Gold127312Gold327304Gold527292Gold727278Gold927262Iridium−73153Iridium0148Iridium127144Iridium327138Iridium527132Iridium727126Iridium927120Magnesium−73159Magnesium0157Magnesium127153Magnesium327149Magnesium527146Magnesium alloy AZ31B0-25100Molybdenum−73143Molybdenum0139Molybdenum127134Molybdenum327126Molybdenum527118Molybdenum727112Molybdenum927105Red brass0-25160Rhodium−73154Rhodium0151Rhodium127146Rhodium327136Rhodium527127Rhodium727121Rhodium927115Silver−73403Silver0428Silver127420Silver327405Silver527389Silver727374Silver927358Tungsten−73197Tungsten0182Tungsten127162Tungsten327139Tungsten527128Tungsten727121Tungsten927115Zinc−73123Zinc0122Zinc127116Zinc327105 Although the sensing devices 20 , 22 , and 24 have been detailed herein, the telescope 10 or its associated system or assembly may additionally include one or more other sensor to sense or gather data pertaining to the surrounding environment or operation of the device, assembly, or system. Some exemplary sensors capable of being electronically coupled with the device, assembly, or system of the present disclosure (either directly connected to the device, assembly, or system of the present disclosure or remotely connected thereto) may include but are not limited to: accelerometers sensing accelerations experienced during rotation, translation, velocity/speed, location traveled, elevation gained; gyroscopes sensing movements during angular orientation and/or rotation, and rotation; altimeters sensing barometric pressure, altitude change, terrain climbed, local pressure changes, submersion in liquid; impellers measuring the amount of fluid passing thereby; Global Positioning sensors sensing location, elevation, distance traveled, velocity/speed; audio sensors sensing local environmental sound levels, or voice detection; Photo/Light sensors sensing ambient light intensity, ambient, Day/night, UV exposure; TV/IR sensors sensing light wavelength; other temperature sensors sensing machine or motor temperature, ambient air temperature, and environmental temperature; and moisture Sensors sensing surrounding moisture levels. The device, assembly, or system of the present disclosure may include wireless communication logic coupled to sensors on the device, assembly, or system. The sensors gather data and provide the data to the wireless communication logic. Then, the wireless communication logic may transmit the data gathered from the sensors to a remote device. Thus, the wireless communication logic may be part of a broader communication system, in which one or several devices, assemblies, or systems of the present disclosure may be networked together to report alerts and, more generally, to be accessed and controlled remotely. Depending on the types of transceivers installed in the device, assembly, or system of the present disclosure, the system may use a variety of protocols (e.g., Wifi, ZigBee, MiWi, Bluetooth) for communication. In one example, each of the devices, assemblies, or systems of the present disclosure may have its own IP address and may communicate directly with a router or gateway. This would typically be the case if the communication protocol is WiFi. In another example, a point-to-point communication protocol like MiWi or ZigBee is used. One or more of the device, assembly, or system of the present disclosure may serve as a repeater, or the devices, assemblies, or systems of the present disclosure may be connected together in a mesh network to relay signals from one device, assembly, or system to the next. However, the individual device, assembly, or system in this scheme typically would not have IP addresses of their own. Instead, one or more of the devices, assemblies, or system of the present disclosure communicates with a repeater that does have an IP address, or another type of address, identifier, or credential needed to communicate with an outside network. The repeater communicates with the router or gateway. In either communication scheme, the router or gateway communicates with a communication network, such as the Internet, although in some embodiments, the communication network may be a private network that uses transmission control protocol/internet protocol (TCP/IP) and other common Internet protocols but does not interface with the broader Internet, or does so only selectively through a firewall. The system that receives and processes signals from the device, assembly, or system of the present disclosure may differ from embodiment to embodiment. In one embodiment, alerts and signals from the device, assembly, or system of the present disclosure are sent through an e-mail or simple message service (SMS; text message) gateway so that they can be sent as e-mails or SMS text messages to a remote device, such as a smartphone, laptop, or tablet computer, monitored by a responsible individual, group of individuals, or department, such as a maintenance department. Thus, if a particular device, assembly, or system of the present disclosure creates an alert because of a data point gathered by one or more sensors, that alert can be sent, in e-mail or SMS form, directly to the individual responsible for fixing it. Of course, e-mail and SMS are only two examples of communication methods that may be used; in other embodiments, different forms of communication may be used. The system also allows individuals to access the device, assembly, or system of the present disclosure for configuration and diagnostic purposes. In that case, the individual processors or microcontrollers of the device, assembly, or system of the present disclosure may be configured to act as Web servers that use a protocol like hypertext transfer protocol (HTTP) to provide an online interface that can be used to configure the device, assembly, or system. In some embodiments, the systems may be used to configure several devices, assemblies, or systems of the present disclosure at once. For example, if several devices, assemblies, or systems are of the same model and are in similar locations in the same location, it may not be necessary to configure the devices, assemblies, or systems individually. Instead, an individual may provide configuration information, including baseline operational parameters, for several devices, assemblies, or systems at once. As described herein, aspects of the present disclosure may include one or more electrical, pneumatic, hydraulic, or other similar secondary components and/or systems therein. The present disclosure is therefore contemplated and will be understood to include any necessary operational components thereof. For example, electrical components will be understood to include any suitable and necessary wiring, fuses, or the like for normal operation thereof. It will be further understood that any connections between various components not explicitly described herein may be made through any suitable means including mechanical fasteners, or more permanent attachment means, such as welding or the like. Alternatively, where feasible and/or desirable, various components of the present disclosure may be integrally formed as a single unit. Various inventive concepts may be embodied as one or more methods, of which an example has been provided. The acts performed as part of the method may be ordered in any suitable way. Accordingly, embodiments may be constructed in which acts are performed in an order different than illustrated, which may include performing some acts simultaneously, even though shown as sequential acts in illustrative embodiments. While various inventive embodiments have been described and illustrated herein, those of ordinary skill in the art will readily envision a variety of other means and/or structures for performing the function and/or obtaining the results and/or one or more of the advantages described herein, and each of such variations and/or modifications is deemed to be within the scope of the inventive embodiments described herein. More generally, those skilled in the art will readily appreciate that all parameters, dimensions, materials, and configurations described herein are meant to be exemplary and that the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the inventive teachings is/are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific inventive embodiments described herein. It is, therefore, to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, inventive embodiments may be practiced otherwise than as specifically described and claimed. Inventive embodiments of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the inventive scope of the present disclosure. The above-described embodiments can be implemented in any of numerous ways. For example, embodiments of technology disclosed herein may be implemented using hardware, software, or a combination thereof. When implemented in software, the software code or instructions can be executed on any suitable processor or collection of processors, whether provided in a single computer or distributed among multiple computers. Furthermore, the instructions or software code can be stored in at least one non-transitory computer readable storage medium. Also, a computer or smartphone may be utilized to execute the software code or instructions via its processors may have one or more input and output devices. These devices can be used, among other things, to present a user interface. Examples of output devices that can be used to provide a user interface include printers or display screens for visual presentation of output and speakers or other sound generating devices for audible presentation of output. Examples of input devices that can be used for a user interface include keyboards, and pointing devices, such as mice, touch pads, and digitizing tablets. As another example, a computer may receive input information through speech recognition or in other audible format. Such computers or smartphones may be interconnected by one or more networks in any suitable form, including a local area network or a wide area network, such as an enterprise network, and intelligent network (IN) or the Internet. Such networks may be based on any suitable technology and may operate according to any suitable protocol and may include wireless networks, wired networks or fiber optic networks. The various methods or processes outlined herein may be coded as software/instructions that is executable on one or more processors that employ any one of a variety of operating systems or platforms. Additionally, such software may be written using any of a number of suitable programming languages and/or programming or scripting tools, and also may be compiled as executable machine language code or intermediate code that is executed on a framework or virtual machine. In this respect, various inventive concepts may be embodied as a computer readable storage medium (or multiple computer readable storage media) (e.g., a computer memory, one or more floppy discs, compact discs, optical discs, magnetic tapes, flash memories, USB flash drives, SD cards, circuit configurations in Field Programmable Gate Arrays or other semiconductor devices, or other non-transitory medium or tangible computer storage medium) encoded with one or more programs that, when executed on one or more computers or other processors, perform methods that implement the various embodiments of the disclosure discussed above. The computer readable medium or media can be transportable, such that the program or programs stored thereon can be loaded onto one or more different computers or other processors to implement various aspects of the present disclosure as discussed above. The terms “program” or “software” or “instructions” are used herein in a generic sense to refer to any type of computer code or set of computer-executable instructions that can be employed to program a computer or other processor to implement various aspects of embodiments as discussed above. Additionally, it should be appreciated that according to one aspect, one or more computer programs that when executed perform methods of the present disclosure need not reside on a single computer or processor, but may be distributed in a modular fashion amongst a number of different computers or processors to implement various aspects of the present disclosure. Computer-executable instructions may be in many forms, such as program modules, executed by one or more computers or other devices. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. Typically, the functionality of the program modules may be combined or distributed as desired in various embodiments. As such, one aspect or embodiment of the present disclosure may be a computer program product including least one non-transitory computer readable storage medium in operative communication with a processor, the storage medium having instructions stored thereon that, when executed by the processor, implement a method or process described herein, wherein the instructions comprise the steps to perform the method(s) or process(es) detailed herein. Also, data structures may be stored in computer-readable media in any suitable form. For simplicity of illustration, data structures may be shown to have fields that are related through location in the data structure. Such relationships may likewise be achieved by assigning storage for the fields with locations in a computer-readable medium that convey relationship between the fields. However, any suitable mechanism may be used to establish a relationship between information in fields of a data structure, including through the use of pointers, tags or other mechanisms that establish relationship between data elements. All definitions, as defined and used herein, should be understood to control over dictionary definitions, definitions in documents incorporated by reference, and/or ordinary meanings of the defined terms. “Logic”, as used herein, includes but is not limited to hardware, firmware, software, and/or combinations of each to perform a function(s) or an action(s), and/or to cause a function or action from another logic, method, and/or system. For example, based on a desired application or needs, logic may include a software controlled microprocessor, discrete logic like a processor (e.g., microprocessor), an application specific integrated circuit (ASIC), a programmed logic device, a memory device containing instructions, an electric device having a memory, or the like. Logic may include one or more gates, combinations of gates, or other circuit components. Logic may also be fully embodied as software. Where multiple logics are described, it may be possible to incorporate the multiple logics into one physical logic. Similarly, where a single logic is described, it may be possible to distribute that single logic between multiple physical logics. Furthermore, the logic(s) presented herein for accomplishing various methods of this system may be directed towards improvements in existing computer-centric or internet-centric technology that may not have previous analog versions. The logic(s) may provide specific functionality directly related to structure that addresses and resolves some problems identified herein. The logic(s) may also provide significantly more advantages to solve these problems by providing an exemplary inventive concept as specific logic structure and concordant functionality of the method and system. Furthermore, the logic(s) may also provide specific computer implemented rules that improve on existing technological processes. The logic(s) provided herein extends beyond merely gathering data, analyzing the information, and displaying the results. Further, portions or all of the present disclosure may rely on underlying equations that are derived from the specific arrangement of the equipment or components as recited herein. Thus, portions of the present disclosure as it relates to the specific arrangement of the components are not directed to abstract ideas. Furthermore, the present disclosure and the appended claims present teachings that involve more than performance of well-understood, routine, and conventional activities previously known to the industry. In some of the method or process of the present disclosure, which may incorporate some aspects of natural phenomenon, the process or method steps are additional features that are new and useful. The articles “a” and “an,” as used herein in the specification and in the claims, unless clearly indicated to the contrary, should be understood to mean “at least one.” The phrase “and/or,” as used herein in the specification and in the claims (if at all), should be understood to mean “either or both” of the elements so conjoined, i.e., elements that are conjunctively present in some cases and disjunctively present in other cases. Multiple elements listed with “and/or” should be construed in the same fashion, i.e., “one or more” of the elements so conjoined. Other elements may optionally be present other than the elements specifically identified by the “and/or” clause, whether related or unrelated to those elements specifically identified. Thus, as a non-limiting example, a reference to “A and/or B”, when used in conjunction with open-ended language such as “comprising” can refer, in one embodiment, to A only (optionally including elements other than B); in another embodiment, to B only (optionally including elements other than A); in yet another embodiment, to both A and B (optionally including other elements); etc. As used herein in the specification and in the claims, “or” should be understood to have the same meaning as “and/or” as defined above. For example, when separating items in a list, “or” or “and/or” shall be interpreted as being inclusive, i.e., the inclusion of at least one, but also including more than one, of a number or list of elements, and, optionally, additional unlisted items. Only terms clearly indicated to the contrary, such as “only one of” or “exactly one of,” or, when used in the claims, “consisting of,” will refer to the inclusion of exactly one element of a number or list of elements. In general, the term “or” as used herein shall only be interpreted as indicating exclusive alternatives (i.e. “one or the other but not both”) when preceded by terms of exclusivity, such as “either,” “one of,” “only one of,” or “exactly one of.” “Consisting essentially of,” when used in the claims, shall have its ordinary meaning as used in the field of patent law. As used herein in the specification and in the claims, the phrase “at least one,” in reference to a list of one or more elements, should be understood to mean at least one element selected from any one or more of the elements in the list of elements, but not necessarily including at least one of each and every element specifically listed within the list of elements and not excluding any combinations of elements in the list of elements. This definition also allows that elements may optionally be present other than the elements specifically identified within the list of elements to which the phrase “at least one” refers, whether related or unrelated to those elements specifically identified. Thus, as a non-limiting example, “at least one of A and B” (or, equivalently, “at least one of A or B,” or, equivalently “at least one of A and/or B”) can refer, in one embodiment, to at least one, optionally including more than one, A, with no B present (and optionally including elements other than B); in another embodiment, to at least one, optionally including more than one, B, with no A present (and optionally including elements other than A); in yet another embodiment, to at least one, optionally including more than one, A, and at least one, optionally including more than one, B (and optionally including other elements); etc. While components of the present disclosure are described herein in relation to each other, it is possible for one of the components disclosed herein to include inventive subject matter, if claimed alone or used alone. In keeping with the above example, if the disclosed embodiments teach the features of components A and B, then there may be inventive subject matter in the combination of A and B, A alone, or B alone, unless otherwise stated herein. As used herein in the specification and in the claims, the term “effecting” or a phrase or claim element beginning with the term “effecting” should be understood to mean to cause something to happen or to bring something about. For example, effecting an event to occur may be caused by actions of a first party even though a second party actually performed the event or had the event occur to the second party. Stated otherwise, effecting refers to one party giving another party the tools, objects, or resources to cause an event to occur. Thus, in this example a claim element of “effecting an event to occur” would mean that a first party is giving a second party the tools or resources needed for the second party to perform the event, however the affirmative single action is the responsibility of the first party to provide the tools or resources to cause said event to occur. When a feature or element is herein referred to as being “on” another feature or element, it can be directly on the other feature or element or intervening features and/or elements may also be present. In contrast, when a feature or element is referred to as being “directly on” another feature or element, there are no intervening features or elements present. It will also be understood that, when a feature or element is referred to as being “connected”, “attached” or “coupled” to another feature or element, it can be directly connected, attached or coupled to the other feature or element or intervening features or elements may be present. In contrast, when a feature or element is referred to as being “directly connected”, “directly attached” or “directly coupled” to another feature or element, there are no intervening features or elements present. Although described or shown with respect to one embodiment, the features and elements so described or shown can apply to other embodiments. It will also be appreciated by those of skill in the art that references to a structure or feature that is disposed “adjacent” another feature may have portions that overlap or underlie the adjacent feature. Spatially relative terms, such as “under”, “below”, “lower”, “over”, “upper”, “above”, “behind”, “in front of”, and the like, may be used herein for ease of description to describe one element or feature's relationship to another element(s) or feature(s) as illustrated in the figures. It will be understood that the spatially relative terms are intended to encompass different orientations of the device in use or operation in addition to the orientation depicted in the figures. For example, if a device in the figures is inverted, elements described as “under” or “beneath” other elements or features would then be oriented “over” the other elements or features. Thus, the exemplary term “under” can encompass both an orientation of over and under. The device may be otherwise oriented (rotated 90 degrees or at other orientations) and the spatially relative descriptors used herein interpreted accordingly. Similarly, the terms “upwardly”, “downwardly”, “vertical”, “horizontal”, “lateral”, “transverse”, “longitudinal”, and the like are used herein for the purpose of explanation only unless specifically indicated otherwise. Although the terms “first” and “second” may be used herein to describe various features/elements, these features/elements should not be limited by these terms, unless the context indicates otherwise. These terms may be used to distinguish one feature/element from another feature/element. Thus, a first feature/element discussed herein could be termed a second feature/element, and similarly, a second feature/element discussed herein could be termed a first feature/element without departing from the teachings of the present invention. An embodiment is an implementation or example of the present disclosure. Reference in the specification to “an embodiment,” “one embodiment,” “some embodiments,” “one particular embodiment,” “an exemplary embodiment,” or “other embodiments,” or the like, means that a particular feature, structure, or characteristic described in connection with the embodiments is included in at least some embodiments, but not necessarily all embodiments, of the invention. The various appearances “an embodiment,” “one embodiment,” “some embodiments,” “one particular embodiment,” “an exemplary embodiment,” or “other embodiments,” or the like, are not necessarily all referring to the same embodiments. If this specification states a component, feature, structure, or characteristic “may”, “might”, or “could” be included, that particular component, feature, structure, or characteristic is not required to be included. If the specification or claim refers to “a” or “an” element, that does not mean there is only one of the element. If the specification or claims refer to “an additional” element, that does not preclude there being more than one of the additional element. As used herein in the specification and claims, including as used in the examples and unless otherwise expressly specified, all numbers may be read as if prefaced by the word “about” or “approximately,” even if the term does not expressly appear. The phrase “about” or “approximately” may be used when describing magnitude and/or position to indicate that the value and/or position described is within a reasonable expected range of values and/or positions. For example, a numeric value may have a value that is +/−0.1% of the stated value (or range of values), +/−1% of the stated value (or range of values), +/−2% of the stated value (or range of values), +/−5% of the stated value (or range of values), +/−10% of the stated value (or range of values), etc. Any numerical range recited herein is intended to include all sub-ranges subsumed therein. Additionally, the method of performing the present disclosure may occur in a sequence different than those described herein. Accordingly, no sequence of the method should be read as a limitation unless explicitly stated. It is recognizable that performing some of the steps of the method in a different order could achieve a similar result. In the claims, as well as in the specification above, all transitional phrases such as “comprising,” “including,” “carrying,” “having,” “containing,” “involving,” “holding,” “composed of,” and the like are to be understood to be open-ended, i.e., to mean including but not limited to. Only the transitional phrases “consisting of” and “consisting essentially of” shall be closed or semi-closed transitional phrases, respectively, as set forth in the United States Patent Office Manual of Patent Examining Procedures. To the extent that the present disclosure has utilized the term “invention” in various titles or sections of this specification, this term was included as required by the formatting requirements of word document submissions pursuant the guidelines/requirements of the United States Patent and Trademark Office and shall not, in any manner, be considered a disavowal of any subject matter. In the foregoing description, certain terms have been used for brevity, clearness, and understanding. No unnecessary limitations are to be implied therefrom beyond the requirement of the prior art because such terms are used for descriptive purposes and are intended to be broadly construed. Moreover, the description and illustration of various embodiments of the disclosure are examples and the disclosure is not limited to the exact details shown or described.",en,PATENT_APPLICATION
114-413-720-321-048,US,20240385273,A1,2024-11-21,US_20240385273_A1_20241121,en,US,20240385273,A1,2024-11-21,US,18463394,2023-09-08,SELECTION OF K-SPACE DATA SEGMENTS BASED ON ASSOCIATED NAVIGATOR IMAGES,en,DE,Siemens Healthineers AG,Forchheim,US,Duke University,"Durham, NC",US,Wolfgang Rehwald,"Chapel Hill, NC",US,1,Raymond J. Kim,"Chapel Hill, NC",G01R33/567,I,F,G01R33/5676,I,F,G01R33/5673,I,L,US,20240385273,A1,2024-11-21,114-413-720-321-048,1,US,20240385273,A1,2024-11-21,114-413-720-321-048,1,UNKNOWN,"A system and method comprises acquisition of a plurality of navigator k-space data segments and a k-space data segment associated with each of the plurality of navigator k-space data segments, generation of a respective navigator image from each of the plurality of navigator k-space data segments, determination, based on the respective navigator images of each of the plurality of navigator k-space data segments, of a first center navigator k-space data segment associated with a most common respiratory position of a subject, determination of a center k-space data segment corresponding to the first center navigator k-space data segment, determination, for each non-center segment of k-space, of a k-space data segment whose respective navigator image is most similar to a first center navigator image generated from the first center navigator k-space data segment, and generation of an image based on the determined k-space data segments.",en,"1 . A magnetic resonance imaging system comprising: a magnet system configured to generate a polarizing magnetic field about at least a portion of a subject; a plurality of gradient coils configured to apply at least one gradient field to the polarizing magnetic field; a radio frequency (RF) system configured to apply an excitation field to the subject and to acquire magnetic resonance (MR) data from the subject; and a processing unit to execute program code to cause the system to: acquire a plurality of navigator k-space data segments and a k-space data segment associated with each of the plurality of navigator k-space data segments; generate a respective navigator image from each of the plurality of navigator k-space data segments; based on the respective navigator images of each of the plurality of navigator k-space data segments, determine a first center navigator k-space data segment associated with a most common respiratory position of the subject; determine a center k-space data segment corresponding to the first center navigator k-space data segment; determine, for each non-center segment of k-space, a k-space data segment whose respective navigator image is most similar to a first center navigator image generated from the first center navigator k-space data segment; and generate an image based on the determined k-space data segments.","2 . A system according to claim 1 , wherein determination of the first center navigator k-space data segment associated with a most common respiratory position of the subject comprises: determination of a first composite navigator image based on the generated navigator images; determination of a difference image between each generated navigator image and the first composite navigator image; determination of a first value for each generated navigator image based on the difference image determined for the generated navigator image; assignment of the first values to a plurality of bins, each of the plurality of bins associated with a respective range of values; determination of a first bin to which a highest number of the first values is assigned; determination of a second composite navigator image based on the navigator images whose first values are assigned to the first bin; determination of a second difference image between each generated navigator image and the second composite navigator image; determination of a second value for each generated navigator image based on the second difference image determined for the generated navigator image; assignment of the second values to a second plurality of bins, each of the second plurality of bins associated with a respective range of values; determination of a second bin to which a highest number of the second values is assigned; determination of a center navigator image whose second value is assigned to the second bin; and determination of the first center navigator k-space data segment as associated with the center navigator image.","3 . A system according to claim 2 , wherein determination of a first value for each generated navigator image based on the difference image determined for the generated navigator image comprises summing of the absolute values of pixels of each difference image to determine a difference value for each generated navigator image, and wherein determination of a second value for each generated navigator image based on the second difference image determined for the generated navigator image comprises summing of the absolute values of pixels of each second difference image to determine a second difference value for each generated navigator image.","4 . A system according to claim 1 , wherein determination of the first center navigator k-space data segment associated with a most common respiratory position of the subject comprises: determination of a first composite navigator image based on the generated navigator images; determination of a first displacement map between each generated navigator image and the first composite navigator image; determination of a first displacement value for each generated navigator image based on the first displacement map determined for the generated navigator image; assignment of the first displacement values to a plurality of bins, each of the plurality of bins associated with a respective range of displacement values; determination of a first bin to which a highest number of the first displacement values is assigned; determination of a second composite navigator image based on the navigator images whose first displacement values are assigned to the first bin; determination of a second displacement map between each generated navigator image and the second composite navigator image; determination of a second displacement value for each generated navigator image based on the second displacement map determined for the generated navigator image; assignment of the second displacement values to a second plurality of bins, each of the second plurality of bins associated with a respective range of displacement values; determination of a second bin to which a highest number of the second displacement values is assigned; determination of a center navigator image whose second displacement value is assigned to the second bin; and determination of the first center navigator k-space data segment as associated with the center navigator image.","5 . A system according to claim 4 , wherein determination of a first displacement value for each generated navigator image comprises: determination of a first two-dimensional displacement vector for each generated navigator image based on the first displacement map determined for the generated navigator image; and conversion of the first two-dimensional displacement vectors to one-dimensional first displacement values, and determination of a second displacement value for each generated navigator image comprises: determination of a second two-dimensional displacement vector for each generated navigator image based on the second displacement map determined for the generated navigator image; and conversion of the second two-dimensional displacement vectors to one-dimensional second displacement values.","6 . A system according to claim 5 , wherein generation of an image based on the determined k-space data segments comprises: determination of a displacement map between each respective navigator image most similar to the first center navigator image and the first center navigator image; zero-filling of a respective k-space for each determined k-space data segment; generation of a respective image from each zero-filled k-space; application of motion correction to each respective image based on the displacement map determined between the respective navigator image and the first center navigator image; and generation of the image based on the motion-corrected images.","7 . A system according to claim 1 , wherein generation of an image based on the determined k-space data segments comprises: determination of a displacement map between each respective navigator image most similar to the first center navigator image and the first center navigator image; zero-filling of a respective k-space for each determined k-space data segment; generation of a respective image from each zero-filled k-space; application of motion correction to each respective image based on the displacement map determined between the respective navigator image and the first center navigator image; and generation of the image based on the motion-corrected images.","8 . A method comprising: determining a plurality of navigator k-space data segments acquired from a subject and a k-space data segment acquired from the subject and associated with each of the plurality of navigator k-space data segments; generating a respective navigator image based on each of the plurality of navigator k-space data segments; based on the respective navigator images of each of the plurality of navigator k-space data segments, determining a first center navigator k-space data segment associated with a first respiratory position of the subject; determining a center k-space data segment corresponding to the first center navigator k-space data segment; determining, for each non-center segment of k-space, a k-space data segment whose respective navigator image is most similar to a first center navigator image generated from the first center navigator k-space data segment; and generating an image based on the determined k-space data segments.","9 . A method according to claim 8 , wherein determining the first center navigator k-space data segment associated with a most common respiratory position of the subject comprises: determining a first composite navigator image based on the generated navigator images; determining a difference image between each generated navigator image and the first composite navigator image; determining a first value for each generated navigator image based on the difference image determined for the generated navigator image; assigning the first values to a plurality of bins, each of the plurality of bins associated with a respective range of values; determining a first bin to which a highest number of the first values is assigned; determining a second composite navigator image based on the navigator images whose first values are assigned to the first bin; determining a second difference image between each generated navigator image and the second composite navigator image; determining a second value for each generated navigator image based on the second difference image determined for the generated navigator image; assigning the second values to a second plurality of bins, each of the second plurality of bins associated with a respective range of values; determining a second bin to which a highest number of the second values is assigned; determining a center navigator image whose second value is assigned to the second bin; and determining the first center navigator k-space data segment as associated with the center navigator image.","10 . A method according to claim 9 , wherein determining a first value for each generated navigator image based on the difference image determined for the generated navigator image comprises summing the absolute values of pixels of each difference image to determine a difference value for each generated navigator image, and wherein determining a second value for each generated navigator image based on the second difference image determined for the generated navigator image comprises summing the absolute values of pixels of each second difference image to determine a second difference value for each generated navigator image.","11 . A method according to claim 8 , wherein determining the first center navigator k-space data segment associated with a most common respiratory position of the subject comprises: determining a first composite navigator image based on the generated navigator images; determining a first displacement map between each generated navigator image and the first composite navigator image; determining a first displacement value for each generated navigator image based on the first displacement map determined for the generated navigator image; assigning the first displacement values to a plurality of bins, each of the plurality of bins associated with a respective range of displacement values; determining a first bin to which a highest number of the first displacement values is assigned; determining a second composite navigator image based on the navigator images whose first displacement values are assigned to the first bin; determining a second displacement map between each generated navigator image and the second composite navigator image; determining a second displacement value for each generated navigator image based on the second displacement map determined for the generated navigator image; assigning the second displacement values to a second plurality of bins, each of the second plurality of bins associated with a respective range of displacement values; determining a second bin to which a highest number of the second displacement values is assigned; determining a center navigator image whose second displacement value is assigned to the second bin; and determining the first center navigator k-space data segment as associated with the center navigator image.","12 . A method according to claim 11 , wherein determining a first displacement value for each generated navigator image comprises: determining a first two-dimensional displacement vector for each generated navigator image based on the first displacement map determined for the generated navigator image; and converting of the first two-dimensional displacement vectors to one-dimensional first displacement values, and determining a second displacement value for each generated navigator image comprises: determining a second two-dimensional displacement vector for each generated navigator image based on the second displacement map determined for the generated navigator image; and converting the second two-dimensional displacement vectors to one-dimensional second displacement values.","13 . A method according to claim 12 , wherein generating an image based on the determined k-space data segments comprises: determining a displacement map between each respective navigator image most similar to the first center navigator image and the first center navigator image; zero-filling a respective k-space for each determined k-space data segment; generating a respective image from each zero-filled k-space; applying motion correction to each respective image based on the displacement map determined between the respective navigator image and the first center navigator image; and generating the image based on the motion-corrected images.","14 . A method according to claim 8 , wherein generating an image based on the determined k-space data segments comprises: determining a displacement map between each respective navigator image most similar to the first center navigator image and the first center navigator image; zero-filling a respective k-space for each determined k-space data segment; generating a respective image from each zero-filled k-space; applying motion correction to each respective image based on the displacement map determined between the respective navigator image and the first center navigator image; and generating the image based on the motion-corrected images.","15 . A non-transitory computer-readable medium storing program code executable by one or more processing units to cause a computing system to: acquire, from a subject, a plurality of navigator k-space data segments and a k-space data segment associated with each of the plurality of navigator k-space data segments; generate a respective navigator image from each of the plurality of navigator k-space data segments; based on the respective navigator images of each of the plurality of navigator k-space data segments, determine a first center navigator k-space data segment associated with a most common respiratory position of the subject; determine a center k-space data segment corresponding to the first center navigator k-space data segment; determine, for each non-center segment of k-space, a k-space data segment whose respective navigator image is most similar to a first center navigator image generated from the first center navigator k-space data segment; and generate an image based on the determined k-space data segments.","16 . A medium according to claim 15 , wherein determination of the first center navigator k-space data segment associated with a most common respiratory position of the subject comprises: determination of a first composite navigator image based on the generated navigator images; determination of a difference image between each generated navigator image and the first composite navigator image; determination of a first value for each generated navigator image based on the difference image determined for the generated navigator image; assignment of the first values to a plurality of bins, each of the plurality of bins associated with a respective range of values; determination of a first bin to which a highest number of the first values is assigned; determination of a second composite navigator image based on the navigator images whose first values are assigned to the first bin; determination of a second difference image between each generated navigator image and the second composite navigator image; determination of a second value for each generated navigator image based on the second difference image determined for the generated navigator image; assignment of the second values to a second plurality of bins, each of the second plurality of bins associated with a respective range of values; determination of a second bin to which a highest number of the second values is assigned; determination of a center navigator image whose second value is assigned to the second bin; and determination of the first center navigator k-space data segment as associated with the center navigator image.","17 . A medium according to claim 15 , wherein determination of the first center navigator k-space data segment associated with a most common respiratory position of the subject comprises: determination of a first composite navigator image based on the generated navigator images; determination of a first displacement map between each generated navigator image and the first composite navigator image; determination of a first displacement value for each generated navigator image based on the first displacement map determined for the generated navigator image; assignment of the first displacement values to a plurality of bins, each of the plurality of bins associated with a respective range of displacement values; determination of a first bin to which a highest number of the first displacement values is assigned; determination of a second composite navigator image based on the navigator images whose first displacement values are assigned to the first bin; determination of a second displacement map between each generated navigator image and the second composite navigator image; determination of a second displacement value for each generated navigator image based on the second displacement map determined for the generated navigator image; assignment of the second displacement values to a second plurality of bins, each of the second plurality of bins associated with a respective range of displacement values; determination of a second bin to which a highest number of the second displacement values is assigned; determination of a center navigator image whose second displacement value is assigned to the second bin; and determination of the first center navigator k-space data segment as associated with the center navigator image.","18 . A medium according to claim 17 , wherein determination of a first displacement value for each generated navigator image comprises: determination of a first two-dimensional displacement vector for each generated navigator image based on the first displacement map determined for the generated navigator image; and conversion of the first two-dimensional displacement vectors to one-dimensional first displacement values, and determination of a second displacement value for each generated navigator image comprises: determination of a second two-dimensional displacement vector for each generated navigator image based on the second displacement map determined for the generated navigator image; and conversion of the second two-dimensional displacement vectors to one-dimensional second displacement values.","19 . A medium according to claim 18 , wherein generation of an image based on the determined k-space data segments comprises: determination of a displacement map between each respective navigator image most similar to the first center navigator image and the first center navigator image; zero-filling of a respective k-space for each determined k-space data segment; generation of a respective image from each zero-filled k-space; application of motion correction to each respective image based on the displacement map determined between the respective navigator image and the first center navigator image; and generation of the image based on the motion-corrected images.","20 . A medium according to claim 15 , wherein generation of an image based on the determined k-space data segments comprises: determination of a displacement map between each respective navigator image most similar to the first center navigator image and the first center navigator image; zero-filling of a respective k-space for each determined k-space data segment; generation of a respective image from each zero-filled k-space; application of motion correction to each respective image based on the displacement map determined between the respective navigator image and the first center navigator image; and generation of the image based on the motion-corrected images.",en,"CROSS-REFERENCE TO RELATED APPLICATIONS This application claims priority to U.S. Provisional Patent Application No. 63/502,974, filed May 18, 2023, the disclosure of which is incorporated herein by reference for all purposes. BACKGROUND A Magnetic Resonance (MR) scanner generates images of patient anatomy using timed sequences of RF pulses. MR imaging is useful in scenarios requiring high contrast between different soft tissues. For example, cardiac MR (CMR) imaging is increasingly used to non-invasively evaluate myocardial structure and function without using the ionizing radiation which is required by other imaging modalities. MR imaging includes the acquisition of k-space data. An image may be calculated from acquired k-space data using a two-dimensional Fourier Transform (FT). Different regions of k-space represent different image properties. For example, the center region of k-space contains image brightness and contrast information, and the edges of k-space contain image sharpness and detail information. A low-resolution image may therefore be calculated using only k-space data from the center region of k-space. A high-resolution image typically requires k-space data from all of k-space. The amount of k-space data required for a high-resolution image is more than can be acquired in a single data readout (i.e., shot), especially in the case of images of moving structures such as the heart. An image produced from the k-space data acquired by a single shot (even if the data were from the center region of k-space) would exhibit poor temporal resolution, and intricate cardiac features would appear blurred. Therefore, generation of a high-resolution image requires acquisition of k-space data using multiple shots, where each shot is taken with precise temporal resolution (i.e., in the same cardiac phase but in a different heartbeat). Each of the multiple shots acquires a different subset of all the lines of k-space. An MR sequence may acquire all the lines of k-space by acquiring different segments of k-space using separate shots. FIG. 1A illustrates segment acquisition according to contiguous reordering. As shown, each shot acquires a set or segment of L contiguous phase encoding (PE) lines, such that two segments from consecutive shots share one border in k-space. Segment 1 , including PE lines 1 through 1 , is acquired in a first shot. A second shot acquires segment 2 , which includes PE lines 1 +1 to 21 . Assuming a k-space consisting of N*L lines, the process continues in the PE direction until N segments are acquired. In CMR, each acquisition a sequence is typically triggered from the R-wave of the subject's ECG signal so that each segment is acquired in the same cardiac phase, but in a different heartbeat. Preferably, the N segments of k-space are acquired while the subject is holding their breath. If the breath is not held, the sequence results in poor image quality because the N k-space segments are likely not acquired during the same respiratory phase. A thusly-generated image may be fuzzy, inhomogeneous, and/or include motion-related ghosting artifacts. Subjects often cannot hold their breath for more than six seconds, particularly in the case of a subject experiencing cardiac and/or pulmonary disease. This time constraint limits the amount of k-space data that can be acquired during a single breath hold and can lead to two-dimensional images exhibiting poor signal-to-noise ratio (SNR), poor spatial resolution and/or poor temporal resolution. This constraint practically excludes the acquisition of three-dimensional MR images while breath holding. Systems are desired to efficiently generate high-quality images using k-space data acquired during free breathing. BRIEF DESCRIPTION OF THE DRAWINGS FIG. 1A illustrates contiguous reordering of k-space data. FIG. 1B illustrates interleaved reordering of k-space data. FIG. 2 is a flow diagram of a process to generate an image from k-space data segments according to some embodiments. FIG. 3 illustrates an inversion recovery-prepared, segmented pulse sequence with navigator data acquisition according to some embodiments. FIG. 4 illustrates acquired navigator k-space data segments and corresponding k-space data segments according to some embodiments. FIG. 5 illustrates determination of a center k-space data segment associated with a common respiratory position according to some embodiments. FIG. 6 illustrates determination of a side k-space data segment associated with a common respiratory position according to some embodiments. FIG. 7 illustrates determination of a side k-space data segment associated with a common respiratory position according to some embodiments. FIG. 8 illustrates determined k-space segments associated with a common respiratory position according to some embodiments. FIGS. 9A and 9B comprise a flow diagram of a process to determine a first center navigator image associated with a common respiratory position based on respective navigator images of each of a plurality of center navigator k-space data segments according to some embodiments. FIG. 10 illustrates generation of a composite navigator image according to some embodiments. FIG. 11 illustrates determination of a subset of navigator images based on the composite navigator image according to some embodiments. FIG. 12 illustrates generation of a second composite navigator image based on the subset of navigator images according to some embodiments. FIG. 13 illustrates determination of a center navigator image based on the second composite navigator image according to some embodiments. FIGS. 14A and 14B illustrate determination of a displacement value representing a displacement between navigator images and composite navigator images according to some embodiments. FIG. 15 comprises a flow diagram of a process to generate a motion-corrected image from k-space data segments according to some embodiments. FIG. 16 illustrates generation of a motion-corrected image based on a segment of k-space data according to some embodiments. FIG. 17 illustrates generation of an image based on a center segment of k-space data according to some embodiments. FIG. 18 illustrates a phase-sensitive inversion recovery pulse sequence including the acquisition of navigator data prior to the acquisition of each inversion recovery-prepared data segment and the acquisition of each non-inversion recovery-prepared reference data segment according to some embodiments. FIG. 19 is a block diagram of an example MR system for use in some embodiments. DETAILED DESCRIPTION The following description is provided to enable any person in the art to make and use the described embodiments. Various modifications will remain apparent to those in the art. Some embodiments provide high-quality images using k-space data acquired during free breathing. Briefly, a free-breathing navigator pulse sequence is executed to acquire segmented k-space data and, in close temporal proximity, associated navigator k-space data. A “most common” respiratory position is determined based on navigator images generated from the navigator k-space data, and k-space data segments which correspond to that position are identified. A high-quality image may then be generated based on the identified data segments. The most common respiratory position may comprise a position in which a region of interest resides for the longest time during a respiratory cycle. Due to the non-linearity of the respiratory cycle, this position is usually not an average of all positions in which the region resides during the cycle. Embodiments may use a two-step process to determine the most common respiratory position. In the first step, a composite navigator image is determined from all of the navigator images. The composite navigator image may be an average of all of the navigator images. Difference images between each navigator image and the composite navigator image are then calculated. Each difference image is reduced to a single difference value, for example by summing up the absolute values of their constituent pixels. A histogram of these difference values is plotted with respect to several bins. The bin with the highest peak is identified, representing a first and coarse approximation of the most common respiratory position. In the second step, the navigator images whose difference values reside in the identified bin are identified. A second composite image is determined based on these navigator images, again for example by averaging this identified subset of navigator images. A second difference value is determined as described above for each of the original navigator images based on the second composite image The second difference values are plotted in a second histogram, and a bin with highest peak is identified. The second difference values within this bin are determined to represent the most common respiratory position. A k-space data segment is identified which includes the center of k-space and which was acquired contemporaneously with the navigator k-space data of a navigator image whose second difference values are closest to the identified bin. This center k-space data segment is determined out of a plurality of center k-space data segments to have been acquired while the subject was at or closest to the most common respiratory position. The other (i.e., “side”) data segments may then be identified by comparing their corresponding navigator images to the navigator image corresponding to the center k-space data segment. The identified k-space data segments may be motion-corrected in some embodiments. Since the identified k-space data segments represent substantially the same respiratory position, only a minimal amount of motion correction is needed. In some embodiments, an image is generated from the zero-filled k-space of each identified data segment, a different non-rigid motion correction is applied to each image, and the motion-corrected images are combined along with a non-motion-corrected image of the center k-space data segment. This implementation advantageously allows the use of non-rigid motion correction in image space, as opposed to the rigid motion correction typically performed in k-space. Embodiments may address the problem of poor free breathing image quality in MR, and specifically in CMR. Embodiments may advantageously assess respiratory motion in the image plane. Embodiments may apply to any k-space trajectory, e.g., cartesian, radial, or elliptical, as well as to any reordering scheme used for acquiring the navigator k-space data or the segmented k-space data. Moreover, precision is increased since the navigator k-space data is acquired in the same plane as the segmented k-space data. FIG. 2 comprises a flow diagram of process 200 to generate an image from k-space data segments according to some embodiments. In some embodiments, various hardware elements of an MRI scanner execute program code to perform process 200 . The steps of process 200 need not be performed by a single device or system. Process 200 and all other processes mentioned herein may be embodied in executable program code read from one or more of non-transitory computer-readable media, such as a disk-based or solid-state hard drive, a DVD-ROM, a Flash drive, and a magnetic tape, and then stored in a compressed, uncompiled and/or encrypted format. In some embodiments, hard-wired circuitry may be used in place of, or in combination with, program code for implementation of processes according to some embodiments. Embodiments are therefore not limited to any specific combination of hardware and software. Initially, a plurality of navigator k-space data segments and a k-space data segment associated with each of the plurality of navigator k-space data segments are acquired at S 210 . The k-space data segments may be acquired by an MR scanner using a suitable pulse sequence. FIG. 3 illustrates a segmented CMR pulse sequence that may be used at S 210 in some embodiments. Embodiments are not limited thereto. As illustrated by the depicted respiration signal, the pulse sequence executes while the subject is free breathing. The FIG. 3 pulse sequence is triggered based on the subject's ECG signal such that all k-space data segments are acquired during the diastolic part of the RR-interval. The acquisition of k-space data segments is not synchronized to the respiratory signal, but only to the ECG signal. Therefore, the respiratory position at which each k-space data segment is acquired is substantially random. It will be assumed that k-space consists of N segments, including one center segment and N−1 side segments. In the present example, the sequence is repeated so that each of k-space data segments SEG 1 to SEG N is acquired M=4 times. The N segments SEG 1 to SEG N may be acquired in continuous order as described with respect to FIG. 1A , but any acquisition order (e.g., partially reversed reordering, interleaved reordering) may be used. Interleaved reordering, illustrated in FIG. 1B , refers to the acquisition of a segment of L lines per shot which do not correspond to a single contiguous region in the PE direction but are instead spaced apart from one another. Assuming a k-space consisting of N*L lines, the L lines of an interleaved-reordered segment in are spaced at a distance of N lines from one another. Accordingly, the entire k-space is traversed in the PE direction by each shot but only L lines out of the total N*L lines are acquired by each shot. After N shots, all N*L lines have been acquired. A navigator k-space data segment is acquired in close temporal proximity to acquisition of each acquired k-space data segment SEG 1 to SEG N. Each navigator k-space data segment and each acquired k-space data segment SEG 1 to SEG N may include a same number of lines, but each navigator k-space data segment includes the same k-space lines, typically located at and near the center of k-space. The navigator k-space data segments will be used to generate navigator images which depict a position reference and facilitate assessment of a subject's respiratory position during which a corresponding one of k-space data segments SEG 1 to SEG N was acquired. Accordingly, although FIG. 3 depicts acquisition of a navigator k-space data segment immediately before its corresponding k-space data segment, in some embodiments a navigator k-space data segment is acquired immediately after acquisition of its corresponding k-space data segment. FIG. 4 illustrates acquired navigator k-space data segments and corresponding k-space data segments according to some embodiments. According to the FIG. 4 example, N=5 and M=4. Thus, the pulse sequence was executed in order to acquire each of the five segments S 1 -S 5 four times. More particularly, navigator k-space data segments 410 and corresponding k-space data segments 415 were acquired during a first repetition (M 1 ), navigator k-space data segments 420 and corresponding k-space data segments 425 were acquired during a second repetition (M 2 ), navigator k-space data segments 430 and corresponding k-space data segments 435 were acquired during a third repetition (M 3 ), and navigator k-space data segments 440 and corresponding k-space data segments 445 were acquired during a fourth repetition (M 4 ). The subscript associated with each navigator k-space data segment N S#M# indicates the k-space data segment to which it corresponds. For example, navigator k-space data segment N S2M4 corresponds to (i.e., was acquired contemporaneously with) k-space data segment S 2 M4 . S 210 may comprise acquisition of the k-space data segments by an MR scanner, or acquisition of previously MR scanner-acquired k-space data segments by a separate computing system. Moreover, S 220 -S 270 may be executed by such a separate computing system or by an MR scanner. At S 220 , a respective navigator image is generated from each of the plurality of navigator k-space data segments. A Fast Fourier Transform (FFT) may be applied to a navigator k-space data segment to generate a respective navigator image as is known in the art. With respect to FIG. 3 , S 220 may include application of a FFT to each navigator k-space data segment (i.e., NAV 1 through NAV 4 ) acquired during each of the four repetitions of the pulse sequence. Because the navigator k-space data segments include only a limited number of k-space lines (e.g., 25 ), navigator images generated therefrom are of lower spatial resolution in the phase encoding direction than images generated from a complete set of k-space lines. A first center navigator k-space data segment associated with a first respiratory position is determined at S 240 based on the respective navigator images of each of the plurality of navigator k-space data segments. The first respiratory position may be the most common respiratory position according to some embodiments. Next, at S 250 , a center k-space data segment associated with the first center navigator k-space data segment is determined. FIG. 5 illustrates S 220 , S 240 and S 250 according to some embodiments. First, acquired navigator k-space data segments 410 - 440 are input to center segment navigator determination component 510 . Each component described herein may be embodied in hardware and/or executable program code, and two or more components may be embodied by the same hardware and/or executable program code. Center segment navigator determination component 510 may generate a respective navigator image from each of navigator k-space data segments 410 - 440 at S 220 . Component 510 then operates to determine a first center navigator k-space data segment associated with a first respiratory position based on the respective navigator images of navigator k-space data segments 410 - 440 at S 240 . Specific details of the determination at S 240 according to various embodiments will be described below. In the FIG. 5 example, navigator k-space data segment N S3M3 is determined as the first center navigator k-space data segment at S 240 . Since k-space data segment S 3 M3 corresponds to navigator k-space data segment N S3M3 , k-space data segment S 3 M3 is determined as the center k-space data segment at S 250 . FIG. 5 further illustrates the association of k-space data segment S 3 M3 with the center lines of cumulative k-space 520 . For each non-center k-space data segment, a respective navigator image most similar to a first center navigator image generated from the first center navigator k-space data segment is determined at S 260 . The similarity may be determined by, for each non-center k-space data segment, calculating the difference navigator image between each respective non-center navigator image and the first center navigator image, then summing the absolute pixel values of each respective difference navigator image, and comparing these sums. According to some embodiments, only the absolute values of the pixels of each difference navigator image within a region of interest (e.g., a central image portion including the heart) contribute to the sums. For each non-center k-space data segment, the navigator image whose sum of the absolute pixel values of its difference navigator image is smaller than those of the other non-center navigator images linked to the same k-space segment is deemed to be most similar to the first center navigator image. Next, at S 270 , the acquired k-space data segments from which the determined navigator images were generated are determined. FIG. 6 continues the above example in the case of first segment S 1 , including lines 1 through m of k-space. As shown, side segment navigator determination component 610 receives the navigator k-space data segments corresponding to each of the four k-space data segments including the lines of segment S 1 . Side segment navigator determination component 610 compares respective navigator images generated from each of the four k-space data segments to the navigator image generated from center navigator k-space data segment N S3M3 and determines at S 260 that the navigator image generated from navigator k-space data segment N S1M4 is most similar to the navigator image generated from center navigator k-space data segment N S3M3 . At S 270 , k-space data segment S 1 M4 is determined to correspond to the navigator image generated from navigator k-space data segment N S1M4 . Accordingly, k-space data segment S 1 M4 is associated with the lines of the first segment of cumulative k-space 520 . FIG. 7 similarly illustrates S 260 and S 270 with respect to the last segment in the PE direction, denoted herein as segment S 5 . Side segment navigator determination component 610 receives the navigator k-space data segments corresponding to each of the four navigator k-space data segments including the lines of segment S 5 . At S 260 , side segment navigator determination component 610 compares respective navigator images generated from each of the four k-space data segments to the navigator image generated from center navigator k-space data segment N S3M3 and determines that the navigator image generated from navigator k-space data segment N S5M3 is most similar to the navigator image generated from center navigator k-space data segment N S3M3 . Component 610 determines that k-space data segment S 5 M3 corresponds to the navigator image generated from navigator k-space data segment N S5M3 at S 270 . K-space data segment S 5 M3 is therefore associated with the lines of segment S 5 of cumulative k-space 520 . S 260 and S 270 are performed for each side (i.e., non-center) k-space data segment, resulting in one k-space data segment being determined for each segment of k-space as shown in FIG. 8 . Next, at S 280 , an image is generated based on the determined k-space data segments. As described above, the image may exhibit high quality despite the acquisition of k-space data segments during free-breathing. FIGS. 9A and 9B comprise a flow diagram of process 900 to perform S 240 according to some embodiments. Process 900 may therefore be executed to determine a first center navigator k-space data segment associated with a most common respiratory position based on respective navigator images generated from each of a plurality of navigator k-space data segments. Initially, at S 910 , a first composite navigator image is determined based on the navigator images generated at S 220 . According to some embodiments, the first composite navigator image is determined by calculating the complex-valued mean of the set of navigator images acquired by each receiver coil. The complex-valued mean images separated by receiver coil may then be combined into a single absolute-valued image by summing, for each pixel, the squares of the complex-valued navigator images of each receiver coil and taking the square root of this sum. In another embodiment, the complex-valued navigator k-space data of all navigator data acquired by each receiver coil is averaged. For each receiver coil, this average is Fourier-transformed into a complex-valued image and these receiver coil images are combined using the above-described root-sum-of-squares process create an absolute-valued first composite navigator image. The first composite navigator image may be determined using any other algorithm which generates a navigator image based on the plurality of navigator images. FIG. 10 illustrates generation of a composite navigator image according to some embodiments. Twenty (i.e., N=5, M=4) acquired navigator k-space data segments 1010 are input to image generation component 1020 , which generates a respective navigator image (i.e., NI S#M# ) 1030 from each of navigator k-space data segments 1010 . Composite image generation component 1040 operates on navigator images 1030 to generate first composite navigator image NI C1 . A difference image between each generated navigator image and the first composite navigator image is determined at S 915 . For example, the first composite navigator image may be subtracted from each generated navigator image to result in a difference image corresponding to each generated navigator image. Next, at S 920 , the absolute values of the pixels of each difference image are summed to determine a difference value for each generated navigator image. The determined differences are assigned to bins of a histogram at S 925 . FIG. 11 illustrates S 915 -S 925 according to some embodiments. Difference calculation component 1110 subtracts first composite navigator image NI C1 from each of navigator images 1030 to generate a difference image (not shown) corresponding to each navigator image 1030 . Difference calculation component 1110 then sums the absolute values of the pixels of each difference image to determine a difference value D 1120 for each navigator image 1030 . FIG. 11 also shows histogram 1130 including nineteen bins. Each bin represents an exclusive range of difference values. After S 925 , each bin maintains a count (represented by a dot) of the difference values 1120 which fall within the range of the bin. The bin having the highest count after S 925 will be referred to as most common bin A. The navigator images with difference values in most common bin A are determined at S 930 . In the case of histogram 1130 , most common bin A, including four difference values 1120 , is bin 1135 . This bin may be referred to as the mode bin, and it represents the most-frequent respiratory position relative to the first composite navigator image, which may be the mean navigator image, i.e., the mean respiratory position. The mean respiratory position may be close but not identical to the most common respiratory position because respiratory motion is non-linear. S 930 therefore consists of determining the navigator images 1030 whose difference values 1120 are assigned to bin 1135 , in this example four navigator images, and consequently better approximate the most common respiratory position. These four navigator images 1030 may be associated with any acquired k-space data segments. A second composite navigator image is determined at S 935 . The second composite navigator image is determined based on the navigator images having difference values in most common bin A. FIG. 12 shows four navigator images 1210 which were determined at S 930 as having difference values in bin 1135 . The subscripts of navigator images 1210 do not conform to the above-used numbering convention in order to indicate that the navigator images determined at S 930 might be associated with any of the acquired segments. Composite image generation component 1240 operates on navigator images 1210 at S 935 to generate second composite navigator image NI C2 . Component 1240 may generate second composite navigator image NI C2 in the same manner as component 1040 generated first composite navigator image NI C1 , but embodiments are not limited thereto. A second difference image between each generated navigator image and the second composite navigator image is determined at S 940 . The absolute values of the pixels of each of the second difference images are summed to determine a second difference value for each generated navigator image at S 945 . Next, at S 950 , the determined differences are assigned to bins of a histogram. FIG. 13 illustrates S 940 -S 950 according to some embodiments. Difference calculation component 1310 subtracts second composite navigator image NI C2 from each of navigator images 1030 to generate a difference image corresponding to each navigator image 1030 , and then sums the absolute values of the pixels of each difference image to determine a second difference value D 2 1320 for each of navigator images 1030 . Histogram 1330 includes nineteen bins representing exclusive ranges of difference values. Each of difference values 1320 is assigned at S 950 to a bin of histogram 1330 whose range includes the difference value. The dot shown within each bin indicates a number of difference values 1320 which fall within the range of the bin. The most common bin of histogram 1330 (i.e., most common bin B) is determined at S 955 . According to FIG. 13 , bin 1335 is the most common bin of histogram 1330 and includes five difference values 1320 . Most common bin B (i.e., bin 1335 ) may typically correspond to the most common respiratory position to a closer degree than most common bin A (i.e., bin 1135 ). As in the present example, the number of difference values within most common bin B may be greater than the number of difference values within most common bin A and the range of difference values of most common bin B may be lower in magnitude than the range of difference values of most common bin A, both of which may indicate that most common bin B more accurately reflects the most common respiratory position than most common bin A. A first center navigator image associated with a first respiratory position is determined based on most common bin B and the second difference values associated with each of the center navigator images at S 960 . For example, stars 1350 - 1356 indicate the second difference values corresponding to each of the four center navigator images generated from the four acquired k-space data segments which include the center line of k-space. The first center navigator image determined at S 960 is the one of these four center navigator images represented by star 1350 whose second difference value is closest to the center value of most common bin B 1335 . Flow then continues to S 250 of process 200 . S 260 of process 200 involves determination, for each non-center k-space data segment, of a respective navigator image most similar to the first center navigator image. According to some embodiments, such similarity may be determined by calculating the displacement fields between the pixels of each respective navigator image and the pixels of the first center navigator image. Again, the displacement fields may be calculated only between pixels within a region of interest. As known in the art, a displacement field represents the displacement of each pixel as a displacement pixel, which is a vector. To create a single value analogous to the “sum of the absolute values of the pixels”, all vectors within a displacement field are added to result in a single vector having an x-component and a y-component, such as (S x , S y ). The procedure is repeated for all navigator images associated with a segment, resulting in one vector (S x , S y ) per navigator image. FIG. 14A illustrates a plot of these vectors, where crosses indicate a navigator image corresponding to a side segment and circled crosses indicate a navigator image corresponding to a center segment. Embodiments may reduce two-dimensional metric of image similarity to a single dimension (akin to a sum of absolute pixel values). First, the main direction of motion is determined by applying a least squares polynomial fit of first order to all vectors (S x , S y ) to generate straight line 1410 . Next, all vectors (S x , S y ) are rotated about the intersection of line 1410 and the y-axis, by the negative of the angle α between the x-axis and line 1410 . This rotation yields transformed vector sums (S T x , S T y ) shown in FIG. 14B . seen in diagram 8011 for which the main motion direction 8015 occurs along the x-axis. The x-component of each transformed vector (S T x , S T y ) is then used to determine the similarity between the navigator image associated with the transformed vector and the first center navigator image, in a manner analogous to the above-described use of the “sum of the absolute pixel values” to determine image similarity. Unlike a sum of absolute values, the x-component of each transformed vector (S T x , S T y ) may be negative, zero, or positive. This one-dimensional displacement metric is used instead of the difference values described with respect to process 900 . Specifically, S 915 and S 920 may be replaced with a determination of a one-dimensional displacement metric for each generated navigator image and indicative of displacement between the generated navigator image and the first composite navigator image. Similarly, S 940 and S 945 may be replaced with a determination of a one-dimensional displacement metric for each generated navigator image and indicative of displacement between the generated navigator image and the second composite navigator image. These one-dimensional displacement metrics may be binned at S 925 and S 950 as described with respect to the difference values, although the ranges of the bins will differ accordingly. FIG. 15 is a flow diagram of process 1500 according to some embodiments. Process 1500 may comprise an embodiment of S 270 and S 280 of process 200 . Prior to S 1510 , S 210 -S 260 may have proceeded according to any of the alternatives mentioned above. A displacement map between each determined respective navigator image and the first center navigator image is determined at S 1510 . The displacement maps may be determined as is known in the art, and the determination may ignore image pixels outside a region of interest. Next, at S 1520 , a k-space is zero-filled for each determined k-space data segment and for a center k-space data segment associated with the first center navigator image. Zero-filling may comprise creating a complete k-space for each data segment, where the lines of the k-space which are not included in the data segment are populated with zero values. A complex-valued image is generated from each zero-filled k-space data segment at S 1530 . At S 1540 , motion correction is applied to each generated image based on the displacement map determined for the navigator image corresponding to the k-space data segment of the generated image. FIG. 16 illustrates execution of S 1510 to S 1540 for a single navigator image according to some embodiments. Displacement calculation component 1610 calculates displacement map D S1 between navigator image N S1M4 determined at S 260 and first center navigator image N S3M3 at S 1510 . K-space S 1 M4ZF is zero-filled at S 1520 except for the k-space data of corresponding k-space data segment I S1M4 . Next, at S 1530 , image generation component 1620 generates image I S1 from zero-filled k-space S 1 M4ZF , for example using an FFT. Motion correction component 1630 applies motion correction to image I S1 based on displacement map D S1 , which was determined for navigator image N S1M4 corresponding to k-space data segment I S1M4 of image I S1 . As mentioned, a k-space is zero-filled at S 1520 for each determined k-space data segment and for a center k-space data segment associated with the first center navigator image. FIG. 17 illustrates processing of center k-space data segment at S 1520 and S 1530 according to some embodiments. K-space S 3 M3ZF is zero-filled at S 1520 except for the k-space data of center k-space data segment S 3 M3 . Image generation component 1620 then generates image I S3 from zero-filled k-space S 3 M3ZF . Image I S3 is not subjected to motion correction according to some embodiments. Continuing with process 1500 , a complex-valued image is generated at S 1550 based on the motion-corrected complex-valued images and the complex-valued image generated from the center k-space data segment. The complex-valued images may be added together or combined in any other suitable manner. Adding together the motion corrected images to create one final high-quality image is suitable if the images to be motion-corrected are complex-valued, the motion-corrected images are complex-valued, and the image generated from the center k-space data segment is complex-valued. FIG. 18 illustrates a Phase-Sensitive Inversion Recovery (PSIR) pulse sequence to acquire IR-prepared k-space data segments (i.e., IR SEG) and corresponding navigator k-space data segments (i.e., IR NAV) according to some embodiments. This sequence is similar to the sequence of FIG. 3 but also comprises the acquisition of PSIR reference data (i.e., non-IR SEG) and associated navigator k-space data segments (i.e., non-IR NAV). The PSIR reference data is acquired during RR-intervals in which no preparatory inversion pulse is played out. According to some embodiments, the acquired IR-prepared k-space data segments and corresponding navigator k-space data segments are processed using any alternative described above to generate an IR image. Analogously, the acquired non-IR-prepared k-space data segments and corresponding navigator k-space data segments may be processed using any alternative described above to generate a PSIR reference image. The IR image and the PSIR reference image may be passed to a known PSIR reconstruction algorithm to produce a PSIR image. Motion correction may be applied to correct the PSIR reference image relative to the IR image prior to application of the PSIR reconstruction algorithm. FIG. 20 illustrates MR system 1 according to some embodiments. MR system 1 includes MR chassis 2 , which defines bore 3 in which patient 4 is disposed. MR chassis 2 includes polarizing main magnet 5 , gradient coils 6 and RF coil 7 arranged about bore 3 . According to some embodiments, polarizing main magnet 5 generates a uniform main magnetic field (B 0 ) and RF coil 7 emits an excitation field (B 1 ). According to MR techniques, a substance (e.g., human tissue) is subjected to a main polarizing magnetic field (i.e., B 0 ), causing the individual magnetic moments of the nuclear spins in the substance to process about the polarizing field in random order at their characteristic Larmor frequency, in an attempt to align with the field. A net magnetic moment is produced in the direction of the polarizing field, and the randomly-oriented magnetic components in the perpendicular plane (the x-y plane) cancel out one another. The substance is then subjected to an excitation field (i.e., B 1 ) created by emission of a radiofrequency (RF) pulse, which is in the x-y plane and near the Larmor frequency, causing the net aligned magnetic moment to rotate into the x-y plane so as to produce a net transverse magnetic moment M t , which is rotating, or spinning, in the x-y plane at the Larmor frequency. The excitation field is terminated, and signals are emitted by the excited spins as they return to their pre-excitation field state. The emitted signals are detected, digitized and processed to reconstruct an image or a spectrum using one of many well-known MR techniques. Gradient coils 6 produce magnetic field gradients G x , G y , and which are used for position-encoding NMR signals. The magnetic field gradients G x , G y , and distort the main magnetic field in a predictable way so that the Larmor frequency of nuclei within the main magnetic field varies as a function of position. Accordingly, an excitation field B 1 which is near a particular Larmor frequency will tip the net aligned moment of those nuclei located at field positions which correspond to the particular Larmor frequency, and signals will be emitted only by those nuclei after the excitation field B 1 is terminated. Gradient coils 6 may consist of three windings, for example, each of which is supplied with current by an amplifier 8 a - 8 c in order to generate a linear gradient field in its respective Cartesian direction (i.e., x, y, or ). Each amplifier 8 a - 8 c includes a digital-analog converter 9 a - 9 c which is controlled by a sequence controller 10 to generate desired gradient pulses at prescribed times. Sequence controller 10 also controls the generation of RF pulses by RF system 11 and RF power amplifier 12 . RF system 11 and RF power amplifier 12 are responsive to a scan prescription and direction from sequence controller 10 to produce RF pulses of the desired frequency, phase, and pulse amplitude waveform. The generated RF pulses may be applied to the whole of RF coil 7 or to one or more local coils or coil arrays. RF coil 7 converts the RF pulses emitted by RF power amplifier 12 , via multiplexer 13 , into a magnetic alternating field in order to excite the nuclei and align the nuclear spins of the object to be examined or the region of the object to be examined. As mentioned above, RF pulses may be emitted in a magnetization preparation step in order to enhance or suppress certain signals. The RF pulses are represented digitally as complex numbers. Sequence controller 10 supplies these numbers in real and imaginary parts to digital-analog converters 14 a - 14 b in RF system 11 to create corresponding analog pulse sequences. Transmission channel 15 modulates the pulse sequences with a radio-frequency carrier signal having a base frequency corresponding to the resonance frequency of the nuclear spins in the volume to be imaged. RF coil 7 both emits radio-frequency pulses as described above and scans the alternating field which is produced as a result of processing nuclear spins, i.e., the nuclear spin echo signals. The received signals are received by multiplexer 13 , amplified by RF amplifier 16 and demodulated in receiving channel 17 of RF system 11 in a phase-sensitive manner. Analog-digital converters 18 a and 18 b convert the demodulated signals into digitized real and imaginary components. Electrocardiogramonitor 19 acquires ECG signals from electrodes placed on patient 4 . Such physiological signals may be used by sequence controller 10 to synchronize, or “gate”, transmitted RF pulses of a spectroscopy pulse sequence based on the heartbeat of patient 4 as described herein. Computing system 30 receives the digitized real and imaginary components from analog-digital converters 18 a and 18 b and may process the components according to known techniques. Such processing may, for example, include reconstructing two-dimensional or three-dimensional images by performing a Fourier transformation of raw k-space data, performing other image reconstruction techniques such as iterative or back-projection reconstruction techniques, applying filters to raw k-space data or to reconstructed images, generating functional magnetic resonance images, calculating motion or flow images, and generating a chemical shift vs. magnitude spectrum. System 30 may comprise any general-purpose or dedicated computing system. Accordingly, system 30 includes one or more processing units 31 (e.g., processors, processor cores, execution threads, etc.) configured to execute processor-executable program code to cause system 30 to operate as described herein, and storage device 32 for storing the program code. Storage device 32 may comprise one or more fixed disks, solid-state random access memory, and/or removable media (e.g., a thumb drive) mounted in a corresponding interface (e.g., a USB port). One or more processing units 31 may execute program code of control program 33 to provide instructions to sequence controller 10 via MR system interface 34 . For example, sequence controller 10 may be instructed to initiate a desired pulse sequence of pulse sequences 37 . In particular, sequence controller 10 may be instructed to control the switching of magnetic field gradients via amplifiers 8 a - 8 c at appropriate times, the transmission of radio-frequency pulses having a specified phase and amplitude at specified times via RF system 11 and RF amplifier 12 , and the readout of the resulting MR signals. The timing of the various pulses of a pulse sequence may be based on physiological data received by ECG monitor interface 36 . Storage device 32 stores MR images 38 generated as described herein. Such images may be provided to terminal 40 via terminal interface 35 of system 30 . Terminal interface 35 may also receive input from terminal 40 , which may be used to provide commands to control program 33 in order to acquire k-space data segments and generate images as described herein. Terminal 40 may comprise a display device and an input device coupled to system 30 . In some embodiments, terminal 40 is a separate computing device such as, but not limited to, a desktop computer, a laptop computer, a tablet computer, and a smartphone. Each element of system 1 may include other elements which are necessary for the operation thereof, as well as additional elements for providing functions other than those described herein. Storage device 32 may also store data and other program code for providing additional functionality and/or which are necessary for operation of system 30 , such as device drivers, operating system files, etc. Executable program code according to the above description may be stored on a form of non-transitory computer-readable media. Computer-readable media includes volatile and nonvolatile, removable, and non-removable media implemented in any method or technology for storage of information such as program code, data structures, program modules or other data. Computer-readable media includes, but is not limited to, random access memory (RAM), read-only memory (ROM), electrically erasable programmable ROM (EEPROM), flash memory or other memory technology, compact disk ROM (CD-ROM), digital volatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired instructions and which may be accessed by a system (e.g., a computer), including by internet or other computer network form of access. The foregoing diagrams represent logical architectures for describing processes according to some embodiments, and actual implementations may include more or different components arranged in other manners. Other topologies may be used in conjunction with other embodiments. Moreover, each component or device described herein may be implemented by any number of devices in communication via any number of other public and/or private networks. Two or more of such computing devices may be located remote from one another and may communicate with one another via any known manner of network(s) and/or a dedicated connection. Each component or device may comprise any number of hardware and/or software elements suitable to provide the functions described herein as well as any other functions. For example, any computing device used in an implementation of a system according to some embodiments may include a processor to execute program code such that the computing device operates as described herein. Embodiments described herein are solely for the purpose of illustration. Those in the art will recognize other embodiments may be practiced with modifications and alterations to that described above.",en,PATENT_APPLICATION
115-013-300-413-986,US,20240384204,A1,2024-11-21,US_20240384204_A1_20241121,en,US,20240384204,A1,2024-11-21,US,18605918,2024-03-15,COMPOSITIONS WITH REDUCED SOLID HYDROXIDE ALKALINITY FOR EFFECTIVE REMOVAL OF PROTEIN SOILS AND ENZYME STABILITY THROUGHOUT MULTI-CYCLE DISPENSING,en,US,ECOLAB USA INC.,"Saint Paul, MN",US,Victor Fuk-Pong Man,"Saint Paul, MN",US,1,"Clinton Hunt, JR.","Saint Paul, MN",US,2,Mark Schomers,"Saint Paul, MN",US,3,Wendy Lo,"Saint Paul, MN",C11D3/386,I,F,C11D3/04,I,L,C11D3/06,I,L,C11D3/386,I,F,C11D3/044,I,L,C11D3/06,I,L,US,20240384204,A1,2024-11-21,115-013-300-413-986,1,US,20240384204,A1,2024-11-21,115-013-300-413-986,1,UNKNOWN,"Alkaline cleaning compositions using a detersive combination of surfactants, chelants and enzymes with reduced alkalinity, including between about 20-47% by percent Na 2 O in the liquid or solid compositions, are disclosed. Methods of using the compositions with optimal protein removal performance that overcome limitations of reduced alkalinity content for use in cleaning, rinsing, sanitizing, and disinfecting are disclosed along with methods that stabilize enzymes for multi-cycle dispensing in solid compositions.",en,"1 . A composition comprising: alkalinity source comprising an alkali metal hydroxide, alkali metal carbonate and/or reagents comprising an organic molecule having at least one hydroxyl group or an alkylene carbonate; a first surfactant comprising a first reverse EO/PO block copolymer of about 10-40% EO; an optional second surfactant comprising at least one of a second reverse EO/PO block copolymer of about 40-50% EO, an alkyl capped alcohol ethoxylate, a capped block copolymer, and alkyl pyrrolidone; a strong chelating agent having a stability constant with calcium that is at least about 1×10 7 ; and a protease enzyme, wherein the composition is a liquid or solid and has between from about 20-47% total alkalinity as measured by percent Na 2 O in the composition.","2 . The composition of claim 1 , wherein the first reverse EO/PO block copolymer is about 20% EO and the second reverse EO/PO block copolymer is about 40% EO.","3 . The composition of claim 1 , wherein the first surfactant comprises from about 0.1 wt-% to about 5 wt-% of the composition and/or wherein the second surfactant(s) comprises from about 0.1 wt-% to about 20 wt-% of the composition.","4 . The composition of claim 1 , wherein the second surfactant is the alkyl capped alcohol ethoxylate having the structure: R 1 —O—(CH 2 CH 2 O) n —R 2 where R 1 is a linear or branched (C 10 -C 18 ) alkyl group, R 2 is C 1 -C 4 , and n is an integer in the range of 1 to 100, or wherein the alkyl capped alcohol ethoxylate is a butyl capped alcohol ethoxylate.","5 . The composition of claim 1 , wherein the second surfactant is the alkyl pyrrolidone is C8 or C10 alkyl pyrrolidone.","6 . The composition of claim 1 , wherein the chelant is selected from the group consisting of NTA, EDTA, DTPA TTHA, MGDA, GLDA, and a ternary polymer comprising acrylic acid/maleic acid/ATBS.","7 . The composition of claim 1 , wherein the protease enzyme is Esperase 6.0T.","8 . The composition of claim 1 , further comprising a hydrotrope and/or water conditioning polymer.","9 . The composition of claim 1 , wherein the alkalinity source comprises alkali metal carbonate and/or alkali metal hydroxide, and wherein the wt-% of the alkali metal carbonate exceeds the wt-% of the alkali metal hydroxide.","10 . The composition of claim 9 , further comprising an alkylene carbonate that is glycerin carbonate, ethylene carbonate, propylene carbonate, or butylene carbonate, and wherein the molar ratio of the initial alkali metal hydroxide to alkylene carbonate combined to make the solid composition is from about 0.5:1 to about 10:1.","11 . The composition of claim 1 , wherein the composition is free of solid alkali metal hydroxide beads, or wherein the composition has at least about 50% less solid alkali metal hydroxide beads compared to a caustic beads containing solid composition that does not contain a polyol or the alkylene carbonate forming the solid composition.","12 . The composition of claim 1 , wherein the alkalinity source comprises a polyol comprising glycol, glycerin or sorbitol and alkali metal hydroxide, and wherein the weight ratio of the alkali metal hydroxide to water in the solid composition is from about 27:73 to about 75:25.","13 . The composition of claim 12 , further comprising water, and wherein the wherein the molar ratio of alkali metal hydroxide to water in the solid is about 1:2.2 to about 1:1.","14 . The composition of claim 1 , wherein the solid composition is formed in-situ and/or wherein the solid composition is contiguous solid that is a pressed, cast, or extruded solid, powder or granule.","15 . The composition of claim 14 , wherein the solid composition is a multi-use composition and the protease enzymes is stabilized therein as measured by enzyme surviving the multi-use dispensing.","16 . The composition of claim 1 , wherein alkalinity source(s) comprises from about 20 wt-% to about 80 wt-%, the chelant comprises from about 2 wt-% to about 15 wt-%, the enzyme comprises from about 1 wt-% to about 5 wt-%, and wherein the first surfactant comprising the reverse EO/PO block copolymer having 10-40% EO comprises from about 0.1 wt-% to about 5 wt-%, and wherein the second surfactant(s) comprise from about 0.1 wt-% to about 20 wt-% of the composition.","17 . A method of use comprising: generating a use solution of the composition according to claim 1 ; contacting an article or surface in need of soil removal with the use solution; and cleaning to remove the soil from the article or surface, wherein the soil comprises protein.","18 . The method of claim 17 , wherein the use solution is between about 50 ppm to about 5,000 ppm of the solid composition, and/or wherein the step of generating a use solution is by diluting a multi-use solid composition and wherein the protease enzyme survives the multi-use dispensing.","19 . The method of claim 17 , wherein the use solution is applied in a ware washing machine and optionally the use solution is contact with the articles of ware therein at a temperature range of about 120-180° F., or wherein the use solution is applied in a laundry or textile care washing machine.","20 . The method of claim 17 , wherein the surface is a hard surface, instrument or ware.",en,"CROSS REFERENCE TO RELATED APPLICATIONS This application claims priority under 35 U.S.C. § 119 to both provisional patent applications U.S. Ser. No. 63/502,259, filed May 15, 2023, titled “COMPOSITIONS WITH REDUCED SOLID HYDROXIDE ALKALINITY FOR EFFECTIVE REMOVAL OF PROTEIN SOILS”, and U.S. Ser. No. 63/607,875, filed Dec. 8, 2023, titled “GLYCEROL CARBONATE BASED SOLID COMPOSITIONS WITH ENZYME STABILITY THROUGHOUT MULTI-CYCLE DISPENSING.” The provisional patent applications are herein incorporated by reference in their entirety, including without limitation, the specification, claims, and abstract, as well as any figures, tables, appendices, or drawings thereof. TECHNICAL FIELD The present disclosure relates to the field of compositions for various alkaline cleaning applications using a detersive combination of surfactants, chelants and enzymes. Beneficially, the compositions provide optimal protein removal performance that overcome limitations of reduced alkalinity content of the compositions, including between about 20-47% by percent Na 2 O in the liquid or solid compositions, for use in cleaning, rinsing, sanitizing, and disinfecting. The compositions further beneficially stabilize enzymes for multi-cycle dispensing in solid compositions. BACKGROUND Alkali metal hydroxides, commonly referred to as caustic, are commonly sold in solid form (e.g. pellets, flakes, blocks) and are frequently used in manufacturing processes. The manufacturing of caustic beads is energy intensive and compositions containing caustic beads are generally hygroscopic. Additionally, there are safety concerns surrounding the transportation and handling of other strong bases, such as alkoxides. Despite challenges of using solid caustic and alkoxides there remains advantages to using solid caustic compositions. For example, storing and transporting of solid concentrates are less expensive than the storage and transporting of liquids. There are also fewer safety and stability challenges associated with transporting and handling of solid compositions. Therefore, it is an object of the disclosure to provide alkaline compositions providing alternatives to the purchase of solid caustic to incorporate into an alkaline composition, namely solid alkaline compositions. The use of alkali metal hydroxide compositions for various warewashing and other applications, such as bottle washing, pulp antifoaming applications, and others further requires the use of surfactants to tailor the cleaning capabilities of the caustic compositions. A particular challenge is providing desired defoaming or antifoaming in these compositions. Accordingly, it is an objective of the claimed disclosure to develop solid caustic compositions that provide highly effective defoaming and antifoaming surfactant packages while maintaining efficacy for soil removal. It is further reported in literature that protein macromolecules maintain certain conformation or overall structure in its natural state which are dictated by secondary and tertiary structures. Denaturation disrupts the alpha helix and beta sheets in a protein and uncoil it into a random shape. The most common observation of the denaturation process is the precipitation and coagulation of the protein. Protein soil presents significant challenges in machine warewashing and CIP cleaning as they are difficult to remove and can produce stable foams. Accordingly, it is an objective to develop compositions with detersive combinations of surfactants, chelants and enzymes that can partially denature proteins and provide required defoaming to avoid cavitation in pumping mechanical action. It is desired to provide antifoaming or defoaming surfactant to be surface active enough to penetrate protein stabilized foam lamellae, partially denature the protein to a certain degree to produce proper defoaming, yet without full denaturation to cause precipitation/coagulation of the protein to cause deposition problem. Other objects, advantages and features of the present disclosure will become apparent from the following specification taken in conjunction with the accompanying drawings. SUMMARY The following objects, features, advantages, aspects, and/or embodiments, are not exhaustive and do not limit the overall disclosure. No single embodiment need provide each and every object, feature, or advantage. Any of the objects, features, advantages, aspects, and/or embodiments disclosed herein can be integrated with one another, either in full or in part. According to embodiments compositions comprise: alkalinity source comprising an alkali metal hydroxide, alkali metal carbonate and/or reagents comprising an organic molecule having at least one hydroxyl group or an alkylene carbonate; a first surfactant comprising a first reverse EO/PO block copolymer of about 10-40% EO; and an optional second surfactant comprising at least one of a second reverse EO/PO block copolymer of about 40-50% EO, an alkyl capped alcohol ethoxylate, a capped block copolymer, and alkyl pyrrolidone; a strong chelating agent having a stability constant with calcium that is at least about 1×10 7 ; and a protease enzyme, wherein the composition is a liquid or solid and has between from about 20-47%, from about 25-40%, or preferably from about 28-37% total alkalinity as measured by percent Na 2 O in the composition. According to additional embodiments methods of generating a use solution of the compositions described herein comprise: contacting an article or surface in need of soil removal with the use solution; and cleaning to remove the soil from the article or surface, wherein the soil comprises protein. While multiple embodiments are disclosed, still other embodiments will become apparent to those skilled in the art from the following detailed description, which shows and describes illustrative embodiments. Accordingly, the drawings and detailed description are to be regarded as illustrative in nature and not restrictive. BRIEF DESCRIPTION OF THE DRAWINGS The patent or application file contains at least one drawing executed in color. Copies of this patent or patent application publication with color drawing(s) will be provided by the Office upon request and payment of the necessary fec. FIG. 1 shows a graph of enzymatic activity evaluated in dispensing run off testing as described in Example 2. FIG. 2 shows an image of ceramic tiles treated with Composition 8 compared to Control after a 10-Cycle machine warewash experiment as described in Example 3. FIG. 3 shows an image of ceramic tiles treated with Composition 8 compared to Control after a 10-Cycle machine warewash experiment as described in Example 3. FIG. 4 shows an image of glassware treated with Composition 8 compared to Control demonstrating both enzyme stability and the improved performance as described in Example 4. FIG. 5 shows a graph of enzymatic activity evaluated in dispensing run off testing as described in Example 5. Various embodiments will be described in detail with reference to the drawings, wherein like reference numerals represent like parts throughout the several views. Reference to various embodiments does not limit the scope of the invention. Figures represented herein are not limitations to the various embodiments according to the invention and are presented for exemplary illustration of the invention. DETAILED DESCRIPTION The embodiments of this disclosure are not limited to particular compositions, methods of making and/or methods of employing the same, which can vary and are understood by skilled artisans. So that the disclosure may be more readily understood, certain terms are first defined. It is further to be understood that all terminology used herein is for the purpose of describing particular embodiments only, and is not intended to be limiting in any manner or scope. For example, as used in this specification and the appended claims, the singular forms “a,” “an” and “the” can include plural referents unless the content clearly indicates otherwise. Further, all units, prefixes, and symbols may be denoted in its SI accepted form. Numeric ranges recited within the specification are inclusive of the numbers defining the range and include each integer within the defined range. Throughout this disclosure, various aspects of this disclosure are presented in a range format. It should be understood that the description in range format is merely for convenience and brevity and should not be construed as an inflexible limitation on the scope of the disclosure. Accordingly, the description of a range should be considered to have specifically disclosed all the possible sub-ranges, fractions, and individual numerical values within that range. As used herein, the term “and/or”, e.g., “X and/or Y” shall be understood to mean either “X and Y” or “X or Y” and shall be taken to provide explicit support for both meanings or for either meaning, e.g. A and/or B includes the options i) A, ii) B or iii) A and B. Unless defined otherwise, all technical and scientific terms used above have the same meaning as commonly understood by one of ordinary skill in the art to which embodiments of the present disclosure pertain. The term “about,” as used herein, refers to variation in the numerical quantity that can occur, for example, through typical measuring techniques and equipment, with respect to any quantifiable variable, including, but not limited to, concentration, mass, volume, time, molecular weight, temperature, pH, humidity, molar ratios, log count of bacteria or viruses, and the like. Further, given solid and liquid handling procedures used in the real world, there is certain inadvertent error and variation that is likely through differences in the manufacture, source, or purity of the ingredients used to make the compositions or carry out the methods and the like. The term “about” also encompasses these variations. Whether or not modified by the term “about,” the claims include equivalents to the quantities. The term “actives” or “percent actives” or “percent by weight actives” or “actives concentration” are used interchangeably herein and refers to the concentration of those ingredients involved in cleaning expressed as a percentage minus inert ingredients such as water or salts. It is also sometimes indicated by a percentage in parentheses, for example, “chemical (10%).” As used herein, the term “alkoxide” refers to a conjugate base of an organic molecule having one or more hydroxyl groups and can be formed through the deprontonation of the hydroxyl group(s), which is a weak acid/base reaction. Alkoxides can be formed through the reaction of an alkali metal hydroxide and an organic molecule having one or more hydroxyl-groups or an alkylene carbonate as disclosed in U.S. Patent Application No.______, claiming priority to U.S. Ser. No. 63/490,838, filed simultaneously herewith and titled “Alkoxide-Based Solidification Via Control of Reaction Equilibrium and Kinetics”, which is incorporated by reference in its entirety. As used herein, the term “alkyl” or “alkyl groups” refers to saturated hydrocarbons having one or more carbon atoms, including straight-chain alkyl groups (e.g., methyl, ethyl, propyl, butyl, pentyl, hexyl, heptyl, octyl, nonyl, decyl, etc.), cyclic alkyl groups (or “cycloalkyl” or “alicyclic” or “carbocyclic” groups) (e.g., cyclopropyl, cyclopentyl, cyclohexyl, cycloheptyl, cyclooctyl, etc.), branched-chain alkyl groups (e.g., isopropyl, tert-butyl, sec-butyl, isobutyl, etc.), and alkyl-substituted alkyl groups (e.g., alkyl-substituted cycloalkyl groups and cycloalkyl-substituted alkyl groups). Unless otherwise specified, the term “alkyl” includes both “unsubstituted alkyls” and “substituted alkyls.” As used herein, the term “substituted alkyls” refers to alkyl groups having substituents replacing one or more hydrogens on one or more carbons of the hydrocarbon backbone. Such substituents may include, for example, alkenyl, alkynyl, halogeno, hydroxyl, alkylcarbonyloxy, arylcarbonyloxy, alkoxycarbonyloxy, aryloxy, aryloxycarbonyloxy, carboxylate, alkylcarbonyl, arylcarbonyl, alkoxycarbonyl, aminocarbonyl, alkylaminocarbonyl, dialkylaminocarbonyl, alkylthiocarbonyl, alkoxyl, phosphate, phosphonato, phosphinato, cyano, amino (including alkyl amino, dialkylamino, arylamino, diarylamino, and alkylarylamino), acylamino (including alkylcarbonylamino, arylcarbonylamino, carbamoyl and ureido), imino, sulfhydryl, alkylthio, arylthio, thiocarboxylate, sulfates, alkylsulfinyl, sulfonates, sulfamoyl, sulfonamido, nitro, trifluoromethyl, cyano, azido, heterocyclic, alkylaryl, or aromatic (including heteroaromatic) groups. In some embodiments, substituted alkyls can include a heterocyclic group. As used herein, the term “heterocyclic group” includes closed ring structures analogous to carbocyclic groups in which one or more of the carbon atoms in the ring is an element other than carbon, for example, nitrogen, sulfur or oxygen. Heterocyclic groups may be saturated or unsaturated. Exemplary heterocyclic groups include, but are not limited to, aziridine, ethylene oxide (epoxides, oxiranes), thiirane (episulfides), dioxirane, azetidine, oxetane, thietane, dioxetane, dithietane, dithiete, azolidine, pyrrolidine, pyrroline, oxolane, dihydrofuran, and furan. As used herein, the term “analog” means a molecular derivative of a molecule. The term is synonymous with the terms “structural analog” or “chemical analog.” As used herein, the term “antimicrobial” refers to a compound or composition that reduces and/or inactivates a microbial population, including, but not limited to bacteria, viruses, fungi, and algae within about 10 minutes or less, about 8 minutes or less, about 5 minutes or less, about 3 minutes or less, about 2 minutes or less, about 1 minute or less, or about 30 seconds or less. Preferably, the term antimicrobial refers to a composition that provides at least about a 3-log, 3.5 log, 4 log, 4.5 log, or 5 log reduction of a microbial population in about 10 minutes or less, about 8 minutes or less, about 5 minutes or less, about 3 minutes or less, about 2 minutes or less, about 1 minute or less, or about 30 seconds or less. As used herein, the term “cleaning” refers to a method used to facilitate or aid in soil removal, bleaching, microbial population reduction, and any combination thereof. As used herein, the term “microorganism” refers to any noncellular or unicellular (including colonial) organism. Microorganisms include all prokaryotes. Microorganisms include bacteria (including cyanobacteria), spores, lichens, fungi, protozoa, virinos, viroids, viruses, phages, and some algae. As used herein, the term “microbe” is synonymous with microorganism. As used herein, the term “disinfectant” refers to an agent that kills all vegetative cells including most recognized pathogenic microorganisms, using the procedure described in A.O.A.C. Use Dilution Methods , Official Methods of Analysis of the Association of Official Analytical Chemists, paragraph 955.14 and applicable sections, 15th Edition, 1990 (EPA Guideline 91-2). According to this reference a disinfectant should provide a 99.999% reduction (5-log order reduction) within 30 seconds at room temperature, 25±2° C., against several test organisms. According to embodiments of the disclosure, a disinfecting composition provides a 99.999% reduction (5-log order reduction) of the desired organisms (including bacterial contaminants) at a use temperature. Further, a disinfectant should provide a 99.99% reduction (4-log order reduction) within 30 seconds at room temperature, 25±2° C., against several test organisms. According to embodiments of the disclosure, a disinfecting composition provides a 99.99% reduction (4-log order reduction) of the desired organisms (including bacterial contaminants) at a use temperature. Further, a disinfectant should provide a 99.9% reduction (3-log order reduction) within 30 seconds at room temperature, 25±2° C., against several test organisms. According to embodiments of the disclosure, a disinfecting composition provides a 99.9% reduction (3-log order reduction) of the desired organisms (including bacterial contaminants) at a use temperature. As used herein, the term “high level disinfection” or “high level disinfectant” refers to a compound or composition that kills substantially all organisms, except high levels of bacterial spores, and is effected with a chemical germicide cleared for marketing as a sterilant by the Food and Drug Administration. As used herein, the term “intermediate-level disinfection” or “intermediate level disinfectant” refers to a compound or composition that kills mycobacteria, most viruses, and bacteria with a chemical germicide registered as a tuberculocide by the Environmental Protection Agency (EPA). As used herein, the term “low-level disinfection” or “low level disinfectant” refers to a compound or composition that kills some viruses and bacteria with a chemical germicide registered as a hospital disinfectant by the EPA. As used herein, the term “exemplary” refers to an example, an instance, or an illustration, and does not indicate a most preferred embodiment unless otherwise stated. As used herein, the phrase “food processing surface” refers to a surface of a tool, a machine, equipment, a structure, a building, or the like that is employed as part of a food processing, preparation, or storage activity. Examples of food processing surfaces include surfaces of food processing or preparation equipment (e.g., slicing, canning, or transport equipment, including flumes), of food processing wares (e.g., utensils, dishware, wash ware, and bar glasses), and of floors, walls, or fixtures of structures in which food processing occurs. Food processing surfaces are found and employed in food anti-spoilage air circulation systems, aseptic packaging sanitizing, food refrigeration and cooler cleaners and sanitizers, ware washing sanitizing, blancher cleaning and sanitizing, food packaging materials, cutting board additives, third-sink sanitizing, beverage chillers and warmers, meat chilling or scalding waters, autodish sanitizers, sanitizing gels, cooling towers, food processing antimicrobial garment sprays, and non-to-low-aqueous food preparation lubricants, oils, and rinse additives. The term “generally” encompasses both “about” and “substantially.” The term “hard surface” refers to a solid, substantially non-flexible surface such as a countertop, tile, floor, wall, panel, window, plumbing fixture, kitchen and bathroom furniture, appliance, engine, circuit board, dish, mirror, window, monitor, touch screen, and thermostat. Hard surfaces are not limited by the material; for example, a hard surface can be glass, metal, tile, vinyl, linoleum, composite, wood, plastic, etc. Hard surfaces may include for example, health care surfaces and food processing surfaces. As used herein, the term “microorganism” refers to any noncellular or unicellular (including colonial) organism. Microorganisms include all prokaryotes. Microorganisms include bacteria (including cyanobacteria), spores, lichens, fungi, protozoa, virinos, viroids, viruses, phages, and some algae. As used herein, the term “microbe” is synonymous with microorganism. As used herein, the term “oligomer” refers to a molecular complex comprised of between one and ten monomeric units. For example, dimers, trimers, and tetramers, are considered oligomers. Furthermore, unless otherwise specifically limited, the term “oligomer” shall include all possible isomeric configurations of the molecule, including, but are not limited to isotactic, syndiotactic and random symmetries, and combinations thereof. Furthermore, unless otherwise specifically limited, the term “oligomer” shall include all possible geometrical configurations of the molecule. As used herein the term “polymer” refers to a molecular complex comprised of a more than ten monomeric units and generally includes, but is not limited to, homopolymers, copolymers, such as for example, block, graft, random and alternating copolymers, terpolymers, and higher “x”mers, further including their analogs, derivatives, combinations, and blends thereof. Furthermore, unless otherwise specifically limited, the term “polymer” shall include all possible isomeric configurations of the molecule, including, but are not limited to isotactic, syndiotactic and random symmetries, and combinations thereof. Furthermore, unless otherwise specifically limited, the term “polymer” shall include all possible geometrical configurations of the molecule. As used herein, the term “sanitizer” refers to an agent that reduces the number of bacterial contaminants to safe levels as judged by public health requirements. In an embodiment, sanitizers for use in this invention will provide at least a 99.999% reduction (5-log order reduction). These reductions can be evaluated using a procedure set out in Germicidal and Detergent Sanitizing Action of Disinfectants, Official Methods of Analysis of the Association of Official Analytical Chemists, paragraph 960.09 and applicable sections, 15th Edition, 1990 (EPA Guideline 91-2). According to this reference a sanitizer should provide a 99.999% reduction (5-log order reduction) within 30 seconds at room temperature, 25±2° C., against several test organisms. As used herein, the term “soft surface” refers to surfaces not classified as hard surfaces, but which are solid surfaces. Soft surfaces, include, but are not limited to, textiles, fabrics, woven surfaces, and non-woven surfaces. Soft surfaces, include, but are not limited to, carpet, curtains, fabrics, hospital partitions, linens, and upholstery. As used herein, the term “soil” or “stain” refers to any soil, including, but not limited to, non-polar oily and/or hydrophobic substances which may or may not contain particulate matter such as industrial soils, mineral clays, sand, natural mineral matter, carbon black, graphite, kaolin, environmental dust, and/or food based soils such as blood, proteinaceous soils, starchy soils, fatty soils, cellulosic soils, etc. The “scope” of the present disclosure is defined by the appended claims, along with the full scope of equivalents to which such claims are entitled. The scope of the disclosure is further qualified as including any possible modification to any of the aspects and/or embodiments disclosed herein which would result in other embodiments, combinations, subcombinations, or the like that would be obvious to those skilled in the art. The term “substantially” refers to a great or significant extent. “Substantially” can thus refer to a plurality, majority, and/or a supermajority of said quantifiable variable, given proper context. As used herein, the term “substantially free” refers to compositions completely lacking the component or having such a small amount of the component that the component does not affect the performance of the composition. The component may be present as an impurity or as a contaminant and shall be less than 0.5 wt-%. In another embodiment, the amount of the component is less than 0.1 wt-% and in yet another embodiment, the amount of component is less than 0.01 wt-%. The term “surfactant” or “surface active agent” refers to an organic chemical that when added to a liquid changes the properties of that liquid at a surface. As used herein, the term “ware” refers to items such as eating and cooking utensils, dishes, and other hard surfaces such as showers, sinks, toilets, bathtubs, countertops, windows, mirrors, transportation vehicles, and floors. As used herein, the term “warewashing” refers to washing, cleaning, or rinsing ware. Ware also refers to items made of plastic. Types of plastics that can be cleaned with the compositions include but are not limited to, those that include polypropylene polymers (PP), polycarbonate polymers (PC), melamine formaldehyde resins or melamine resin (melamine), acrylonitrile-butadiene-styrene polymers (ABS), and polysulfone polymers (PS). Other exemplary plastics that can be cleaned using the compounds and compositions of the disclosure include polyethylene terephthalate (PET). The term “weight percent,” “wt-%,” “percent by weight,” “% by weight,” and variations thereof, as used herein, refer to the concentration of a substance as the weight of that substance divided by the total weight of the composition and multiplied by 100. It is understood that, as used here, “percent,” “%,” and the like are intended to be synonymous with “weight percent,” “wt-%,” etc. The solid compositions, methods of making the compositions, and methods of use of the present disclosure may comprise, consist essentially of, or consist of the components and ingredients of the present disclosure as well as other ingredients described herein. As used herein, “consisting essentially of” means that the methods and compositions may include additional steps, components or ingredients, but only if the additional steps, components or ingredients do not materially alter the basic and novel characteristics of the claimed methods and compositions. Compositions The compositions employing detersive combinations of surfactants, chelants and enzymes with alkalinity as described herein comprise an alkalinity source, a combination of surfactants including a first surfactant comprising a first reverse EO/PO block copolymer, preferably a reverse EO/PO block copolymer of about 10-40% EO, most preferably a reverse EO/PO block copolymer of about 20% EO, and a second surfactant comprising at least one of a second reverse EO/PO block copolymer of about 40-50% EO, an alkyl capped alcohol ethoxylate, a capped block copolymer, and alkyl pyrrolidone, for protein soil removal, a strong chelating agent, and a protease enzyme. The compositions can be solid or liquid. In aspects, solid compositions are preferred as the compositions overcome limitations in the art of providing stable alkaline solids that do not require use of sodium hydroxide beads. In embodiments the solid compositions can further comprise at least one additional functional ingredient, such as a water conditioning agent, e.g. water conditioning polymers and/or hydrotrope. Exemplary ranges of the compositions according to the disclosure are shown in Tables 1A-1B each in weight percentage. Exemplary ranges of the reagents to make the solid compositions are shown in Table IC where the simplified term “polyol” is used, however also includes the scope of the broader description contained herein of organic molecules having at least one hydroxyl group (i.e. includes polyols). While the components may have a percent actives of 100%, it is noted that Tables 1A-1D do not recite the percent actives of the components, but rather, recites the total weight percentage of the raw materials (i.e. active concentration plus inert ingredients). TABLE 1AFirstSecondThirdExem-Exem-Exem-plaryplaryplaryRangeRangeRangeMaterialwt.-%wt.-%wt.-%First Surfactant0.1-50.5-51-5[Reverse EO/PO block copolymer ofabout 10-40% EO]Second Surfactant(s)0.1-201-201-10[at least one of a second reverse EO/POblock copolymer of about 40-50% EO, analkyl capped alcohol ethoxylate, a cappedblock copolymer, and alkyl pyrrolidone]Alkalinity source(s)20-8025-8030-80Chelant1-202-152-15Enzyme0.1-101-101-5Additional Functional Ingredients0-500-300-25Total100100100 TABLE 1BFirstSecondThirdExem-Exem-Exem-plaryplaryplaryRangeRangeRangeMaterialwt.-%wt.-%wt.-%First Surfactant0.1-50.5-51-5[Reverse EO/PO block copolymer ofabout 10-40% EO]Second Surfactant(s)0.1-201-201-10[at least one of a second reverse EO/POblock copolymer of about 40-50% EO, analkyl capped alcohol ethoxylate, a cappedblock copolymer, and alkyl pyrrolidone]Alkalinity source(s)20-8025-8030-80Chelant1-202-152-15Enzyme0.1-101-101-5Additional Functional Ingredients0-500-300-25Total100100100 TABLE 1CFirstSecondThirdExem-Exem-Exem-plaryplaryplaryRangeRangeRangeMaterialwt.-%wt.-%wt.-%First Surfactant0.1-50.5-51-5[Reverse EO/PO block copolymer ofabout 10-40% EO]Optional Second Surfactant(s)0-200-200-10[at least one of a second reverse EO/POblock copolymer of about 40-50% EO, analkyl capped alcohol ethoxylate, a cappedblock copolymer, and alkyl pyrrolidone]Alkalinity source(s)20-8025-8030-80Chelant1-202-152-15Enzyme0.1-101-101-5Additional Functional Ingredients0-500-300-25Total100100100 TABLE 1DFirstSecondThirdExem-Exem-Exem-plaryplaryplaryRangeRangeRangeMaterialwt.-%wt.-%wt.-%First Surfactant0.1-50.5-51-5[Reverse EO/PO block copolymer ofabout 10-40% EO]Second Surfactant(s)0.1-201-201-10[at least one of a second reverse EO/POblock copolymer of about 40-50% EO, analkyl capped alcohol ethoxylate, a cappedblock copolymer, and alkyl pyrrolidone]Alkali metal hydroxide20-8025-8030-80Polyol or Alkylene CarbonatePolyol1-201-151-10Alkylene Carbonate1-401-205-20Chelant1-202-152-15Enzyme0.1-101-101-5Additional Functional Ingredients0-500-300-25Total100100100 Reverse EO/PO Block Copolymer Reverse EO/PO block copolymers are included in the compositions as disclosed herein. A “reverse” EO/PO block copolymer structure has EO groups on the inside and the PO groups are on the outside, (PO) Y (EO) X (PO) Y wherein EO represents an ethylene oxide group, PO represents a propylene oxide group, and X and Y reflect the average molecular proportion of each alkylene oxide monomer in the overall block copolymer composition. The reverse EO/PO block copolymer surfactants can be linear or branched. Typical reverse block copolymers useful as defoamers have ethoxylation up to about 20% and those useful as wetting agents have ethoxylation between about 20% and 40%. Additionally, reverse block copolymers due not usually exhibit both good wetting and defoaming properties, instead they are typically selected for one or the other based on the degree of ethoxylation. In embodiments, the degree of ethoxylation of the first reverse EO/PO block copolymers included as a first surfactant for protein soil defoaming is about 10-40%, about 20-40%, most preferably about 20%. In embodiments of the first surfactant, the propoxylation of the first reverse EO/PO block copolymers is about 60-90%, about 60-80%, most preferably about 80%. In addition, without being limited according to the disclosure, all ranges recited are inclusive of the numbers defining the range and include each integer within the defined range. Commercially available examples of the reverse EO/PO block copolymers included as a first surfactant for protein soil defoaming include for example PLURONIC 25R2 and SURFONIC LD-097. In a preferred embodiment the first surfactant is a reverse EO/PO block copolymer of about 20-40% EO. In a further preferred embodiment the first surfactant is a reverse EO/PO block copolymer having about 20% EO/80% PO regardless of the number of arms in the surfactant structure. In an embodiment, the ethoxylation of the reverse EO/PO block copolymers included as a second surfactant for protein soil cleaning (i.e. removal) is about 40-50% and can have a linear or branched (i.e. arms) structure. In embodiments of the second surfactant, the propoxylation of the reverse EO/PO block copolymers is about 50-60%. In addition, without being limited according to the disclosure, all ranges recited are inclusive of the numbers defining the range and include each integer within the defined range. Commercially available examples of the EO/PO block copolymers included as a second surfactant for protein soil removal include for example TETRONIC 90R4. In a preferred embodiment the second surfactant is a reverse EO/PO block copolymer having about 40% EO/60% PO. According to various embodiments the reverse EO/PO block copolymer(s) included are included in an amount of about 0.1 wt-% to about 50 wt-%, about 0.5 wt-% to about 20 wt-%, or about 0.5 wt-% to about 2 wt-%. According to various embodiments of the compositions, the reverse EO/PO block copolymers is included as a first surfactant for protein soil defoaming in an amount of about 0.1 wt-% to about 10 wt-%, about 0.1 wt-% to about 5 wt-%, about 0.5 wt-% to about 5 wt-%, or about 0.5 wt-% to about 2 wt-%. According to various embodiments of the compositions, the reverse EO/PO block copolymers is included as a second surfactant for protein soil cleaning in an amount of about 0 wt-% to about 15 wt-%, about 0.5 wt-% to about 15 wt-%, about 1 wt-% to about 10 wt-%, or about 2 wt-% to about 10 wt-%. Alkyl Capped Alcohol Ethoxylate Alkyl capped alcohol ethoxylates can be included in compositions as disclosed herein. Alkyl capped alcohol ethoxylate compounds have the following structure: R 1 —O—(CH 2 CH 2 O) n —R 2 where R 1 is a linear or branched (C 10 -C 18 ) alkyl group, R 2 is C 1 -C 4 , and n is an integer in the range of 1 to 100. In preferred embodiments the alkyl capped alcohol ethoxylate is a butyl capped alcohol ethoxylate, such as for example a lauryl fatty alcohol ethoxylate butyl ether or coconut fatty alcohol ethoxylate butyl ether. Commercially available examples of the alkyl capped alcohol ethoxylates included as a second surfactant for protein soil removal include for example surfactants sold under the tradename DEHYPON LT or GENAPOL BE-2810 and GENAPOL BE-2410. According to various embodiments of the compositions, the alkyl capped alcohol ethoxylate is included as a second surfactant for protein soil defoaming in an amount of about 0 wt-% to about 10 wt-%, about 0.1 wt-% to about 10 wt-%, about 0.1 wt-% to about 5 wt-%, about 0.5 wt-% to about 5 wt-%, or about 0.5 wt-% to about 2 wt-%. Alkyl Pyrrolidone An alkyl pyrrolidone surfactant can be included in compositions as a second surfactant for protein soil removal as disclosed herein. Pyrrolidones are heterocyclic ketones derived from a pyrrolidone. Alkyl pyrrolidones have the general structure wherein R is a C6-C20 alkyl or R 1 NHCOR 2 , wherein R 1 is C1-C6 alkyl and R 2 is C6-C20 alkyl. In embodiments the alkyl pyrrolidone has the general structure shown above wherein R is C8-C10 alkyl pyrrolidone. In preferred embodiments the alkyl pyrrolidone is a C8 or C10 alkyl pyrrolidone. An example of a commercially available C8 alkyl pyrrolidone (1-octyl-2-pyrrolidone) and a C12 alkyl pyrrolidone are available under the tradename SURFADONE®. According to various embodiments of the solid compositions, the alkyl pyrrolidone is included as a second surfactant for protein soil cleaning in an amount of about 0 wt-% to about 15 wt-%, about 0.5 wt-% to about 15 wt-%, about 1 wt-% to about 10 wt-%, or about 2 wt-% to about 10 wt-%. Capped Block Copolymers Capped block copolymers can be included in the surfactant compositions and/or the solid compositions as disclosed herein. Capped block copolymers are disclosed in U.S. Patent Application No.______, claiming priority to U.S. Ser. No. 63/490,857, filed simultaneously herewith and titled “Capped Block Copolymers, Their Synthesis, Manufacture, and Methods of Use”, which is incorporated by reference in its entirety. Preferably, the capped block copolymers are multiarmed. Preferred block copolymers may have from about 1 to about 100 moles of EO and from about 1 to about 100 moles of PO, more preferably from about 1 to about 50 moles EO and from about 1 to about 50 moles PO. Some examples of block copolymers include: -(PO) Y (EO) X -(EO) X (PO) Y -(EO) X (PO) Y (EO) X -(PO) Y (EO) X (PO) Y wherein EO represents an ethylene oxide group, PO represents a propylene oxide group, and X and Y reflect the average molecular proportion of each alkylene oxide monomer in the overall block copolymer composition. A preferred EO/PO copolymer is represented by the formula (EO) X (PO) Y (EO) X . In another embodiment, a preferred EO/PO copolymer is represented by the formula (PO) Y (EO) X (PO) Y . Preferably X is in the range of about 1 to about 100 and Y is in the range of about 1 to about 100. In a more preferred embodiment, X is in the range of about 5 to about 90 and Y is in the range of about 5 to about 90. Preferably, X plus Y is in the range of about 2 to about 200, more preferably about 10 to about 180, still more preferably about 15 to about 150. It should be understood that each X and Y in a molecule can be different. In a preferred embodiment, the block copolymer can have a molecular weight (Mn-number average mw) greater than about 200 and less than about 25,000, more preferably from about 500 to about 25,000, most preferably from about 1000 to about 20,000. In embodiments the capped block copolymers provide the advantage of exhibiting good wetting and defoaming properties; this is highly beneficial as there is typically a trade-off in these properties such that high defoaming properties may come at the expense of wetting properties and high wetting properties may come at the expense of defoaming properties. A preferred embodiment for the capped block copolymer comprises a multiarm block copolymer comprising a polyfunctional moiety and at least 2 alkoxylated arms each of which is selected from the group consisting of -(PO) Y (EO) X , -(EO) X (PO) Y , -(EO) X (PO) Y (EO) X , and -(PO) Y (EO) X (PO) Y ; wherein X is about 1 to about 100, and Y is about 1 to about 100; wherein each of the alkxoylated arms comprises a terminus and is capped with a hydrophobic group at the terminus. These EO/PO block copolymers can include a compact alcohol EO/PO surfactant where the EO and PO groups are in small block form, or random form. In other embodiments, the alkyl alkoxylate includes an ethylene oxide, a propylene oxide, a butylene oxide, a pentalene oxide, a hexylene oxide, a heptalene oxide, an octalene oxide, a nonalene oxide, a decylene oxide, and mixtures thereof. The alkyl group can be linear or branched and is preferably C 1 -C 18 , more preferably C 10 -C 18 ; most preferably it is a branched alkyl group. Exemplary commercially available surfactants are available, for example, under the tradename PLURONIC® and PLURONIC R®, TETRONIC®, and SURFONIC®. In a preferred embodiment, the block copolymer comprises a linear or multiarmed EO/PO structures. Most preferably, the block copolymer is a “reverse” block copolymer with the EO inside and the PO on the terminal end. Non-limiting examples are shown below: where B is an organic molecule with a polyfunctional moieties, such as a polyol, ethylene diamine, or diethylenetriamine, etc, as the starting point where the arms attach. It should be understood that these are not representative of the orientations of the arms, but merely representative of the potential formulations for purposes of illustrating the attachment of the alkoxylated arms to the starting polyfunctional moiety. Further, X and Y are defined further below where the degree of ethoxylation and propoxylation are discussed. The capped block copolymers disclosed herein include what is often referred to as a “reverse” structure, that is, the EO groups are on the inside and the PO groups are on the outside, (PO) Y (EO) X (PO) Y . Typical reverse block copolymers useful as defoamers have ethoxylation up to about 20% and those useful as wetting agents have ethoxylation between about 20% and 40%. However, typical reverse block copolymers for defoamer or wetting are not capped. Additionally, reverse block copolymers due not usually exhibit both good wetting and defoaming properties, instead they are typically selected for one or the other based on the degree of ethoxylation. Preferably the ethoxylation is greater than about 20%, more preferably greater than about 20% and up to about 60%, still more preferably from about 25% to about 55%, even more preferably from about 30% to about 50%, still more preferably from about 35% to about 45%, most preferably about 40%. Preferably the propoxylation is less than about 80%, more preferably from about 40% to less than about 80%, still more preferably from about 45% to about 75%, even more preferably from about 50% to about 70%, still more preferably from about 55% to about 65%, most preferably about 60%. As used herein “arm(s)” refers to the alkoxylated chains; thus a multiarmed capped block copolymer would have more than one alkoxylated chain. The capped block copolymers preferably are multiarmed having at least 2 arms, more preferably at least 3 arms, even more preferably 3-6 arms, still more preferably 4 or 5 arms, and most preferred 4 arms. Preferably, the arms are formed by a branched alkyl group (backbone), which the block copolymer arms attach to. Non-limiting examples of 2 arms, 3 arms, and 4 arms are shown below for illustrative purposes: where R is a hydrophobic capping group as disclosed herein, X and Y are as defined above, each preferably between 1 and 100; and the EO/PO arms would be attached to a single backbone such that a single molecule is formed with at least 2 alkoxylated arms, at least 3 alkoxylated arms, at least 4 alkoxylated arms, at least 5 alkoxylated arms, or at least 6 alkoxylated arms. It should be understood that the block copolymer can have arms of different degrees of ethoxylation and/or propoxylation. Additionally, different arms of the block copolymer can have different hydrophobic capping groups at the terminus of each. It should be understood that the number of arms, nature of the alkyl backbone, and percentages of ethoxylation and propoxylation on the arms can be determined by using a preexisting block copolymer surfactant and capping it according to the methods disclosed herein. The multi-arm capped, reverse block copolymers are capped, i.e., the terminus of each arm is capped with a capping chemistry. Preferred capping chemistries are hydrophobic groups. More preferably, the hydrophobic group comprises a benzyl group and/or a substituted silyl group (R 1 R 2 R 3 Si—) shown below: where each of R 1 , R 2 , and R 3 comprises an alkyl group, a phenol group, or tert-butyl. Preferred alkyl groups for R1, R2, and/or R3 include straight-chain alkyl groups having between 1 and 10 carbons (i.e., methyl, ethyl, propyl, butyl, pentyl, hexyl, heptyl, octyl, nonyl, and decyl); cyclic alkyl groups (or “cycloalkyl” or “alicyclic” or “carbocyclic” groups), including, but not limited to, cyclopropyl, cyclopentyl, cyclohexyl, cycloheptyl, cyclooctyl, cyclononyl, and cyclodecyl; branched-chain alkyl groups, including, but not limited to isopropyl, tert-butyl, sec-butyl, isobutyl; and alkyl-substituted alkyl groups, including but not limited to, alkyl-substituted cycloalkyl groups and cycloalkyl-substituted alkyl groups. More preferably the hydrophobic group comprises a benzyl group, trimethylsilyl (TMS), triethylsilyl (TES), tert-butyldimethylsilyl (TBS), tert-butyldiphenylsilyl (TBDPS), triisopropylsilyl (TIPS), or a combination thereof. These preferred silyl-based capping chemistries are shown below: In the instance where there is a combination of hydrophobic groups utilized, this is due to capping of the different arms (e.g., two arms capped with TIPS and two arms capped with a benzyl group). Most preferably the hydrophobic group comprises a benzyl group, trimethylsilyl (TMS), triisopropylsilyl (TIPS), or a combination thereof. In an embodiment where the block copolymer has two arms, one or both arms can be capped. In an embodiment where the block copolymer has three arms, one, two or three arms can be capped. In an embodiment where the block copolymer has four arms, one, two, three or four arms can be capped. In an embodiment where the block copolymer has five arms, one, two, three, four or five arms can be capped. In an embodiment where the block copolymer has six arms, one, two, three, four, five, or six arms can be capped. The ratio of capped to uncapped arms can impact the thermal stability of the capped block copolymers. Thermal stability is increased with increased capping; thus, it is preferred to have a ratio of at least greater than 1:1 for capped to uncapped arms, most preferably all arms being capped for high temperature use applications. Thus, in applications requiring thermal stability it is preferable to have at least two of three arms capped, at least three of four arms capped, at least four of five arms capped, at least four of six arms capped, at least five of six arms capped, most preferably all arms capped. Retaining some uncapped arms improves viscoelasticity of the capped block copolymers. Most preferably we found that a ratio of about 2:1 to about 4:1 capped to uncapped arms; most preferably about 3:1 capped to uncapped arms. Thus, in applications where viscoelasticity is desired, two of three arms are capped, three of four arms are capped, three of five arms are capped, four of five arms are capped, four of six arms are capped, five of six arms are capped. Beneficially, the capped block copolymers disclosed herein have a low surface tension. Preferably a surface tension of less than about 35 dynes/cm, more preferably less than about 34 dynes/cm, still more preferably less than about 33 dynes/cm, even more preferably less than about 32 dynes/cm, yet more preferably less than about 31 dynes/cm, still more preferably less than about 30 dynes, even more preferably less than about 29 dynes, yet more preferably less than about 28 dynes/cm, still more preferably less than about 27 dynes/cm, even more preferably less than about 26 dynes/cm, yet more preferably less than about 25 dynes/cm, still more preferably less than about 24 dynes, even more preferably less than about 23 dynes, yet more preferably less than about 22 dynes/cm, still more preferably less than about 21 dynes, most preferably about 20 dynes/cm or less; when tested under ambient temperature and humidity. As discussed above, the problems associated with proteinaceous soils are well known and particularly problematic for machine warewash and CIP cleaning. Without wishing to be bound by the theory, we believe for optimal performance, a defoaming surfactant should be surface active enough to penetrate protein stabilized foam lamellae, partially denature the protein to a certain degree to produce proper defoaming, yet without full denaturation to cause precipitation/coagulation of the protein to cause deposition problem. According to various embodiments of the compositions, the capped block copolymers are included as a second surfactant in an amount of about 0 wt-% to about 15 wt-%, about 0.5 wt-% to about 15 wt-%, about 1 wt-% to about 10 wt-%, or about 2 wt-% to about 10 wt-%. Alkalinity Source In embodiments, the compositions contain at least one alkalinity source. Alkalinity sources can include alkali metal hydroxides, alkali metal carbonates, and/or reagents combined to form an alkoxide solid, including alkali metal hydroxide and reagents comprising an organic molecule having at least one hydroxyl group or an alkylene carbonate. Embodiments as described herein do not require the formation of alkoxide solids from the reagents. In some embodiments the primary alkalinity source can include an alkali metal carbonate with substantially less alkali metal hydroxide while also including an organic molecule having at least one hydroxyl group or an alkylene carbonate, wherein the hydroxide does not form an alkoxide with the organic molecule having at least one hydroxyl group or the alkylene carbonate. Alkalinity sources can include an effective amount of one or more alkalinity sources. An effective amount of one or more alkaline sources should be considered as an amount that provides a composition having a pH between about 7 and about 14. In a particular embodiment the end-use compositions can have a pH of between about 7.5 and about 12.5. In embodiments the alkalinity source in the final composition (including alkalinity and/or neutralization from additional components in the composition) provides a total alkalinity as measured by percentage of Na 2 O is from about 20-47%, from about 25-40%, or preferably from about 28-37%. In embodiments the total alkalinity as measured by percentage of Na 2 O is from about 20-47%, from about 25-40%, or preferably from about 28-37%, has more alkalinity provided by carbonate alkalinity source compared to caustic alkalinity source. Total alkalinity can measure alkalinity from various sources in addition to sodium hydroxide, including less alkalinity sources such as, for example, sodium bicarbonate. The total alkalinity is measured by standard acid-based titration using HCl in a liquid or in a solid dissolved in water to generate a liquid. In embodiments the alkalinity source in the final composition is less than about 70:30 alkali metal hydroxide to water weight ratio to provide improved cleaning performance in comparison to compositions comprising additional alkali metal hydroxide. Alkali Metal Carbonate In embodiments, the compositions can contain at least one alkali metal carbonate. In an embodiment of the disclosure, any suitable source of carbonate may be used. In an embodiment, an alkali metal carbonate source may be used, for example, sodium carbonate, potassium carbonate, lithium carbonate, and combinations thereof. The alkali metal carbonate can be in the compositions in an amount of about 1 wt-% to about 99.9 wt-%, about 10 wt-% to about 90 wt-%, about 20 wt-% to about 90 wt-%, about 30 wt-% to about 90 wt-%, about 30 wt-% to about 80 wt-%, about 40 wt-% to about 85 wt-%, or about 30 wt-% to about 70 wt-%. Alkali Metal Hydroxide In embodiments, the compositions contain at least one alkali metal hydroxide as a caustic source. As referred to herein, caustic is synonymous with hydroxide. While not wishing to be bound by a particular theory or mechanism of action, it is believed that the caustic plays a role in partially denaturing proteinaceous soils to aid in their removal and is thus particularly suitable to cleaning compositions used to clean soils comprising proteinaceous soils. In an embodiment of the disclosure, any suitable source of caustic may be used. In an embodiment, an alkali metal caustic source may be used. For example, caustic sources may be in the form of sodium hydroxide, potassium hydroxide, lithium hydroxide, derivatives thereof, or and combinations thereof. An example of a derivative of a caustic source is a preformed alkoxide. In the methods of making the solid composition the alkali metal hydroxide is a solution or a liquid alkali metal hydroxide. In further embodiments additional caustic source can be included in the form of a solid, such as caustic beads, pellets, flakes, powder, granules, and the like may be combined with the liquid alkali metal hydroxide. In embodiments of the solid compositions an alkali metal hydroxide is reacted with an organic molecule having one or more hydroxyl-groups or an alkylene carbonate as disclosed in U.S. Patent Application No. 63/490,838, claiming priority to U.S. Ser. No. 63/490,838, filed simultaneously herewith and titled “Alkoxide-Based Solidification Via Control of Reaction Equilibrium and Kinetics”, which is incorporated by reference in its entirety. In an embodiment a higher active caustic liquid is preferred for the control of equilibrium reaction and kinetics for the generation of solid compositions. In an embodiment, the molar ratio of caustic to reagent (e.g. propylene glycol) is about 1:1 to about 10:1 molar ratio, about 1:1 to about 8:1 molar ratio, about 1:1 to about 6:1 molar ratio, and preferably about 1:1. In exemplary embodiments of the Examples, the reaction of glycol reagents is faster to produce the compositions, namely solid compositions. In an embodiment, a concentrated caustic can be used in the methods of making the compositions, namely solid compositions. In an embodiment, 70% NaOH is preferred over a 50% NaOH to provide the 1:1 (or greater) molar ratio of caustic to reagent. In preferred embodiments, a concentrated alkali metal hydroxide comprises greater than 50% (actives basis) liquid alkali metal hydroxide. In some embodiments, the concentrated alkali metal hydroxide is from about 69% to about 74% (actives basis) liquid alkali metal hydroxide, preferably from about 70% to about 73% (actives basis) liquid alkali metal hydroxide. The concentrated alkali metal hydroxide is maintained at sufficiently high temperatures to prevent premature solidification. In an embodiment the concentrated alkali metal hydroxide is maintained, handled or otherwise processed at a temperature of at least about 66° C., or from about 66° C. to about 85° C. In alternative embodiments the concentrated alkali metal hydroxide can include using a mixture of caustic beads and caustic liquid, wherein the methods of making the solid compositions beneficially reduce the use of caustic beads. In such an embodiment, there may still be an initial step of concentrating an alkali metal hydroxide, such as by dissolving a solid alkali metal hydroxide in a liquid alkali metal hydroxide having 50% (actives basis) or less, to provide the concentrated alkali metal hydroxide. The alkali metal carbonate can be in the solid compositions in an amount of about 1 wt-% to about 99.9 wt-%, about 10 wt-% to about 90 wt-%, about 20 wt-% to about 90 wt-%, about 30 wt-% to about 90 wt-%, about 30 wt-% to about 80 wt-%, about 40 wt-% to about 85 wt-%, or about 30 wt-% to about 70 wt-%. The alkali metal carbonate can be in the solid compositions in further embodiments in an amount of about 1 wt-% to about 99.9 wt-%, about 1 wt-% to about 70 wt-%, about 1 wt-% to about 50 wt-%, about 5 wt-% to about 50 wt-%, about 5 wt-% to about 40 wt-%, or about 5 wt-% to about 30 wt-%. In alternative embodiments the alkali metal hydroxide can be present to make the solid compositions in an amount of about 1 wt-% to about 99.9 wt-%, about 10 wt-% to about 90 wt-%, about 20 wt-% to about 90 wt-%, about 30 wt-% to about 90 wt-%, about 30 wt-% to about 80 wt-%, or about 30 wt-% to about 70 wt-%. In other embodiments, the caustic solution includes from about 1 wt-% to about 90 wt-% of the total caustic to make the solid composition. In still other embodiments, the caustic solution includes from about 10 wt-% to about 90 wt-% of the total caustic to make the solid composition. As described herein a liquid and a solid caustic can be combined to make the solid composition. In some embodiments where a solid caustic is employed the alkali metal hydroxide of the composition comprises less than about 40 wt-% solid caustic bead. In further embodiments where a solid caustic is employed, the composition has at least about 20% less solid caustic bead compared to a solid composition that does not contain the alkali metal hydroxide and reagent (e.g. propylene glycol). Polyol In some embodiments, the solid compositions contain at least one polyol to react with the alkali metal hydroxide to form the solid compositions. Polyols include C1-C22 alcohol, a glycol, or derivative thereof, or a combination thereof. In embodiments, a polyol is a diol, triol, and/or polyol containing more than 3 hydroxyl groups. Diols include for example, ethylene glycol, propylene glycol, hexylene glycol, tetramethylene glycol (1,4-Butanediol), etc. An exemplary triol is glycerin. An exemplary polyol is D-Sorbitol (6 hydroxyl groups). Exemplary polyols include glycols and derivatives thereof including, ethylene glycol, propylene glycol, hexylene glycol, ethylene glycol phenyl ether, propylene glycol n-propyl ether, propylene glycol phenyl ether, dipropylene glycol n-propyl ether, and the like. Further exemplary glycerols and derivatives include, glycerol ethyl hexyl glyceryl ether, glycerin, glycerol formal, glycerol ketal, and the like. Exemplary polyols, diols and derivatives include, 3-butanediol, 1,4-butanediol, 2-ethy-1,3,-hexanediol, 1-3-propane diol, 2-methyl-2-propyl-1,3-propanediol, and the like. A preferred polyol is glycerin. In an embodiment crude glycerin (˜85% active) is the preferred polyol. The terms glycerin and glycerol may be used interchangeably. In addition to polyols, additional organic molecules having at least one hydroxyl group to react with the caustic source to form the solid compositions as disclosed in U.S. Patent Application No. 63/490,838, claiming priority to U.S. Ser. No. 63/490,838, filed simultaneously herewith and titled “Alkoxide-Based Solidification Via Control of Reaction Equilibrium and Kinetics”, which is incorporated by reference in its entirety. According to various embodiments the polyol is included as a reagent to make the solid compositions in an amount of about 1 wt-% to about 30 wt-%, about 1 wt-% to about 20 wt-%, about 1 wt-% to about 15 wt-%, about 1 wt-% to about 10 wt-%, or about 2 wt-% to about 10 wt-%. Alkylene Carbonates In some embodiments, the solid compositions include an alkylene carbonate to react with the caustic source to form the solid composition. Any suitable alkylene carbonate may be used. Exemplary alkylene carbonates include for example, glycerin carbonate (or glycerol carbonate), ethylene carbonate, propylene carbonate, butylene carbonate, carbonate esters, and the like. A carbonate ester has a carbonyl group flanked by two alkoxy groups. In an embodiment, a cyclic organic ester is provided in the carbonate structure, such as shown for ethylene carbonate, propylene carbonate, and butylene carbonate, as follows respectively, is employed, however, any chain length of the alkyl group can be employed: Alkylene carbonates are commercially-available (Huntsman, available under Jeffsol® tradename) and often referred to as glycol carbonates or cyclic carbonates. They are often used as reactive intermediates to replace ethylene and propylene oxides and ethylene and propylene glycols. In some embodiments glycerin carbonate is a preferred alkalinity source and/or reagent. Glycerol carbonate is a carbonate ester. Glycerol carbonate can be obtainable by esterifying ethylene carbonate or dimethyl carbonate with glycerol, the by-products produced being ethylene glycol or methanol, respectively. A further synthesis route starts from glycidol (2,3-epoxy-1-propanol), which is reacted with CO 2 under pressure in the presence of catalysts to give glycerol carbonate. Glycerol carbonate is a clear, readily mobile liquid which has a density of 1.398 gem-3 and boils at 125-130° C. (0.15 mbar). In embodiments the molar ratio of the initial alkali metal hydroxide to alkylene carbonate combined to make the solid composition is from about 0.5:1 to about 10:1, or from about 0:71:1 to about 9.8:1. According to various embodiments the alkylene carbonate is included as a reagent to make the solid compositions in an amount of about 1 wt-% to about 50 wt-%, about 1 wt-% to about 40 wt-%, about 1 wt-% to about 20 wt-%, about 1 wt-% to about 15 wt-%, or about 5 wt-% to about 15 wt-%. Chelants The compositions include a strong chelant (also referred to as a chelating agent). The inclusion of a strong chelant in combination with the surfactants and enzyme provide optimal protein removal performance that overcome limitations of reduced alkalinity content (i.e. from about 20-47%, from about 25-40%, or preferably from about 28-37% total alkalinity as measured by percent Na 2 O in the composition) of the compositions. Examples of chelating agents include phosphonic acid and phosphonates, phosphates, aminocarboxylates and their derivatives, pyrophosphates, ethylenediamine and ethylenetriamine derivatives, hydroxyacids, and mono-, di-, and tri-carboxylates and their corresponding acids. In certain embodiments the composition is phosphate free. Exemplary chelating agents include methylglycine-N,N-diacetic acid (MGDA); glutamic acid-N,N-diacetic acid (GLDA); ethylenediaminetetraacetic acid (EDTA); diethylenetriaminepentacetic acid (DTPA); nitrilotriacetic acid (NTA); Triethylenetetramine-N,N,N′,N″,N′″,N′″-hexaacetic acid (TTHA); Aspartic acid-N,N-diacetic acid (ASDA) and alkali, alkali earth metal, transition metal and/or ammonium salts thereof. In a further embodiment a biodegradable chelating agent, such as an aminocarboxylate is preferred. Chelants suitable for use in the compositions preferably have a stability constant with calcium that is between about 1×10 7 and about 1×10 11 . In some embodiments, the compositions include a chelant comprising an aminocarboxylate selected from the group consisting of selected from the group consisting of NTA, EDTA, DTPA TTHA, MGDA, and GLDA. In some embodiments, the compositions include a chelant comprising an aminocarboxylate selected from the group consisting of selected from the group consisting of NTA, EDTA, DTPA, MGDA, and GLDA each having a stability constant with calcium that is between about 1×10 7 and about 1×10 11 . In some embodiments, the solid compositions include a chelant comprising sodium gluconate. Without being limited to a particular mechanism of action the inclusion of a strong chelant, namely a biodegradable aminocarboxylate, benefits by increasing the melting point of the solid formed by a caustic/polyol/chelant by further bonding with the chelant. In some embodiments, the compositions include a chelant comprising a ternary polymer, such as an acrylic acid/maleic acid/ATBS (Acrylamide t-butyl sulfonic acid, N-t-butyl acrylamide sulfonic acid). In an exemplary embodiment the chelant comprises 85% acrylic acid/10% maleic acid/5% ATBS. The chelant can be in the compositions in an amount of about 1 wt-% to about 20 wt-%, about 2 wt-% to about 20 wt-%, or about 2 wt-% to about 15 wt-%. Enzyme The compositions include a protease enzyme in combination with the surfactants and strong chelant to provide optimal protein removal performance that overcome limitations of reduced alkalinity content (i.e. from about 20-47%, from about 25-40%, or preferably from about 28-37% total alkalinity as measured by percent Na 2 O in the composition) of the compositions. Beneficially and without being limited to a mechanism of action, the protease enzyme can be included in the solid compositions as a result of the lower total alkalinity in the compositions compared to conventional caustic-based solids despite compositions including greater wt-% of water contained therein for both liquid and solid compositions. Various protease enzymes or mixture of proteases, from any source, can be used in the compositions, provided that the selected enzyme is stable in the desired pH range (between about 7.5 and about 12.5). For example, the protease enzymes can be derived from a plant, an animal, or a microorganism such as a yeast, a mold, or a bacterium. Preferred protease enzymes include, but are not limited to, the enzymes derived from Bacillus subtilis, Bacillus lentus, Bacillus licheniformis and Streptomyces griseus . Protease enzymes derived from B. subtilis are most preferred. The protease can be purified or a component of a microbial extract, and either wild type or variant (either chemical or recombinant). Exemplary proteases are commercially available under the following trade names Alcalase®, Blaze®, Savinase®, Esperase®, and Progress UNO™ (also sold under the name Everis DUO™) each available from Novozymes. In embodiments the protease enzymes survive as stable enzymes suitable for the cleaning and methods of use described herein over multi-use dispensing cycles as solid compositions representing a significant improvement over conventional enzyme stability in liquid and solid compositions where processing/manufacturing and dispensing are known to significantly reduce concentrations of viable enzymes. It is generally expected for substantial amounts of enzyme to be lost in the processing and thereafter the dispensing of a solid composition to generate a concentrate or use solution. The use of a dry solid containing enzymes does not ensure the enzyme remains stable and effective for use throughout the processing and dispensing, namely generating of a use solution for dispensing. Beneficially, as described herein, the protease enzymes when contacted with water in a dispensing step to generate a concentrate or a use solution from a solid, does not exhibit loss of a substantial enzyme concentration, including over multi-use dispensing cycles. In some embodiments the enzyme concentration is not substantially changed or exhibits less than about a 10% loss in enzyme concentration as can be measured by QA™ 476 (Proteolytic Enzyme Activity by Plate Reader). The enzyme can be in the compositions in an amount of about 0.1 wt-% to about 10 wt-%, about 0.1 wt-% to about 5 wt-%, about 0.5 wt-% to about 5 wt-%, or about 1 wt-% to about 5 wt-%. Additional Functional Ingredients The compositions may further include additional functional materials or additives that provide a beneficial property, e.g., for a particular use. Examples of conventional additives include one or more of additional alkalinity source, additional surfactants, detersive polymer, cleaning agent, rinse aid composition, softeners, pH modifier, source of acidity, anti-corrosion agent, secondary hardening agent, solubility modifier, detergent builder, detergent filler, defoamer, anti-redeposition agent, antimicrobial sanitizers and/or antimicrobial agents, rinse aids, a threshold agent or system, aesthetic enhancing agent (i.e., dye, odorant, perfume), optical brightener, bleaching agent, enzyme, effervescent agent, activator for an active oxygen compound, other such additives or functional ingredients, and the like, and mixtures thereof. Adjuvants and other additive ingredients will vary according to the type of composition being manufactured, and the intended end use of the solid composition. In some embodiments, the surfactant compositions for liquid or solid compositions and the compositions themselves are free of silicone surfactants. In some embodiments, the surfactant compositions for liquid compositions, further include a hydrotrope, viscosity modifier, solvent, water carrier, or derivatives or combinations thereof. In some embodiments, the surfactant compositions for liquid or solid compositions further include an additional surfactant. Additional nonionic, anionic, amphoteric and zwitterionic surfactants are disclosed, for example in U.S. Patent Application No. , claiming priority to U.S. Ser. No. 63/490,857, filed simultaneously herewith and titled “Capped Block Copolymers, Their Synthesis, Manufacture, and Methods of Use”, which is incorporated by reference in its entirety. In some embodiments, the surfactant compositions for liquid or solid compositions further include a water conditioning agent. The term “water conditioning agent” refers to a compound that inhibits crystallization of water hardness ions from solution or disperses mineral scale including but not limited to calcium carbonate. Water conditioning agents can include polymeric and small molecule water conditioning agents. Organic small molecule water conditioning agents are typically organocarboxylate compounds or organophosphate water conditioning agents. Polymeric water conditioning agents commonly comprise polyanionic compositions such as polyacrylic acid compounds. Additional examples of water conditioning polymers includes polyacrylic acid homopolymer or alkali metal salt thereof, i.e., sodium polyacrylate. The polyacrylic acid homopolymers can contains a polymerization unit derived from the monomer selected from the group consisting of acrylic acid, methacrylic acid, methyl acrylate, methyl methacrylate, ethyl acrylate, ethyl methacrylate, butyl acrylate, butyl methacrylate, iso-butyl acrylate, iso-butyl methacrylate, iso-octyl acrylate, iso-octyl methacrylate, cyclohexyl acrylate, cyclohexyl methacrylate, glycidyl acrylate, glycidyl methacrylate, hydroxyethyl acrylate, hydroxypropyl acrylate, 2-hydroxyethyl acrylate, 2-hydroxyethyl methacrylate, 2-hydroxypropyl acrylate, 2-hydroxypropyl methacrylate, and hydroxypropyl methacrylate and a mixture thereof, among which acrylic acid, methacrylic acid, methyl acrylate, methyl methacrylate, butyl acrylate, butyl methacrylate, iso-butyl acrylate, iso-butyl methacrylate, hydroxyethyl acrylate, 2-hydroxyethyl acrylate, 2-hydroxyethyl methacrylate, 2-hydroxypropyl acrylate, and 2-hydroxypropyl methacrylate, and a mixture thereof are preferred. Preferred are polyacrylic acids, (C 3 H 4 O 2 ) n or 2-Propenoic acid homopolymers; Acrylic acid polymer; Poly(acrylic acid); Propenoic acid polymer; PAA have the following structural formula: where n is any integer. One source of commercially available polyacrylates (polyacrylic acid homopolymers) includes the Acusol® 445 series from The Dow Chemical Company, Wilmington Delaware, USA, including, for example, Acusol® 445 (acrylic acid polymer, 48% total solids) (4500 MW), Acusol® 445N (sodium acrylate homopolymer, 45% total solids) (4500MW), and Acusol® 445ND (powdered sodium acrylate homopolymer, 93% total solids) (4500MW) Other polyacrylates (polyacrylic acid homopolymers) commercially available from Dow Chemical Company include, but are not limited to Acusol 929 (10,000 MW) and Acumer® 1510. Yet another example of a commercially available polyacrylic acid is AQUATREAT® AR-6 (100,000 MW) from AkzoNobel Strawinskylaan 2555 1077 ZZ Amsterdam Postbus 75730 1070 AS Amsterdam. Other suitable polyacrylates (polyacrylic acid homopolymers) include, but are not limited to those obtained from additional suppliers such as Aldrich Chemicals, Milwaukee, Wis., and ACROS Organics and Fine Chemicals, Pittsburgh, Pa, BASF Corporation and SNF Inc. The homopolymers, copolymers, and/or terpolymers may be present in a composition from about 0.01 wt-% to about 30 wt-%. Maleic anhydride/olefin copolymers are copolymers of polymaleic anhydrides and olefins. Maleic anhydride (C 2 H 2 (CO) 2 O has the following structure: A part of the maleic anhydride can be replaced by maleimide, N-alkyl(C 1-4 ) maleimides, N-phenyl-maleimide, fumaric acid, itaconic acid, citraconic acid, aconitic acid, crotonic acid, cinnamic 10 acid, alkyl (C 1-18 ) esters of the foregoing acids, cycloalkyl(C 3-8 ) esters of the foregoing acids, sulfated castor oil, or the like. At least 95 wt % of the maleic anhydride polymers, copolymers, or terpolymers have a number average molecular weight of in the range between about 700 and about 20,000, preferably between about 1000 and about 100,000. A variety of linear and branched chain alpha-olefins can be used for the purposes of this disclosure. Particularly useful alpha-olefins are dienes containing 4 to 18 carbon atoms, such as butadiene, chloroprene, isoprene, and 2-methyl-1,5-hexadiene; 1-alkenes containing 4 to 8 carbon atoms, preferably C 4-10 , such as isobutylene, 1-butene, 1-hexene, 1-octene, and the like. In a preferred embodiment, particularly suitable maleic anhydride/olefin copolymers have a molecular weight between about 1000 and about 50,000, in a preferred embodiment between about 5000 and about 20,000, and in a most preferred embodiment between about 7500 and about 12,500. Examples of maleic anhydride/olefin copolymers which may be used include, but are not limited to, Acusol 460N from The Dow Chemical Company, Wilmington Delaware, USA. The maleic anhydride/olefin copolymer may be present in a composition from about 0.01 wt-% to about 30 wt-%. Additional polymers include polycarboxylic acid polymers, include, but are not limited to, polymaleic acid homopolymers, polyacrylic acid copolymers, and maleic anhydride/olefin copolymers. Polymaleic acid (C 4 H 2 O 3 ) X or hydrolyzed polymaleic anhydride or cis-2-butenedioic acid homopolymer, has the structural formula: where n and m are any integer. Examples of polymaleic acid homopolymers, copolymers, and/or terpolymers (and salts thereof) which may be used are particularly preferred are those with a molecular weight of about 0 and about 5000, more preferably between about 200 and about 2000 (can you confirm these MWs). Commercially available polymaleic acid homopolymers include the Belclene 200 series of maleic acid homopolymers from BWA™ Water Additives, 979 Lakeside Parkway, Suite 925 Tucker, GA 30084, USA and Aquatreat AR-801 available from AkzoNobel. The polymaleic acid homopolymers, copolymers, and/or terpolymers may be present in a composition from about 0.01 wt-% to about 30 wt-%. Inorganic water conditioning agents include, but are not limited to, sodium tripolyphosphate and other higher linear and cyclic polyphosphates species. Suitable condensed phosphates include sodium and potassium orthophosphate, sodium and potassium pyrophosphate, sodium tripolyphosphate, and sodium hexametaphosphate. A condensed phosphate may also assist, to a limited extent, in solidification of the solid detergent composition by fixing the free water present in the composition as water of hydration. Examples of phosphonates included, but are not limited to: 1-hydroxyethane-1,1-diphosphonic acid, CH 3 C(OH)[PO(OH) 2 ] 2 ; aminotri(methylenephosphonic acid), N[CH 2 PO(OH) 2 ] 3 ; aminotri(methylenephosphonate), sodium salt (ATMP), N[CH 2 PO(ONa) 2 ] 3 ; 2-hydroxyethyliminobis(methylenephosphonic acid), HOCH 2 CH 2 N[CH 2 PO(OH) 2 ] 2 ; diethylenetriaminepenta(methylenephosphonic acid), (HO) 2 POCH 2 N[CH 2 CH 2 N[CH 2 PO(OH) 2 ] 2 ] 2 ; diethylenetriaminepenta(methylenephosphonate), sodium salt (DTPMP), C 9 H 28-x N 3 Na x O 15 P 5 (x=7); hexamethylenediamine (tetramethylenephosphonate), potassium salt, C 10 H 28-x N 2 K x O 12 P 4 (x=6); bis(hexamethylene)triamine (pentamethylenephosphonic acid), (HO 2 )POCH 2 N [(CH 2 ) 6 N[CH 2 PO(OH) 2 ] 2 ] 2 ; and phosphorus acid, H 3 PO 3 . A preferred phosphonate combination is ATMP and DTPMP. A neutralized or alkaline phosphonate, or a combination of the phosphonate with an alkali source before being added into the mixture such that there is little or no heat or gas generated by a neutralization reaction when the phosphonate is added is preferred. In some embodiments, the surfactant compositions for solid compositions further include a hydrotrope, viscosity modifier, solvent, water carrier, or derivatives or combinations thereof. In some embodiments, the solid compositions further include a functional anhydrous material to absorb excess water from the mixture of hydrated solids in the composition. Examples of such a functional anhydrous material, include, but are not limited to, sodium carbonate (ash), sodium sulfate, and the like. Without being limited to a particular mechanism, the addition of a functional anhydrous material forms hydrate compounds upon contact with excess water, thus removing the excess water from the mixture. In some embodiments, the surfactant compositions for liquid or solid compositions further include an anti-redeposition agent, namely that is capable of facilitating sustained suspension of soils in a cleaning or rinse solution and preventing removed soils from being redeposited onto the substrate being cleaned and/or rinsed. Some examples of suitable anti-redeposition agents can include fatty acid amides, fluorocarbon surfactants, complex phosphate esters, styrene maleic anhydride copolymers, and cellulosic derivatives such as hydroxyethyl cellulose, hydroxypropyl cellulose, and the like. A composition can include up to about 10 wt-%, and in some embodiments, in the range of about 1 to about 5 wt-%, of an anti-redeposition agent. In some embodiments, the surfactant compositions for liquid or solid compositions further include one or more functional polydimethylsiloxones. For example, in some embodiments, a polyalkylene oxide-modified polydimethylsiloxane, nonionic surfactant or a polybetaine-modified polysiloxane amphoteric surfactant can be employed as an additive. Both, in some embodiments, are linear polysiloxane copolymers to which polyethers or polybetaines have been grafted through a hydrosilation reaction. Some examples of specific siloxane surfactants are known as SILWET® surfactants available from Union Carbide or ABIL® polyether or polybetaine polysiloxane copolymers available from Goldschmidt Chemical Corp., and described in U.S. Pat. No. 4,654,161 which patent is incorporated herein by reference. In some embodiments, the particular siloxanes used can be described as having, e.g., low surface tension, high wetting ability and excellent lubricity. For example, these surfactants are said to be among the few capable of wetting polytetrafluoroethylene surfaces. The siloxane surfactant employed as an additive can be used alone or in combination with a fluorochemical surfactant. In some embodiments, the fluorochemical surfactant employed as an additive optionally in combination with a silane, can be, for example, a nonionic fluorohydrocarbon, for example, fluorinated alkyl polyoxyethylene ethanols, fluorinated alkyl alkoxylate and fluorinated alkyl esters. Further description of such functional polydimethylsiloxones and/or fluorochemical surfactants are described in U.S. Pat. Nos. 5,880,088; 5,880,089; and 5,603,776, all of which patents are incorporated herein by reference. We have found, for example, that the use of certain polysiloxane copolymers in a mixture with hydrocarbon surfactants provides excellent rinse aids on plastic ware. We have also found that the combination of certain silicone polysiloxane copolymers and fluorocarbon surfactants with conventional hydrocarbon surfactants also provide excellent rinse aids on plastic ware. This combination has been found to be better than the individual components except with certain polyalkylene oxide-modified polydimethylsiloxanes and polybetaine polysiloxane copolymers, where the effectiveness is about equivalent. Therefore, some embodiments encompass the polysiloxane copolymers alone and the combination with the fluorocarbon surfactant can involve polyether polysiloxanes, the nonionic siloxane surfactants. The amphoteric siloxane surfactants, the polybetaine polysiloxane copolymers may be employed alone as the additive in the end-use compositions to provide the same results. In some embodiments, the end-use compositions may include functional polydimethylsiloxones in an amount in the range of up to about 10 wt. %. For example, some embodiments may include in the range of about 0.1 to 10 wt. % of a polyalkylene oxide-modified polydimethylsiloxane or a polybetaine-modified polysiloxane, optionally in combination with about 0.1 to 10 wt. % of a fluorinated hydrocarbon nonionic surfactant. The compositions can be formulated as liquids or solids. Liquids can include for example, a slurry, or structured liquid, or emulsion. The solid compositions refer to “solid forms” that are hardened compositions that will not flow and will substantially retain its shape under moderate stress or pressure or mere gravity. The degree of hardness of the solid cast composition may range from that of a fused solid product which is relatively dense and hard, for example, like concrete, to a consistency characterized as being a hardened paste. In addition, the term “solid” refers to the state of the composition under the expected conditions of storage and use of the solid composition. In general, it is expected that the composition will remain in solid form when exposed to temperatures of up to approximately 100° F. and particularly greater than approximately 120° F. The solid compositions may take forms including, but not limited to: a pressed solid; a cast solid product; an extruded, molded or formed solid pellet, block, tablet, powder, granule, flake; or the formed solid can thereafter be ground or formed into a powder, granule, or flake. In an exemplary embodiment, extruded pellet materials formed have a weight of between approximately 1 gram and 50 grams, or 50 grams and approximately 250 grams, extruded solids generally have a weight of approximately 100 grams or greater, and solid blocks generally have a mass of between approximately 1 and approximately 10 kilograms. The solid compositions provide for a stabilized source of functional materials. In some embodiments employing an alkoxide solidification (e.g. alkali metal hydroxide and an alkylene carbonate) and/or solid compositions employing the various alkalinity sources without alkoxides, provide a storage stable solid composition i including the protease enzyme, beneficially wherein the protease enzyme is stable during cast processing, storage, and wet conditions during dispensing cycles. Beneficially the solid compositions have not only storage, shelf, and dimensional stability but also provide a superior enzyme stability within the solid compositions and in their use solutions during multiple dispensing cycles. In embodiments the storage, shelf, and dimensional stability of the solids include under an elevated temperatures. The enzymatic activity in these solid compositions and use solutions thereof is retained over time and under elevated temperature conditions. In embodiments the solid compositions and use solutions thereof are stable at room temperature, and further stable at temperatures up to about 50° C. Still further such shelf stability of the use solutions may be important for applications of use that keep a use dilution for use over an extended period of times, such as hours, days, or weeks. Beneficially, the use compositions of the solid compositions maintain shelf stability for at least about 1 year, or at least about 6 months, at room temperature. Moreover, the solid compositions maintain shelf stability in solid form, including at elevated storage temperatures, including for example at temperatures up to at least 40° C. (or 100° F.) for at least 8 weeks with a growth exponent (or change in dimension of the solid in any direction) of less than about 3%, demonstrating shelf stability at room temperature or ambient temperatures for at least about 1 year. It was unexpected for the solid compositions to exhibit both solid stability and use composition stability with maintained enzyme stability for extended periods of time. In some embodiments, the solid compositions have a dimensional stability and has a growth exponent of less than about 3% if heated at a temperature of about 122° F. In some other embodiments, the solid detergent has a dimensional stability and has a growth exponent of less than about 2% if heated at a temperature of about 122° F. In some embodiments, the solid composition may be dissolved, for example, in an aqueous or other medium, to create a concentrated and/or use solution. The solution may be directed to a storage reservoir for later use and/or dilution, or may be applied directly to a point of use. Alternatively, the solid alkaline composition is provided in the form of a unit dose, typically provided as a cast solid, an extruded pellet, or a tablet having a size of between approximately 1 gram and approximately 100 grams. In another alternative, multiple-use solids can be provided, such as a block or a plurality of pellets, and can be repeatedly used to generate aqueous compositions for multiple cycles. Methods of Use The compositions beneficially provide highly effective protein removal and synergy as a result of the alkalinity, detersive and defoaming surfactants, strong chelant and protease enzyme. Various applications of use and methods of using are described herein. In some embodiments warewashing compositions, rinse aid or bottle washing anti-foam compositions are provided. Various applications where protein removal is desired are envisioned with the compositions and methods described herein. The methods of using the compositions described herein include generating a use solution (of either a liquid or solid composition, including single or multi-use solids), contacting an article or surface in need of protein removal and/or defoaming and cleaning, disinfecting, and/or sanitizing with the use solution, and cleaning, disinfecting, and/or sanitizing the article or surface. The compositions can be referred to herein as cleaning compositions which are suitable for various applications of use. In embodiments the pH of a use solution generated from dilution of the compositions is at least about 7.5, from about 7.5 to about 14, or from about 7.5 to about 12. As referred to herein, the term ‘antifoaming’ means the compositions do not generate significant or any foam. The antifoaming prevents the generation of foam, whereas ‘defoaming’ means the surfactant composition and the compositions employing the same reduce the presence of foam. In some embodiments, where a solid is provided, the solid composition does not slough during dispensing to generate the use solution. In embodiments where a solid is provided, the solid composition provides a stable protease enzyme. In further embodiments for solid compositions that are multi-use compositions, the protease enzyme is beneficially stabilized to survive multi-use dispensing. In such embodiments, a concentrate or use solution is formed by contacting the solid composition with water to generate the solution over multiple dispensing cycles. In some embodiments, the solid is dispensed over multiple days or weeks based on the number of cycles run with the multi-use solid compositions. In some embodiments, the solid is dispensed from about 5 cycles to about 100 cycles, or from about 20 cycles to about 100 cycles. Beneficially, the protease enzyme concentration in the concentrate or use solution therefrom is stable and substantially no enzyme concentration is lost by the multi-use dispensing cycles. Without being limited to a particular mechanism of action the enzyme stability in the multi-use solid compositions is afforded by the binding of the alkoxide solid and/or reagents for forming alkoxides, e.g. alkylene carbonates such as glycerol carbonate, afforded in solid compositions than can include a significant water concentration as described herein. In various embodiments, it is a benefit that the compositions comprise from about 20-47%, from about 25-40%, or preferably from about 28-37% total alkalinity as measured by percent Na 2 O in the composition to provide improved cleaning performance in comparison to compositions comprising additional alkali metal hydroxide. In other embodiments, it is a benefit that the compositions comprise less than about 70:30 alkali metal hydroxide to water weight ratio to provide improved cleaning performance in comparison to compositions comprising additional alkali metal hydroxide. In either of these embodiments, the compositions are less concentrated caustic compositions (or less concentrated alkaline compositions) and therefore allow for additional performance additives, including the surfactants, chelant and enzyme to improve the cleaning performance. For example, in an embodiment the compositions can contain less active caustic (after neutralization from acidic components) in the composition compared to inline solid caustic machine ware washing detergents and provide at least the same or improved cleaning efficacy. In other embodiments, compositions employing greater alkalinity are preferred, such as for example, bottle washing, rinsing and/or pulp processing. In such embodiments, solid or liquid compositions employing the surfactant compositions are preferred. The present disclosure includes methods of using the compositions for various cleaning applications. These cleaning compositions can operate on an article, surface, in a body or stream of water or a gas, or the like, by contacting the article, surface, body, or stream with a composition of the disclosure. Contacting can include any of numerous methods for applying a cleaning composition of the disclosure, such as spraying the compositions, immersing the article in compositions, foam or gel treating the article with the compounds or composition, or a combination thereof. It should be understood that the concentration of the ingredients in the compositions will vary depending on whether the cleaning composition is provided as a concentrate or as a use solution. A use solution may be prepared from the concentrate by diluting the concentrate with water at a dilution ratio that provides a use solution having desired detersive properties. Exemplary industries in which the present methods can be used include, but are not limited to: food service industry; food and beverage industry; and the pharmaceutical manufacturing industry. Suitable uses for the compositions and methods of the invention may include, for example, bottle washing, machine warewashing, pulp antifoaming rinse aid, and the like. The present methods can also be used to remove various types of soils. Such other soils include, but are not limited to, starch, cellulosic fiber, protein, simple carbohydrates and combinations of any of these soil types with mineral complexes. Examples of specific food soils that are effectively removed using the present methods include, but are not limited to, soils generated in the manufacture and processing meat, poultry, vegetables and fruit, bakery goods, soft drinks, brewing and fermentation residues, soils generated in sugar beet and cane processing and processed foods containing these ingredients and associated ingredients such as juices, sauces and condiments. These soils can develop on environmental surfaces such as walls and floors, freezers and cooling systems, heat exchange equipment surfaces, conveyor surfaces and on other surfaces during the manufacturing and packaging process. In further embodiments, the methods of employing cleaning compositions are particularly suited for use in closed systems, e.g. dish or ware washing systems for cleaning, bottle washing systems and processes, sanitizing and/or disinfecting articles and surfaces. The method includes contacting an article or surface with a cleaning composition or a cleaning use composition to wash the surface. The method can contact the liquid to any of a variety of surfaces or objects including surfaces or articles including those made of glass, ceramic, plastic, porcelain, aluminum, or the like. The phrase “washing a surface with a wash solution (or a use solution or a cleaning composition)” refers to the circulation of a cleaning composition solution to remove substantially all soil from the treated surfaces (e.g. ware) and to keep that soil suspended or dissolved. In an embodiment, this step may be conducted where the temperature of the rinse water is up to about 140° F., preferably in the range of 100° F. to 140° F., preferably in the range of 110° F. to 140° F., and most preferably in the range of 120° F. to 140° F. As referred to herein, “low temperature” refers to those rinse water temperatures below about 140° F. For example, conventional rinse temperature for ware washing occurs above 140° F., such as from about 140° F. to about 190° F., particularly between about 145° F. to about 180° F. In an aspect, the methods employing a low temperature further employ a sanitizer. Contacting can include any of numerous methods for applying a cleaning composition, such as spraying the composition, immersing the object in the composition, or a combination thereof. A concentrate or use concentration of a composition can be applied to or brought into contact with an article by any conventional method or apparatus for applying a cleaning composition to an object. For example, the object can be wiped with, sprayed with, and/or immersed in the composition, or a use solution made from the composition. The composition can be sprayed, or wiped onto a surface; the composition can be caused to flow over the surface, or the surface can be dipped into the composition. Contacting can be manual or by machine. Before contacting an article or surface, a concentrate cleaning composition may be first diluted with water at the location of use to provide the use solution. When the composition is used in an automatic warewashing or dishwashing machine, it is expected that that the location of use will be inside the automatic warewashing machine. Depending on the machine, the composition may be provided in a unit dose form or in a multi-use form. In larger warewashing machines, a large quantity of composition may be provided in a compartment that allows for the release of a single dose amount of the composition for each wash cycle. Such a compartment may be provided as part of the warewashing machine or as a separate structure connected to the warewashing machine. The cleaning composition may also be dispensed from a spray-type dispenser, such as that disclosed in U.S. Pat. Nos. 4,826,661, 4,690,305, 4,687,121, 4,426,362 and in U.S. Pat. Nos. Reissue 32,763 and 32,818, the disclosures of which are incorporated by reference herein. Briefly, a spray-type dispenser functions by impinging a water spray upon an exposed surface of the composition, and then immediately directing the use solution out of the dispenser to a storage reservoir or directly to a point of use. If necessary, in some embodiments, when used, the product may be removed from the packaging and inserted into the dispenser. The methods may further employ one or more rinse steps for the treated articles or surfaces. In some embodiments, the cleaning compositions include killing one or more of the pathogenic bacteria associated with health care surfaces and environments including, but not limited to, Salmonella typhimurium, Staphylococcus aureus , methicillin resistant Staphylococcus aureus, Salmonella choleraesurus, Pseudomonas aeruginosa, Escherichia coli , mycobacteria, yeast, and mold. The cleaning compositions have activity against a wide variety of microorganisms such as Gram positive (for example, Listeria monocytogenes or Staphylococcus aureus ) and Gram negative (for example, Escherichia coli or Pseudomonas aeruginosa ) bacteria, yeast, molds, bacterial spores, viruses, etc. The compounds and compositions of the present disclosure, as described above, have activity against a wide variety of human pathogens. The cleaning compositions can kill a wide variety of microorganisms on a food processing surface, on the surface of a food product, in water used for washing or processing of food product, on a health care surface, or in a health care environment. The present methods can be used to achieve any suitable reduction of the microbial population in and/or on the target or the treated target composition. In some embodiments, the present methods can be used to reduce the microbial population in and/or on the target or the treated target composition by at least one log10. In other embodiments, the present methods can be used to reduce the microbial population in and/or on the target or the treated target composition by at least two log10. In still other embodiments, the present methods can be used to reduce the microbial population in and/or on the target or the treated target composition by at least three log10. In still other embodiments, the present methods can be used to reduce the microbial population in and/or on the target or the treated target composition by at least five log10. Without limiting the scope of disclosure, the numeric ranges are inclusive of the numbers defining the range and include each integer within the defined range. The cleaning compositions can be used for a variety of domestic or industrial applications, e.g., to reduce microbial or viral populations on a surface or object or in a body or stream of water. The cleaning compositions can be applied in a variety of areas including kitchens, bathrooms, factories, hospitals, dental offices and food plants, and can be applied to a variety of hard or soft surfaces having smooth, irregular or porous topography. Suitable hard surfaces include, for example, architectural surfaces (e.g., floors, walls, windows, sinks, tables, countertops and signs); eating utensils; hard-surface medical or surgical instruments and devices; and hard-surface packaging. Such hard surfaces can be made from a variety of materials including, for example, ceramic, metal, glass, wood or hard plastic. Suitable soft surfaces include, for example paper; filter media; hospital and surgical linens and garments; soft-surface medical or surgical instruments and devices; and soft-surface packaging. Such soft surfaces can be made from a variety of materials including, for example, paper, fiber, woven or nonwoven fabric, soft plastics and elastomers. The cleaning compositions can also be applied to soft surfaces such as food and skin (e.g., a hand). The present compounds can be employed as a non-foaming environmental sanitizer or disinfectant. The cleaning compositions can be applied to microbes or to soiled or cleaned surfaces using a variety of methods. These methods can operate on an object, surface, in a body or stream of water or a gas, or the like, by contacting the object, surface, body, or stream with a compound of the disclosure. Contacting can include any of numerous methods for applying a compound, such as spraying the compound, immersing the object in the compound, foam or gel treating the object with the compound, or a combination thereof. A concentrate or use concentration of a cleaning composition can be applied to or brought into contact with an object by any conventional method or apparatus for applying an antimicrobial or cleaning compound to an object. For example, the object can be wiped with, sprayed with, foamed on, and/or immersed in the compound, or a use solution made from the composition. The cleaning composition can be sprayed, foamed, or wiped onto a surface; the composition can be caused to flow over the surface, or the surface can be dipped into the cleaning composition. Contacting can be manual or by machine. Food processing surfaces, food products, food processing or transport waters, and the like can be treated with liquid, foam, gel, aerosol, gas, wax, solid, or powdered stabilized compounds according to the disclosure, or solutions containing these compounds. Cleaning compositions of the disclosure can be formulated and sold for use as is, or as solvent or solid concentrates. If desired, such concentrates can be used full-strength as sanitizing rinse compositions. However, the concentrates typically will be diluted with a fluid (e.g., water) that subsequently forms the dilute phase or a use solution. Preferably, the concentrate forms a single phase before such dilution and remains so while stored in the container in which it will be sold. When combined with water or other desired diluting fluid at an appropriate dilution level and subjected to mild agitation (e.g., by stirring or pumping the composition), some compositions of the disclosure will form a pseudo-stable dispersion, and other compositions of the disclosure will form a clear or quasi-stable solution or dispersion. If a pseudo-stable composition is formed, then the composition preferably remains in the pseudo-stable state for a sufficiently long period so that the composition can be applied to a surface before the onset of phase separation. The pseudo-stable state need only last for a few seconds when suitably rapid application techniques such as spraying are employed, or when agitation during application is employed. The pseudo-stable state desirably lasts for at least one minute or more after mixing and while the composition is stored in a suitable vessel, and preferably lasts for five minutes or more after mixing. Often normal refilling or replenishment of the applicator (e.g., by dipping the applicator in the composition) will provide sufficient agitation to preserve the pseudo-stable state of the composition during application. The various applications of use described herein provide the cleaning compositions to a surface and/or water source. Beneficially, the cleaning compositions of the disclosure are fast-acting. However, the present methods require a certain minimal contact time of the compositions with the surface or product in need of treatment for occurrence of sufficient antimicrobial effect. The contact time can vary with concentration of the use compositions, method of applying the use compositions, temperature of the use compositions, pH of the use compositions, amount of the surface or product to be treated, amount of soil or substrates on/in the surface or product to be treated, or the like. The contact or exposure time can be about 15 seconds, at least about 15 seconds, about 30 seconds or greater than 30 seconds. In some embodiments, the exposure time is about 1 to 5 minutes. In other embodiments, the exposure time is a few minutes to hours. In other embodiments, the exposure time is a few hours to days. The contact time will further vary based upon the use concentration of actives of compositions according to the disclosure. All publications and patent applications in this specification are indicative of the level of ordinary skill in the art to which this disclosure pertains. All publications and patent applications are herein incorporated by reference to the same extent as if each individual publication or patent application was specifically and individually indicated as incorporated by reference. EMBODIMENTS The present disclosure is further defined by the following numbered embodiments: 1. Compositions comprise: alkalinity source comprising an alkali metal hydroxide, alkali metal carbonate and/or reagents comprising an organic molecule having at least one hydroxyl group or an alkylene carbonate; a first surfactant comprising a first reverse EO/PO block copolymer of about 10-40% EO; and an optional second surfactant comprising at least one of a second reverse EO/PO block copolymer of about 40-50% EO, an alkyl capped alcohol ethoxylate, a capped block copolymer, and alkyl pyrrolidone; a strong chelating agent having a stability constant with calcium that is at least about 1×10 7 ; and a protease enzyme, wherein the composition is a liquid or solid and has between from about 20-47%, from about 25-40%, or preferably from about 28-37% total alkalinity as measured by percent Na 2 O in the composition. 2. The composition of embodiment 1, wherein the first reverse EO/PO block copolymer is about 20% EO. 3. The composition of any one of embodiments 1-2, wherein second reverse EO/PO block copolymer is about 40% EO. 4. The composition of any one of embodiments 1-3, wherein the first surfactant comprises from about 0.1 wt-% to about 5 wt-% of the composition and/or wherein the second surfactant(s) comprises from about 0.1 wt-% to about 20 wt-% of the composition. 5. The composition of any one of embodiments 1-4, wherein the second surfactant is the alkyl capped alcohol ethoxylate having the structure: R 1 —O—(CH 2 CH 2 O) n —R 2 where R 1 is a linear or branched (C 10 -C 18 ) alkyl group, R 2 is C 1 -C 4 , and n is an integer in the range of 1 to 100, or preferably wherein the alkyl capped alcohol ethoxylate is a butyl capped alcohol ethoxylate. 6. The composition of any one of embodiments 1-5, wherein the second surfactant is the alkyl pyrrolidone is C8 or C10 alkyl pyrrolidone. 7. The composition of any one of embodiments 1-6, wherein the chelant is selected from the group consisting of NTA, EDTA, DTPA TTHA, MGDA, GLDA, and a ternary polymer comprising acrylic acid/maleic acid/ATBS. 8. The composition of any one of embodiments 1-7, wherein the protease enzyme is Esperase 6.0T. 9. The composition of any one of embodiments 1-8, further comprising a hydrotrope and/or water conditioning polymer. 10. The composition of any one of embodiments 1-9, wherein the alkalinity source comprises alkali metal carbonate and/or alkali metal hydroxide, and preferably wherein the alkalinity source comprises both alkali metal carbonate and alkali metal hydroxide, wherein the wt-% of the alkali metal carbonate exceeds the wt-% of the alkali metal hydroxide. 11. The composition of embodiment 10, further comprising an alkylene carbonate that is glycerin carbonate, ethylene carbonate, propylene carbonate, or butylene carbonate, and wherein the molar ratio of the initial alkali metal hydroxide to alkylene carbonate combined to make the solid composition is from about 0.5:1 to about 10:1. 12. The composition of any one of embodiments 10-11, wherein the composition is free of solid alkali metal hydroxide beads, or wherein the composition has at least about 50% less solid alkali metal hydroxide beads compared to a caustic beads containing solid composition that does not contain a polyol or the alkylene carbonate forming the solid composition. 13. The composition of any one of embodiments 1-9, wherein the alkalinity source comprises an organic molecule having at least one hydroxyl group and alkali metal hydroxide. 14. The composition of embodiment 13, wherein the weight ratio of the alkali metal hydroxide to water in the solid composition is from about 27:73 to about 75:25, from about 50:50 to about 75:25, or from about 60:40 to about 70:30. 15. The composition of embodiment 13, further comprising water, and wherein the wherein the molar ratio of alkali metal hydroxide to water in the solid is about 1:2.2 to about 1:1. 16. The composition of any one of embodiments 13-15, wherein the organic molecule having at least one hydroxyl group is a polyol, and optionally wherein the polyol comprises glycol, glycerin or sorbitol. 17. The composition of any one of embodiments 13-16, wherein the composition has at least about 20% less solid alkali metal hydroxide beads, or wherein the composition has at least about 40% less solid alkali metal hydroxide beads compared to a solid composition that does not contain the alkali metal hydroxide and the organic molecule having at least one hydroxyl group forming the solid composition. 18. The composition of any one of embodiments 1-17, wherein the solid composition is formed in-situ. 19. The composition of any one of embodiments 1-17, wherein the solid composition is contiguous solid, powder or granule. 20. The composition of embodiment 19, wherein the solid composition is a pressed, cast, or extruded solid, and/or wherein the solid composition is a multi-use composition and the protease enzymes is stabilized therein as measured by enzyme surviving the multi-use dispensing. 21. The composition of any one of embodiments 1-20, wherein alkalinity source(s) comprises from about 20 wt-% to about 80 wt-%, the chelant comprises from about 2 wt-% to about 15 wt-%, the enzyme comprises from about 1 wt-% to about 5 wt-%, and wherein the first and second surfactants comprise from about 0.1 wt-% to about 50 wt-% of the composition. 22. The composition of embodiment 21, wherein the first surfactant comprising the reverse EO/PO block copolymer having 10-40% EO comprises from about 0.1 wt-% to about 5 wt-%, and wherein the second surfactant(s) comprise from about 0.1 wt-% to about 20 wt-% of the composition. 23. Methods of generating a use solution of the compositions described herein comprise: contacting an article or surface in need of soil removal with the use solution; and cleaning to remove the soil from the article or surface, wherein the soil comprises protein. 24. The method of embodiment 23, wherein the use solution is between about 50 ppm to about 5,000 ppm, or about 100 ppm to about 2,000 ppm of the solid composition. 25. The method of any one of embodiments 23-24, wherein the use solution is applied in a ware washing machine and optionally the use solution is contact with the articles of ware therein at a temperature range of about 120-180° F. 26. The method of any one of embodiments 23-25, wherein the use solution is applied in a ware wash machine. 27. The method of any one of embodiments 23-24, wherein the use solution is applied in a laundry or textile care washing machine. 28. The method of any one of embodiments 23-24, wherein the surface is a hard surface, instrument or ware. 29. The method of any one of embodiments 23-28, wherein the step of generating a use solution is by diluting a multi-use solid composition and wherein the protease enzyme survives the multi-use dispensing. EXAMPLES Embodiments of the present disclosure are further defined in the following non-limiting Examples. It should be understood that these Examples, while indicating certain embodiments of the disclosure, are given by way of illustration only. From the above discussion and these Examples, one skilled in the art can ascertain the essential characteristics of this disclosure, and without departing from the spirit and scope thereof, can make various changes and modifications of the embodiments of the disclosure to adapt it to various usages and conditions. Thus, various modifications of the embodiments of the disclosure, in addition to those shown and described herein, will be apparent to those skilled in the art from the foregoing description. Such modifications are also intended to fall within the scope of the appended claims. The following ingredients are utilized in the Examples: Acusol 448—an acrylic acid dispersant copolymer, commercially available from Dow Chemical. Achieve Choice 150 T—Achieve® Advance 150 T is a compact amylase enzyme. Esperase 6.0T: minimum enzyme activity of 6.0 KNPU/g. and is in the class of subtilisin derived from bacillus subtillis (EC 3.4.21.62) Belclene 200—a poly maleic acid (50%), commercially available from Italmatch. Briquest 301 (ATMP) (50%)—aminotris(methylenephosphonic acid) chelant. Dissolvine DZ (DTPA)—Diethylenetriaminepentaacetic Acid (DTPA) chelant. LD 097—a polyoxypropylene polyoxyethylene reverse block copolymer, 26% EO, 30% PO. Nalco DVS3C 008 —a phosphinic acid, sodium salt. Pluronic 25R 2 —a reverse block copolymer, 20% EO, commercially available from BASF. Tetronic 90R 4 —ethylenediamine tetrakis(ethoxylate-block-propoxylate) tetrol, commercially available from BASF. Chelant00929—85% ACRYLIC ACID, 10% MALEIC ACID, 5% ATBS, noncommercially available polymer ternary polymer. Pluronic 25-R-2: long chain EO/PO block copolymer Pluronic N3—a reverse block copolymer, PO EO PO Block Copolymer 1:1 Blend of MW 3100-20% EO & MW 3600-40% EO; 96%; nonionic surfactant. Commercially available sodium hydroxide (50%), water, sodium aluminate, sodium xylene sulfonate (40%, SXS), sodium xylene sulfonate (96%, SXS), glycerol carbonate, sodium carbonate, D-sorbitol, and esperase enzyme. Example 1 10-cycle warewash protein soil removal testing was conducted on various experimental formulations. To test the ability of compositions to clean, 2 coated ceramic tiles were used. The ceramic tiles were cleaned prior to use. A food soil solution was prepared using a 50/50 combination of Hot point soil and dosed at 2000 ppm soil. The soil included two cans of Dinty Moore Beef Stew (1360 grams), one large can of tomato sauce (822 grams), 15.5 sticks of Blue Bonnet Margarine (1746 grams) and powered milk (436.4 grams). The hot point soil was added to the machine to maintain a sump concentration of about 2000 ppm. In addition, the ceramic tiles were painted with a 1:1 mixture of whole milk and cream of chicken soup. In some experiments, both glass and plastic wares were tested for both protein removal and soil redepositon. The dish machine was turned on and allowed to reach between 150-160° F. After the dish machine reached the proper temperature, 1000 ppm of detergent was dosed per cycle with 1000 ppm of detergent and 5 grain water. Testing was conducted in a Hobart AM15 ware wash machine. The final rinse temperature was adjusted to about 180-195° F. The ceramic tiles and/or glass and plastic were placed in the dish machine. The dish machine was then started and run through an automatic cycle. At the beginning of each cycle the appropriate amount of hot point soil was added to maintain the sump concentration of 2000 ppm. When the 10 cycles ended, the ceramic tiles were allowed to dry overnight. Thereafter they were graded for spots and film accumulation (visual). The ceramic tiles were then graded for protein accumulation using Coomassie Brilliant Blue R stain followed by destaining with an aqueous acetic acid/methanol solution. The Coomassie Brilliant Blue R stain was prepared by combining 1.25 g of Coomassie Brilliant Blue R dye with 45 mL of acetic acid and 455 mL of 50% methanol in distilled water. The destaining solution consisted of 45% methanol and 10% acetic acid in distilled water. Blue dye from the Coomassie blue highlighted the protein present on the tiles. The darkest areas on the tiles showed where the most protein was present. The resulting color was measured using a CIELAB type protocol (as described in the reference: Measurement and Control of the Optical Properties of Paper , by S. J. Popson et al., Technidyne Corp., New Albany, Ind. (1996)). The results are measured according to the following data: L*—The light to dark number in the color solid. 0=totally black, 100=totally white. This is the number used for Percent Soil Removal calculations.a*—The red to green number in the color solid. A positive number is toward red and a negative number is toward green.b*—The yellow to blue number in the color solid. A positive number is toward yellow and a negative number is toward blue. The first test formulation incorporated the alkylene carbonate, alkali metal hydroxide, and alkali metal carbonate alkalinity (eliminating use of caustic beads) with surfactants and an MGDA chelant as shown in Table 2. This formulation included an alkaline cast solid with MGDA and no enzyme. TABLE 2Description - Composition 1Wt-%NaOH 50%20.42Belclene 200 Poly Maleic Acid (50%)3.00Acusol 4483.00Briquest 301 (ATMP) 50%1.04Trilon M Granules (MGDA)8.05Pluronic 25R2 (Defoamer)0.75Glycerol Carbonate9.97Sodium Xylene Sulfonate 96%1.97Sodium Carbonate, anhyd.50.72Esperase 6.0T1.08Achieve Choice 150 T0.10Total100% Alkalinity, Na 2 O37%Reflectance (CIELAB), L*, a*, b*L* = 18.9a* = 0.75b* = −16.1 The L*, a*, b* results measured and reported in Table 2 show poor warewash results (which were confirmed by visual assessment of ceramic tiles) with level of blueness indicating poor to marginal removal of protein soil. Additional testing was conducted using another zero percent caustic bead formulation with alternative chelant and polymer composition with the inclusion of an enzyme as shown in Table 3. This formulation resulted in an alkaline cast solid with MGDA, ATMP, Belclene and enzyme. TABLE 3Description - Composition 2Wt-%NaOH 50%32.96Nalco DVS3C008 (Phosphinic acid,3.41sodium salt)Acusol 4488.28Sodium Aluminate0.24Trilon M Granules (MGDA)8.23Pluronic 25R2 (Defoamer)1.04Tetronic 90R42.56Sodium Xylene Sulfonate 40%3.12Sodium Carbonate, anhyd.29.83Glycerol Carbonate10.34Total100% Alkalinity, Na 2 O28%No Reflectance Data available (CIELAB), L*, a*, b* No L*, a*, b* results were measured for the evaluation of Composition 2 (which were confirmed by visual assessment of ceramic tiles) with level of blueness indicating poor to marginal ADW performance in a 10 Cycle Machine Warewash evaluation. Additional testing with was conducted using another zero percent caustic bead formulation with alternative chelant and enzyme compositions as shown in Table 4. This formulation included an alkaline cast solid with DTPA and 3% enzyme. TABLE 4Description - Composition 3Wt-%NaOH 50%20.16Nalco DVS3C008 (Phosphinic acid,3.01sodium salt)Acusol 4487.60Sodium Aluminate 45%0.23Pluronic 25R2 (Defoamer)1.00LD 0972.52Sodium Xylene Sulfonate 96%1.05DTPA3.56D-sorbitol 70%3.02Sodium Carbonate, anhyd.47.06Glycerol Carbonate7.71Esperase 6.0T3.09Total100% Alkalinity, Na 2 O33%Reflectance (CIELAB), L*, a*, b*L* = 92.8a* = −0.2b* = 2.7 The L*, a*, b* results measured and reported in Table 4 (which were confirmed by visual assessment of ceramic tiles) with ceramic tiles from a duplicate run testing Composition 3 showing improved performance with the DTPA chelant and enzyme. Additional testing with was conducted using another zero percent caustic bead formulation with alternative chelant and enzyme compositions as shown in Table 5. This formulation included an alkaline cast solid with MGDA and 3% enzyme. TABLE 5Description - Composition 4Wt-%NaOH 50%20.01Nalco DVS3C008 (Phosphinic acid,3.48sodium salt)Acusol 4487.98Sodium Aluminate 45%0.21Pluronic 25R2 (Defoamer)1.04LD 0972.48Sodium Xylene Sulfonate 96%0.59MGDA5.08D-sorbitol 70%0.52Sodium Carbonate, anhyd.46.63Glycerol Carbonate8.96Esperase 6.0T3.00Total100% Alkalinity, Na 2 O34%Reflectance (CIELAB), L*, a*, b*L* = 88.7a* = −1.0b* = −2.7 The L*, a*, b* results measured and reported in Table 5 (which were confirmed by visual assessment of ceramic tiles) showing improved protein soil removal in comparison to the caustic bead containing control. The results with Composition 4 (zero percent caustic beads) after a 10-Cycle Machine Warewash experiment show performance, based on protein removal (degree of blueness), is better than a control (caustic beads containing without enzyme). Additional testing was conducted using another zero percent caustic bead formulation with alternative chelant and enzyme compositions as shown in Table 6. This formulation included an alkaline cast solid with Chelant00929 polymer and 3% enzyme. TABLE 6Description - Composition 5Wt-%NaOH 50%20.70Nalco DVS3C008 (Phosphinic acid,3.60sodium salt)Chelant00929 (85% Acrylic Acid, 10%12.40Maleic Acid, 5% ATBS)Sodium Aluminate 45%0.20Pluronic 25R2 (Defoamer)2.00Tetronic 90R42.60Sodium Xylene Sulfonate 96%1.30Sodium Carbonate, anhyd.46.70Glycerol Carbonate7.70Esperase 6.0T1.00Total100% Alkalinity, Na 2 O33%No Reflectance Data available (CIELAB), L*, a*, b* Composition 5 containing 25R2, 90R4 and Esperase 6.0T (3%) was evaluated for Protein Removal with reduced enzyme concentration. The protein removal results were poor to fair protein removal, noting that no chelant was used in this formulation. Visual assessment of ceramic tiles confirmed poor removal with remaining dye on the ceramic tiles with the composition that did not include a strong chelant. Additional testing was conducted using another zero percent caustic bead formulation with alternative chelant and enzyme compositions as shown in Table 7. This formulation included an alkaline cast solid with Chelant00929 and 1% enzyme. TABLE 7Description - Composition 6Wt-%NaOH 50%20.70Nalco DVS3C008 (Phosphinic acid,3.60sodium salt)Chelant00929 (85% Acrylic Acid, 10%12.40Maleic Acid, 5% ATBS)Sodium Aluminate 45%0.20Pluronic 25R2 (Defoamer)2.00Soft water2.10Tetronic 90R42.60Sodium Xylene Sulfonate 96%1.00Sodium Carbonate, anhyd.46.70Glycerol Carbonate7.70Esperase 6.0T1.00Total100% Alkalinity, Na 2 O33%No Reflectance Data available (CIELAB), L*, a*, b* Composition 6 with Esperase 6.0T without chelant did not show improved protein soil removal in comparison to the control and in comparison to evaluated formulations with a chelant (e.g. DTPA) included. Additional testing was conducted as a Control comparison using caustic bead and sodium NaOH with a higher alkalinity composition (47% Na 2 O) and various chelant, polymer/surfactant compositions as shown in Table 8. TABLE 8Description - Composition 7Wt-%NaOH 50%17.18Caustic bead54.14Sodium gluconate7.63Nalco DVS3C008 (Phosphinic acid,5.00sodium salt)Propoxy-Ethoxy N-30.75Sodium Aluminate 45%0.21Hexylene glycol0.10Soft water0.50Acusol 4488.00Sodium Xylene Sulfonate 96%1.00Sodium sulfate (anhydrous)6.49Total100% Alkalinity, Na 2 O47%Reflectance (CIELAB), L*, a*, b*L* = 86.075a* = −0.05b* = −8.80 The results testing Composition 7 (an Inline Solid Caustic Bead detergent composition) was included as a control. Performance, based on protein removal (degree of blueness) is less than experimental formulations containing enzyme and Chelant. CIELAB values for this control are 86.075, −0.05, −8.8 (L*, a*, b*, respectively). Example 2 The solid alkaline composition according to Table 9 was evaluated for enzyme stability in dispensing in light of the favorable protein removal results of similarly evaluated compositions in Example 1. A commercially-available control was used as well to compare the evaluated composition with approximately 40% sodium carbonate to a control formulation with approximately 80% sodium carbonate. The control contained 0.5% enzyme while the Composition 8 contained 2% enzyme; however the testing evaluated percentage of retained activity, which is calculated based on the amount of enzyme in each starting formula such that the difference in enzyme formulation is adjusted for. TABLE 9Description - Composition 8Wt-%NaOH 50%7.4MGDA 40%12.2SURFONIC LD-0972.6Sodium Aluminate 45%0.3Nalco DVS3C008 (Phosphinic acid,5.2sodium salt)Pluronic 25R2<1%Soft water14.8Acusol 4488.00Sodium Carbonate40.6Glycerol Carbonate6.2Esperase 6.0T2.0Total100% Alkalinity, Na 2 O21.5Molar Ratio Water:Carbonate4.68:1 During dispensing of the solid composition, run off was collected at various points throughout the test. Around 50 grams of dispensed liquid was collected on the initial steady state cycle, and then cycles 50,100,150, 200, 250, and 300. The samples were then kept in a −50 degrees Celsius freezer to preserve the enzyme. The enzyme stability was measured by QA™ 476 (Proteolytic Enzyme Activity by Plate Reader) on each sample and then the enzyme activity could be measured throughout the dispense test to calculate the percentage of retained enzymatic activity. The results are shown in FIG. 1 where showing enzyme activity remains relatively constant throughout the dispense test for both Composition 8 and Controls. A solid sample of the Composition 8 showed 49% enzyme recovery and consistently showed 50% enzyme activity, indicating that although about half the enzyme is denatured in processing the solid, no further degradation occurs during dispensing. Beneficially the results are achieved without inclusion of other conventional enzyme stabilizing agents, e.g. borates. The variability depicted in FIG. 1 between the cycles is impacted by homogeneity of solid compositions as evaluated. This is depicted from the comparison of the Control (using two different blocks of the same composition and showing variation). The retained activity is based on what an expected concentration, as a result % in excess of 100% show variation in the run off collected from a particular cycle and does not indicate gained enzymatic activity relative to the amount in the solid composition. Example 3 The enzyme stability is further confirmed through performance testing for protein removal. The Composition 8 was further compared to the Control according to the methods of Example 1 wherein the Control is a substantially higher alkalinity detergent without enzyme. The results are shown in FIG. 2 showing excellent performance in removing protein soils and under processing conditions with 5 grain water. The results are further shown in FIG. 3 where caked on soils were evaluated for removal after the same 10-cycle testing methods, again showing significant improvement by Composition 8. Example 4 A 50-cycle test was run on glassware to evaluate and compare performance of Composition 8 was to the Control for protein redeposition. The results are shown in FIG. 4 where Composition 8 again outperformed the Control and demonstrating both enzyme stability and the improved performance with the solid compositions including surfactant, enzyme and chelant with the alkalinity sources. Example 5 Additional testing was repeated according to the method in Example 2 for further confirmation of enzyme stability throughout the multicycle dispensing of a solid composition. The same solid alkaline composition according to Table 9 (Composition 8) was evaluated for enzyme stability in dispensing with modification to the method and cycles where runoff was collected as described further herein. During dispensing of the solid composition, run off was collected at various points throughout the test. Around 50 grams of dispensed liquid was collected on the initial steady state cycle, and then cycles before 50, multiple cycles before 100, approximately 175, and again between 200 and 250. The results are shown in FIG. 5 where showing enzyme activity remains relatively constant throughout the dispense test for Composition 8. A solid sample of the Composition 8 confirmed that 86% of the enzyme survived the production process. The retained activity was variable while maintaining high recovery. Again it is noted that the variability is likely due homogeneity of solid compositions as evaluated. The retained activity is based on what an expected concentration, as a result % in excess of 100% show variation in the run off collected from a particular cycle and does not indicate gained enzymatic activity relative to the amount in the solid composition. It is to be understood that while the invention has been described in conjunction with the detailed description thereof, the foregoing description is intended to illustrate, and not limit the scope of the invention, which is defined by the scope of the appended claims. Other embodiments, advantages, and modifications are within the scope of the following claims. Any reference to accompanying drawings which form a part hereof, are shown, by way of illustration only. It is understood that other embodiments may be utilized, and structural changes may be made without departing from the scope of the present disclosure. All publications discussed and/or referenced herein are incorporated herein in their entirety. The features disclosed in the foregoing description, or the following claims, or the accompanying drawings, expressed in their specific forms or in terms of a means for performing the disclosed function, or a method or process for attaining the disclosed result, as appropriate, may, separately, or in any combination of such features, be utilized for realizing the invention in diverse forms thereof.",en,PATENT_APPLICATION
122-466-154-959-619,US,20240382087,A1,2024-11-21,US_20240382087_A1_20241121,en,US,20240382087,A1,2024-11-21,US,18786191,2024-07-26,COMPACT AUTOCYLINDER COMPENSATION MODULE FOR AUTOREFRACTOR AND AUTOREFRACTOR WITH AUTOCYLINDER COMPENSATION MODULE,en,US,"AMO DEVELOPMENT, LLC","Irvine, CA",US,Lyle Kordonowy,"Sandia Park, NM",US,1,Daniel Medina,"Albuquerque, NM",A61B3/103,I,F,A61B3/00,I,L,A61B3/10,I,L,A61B3/1035,I,F,A61B3/0025,I,L,A61B3/102,I,L,A61B2562/0233,A,L,US,20240382087,A1,2024-11-21,122-466-154-959-619,1,US,20240382087,A1,2024-11-21,122-466-154-959-619,1,UNKNOWN,"An instrument includes: an aberrometer; a corneal topographer; an optical coherence tomographer; and a fixation target subsystem. The fixation target subsystem includes a fixation target and a Stokes cell disposed in an optical path between the fixation target and the eye, wherein the Stokes cell includes a first rotation stage having a first cylinder lens and a second rotation stage having a second cylinder lens, wherein; and a controller configured for controlling a rotation of the first rotation stage and the second rotation stage for correcting for an astigmatism of the eye.",en,1 - 7 . (canceled),"8 . A method, comprising: sending a collimated light beam through a Stokes cell onto a wavefront sensor, wherein the Stokes cell includes a first cylinder lens attached to a first code wheel and a second cylinder lens attached to a second code wheel; while keeping the first cylinder lens of the Stokes cell fixed, rotating the second cylinder lens and second code wheel until the wavefront sensor detects a maximum astigmatism in the light beam; rotating the first and second cylinder lenses and first and second code wheels together to line up to a zero angle reference on the wavefront sensor; rotating the first and second cylinder lenses and first and second code wheels together in a known direction to ascertain a first number of encoder counts to a first reference orientation for the first code wheel and to ascertain a second number of encoder counts to a second reference orientation for the second code wheel; and providing as an output a first value of the first number of encoder counts and a second value of the second number of encoder counts.","9 . The method of claim 8 , wherein providing the output comprises displaying the first value and the second value on a display device.","10 . The method of claim 8 , wherein providing the output comprises storing the first value and the second value in a memory device.","11 . The method of claim 10 , further comprising a motor controller reading the first value and the second value from the memory and using the first value and the second value to rotate the first cylinder lens and the second cylinder lens into desired orientations.","12 . The method of claim 8 , further comprising the wavefront sensor detecting a first strength of the first cylinder lens and a second strength of the second cylinder lens from the light beam.","13 . An instrument for measuring at least one characteristic of an eye, the instrument comprising: an aberrometer; a corneal topographer; an optical coherence tomographer; and a fixation target subsystem, wherein the fixation target subsystem includes a fixation target and a Stokes cell disposed in an optical path between the fixation target and the eye, wherein the Stokes cell includes a first rotation stage having a first cylinder lens and a second rotation stage having a second cylinder lens; and at least one controller configured for controlling a first rotation of the first rotation stage and a second rotation of the second rotation stage for correcting for an astigmatism of the eye.","14 . The instrument of claim 13 , wherein the aberrometer is configured to determine the astigmatism of the eye.","15 . The instrument of claim 13 , wherein the first rotation stage further includes a first code wheel disposed around a periphery of the first cylinder lens, wherein the first code wheel has disposed thereon a first code wheel pattern including a first indicia of a first reference orientation of the first code wheel, wherein the second rotation stage further includes a second code wheel disposed around a periphery of the second cylinder lens, wherein the second code wheel has disposed thereon a second code wheel pattern including a second indicia of a second reference orientation of the second code wheel, wherein the first and second code wheels are oriented with the first code wheel pattern and the second code wheel pattern facing each other, and wherein the Stokes cell further comprises an encoder readout card disposed between the first and second rotation stages, wherein the encoder readout card includes: a first optical sensor disposed on a first side of the encoder readout card and oriented to read encoder counts from the first code wheel as the first rotation stage is rotated, and a second optical sensor disposed on a second side of the encoder readout card and oriented to read encoder counts from the second code wheel as the first rotation stage is rotated.","16 . The instrument of claim 15 , wherein the first rotation stage includes a first gear ring, with gear teeth facing outward, surrounding a periphery of the first code wheel, wherein the second rotation stage includes a second gear ring, with gear teeth facing outward, surrounding a periphery of the second code wheel, wherein the Stokes cell further comprises: a first stepper motor connected to a first shaft, wherein the first shaft has a first gear disposed thereon, wherein the first gear engages the first gear ring such the first stepper motor rotates the first rotation stage; and a second stepper motor connected to a second shaft, wherein the second shaft has a second gear disposed thereon, wherein the second gear engages the second gear ring such the second stepper motor rotates the second rotation stage, wherein the first stepper motor and the second stepper motor are both disposed on one side of the second rotation stage.","17 . The instrument of claim 16 , wherein the first code wheel pattern comprises a plurality of radially-extending reflective regions and radially-extending opaque regions which alternate in a circumferential direction on the first code wheel.","18 . The instrument of claim 17 , wherein a distance between the first cylinder lens and the second cylinder lens is less than 7 mm.","19 . The instrument of claim 18 , wherein the distance between the first cylinder lens and the second cylinder lens is less than 4 mm.","20 . The instrument of claim 17 , wherein the at least one controller is configured to control the first and second stepper motors to rotate the first rotation stage to a first orientation and to rotate the second rotation stage to a second orientation.",en,"CROSS-REFERENCE TO RELATED APPLICATIONS This application is a divisional of and claims priority to U.S. patent application Ser. No. 17/237,001, filed Apr. 21, 2021, the entire contents of which is hereby incorporated by reference in its entirety. TECHNICAL FIELD Embodiments of this invention pertain to eye measurement systems and methods, and more particularly, to autorefractors. BACKGROUND Various types of eye measurement instruments and methods are known, including autorefractors, wavefront aberrometers, corneal topographers and optical coherence topography (OCT) systems. An autorefractor is a computer-controlled machine used during an eye examination to provide an objective measurement of the refractive error for an eye which can be used to generate a prescription for glasses or contact lenses. This is achieved by measuring how light is changed as it enters a person's eye. Wavefront aberrometry measures the way a wavefront of light passes through the cornea and the crystalline lens of an eye, which are the refractive components of the eye. Distortions that occur as light travels through the eye are called aberrations, representing specific vision errors. Various types of wavefront aberrometers and methods are known, including Tscherning aberrometers, retinal ray tracing, and Shack-Hartmann aberrometers. Corneal topography, also sometimes referred to as photokeratoscopy and videokeratoscopy, is a technique that is used to map the curved surface of the cornea. Corneal topography data can help measure the quality of vision as well as assist in eye surgery and the fitting of contact lenses. Various types of corneal topographers and methods are known, including Placido ring topographers, Scheimpflug imagers, and more recently, point source color LED topographers (CLT). Optical coherence tomography (OCT) is a method of interferometry that determines the scattering profile of a sample along the OCT beam. OCT systems can operate in the time domain (TD-OCT) or the frequency domain (FD-OCT). FD-OCT techniques have significant advantages in speed and signal-to-noise ratio as com pared to TD-OCT. The spectral information discrimination in FD-OCT is typically accomplished by using a dispersive spectrometer in the detection arm in the case of spectral domain OCT (SD-OCT) or rapidly scanning a swept laser source in the case of swept-source OCT (SS-OCT). FIG. 1 is a schematic drawing of a portion of a human eye 101 which can be used in the explanations below. Eye 101 includes, in relevant part, a cornea 402 , an iris 404 , a lens 406 , a sclera 408 and a retina 409 . Autorefractors include a fixation target that a patient or subject can view while the autorefraction measurements are performed. The fixation target is translated linearly to draw the focus of the patient's eye to the most distant refractive state of which it is capable, referred to as “the far point.” However, about ten percent of people respond poorly to the fixation target in a typical autorefractor and the eye fails to reach a state of focusing at its far point. In particular, patients with astigmatism, and especially those with strong astigmatism, tend to respond poorly to the fixation target. To improve the target for people with astigmatism, an autorefractor can contain an optical device known as a Stokes cell. Two cylinder-lenses in two motorized rotation stages can correct for the strength and axis of a patient's astigmatism. However, the size of the motors and rotation stages of existing Stokes cells creates mechanical difficulties in making a practical instrument. In particular, for example, multifunction eye measurement instruments that combine refractive, wavefront, corneal topography and other measurement modalities are desirable for a variety of reasons, including improved diagnostic capabilities. However it is particularly difficult to provide such a multifunction instrument with a Stokes cell in the path of the fixation target, especially if the instrument is meant to be small enough for a doctor's clinic. Accordingly, it would be desirable to provide a more compact Stokes cell than those previously known. It would be particularly advantageous to provide a multifunction eye measurement instrument which includes such a Stokes cell. SUMMARY OF THE INVENTION In one aspect, a device comprises a first rotation stage and a second rotation stage disposed opposite the first rotation stage. The first rotation stage includes: a first cylinder lens, a first code wheel disposed around a periphery of the first cylinder lens, wherein the first code wheel has disposed thereon a first code wheel pattern including a first indicia of a first reference orientation of the first code wheel, and a first gear ring, with gear teeth facing outward, surrounding a periphery of the first code wheel. The second rotation stage includes: a second cylinder lens, a second code wheel disposed around a periphery of the second cylinder lens, wherein the second code wheel has disposed thereon a second code wheel pattern including a second indicia of a second reference orientation of the second code wheel, and a second gear ring, with gear teeth facing outward, surrounding a periphery of the second code wheel. The first and second code wheels are oriented with the first code wheel pattern and the second code wheel pattern facing each other. The device also comprises: a first stepper motor connected to a first shaft, wherein the first shaft has a first gear disposed thereon, wherein the first gear engages the first gear ring such the first stepper motor rotates the first rotation stage; a second stepper motor connected to a second shaft, wherein the second shaft has a second gear disposed thereon, wherein the second gear engages the second gear ring such the second stepper motor rotates the second rotation stage, wherein the first stepper motor and the second stepper motor are both disposed on one side of the second rotation stage; and an encoder readout card disposed between the first and second rotation stages. The encoder readout card includes: a first optical sensor disposed on a first side of the encoder readout card and oriented to read encoder counts from the first code wheel as the first rotation stage is rotated, and a second optical sensor disposed on a second side of the encoder readout card and oriented to read encoder counts from the second code wheel as the first rotation stage is rotated. In some embodiments, the first code wheel pattern comprises a plurality of radially-extending reflective regions and radially-extending opaque regions which alternate in a circumferential direction on the first code wheel. In some embodiments, one of the first and second cylinder lenses is a plano-concave lens and the other of the first and second cylinder lenses is a plano-convex lens. In some embodiments, the distance between the first cylinder lens and the second cylinder lens is less than 7 mm. In some embodiments, the distance between the first cylinder lens and the second cylinder lens is less than 4 mm. In some embodiments, the encoder readout card includes a memory, and the memory stores a first value of a first angular distance between the first reference orientation of the first code wheel and a first cylinder lens axis of the first cylinder lens, and wherein the memory stores a second value of a second angular distance between the second reference orientation of the second code wheel and a second cylinder lens axis of the second cylinder lens. In some embodiments, the device also comprises at least one motor controller configured to control the first and second stepper motors to rotate the first rotation stage to a first orientation and to rotate the second rotation stage to a second orientation. In another aspect, a method comprises: sending a collimated light beam through a Stokes cell onto a wavefront sensor, wherein the Stokes cell includes a first cylinder lens attached to a first code wheel and a second cylinder lens attached to a second code wheel; while keeping the first cylinder lens of the Stokes cell fixed, rotating the second cylinder lens and second code wheel until the wavefront sensor detects a maximum astigmatism in the light beam; rotating the first and second cylinder lenses and first and second code wheels together to line up to a zero angle reference on the wavefront sensor; rotating the first and second cylinder lenses and first and second code wheels together in a known direction to ascertain a first number of encoder counts to a first reference orientation for the first code wheel and to ascertain a second number of encoder counts to a second reference orientation for the second code wheel; and providing as an output a first value of the first number of encoder counts and a second value of the second number of encoder counts. In some embodiments, providing the output comprises displaying the first value and the second value on a display device. In some embodiments, providing the output comprises storing the first value and the second value in a memory device. In some embodiments, the method further comprises a motor controller reading the first value and the second value from the memory and using the first value and the second value to rotate the first cylinder lens and the second cylinder lens into desired orientations. In some embodiments, the method further comprises the wavefront sensor detecting a first strength of the first cylinder lens and a second strength of the second cylinder lens from the light beam. In yet another aspect, an instrument is provided for measuring at least one characteristic of an eye. The instrument comprises: an aberrometer; a corneal topographer; an optical coherence tomographer; and a fixation target subsystem. The fixation target subsystem includes a fixation target and a Stokes cell disposed in an optical path between the fixation target and the eye. The Stokes cell includes a first rotation stage having a first cylinder lens and a second rotation stage having a second cylinder lens. The instrument includes at least one controller configured for controlling a first rotation of the first rotation stage and a second rotation of the second rotation stage for correcting for an astigmatism of the eye. In some embodiments, the aberrometer is configured to determine the astigmatism of the eye. In some embodiments, the first rotation stage further includes a first code wheel disposed around a periphery of the first cylinder lens, wherein the first code wheel has disposed thereon a first code wheel pattern including a first indicia of a first reference orientation of the first code wheel, and the second rotation stage further includes a second code wheel disposed around a periphery of the second cylinder lens, wherein the second code wheel has disposed thereon a second code wheel pattern including a second indicia of a second reference orientation of the second code wheel, wherein the first and second code wheels are oriented with the first code wheel pattern and the second code wheel pattern facing each other, and wherein the Stokes cell further comprises an encoder readout card disposed between the first and second rotation stages, wherein the encoder readout card includes: a first optical sensor disposed on a first side of the encoder readout card and oriented to read encoder counts from the first code wheel as the first rotation stage is rotated, and a second optical sensor disposed on a second side of the encoder readout card and oriented to read encoder counts from the second code wheel as the first rotation stage is rotated. In some embodiments, the first rotation stage includes a first gear ring, with gear teeth facing outward, surrounding a periphery of the first code wheel, the second rotation stage includes a second gear ring, with gear teeth facing outward, surrounding a periphery of the second code wheel, and the Stokes cell further comprises: a first stepper motor connected to a first shaft, wherein the first shaft has a first gear disposed thereon, wherein the first gear engages the first gear ring such the first stepper motor rotates the first rotation stage; and a second stepper motor connected to a second shaft, wherein the second shaft has a second gear disposed thereon, wherein the second gear engages the second gear ring such the second stepper motor rotates the second rotation stage, wherein the first stepper motor and the second stepper motor are both disposed on one side of the second rotation stage. In some embodiments, the first code wheel pattern comprises a plurality of radially-extending reflective regions and radially-extending opaque regions which alternate in a circumferential direction on the first code wheel. In some embodiments, the distance between the first cylinder lens and the second cylinder lens is less than 7 mm. In some embodiments, the distance between the first cylinder lens and the second cylinder lens is less than 4 mm. In some embodiments, the at least one controller is configured to control the first and second stepper motors to rotate the first rotation stage to a first orientation and to rotate the second rotation stage to a second orientation. BRIEF DESCRIPTION OF THE DRAWINGS The novel features of the invention are set forth with particularity in the appended claims. A better understanding of the features and advantages will be facilitated by referring to the following detailed description that sets forth illustrative embodiments using principles of the invention, as well as to the accompanying drawings, in which like numerals refer to like parts throughout the different views. Like parts, however, do not always have like reference numerals. Further, the drawings are not drawn to scale, and emphasis has instead been placed on illustrating the principles of the invention. All illustrations are intended to convey concepts, where relative sizes, shapes, and other detailed attributes may be illustrated schematically rather than depicted literally or precisely. FIG. 1 is a schematic drawing of a portion of a human eye. FIG. 2 is a functional block diagram of a portion of an example embodiment of an eye measurement instrument. FIG. 3 illustrates an example embodiment of a multifunction eye measurement instrument that combines refractive, wavefront, corneal topography and other measurement modalities, and includes a Stokes cell in an optical path with a fixation target. FIG. 4 is a side view of an example embodiment of a Stokes cell. FIG. 5 is a front cutaway view of the Stokes cell of FIG. 4 , taken along line A-A. FIG. 6 is a perspective view of the Stokes cell of FIG. 4 FIG. 7 illustrates a relative size of the Stokes cell of FIG. 4 FIG. 8 is a front view of a portion of a Stokes cell according to some embodiments, in particular showing a code wheel or encoder disk. FIG. 9A is a side view of the code wheel or encoder disk of FIG. 8 . FIG. 9B is a magnified side view of the code wheel or encoder disk of FIG. 8 . FIG. 10 is a flowchart of an example embodiment of a method of calibrating a Stokes cell. FIG. 11A illustrates a front perspective view showing an optical measurement system according to many embodiments. FIG. 11B illustrates a rear perspective view showing an optical measurement system according to many embodiments. FIG. 11C illustrates a side perspective view showing an optical measurement system according to many embodiments. FIG. 12 is a block diagram of a system including an optical measurement instrument, and a position of an eye relative to the system according to one or more embodiments described herein which may be used by the optical measurement. FIGS. 13A and 13B illustrate together an assembly illustrating a suitable configuration and integration of an optical coherence tomographer subsystem, a wavefront aberrometer subsystem, a corneal topographer subsystem, an iris imaging subsystem, a fixation target subsystem and a split-prism rangefinder according to a non-limiting embodiment of the present invention. DETAILED DESCRIPTION Exemplary embodiments of optical measurement systems and methods for measuring aberrations of an eye to illustrate various aspects and advantages of these devices and methods are described below. However, it should be understood that the principles involved in these devices and methods can be employed in a variety of other contexts, and therefore the novel devices and method disclosed and claimed here should not be construed as being limited to the example embodiments described below. As used herein the term “light source” means a source of electromagnetic radiation, particularly a source in or near the visible band of the electromagnetic spectrum, for example, in the infrared, near infrared, or ultraviolet bands of the electromagnetic radiation. As used herein, the term “light” may be extended to mean electromagnetic radiation in or near the visible band of the electromagnetic spectrum, for example, in the infrared, near infrared, or ultraviolet bands of the electromagnetic radiation. As used herein, “approximately” means with 30% (i.e., +/−30%) of a nominal value. FIG. 2 is a functional block diagram of a portion of an example embodiment of an eye measurement instrument 2000 . Eye measurement instrument 2000 includes, among other things, a wavefront aberrometer subsystem and a fixation target subsystem 50 . The wavefront aberrometer subsystem is not shown in detail in FIG. 2 , but may be implemented as wavefront aberrometer subsystem 150 described below with respect to FIG. 13B , and includes wavefront sensor or detector 155 , which may be a may be a Shack-Hartmann wavefront sensor as described below with respect to FIG. 13B . Fixation target subsystem 50 includes a fixation target 182 and an autocylinder compensation module 2010 which is configured to correct for the strength and axis of a patient's astigmatism when viewing fixation target 182 . Here, autocylinder compensation module 2010 is a Stokes cell. Fixation target subsystem 50 includes an optical system for providing an image of fixation target 182 to eye 101 . The optical system comprises a first lens 175 , a second lens 185 , a beamsplitter 183 , Stokes cell 2010 , and third lens 184 , all disposed along an optical axis 2002 . As shown in FIG. 2 , some components of fixation target 50 and the wavefront aberrometer subsystem may be disposed on a movable stage or platform 166 , which may be referred to as a Badal stage. Stokes cell 2010 includes a pair of individually rotatable cylinder lenses whose orientation may be individually controlled by a motor controller 2020 . Motor controller may be in communication with a master controller 60 for eye measurement instrument 2000 , which in turn may include a processor 61 for controlling movement of movable stage 166 and for processing wavefront data from wavefront sensor 155 . Next follows a description of the design of the optics layout of eye measurement instrument 2000 . Stokes cell 2010 is located at the focal point of second lens 185 so an image of eye 101 forms there. At this location, the cylinder lenses of Stokes cell 2010 can be rotated by motor controller 2020 to compensate for the naturally occurring astigmatism of eye 101 without the patient seeing any stretching or distorting of target 182 as the lenses rotate. Measurement instrument 2000 has wavefront sensor 166 and target path on movable (Badal) stage 166 . So moving movable stage 166 brings wavefront sensor 166 into its best range also presents target 182 to eye 101 at the same time. This is simply one particular implementation that has an advantage of using one stage for two purposes. However other implementations are possible. In operation, a patient looks into eye measurement instrument 2000 and eye measurement instrument 2000 measures the refraction of eye 101 . A probe beam from the wavefront aberrometer subsystem reflects off beamsplitter 183 into eye 101 and hits the retina. Light scatters off the retina, out of the eye, back through first lens 175 , reflects off beamsplitter 183 and onto wavefront sensor 155 that reads spherical equivalent, cylinder and axis of eye 101 , in conjunction with processor 61 . During the process, the patient sees target 182 through first lens 175 . After an initial measurement of the astigmatism of eye 101 is made by wavefront sensor 155 and processor 61 , motor controller 2020 rotates the cylinder lenses of Stokes cell 2010 to correct both the amount and axis of the astigmatism of eye 101 . Subsequently, the patient can see a clear target 182 if movable stage 166 is positioned to match the refraction of eye 101 . For a final measurement, processor 61 of master controller 60 moves movable stage 166 in a more distant direction from clear focus to draw eye 101 to its far point. At that point, all axes of target 182 appear equally blurred since Stokes Cell 2010 corrects the astigmatism of eye 101 . This is a more effective fogged target than one that would have differing amounts of focus stimulus along different angles. Here, movable stage 166 has disposed thereon all the optical components except for first lens 175 . Movable stage 166 may provide measurement instrument 2000 with the wide spherical range to measure the entire ametropic range of patients, e.g., from about −20 Diopters to +10 Diopters, whereas wavefront sensor 155 typically only covers about a six diopter spherical and astigmatic range by itself. As noted above, the size of the motors and rotation stages of existing Stokes cells creates mechanical difficulties in making a practical instrument. In particular, for example, multifunction eye measurement instruments that combine refractive, wavefront, corneal topography and other measurement modalities. For example, FIG. 3 illustrates an example embodiment of a multifunction eye measurement instrument 3000 that combines refractive, wavefront, corneal topography and other measurement modalities. Various elements of eye measurement instrument 2000 are included in multifunction eye measurement instrument 3000 and shown in FIG. 3 . Multifunction eye measurement instrument 3000 further includes a corneal topographer and an optical coherence tomographer which are not labeled in FIG. 3 so as to not obscure the other elements which are labeled. In some embodiments, multifunction eye measurement instrument may be constructed as discussed below with respect to FIGS. 12 and 13A-13B . Of note, FIG. 3 illustrates the relative small area which is available in measurement instrument 3000 to accommodate a Stokes cell. Here, measurement instrument 3000 includes a Stokes cell 4000 as an embodiment of a compact autocylinder compensation module for autorefraction measurements by multifunction eye measurement instrument 3000 . A Stokes cell as described herein may have the following characteristics. The Stokes cell has two cylinder-lenses with equal magnitudes of power, one plus, one minus (one cylinder lens may be plano-convex and the other plano-concave). Each lens is mounted in a rotation stage that has a hole or aperture in the middle, so light can pass through the lenses. The two lenses can be rotated independently (e.g. via separate motors) about a common axis with an index which identifies a reference position with respect to the axis of the lenses, so a motor controller can rotate each cylinder lens to known positions. Stokes cell 400 may have the following characteristics. Each rotation stage has an annular encoder strip or encoder wheel attached to it, surrounding the periphery of the cylinder lens. Each annular encoder strip has an indicia of a reference, or home, position—e.g., it may have a home mark on it which identifies the axis of the cylinder lens. The rotation stages are arranged so the annular encoder strips face inward, toward each other. An encoder readout card is positioned in between the rotation stages. The card is double-sided with optical sensors on both sides to read the stage positions from the encoder strips. Each rotation stage has a gear ring with teeth facing outward. The gear ring on each stage is driven by a motor that has a gear on a shaft. The motors are mounted on brackets with slots so the gear meshing may be adjusted for smooth operation and minimal backlash. Both motors are mounted on the same side of Stokes cell 4000 to reduce overall size. The motors are small so an optical beam can pass in between them. A motor controller is connected to the motors and to the encoder readout card. The motor controller is located remotely from Stokes cell 4000 in order to conserve space in the optics area. The motor controller communicates with an overall system controller which also communicates with one or more devices that measure the refraction of the eye. The overall system controller commands the motor controller to set a desired astigmatism strength and axis with the motor controller calculating appropriate stage positions. The Stokes cell has a mounting bracket that allows the lenses to be aligned to the optical axis of the eye measurement instrument. The lenses may be glued into the rotation stages with any orientation. The lens axis in the stage may be measured using a setup with a collimated beam and the wavefront sensor. The distance between the cylinder lenses is small, for example less than 7 mm, and beneficially less than 5 mm, and even more beneficially less than 4 mm. Further details regarding Stokes cell 4000 are described below with respect to FIGS. 4-7 . In particular, FIG. 4 is a side view of Stokes cell 4000 . FIG. 5 is a front cutaway view of Stokes cell 4000 of FIG. 4 , taken along line A-A. FIG. 6 is a perspective view of Stokes cell 4000 of FIG. 4 . Stokes cell 4000 includes a first rotation stage 4002 and a second rotation stage 4004 . First rotation stage 4002 includes a first cylinder lens 4010 , a first code wheel disposed around the periphery of first cylinder lens 4010 , and a first gear ring 4014 with gear teeth facing outward surrounding the periphery of the first code wheel. A first encoder reader is provided for reading an orientation of first lens 4010 via the first code wheel. Second rotation stage 4004 includes a second cylinder lens 4020 , a second code wheel 4022 disposed around the periphery of second lens 4020 , and a second gear ring 4024 with gear teeth facing outward surrounding the periphery of second code wheel 4022 . A second encoder reader 4023 is provided for reading an orientation of second lens 4020 via second code wheel 4024 . Each of first and second rotation stages 4002 and 4004 has a hole or aperture 4050 so that light can pass through first and second cylinder lenses 4010 and 4020 . Optical axis 2002 indicates the optical beam path through first and second cylinder lenses 4010 and 4020 . First and second rotation stages 4002 and 4004 are arranged so that the annular encoder wheels face inward, toward each other. Stokes cell 4000 also includes an encoder readout card 4030 which is positioned in between first and second rotation stages 4002 and 4004 . Beneficially, encoder readout card 4030 is double-sided with optical sensors on both sides to read the stage positions from the encoder wheels. Gear rings 4014 and 4024 are driven by corresponding motors 4018 and 4028 , each of which is connected to a corresponding gear 4016 and 4026 , mounted on a shaft of the motor. Beneficially, motors 4018 and 4028 are mounted on brackets with slots so the gear meshing may be adjusted for smooth operation and minimal backlash. Both motors 4018 and 4028 are mounted on the same side of Stokes cell 4000 to reduce overall size. The motors are small so an optical beam can pass in between them. Motor controller 2020 may be connected to motors 4018 and 4028 and to encoder readout card 4030 . Although a single motor controller is shown in FIG. 2 , in some embodiments separate motor controllers may be provided for motors 4018 and 4028 . Beneficially, in an eye measurement instrument such as eye measurement instrument 3000 , motor controller 2020 may be located remotely from Stokes cell 4000 in order to conserve space in the optics area. Motor controller 2020 may communicate with overall system controller 60 which also may communicate with one or more devices that measure characteristics of eye 101 , for example devices involved in the measurement of wavefront aberrations, corneal topography, and/or optical coherence tomography. Overall system controller 60 may command motor controller 2020 to set a desired astigmatism strength and axis, calculating appropriate stage orientations for first and second stages 4002 and 4002 for first and second cylinder lenses 4010 and 4020 . Stokes cell 4000 also has a mounting bracket 4050 that allows first and second cylinder lenses 4010 and 4020 to be aligned to the optical axis of the eye measurement instrument. First and second cylinder lenses 4010 and 4020 may be glued into rotation stages 4002 and 4004 with any orientation. The lens axis in each stage may be measured using a setup with a collimated beam and the wavefront sensor, as described below with respect to FIG. 10 . In some embodiments, each of first motor 4018 and second motor 4028 may be implemented via a model 106-1301 stepper motor from LIN Engineering. This particular model has a width of about 16 mm, a length of about 33 mm and step angles of 3.46 degrees. FIG. 7 illustrates a relative size of the Stokes cell 400 by comparing it to a quarter. FIG. 8 is a front view of a code wheel or encoder disk 8000 which may be included in a Stokes cell, such as Stokes cell 4000 . FIG. 9A is a side view of code wheel or encoder disk 8000 . FIG. 9B is a magnified side view of a portion 8050 of code wheel or encoder disk 8000 . Code wheel 8000 has a central aperture 8002 in which may be disposed a cylinder lens of the Stokes cell to which code wheel 8000 belongs. Code wheel 8000 also has a code wheel pattern 8010 provided thereon. Code wheel pattern 8010 comprises a series of radially extending opaque regions 8012 and radially extending reflective regions or lines 8014 which alternate in a circumferential direction around code wheel 8000 . Code wheel pattern 8010 also includes an opaque reference or index region or line 8016 as an indicia of a reference, or home, position of code wheel 8000 . This may be used to determine an orientation of code wheel 8000 (and consequently an orientation of a cylinder lens disposed within aperture 8002 ). Code wheel 8000 also has a reflective annular region 8004 disposed between aperture 8002 and pattern 8010 . Beneficially, code wheel 8000 comprises a base reflective material, such as a nickel alloy, on which has been provided a plurality of radially extending opaque lines 8014 , separated and spaced apart from each other by a reflective region 8012 , and opaque reference line 8016 . In some embodiments, the opaque material may comprise a copper alloy. In some embodiments, each opaque line 8014 may span an angle of 0.27 degrees, and each reflective region may span an angle of 0.27 degrees. In that case, reference line 8016 may span an angle which is three times greater than the angle of opaque lines 8014 —e.g., 0.81 degrees. In that case, code wheel pattern 8010 may comprise 664 opaque lines 8014 separated from each other by an approximately equal number of reflective regions 8012 . As code wheel 8000 is rotated by a motor, such as motor 4018 or 4028 , the alternating series of opaque lines 8014 and reflective regions 8012 may be read by an encoder of encoder readout card 4040 to determine the orientation of code wheel 8000 , using reference line 8016 as a reference or home position. In some embodiments, the encoders (one for each code wheel) of encoder readout card 4030 may be implemented with a model AEDR-850x three-channel reflective incremental encoder from Avago Technologies. FIG. 10 is a flowchart of an example embodiment of a method 10000 of calibrating a Stokes cell such as Stokes cell 4000 . An operation 10010 includes sending a collimated light beam through the Stokes cell onto a wavefront sensor. An operation 10020 includes keeping one cylinder lens of the Stokes cell fixed while rotating the other cylinder lens until the wavefront sensor reads maximum astigmatism for the light beam. An operation 10030 includes rotating the two cylinder lenses together to line up to a zero angle reference on the wavefront sensor. An operation 10040 includes rotating the two cylinder lenses together in a known direction to find the number of encoder counts to the home or reference position for each of the code wheels. This may be referred to as the Axis Home Offset for the code wheel, and indicates the angular distance between the reference or home mark on the code wheel and the cylinder lens axis. Here, each encoder count may correspond to a transition between an opaque region and a reflective region on the code wheel pattern. Alternatively, each encoder count may correspond to an interval from one reflective region to the next reflective region in the code wheel pattern as the code wheel is rotated. An operation 10050 includes providing an output of the Axis Home Offset for each cylinder lens of the Stokes cell, for example as a value written to a file in a memory of the eye measurement instrument, or displayed on a display device, etc. An optional operation 10060 includes recording or storing the Axis Home Offset for each cylinder lens in a memory of the encoder readout card. Subsequently the values of the Axis Home Offsets could be read by a motor controller that drives the rotation stages of the Stokes cell. In an optional operation 10070 , the wavefront sensor also determines the strength of the cylinder lenses for the Axis Home Offset, and that information can be provided as an output or recorded or stored in the encoder readout card. The principles of a Stokes cell as described above may be applied to an optical measurement instrument which includes additional functionality, such as the ability to measure corneal topography and/or to make wavefront aberrometry measurements for the eye. Embodiments of such an optical measurement instrument, and methods of operation thereof, will now be described. As shown in FIGS. 11A-11C , an optical measurement system 1 , according to many embodiments, is operable to provide for a plurality of measurements of the human eye, including wavefront aberrometry measurements, corneal topography measurements, and optical coherence tomography measurements to measure characteristics of the cornea, the lens capsule, the lens and the retina. Optical measurement system 1 includes a main unit 2 which comprises a base 3 and includes many primary subsystems of many embodiments of optical measurement system 1 . For example, externally visible subsystems include a touch-screen display control panel 7 , a patient interface 4 and a joystick 8 . Patient interface 4 may include one or more structures configured to hold a patient's head in a stable, immobile and comfortable position during the diagnostic measurements while also maintaining the eye of the patient in a suitable alignment with the diagnostic system. In a particularly preferred embodiment, the eye of the patient remains in substantially the same position relative to the diagnostic system for all diagnostic and imaging measurements performed by optical measurement system 1 . In one embodiment patient interface 4 includes a chin support 6 and/or a forehead rest 5 configured to hold the head of the patient in a single, uniform position suitably aligned with respect to optical measurement system 1 throughout the diagnostic measurement. As shown in FIG. 11C , the optical measurement system 1 may be disposed so that the patient may be seated in a patient chair 9 . Patient chair 9 can be configured to be adjusted and oriented in three axes (x, y, and z) so that the patent's head can be at a suitable height and lateral position for placement on the patient interface. In many embodiments, optical measurement system 1 may include external communication connections. For example, optical measurement system 1 can include a network connection (e.g., an RJ45 network connection or WiFi) for connecting optical measurement system 1 to a network. The network connection can be used to enable network printing of diagnostic reports, remote access to view patient diagnostic reports, and remote access to perform system diagnostics. Optical measurement system 1 can include a video output port (e.g., HDMI) that can be used to output video of diagnostic measurements performed by optical measurement system 1 . The output video can be displayed on a display device, for example an external monitor for viewing by physicians or users. The output video can also be recorded for, for example, archival or training purposes. Optical measurement system 1 can include one or more data output ports (e.g., USB) to enable export of patient diagnostic reports to, for example, a data storage device or a computer readable medium, for example a non-volatile computer readable medium, coupled to a laser cataract surgery device for use of the diagnostic measurements in conducting laser cataract surgeries. The diagnostic reports stored on the data storage device or computer readable medium can then be accessed at a later time for any suitable purpose such as, for example, printing from an external computer in the case where the user without access to network based printing or for use during cataract surgery, including laser cataract surgery. Other uses of network data include obtaining service logs, outcomes analysis and algorithm improvement. FIG. 12 is a block diagram of optical measurement system 1 according to one or more embodiments described herein. Optical measurement system 1 includes: an optical coherence tomography (OCT) subsystem 10 , a wavefront aberrometer subsystem 20 , and a corneal topographer subsystem 30 for measuring one or more characteristics of a subject's eye. Optical measurement system 1 may further include an iris imaging subsystem 40 , a fixation target subsystem 50 , a controller 60 , including one or more processor(s) 61 and memory 62 , a display 70 and an operator interface 80 . Optical measurement system 1 further includes patient interface 4 for a subject to present his or her eye 101 for measurement by optical measurement system 1 . As noted above, optical coherence tomography subsystem 10 may be configured to measure the spatial disposition (e.g., three-dimensional coordinates such as X, Y, and Z of points on boundaries) of eye structures in three dimensions. Such structure of interest can include, for example, the anterior surface of the cornea, the posterior surface of the cornea, the anterior portion of the lens capsule, the posterior portion of the lens capsule, the anterior surface of the crystalline lens, the posterior surface of the crystalline lens, the iris, the pupil, the limbus and/or the retina. The spatial disposition of the structures of interest and/or of suitable matching geometric modeling such as surfaces and curves can be generated and/or used by controller 60 for a number of purposes, including, in some embodiment to program and control a subsequent laser-assisted surgical procedure. The spatial disposition of the structures of interest and/or of suitable matching geometric modeling can also be used to determine a wide variety of parameters. Beneficially, optical coherence tomography subsystem 10 may employ swept source optical coherence tomography (SS-OCT) or spectral domain OCT (SDOCT). In some embodiments, OCT subsystem 10 may include OCT scanning subsystem 3000 . Wavefront aberrometer subsystem 20 is configured to measure ocular aberrations, which may include low and high order aberrations, by measuring the wavefront emerging from the eye by, for example a Shack-Hartman wavefront sensor. Corneal topographer subsystem 30 may apply any number of modalities to measure the shape of the cornea including one or more of a keratometry reading of the eye, a corneal topography of the eye, an optical coherence tomography of the eye, a Placido disc topography of the eye, a reflection of a plurality of points from the cornea topography of the eye, a grid reflected from the cornea of the eye topography, a Hartmann-Shack measurement of the eye, a Scheimpflug image topography of the eye, a confocal tomography of the eye, a Helmholtz source topographer, or a low coherence reflectometry of the eye. The shape of the cornea should generally be measured while the patient is engaged with patient interface 4 . Fixation target subsystem 50 is configured to control the patient's accommodation and alignment direction, because it is often desired to measure the refraction and wavefront aberrations when an eye under measurement is focused at its far point Images captured by corneal topographer subsystem 10 , wavefront aberrometer 20 , optical coherence tomographer subsystem 30 or camera 40 may be displayed with a display of operator interface 80 or display 70 of optical measurement system 1 , respectively. Operator interface 80 may also be used to modify, distort, or transform any of the displayed images. Shared optics 55 provide a common propagation path that is disposed between patient interface 4 and each of optical coherence tomography (OCT) subsystem 10 , wavefront aberrometer subsystem 20 , corneal topographer subsystem 30 , and in some embodiments, camera 40 , and fixation target subsystem 50 . In many embodiments, shared optics 55 may comprise a number of optical elements, including mirrors, lenses and beam combiners to receive the emission from the respective subsystem to the patient's eye and, in some cases, to redirect the emission from a patient's eye along the common propagation path to an appropriate director. Controller 60 controls the operation of optical measurement system 1 and can receive input from any of optical coherence tomographer (OCT) subsystem 10 , wavefront aberrometer subsystem 20 , corneal topographer subsystem 30 for measuring one or more characteristics of a subject's eye, camera 40 , fixation target subsystem 50 , display 70 and operator interface 80 via communication paths 58 . Controller 60 can include any suitable components, such as one or more processor, one or more field-programmable gate array (FPGA), and one or more memory storage devices. In many embodiments, controller 60 controls display 70 to provide for user control over the laser eye surgery procedure for pre-cataract procedure planning according to user specified treatment parameters as well as to provide user control over the laser eye surgery procedure. Communication paths 58 can be implemented in any suitable configuration, including any suitable shared or dedicated communication paths between controller 60 and the respective system components. Operator interface 80 can include any suitable user input device suitable to provide user input to controller 60 . For example, user interface devices 80 can include devices such as joystick 8 , a keyboard, or a touchscreen display. FIGS. 13A and 13B are simplified block diagrams illustrating an assembly 100 according to many embodiments which may be included in optical measurement system 1 . Assembly 100 is a non-limiting example of suitable configurations and integration of an optical coherence tomography (OCT) subsystem 190 , a wavefront aberrometer subsystem 150 , a corneal topographer subsystem 140 for measuring one or more characteristics of a subject's eye 101 , camera 40 , a fixation target subsystem 180 and shared optics. The shared optics generally comprise one or more components of a first optical system 170 disposed along a central axis 102 passing through the opening or aperture 114 of the structure 110 . First optical system 170 directs light from the various light sources along the central axis 102 towards an eye 101 and establishes a shared or common optical path along which the light from the various light sources travel to eye 101 . In one embodiment, optical system 170 comprises a quarter wave plate 171 , a first beamsplitter 172 , a second beamsplitter 1715 , an optical element (e.g., a lens) 174 , a lens 1710 , a third beamsplitter 176 , and a structure including an aperture 178 . Additional optical systems may be used in assembly 100 to direct light beams from one or more light sources to the first optical system 170 . For example, a second optical system 160 directs light to the first optical system 170 from wavefront aberrometer subsystem 150 and comprises mirror 153 , beam splitter 183 and lens 185 . Other configurations of assembly 100 may be possible and may be apparent to a person of skill in the art. Corneal topographer subsystem 140 comprises a structure 110 having a principal surface 112 with an opening or aperture 114 therein; a plurality of first (or peripheral) light sources 120 provided on the principal surface 112 of structure 110 ; a Helmholtz light source 130 ; and a detector, photodetector, or detector array 141 , for example a camera. In one embodiment, structure 110 has the shape of an elongated oval or “zeppelin” with openings or apertures at either end thereof. An example of such a structure is disclosed in Yobani Meji′a-Barbosa et al., “Object surface for applying a modified Hartmann test to measure corneal topography,” APPLIED OPTICS, Vol. 40, No. 31 (Nov. 1, 2001) (“Meji′a-Barbosa”). In some embodiments, principal surface 112 of structure 110 is concave when viewed from the cornea of eye 101 , as illustrated in FIG. 13A . In one embodiment where principal surface 112 is concave, principal surface 112 has the shape of a conical frustum. Alternatively, principal surface 112 may have a shape of hemisphere or some other portion of a sphere, with an opening or aperture therein. Also alternatively, principal surface 112 may have the shape of a modified sphere or conical frustum, with a side portion removed. Beneficially, such an arrangement may improve the ergonomics of assembly 100 by more easily allowing structure 110 to be more closely located to a subject's eye 1001 without being obstructed by the subject's nose. Of course, a variety of other configurations and shapes for principal surface 112 are possible. In the embodiment of FIG. 13A , the plurality of first light sources 120 are provided on the principal surface 112 of structure 110 so as to illuminate the cornea of eye 101 . In one embodiment, light sources 122 may comprise individual light generating elements or lamps, such as light emitting diodes (LEDs) and/or the tips of the individual optical fibers of a fiber bundle. Alternatively, principal surface 112 of structure 110 may have a plurality of holes or apertures therein, and one or more backlight lamps, which may include reflectors and/or diffusers, may be provided for passing lighting through the holes to form the plurality of first light sources 120 which project light onto the cornea of eye 101 . Other arrangements are possible. In another embodiment, structure 110 is omitted from assembly 100 , and the first light sources 120 may be independently suspended (e.g., as separate optical fibers) to form a group of first light sources 120 arranged around a central axis, the group being separated from the axis by a radial distance defining an aperture in the group (corresponding generally to the aperture 114 in the structure 110 illustrated in FIG. 13A ). In operation, a ray (solid line) from one of the first light sources 120 is reflected by the cornea and passes through optical system 170 , to appear as a light spot on detector array 141 . It will be appreciated that this ray is representative of a small bundle of rays that make it through optical system 170 and onto detector array 141 , all of which will focus to substantially the same location on detector array 141 . Other rays from that first light source 120 are either blocked by the aperture 178 or are otherwise scattered so as to not pass through the optical system 170 . In similar fashion, light from the other first light sources 120 are imaged onto detector array 141 such that each one of first light sources 120 is imaged or mapped to a location on detector array 141 that may be correlated to a particular reflection location on the cornea of eye 101 and/or the shape of the cornea. Thus, detector array 141 detects the light spots projected thereon and provides corresponding output signals to a processor of controller 60 ( FIG. 12 ). The processor determines the locations and/or shape of the light spots on detector array 141 , and compares these locations and/or shapes to those expected for a standard or model cornea, thereby allowing the processor of controller 60 to determine the corneal topography. Alternatively, other ways of processing the spot images on detector array 141 may be used to determine the corneal topography of eye 101 , or other information related to the characterization of eye 101 . Detector array 141 comprises a plurality of light detecting elements arranged in a two dimensional array. In one embodiment, detector array 141 comprises such a charge-coupled device (CCD), such as may be found in a video camera. However, other arrangements such as a CMOS array, or another electronic photosensitive device, may be employed instead. Beneficially, the video output signal(s) of detector array 141 are provided to processor 60 which processes these output signals as described in greater detail below. Assembly 100 also comprises a Helmholtz light source 130 configured according to the Helmholtz principle. As used herein, the term “Helmholtz source” or “Helmholtz light source” means one or a plurality of individual light sources disposed such that light from each of the individual light sources passes through an optical element having optical power, reflects off of a reference or test object, passes through the optical element, and is received by a detector, wherein light from the Helmholtz source is used to determine geometric and/or optical information of at least a portion of a surface of the reference or test object. In general, it is a characteristic of Helmholtz sources that the signal at the detector is independent of the relative position of the test or reference object relative to the Helmholtz source. As used herein, the term “optical element” means an element that refracts, reflects, and/or diffracts light and has either positive or negative optical power. In such embodiments, the Helmholtz light source 130 is located at optical infinity with respect to eye 101 . The Helmholtz principle includes the use of such infinite sources in combination with a telecentric detector system: i.e., a system that places the detector array at optical infinity with respect to the surface under measurement, in addition to insuring that the principal measured ray leaving the surface is parallel to the optical axis of the instrument. The Helmholtz corneal measurement principle has the Helmholtz light source at optical infinity and the telecentric observing system so that detector array 141 is also optically at an infinite distance from the images of the sources formed by the cornea. Such a measurement system is insensitive to axial misalignment of the corneal surface with respect to the instrument. In one embodiment, the Helmholtz light source 130 comprises a second light source 132 which may comprise a plurality of lamps, such as LEDs or optical fiber tips. In one embodiment, second light source 132 comprises an LED and a plate 133 with plurality of holes or apertures in a surface that are illuminated by one or more backlight lamps with an optical element 131 , which may comprise diffusers. In one embodiment, lamps of second light sources 132 are located off the central optical axis 102 of assembly 100 , and light from second light sources 132 is directed toward optical element 171 by third beamsplitter 176 . The operation of the topographer portion of assembly 100 may be conducted with the combined use of first light source 120 and the Helmholtz light source 130 . In operation, detector array 141 detects the light spots projected thereon from both Helmholtz light source 130 (detected at a central portion of detector array 141 ) and first light sources 120 (detected at a peripheral portion of detector array 141 ) and provides corresponding output signals to processor. In general, the images of first light sources 120 that appear on detector array 141 emanate from an outer region of the surface of the cornea, and the images of Helmholtz light source 130 that appear on detector array 141 emanate from a central or paraxial region of the surface of the cornea. Accordingly, even though information about the central region of the corneal surface (e.g., surface curvature) cannot be determined from the images of first light sources 120 on detector array 141 , such information can be determined from the images of Helmholtz light source 130 on detector array 141 . A processor of controller 60 determines the locations and/or shapes of the light spots on detector array 141 , and compares these locations and/or shapes to those expected based for a standard or model cornea, thereby allowing the processor to determine the corneal topography of eye 101 . Accordingly, the topography of the entire corneal surface can be characterized by assembly 100 without a “hole” or missing data from the central corneal region. As seen in FIG. 13B , assembly 100 also includes Stokes cell 4000 in the optical path between target 182 and eye 101 . In some embodiments, contemporaneous with obtaining the eye measurement data (e.g., wavefront aberrometry data and/or corneal topographer data) for eye 101 , an image of sclera 408 of eye 101 may be captured by detector array 141 . The image may be processed by a processor (e.g., processor 61 of controller 60 ) executing a pattern recognition algorithm as known in the art to identify unique features of sclera 408 , for example scleral blood vessels. Processor 61 may execute a pattern recognition algorithm as a set of computer instructions stored in a memory (e.g., memory 62 ) associated with processor 61 . Processor 61 may use the identified features from the image of eye 101 as fiducials or registration markers for the eye measurement data for eye 101 . In some embodiments, processor 61 may store in memory 62 the eye measurement data (e.g., wavefront aberrometry data and/or corneal topographer data), a first image of eye 101 focused at the appropriate image plane for the eye measurement data (e.g., focused at iris 404 for wavefront measurement data), a second image of eye 101 focused at the fiducials (e.g., scleral blood vessels), and registration data which registers the eye measurement data to the locations of the identified features or fiducials in the image of eye 101 . This set of data may be used by a surgical instrument in a subsequent surgery. For example, the surgical instrument may include a camera which is able to capture an image of eye 101 , including the fiducials. By mapping the fiducials identified by assembly 100 to the same fiducials observed by the camera of the surgical instrument, the eye measurement data may be registered to the locations of the fiducials observed by the camera of the surgical instrument via the registration data of assembly 100 . Wavefront aberrometer subsystem 150 of assembly 100 comprises a third light source 152 providing a probe beam and a wavefront sensor 155 . Wavefront aberrometer subsystem 150 preferably further comprises a collimating lens 154 , a polarizing beamsplitter 156 , an adjustable telescope comprising a first optical element, lens 163 and a second optical element, lens 164 , a movable stage or platform 166 , and a dynamic-range limiting aperture 165 for limiting a dynamic range of light provided to wavefront sensor 155 so as to preclude data ambiguity. Light from the wavefront aberrometer subsystem is directed to one of the constituent optical elements of the optical system 170 disposed along a central axis 102 passing through the opening or aperture 114 of the structure 110 . It will be appreciated by those of skill in the art that the lenses 163 , 164 , or any of the other lenses discussed herein, may be replaced or supplemented by another type of converging or diverging optical element, such as a diffractive optical element. Light source 152 may be an 840 nm SLD (super luminescent laser diode). An SLD is similar to a laser in that the light originates from a very small emitter area. However, unlike a laser, the spectral width of the SLD is very broad, about 40 nm. This tends to reduce speckle effects and improve the images that are used for wavefront measurements. Beneficially, wavefront sensor 155 may be a Shack-Hartmann wavefront sensor comprising a detector array and a plurality of lenslets for focusing received light onto its detector array. In that case, the detector array may be a CCD, a CMOS array, or another electronic photosensitive device. However, other wavefront sensors may be employed instead. Embodiments of wavefront sensors which may be employed in one or more systems described herein are described in U.S. Pat. No. 6,550,917, issued to Neal et al. on Apr. 22, 2003, and U.S. Pat. No. 5,777,719, issued to Williams et al. on Jul. 7, 1998, both of which patents are hereby incorporated herein by reference in their entirety. The aperture or opening in the middle of the group of first light sources 120 (e.g., aperture 114 in principal surface 112 of structure 110 ) allows assembly 100 to provide a probe beam into eye 101 to characterize its total ocular aberrations. Accordingly, third light source 152 supplies a probe beam through a light source polarizing beam splitter 156 and polarizing beam splitter 162 to first beamsplitter 172 of optical system 170 . First beamsplitter 172 directs the probe beam through aperture 114 to eye 101 . Preferably, light from the probe beam is scattered from the retina of eye 100 , and at least a portion of the scattered light passes back through aperture 114 to first beamsplitter 172 . First beamsplitter 172 directs the back scattered light back through beam splitter 172 to polarizing beamsplitter 162 , mirror 153 to wavefront sensor 155 . Wavefront sensor 155 outputs signals to a processor of controller 60 which uses the signals to determine ocular aberrations of eye 101 . Preferably, the processor is able to better characterize eye 101 by considering the corneal topography of eye 101 measured by corneal topography subsystem 140 , which may also be determined by the processor based on outputs of detector array 141 , as explained above. In operation of wavefront aberrometer subsystem 150 , light from light source 152 is collimated by lens 154 . The light passes through light source polarizing beam splitter 156 . The light entering light source polarizing beam splitter 156 is partially polarized. Light source polarizing beam splitter 156 reflects light having a first, S, polarization, and transmits light having a second, P, polarization so the exiting light is 100% linearly polarized. In this case, S and P refer to polarization directions relative to the hypotenuse in light source polarizing beam splitter 156 . Light from light source polarizing beam splitter 156 enters polarizing beamsplitter 162 . The hypotenuse of polarizing beamsplitter 162 is rotated 90 degrees relative to the hypotenuse of light source polarizing beamsplitter 156 so the light is now S polarized relative the hypotenuse of polarizing beamsplitter 162 and therefore the light reflects upwards. The light from polarizing beamsplitter 162 travels upward and passes through toward beam splitter 172 , retaining its S polarization, and then travels through quarter wave plate 171 . Quarter wave plate 171 converts the light to circular polarization. The light then travels through aperture 114 in principal surface 112 of structure 110 to eye 101 . Preferably, the beam diameter on the cornea is between 1 and 2 mm. Then the light travels through the cornea and focuses onto the retina of eye 101 . The focused spot of light becomes a light source that is used to characterize eye 101 with wavefront sensor 155 . Light from the probe beam that impinges on the retina of eye 101 scatters in various directions. Some of the light reflects back as a semi-collimated beam back towards assembly 100 . Upon scattering, about 90% of the light retains its polarization. So the light traveling back towards assembly is substantially still circularly polarized. The light then travels through aperture 114 in principal surface 112 of structure 110 , through quarterwave plate 171 , and is converted back to linear polarization. Quarterwave plate 171 converts the polarization of the light from the eye's retina so that it is P polarized, in contrast to probe beam received from third light source 150 having the S polarization. This P polarized light then reflects off of first beamsplitter 172 , and then reaches polarizing beamsplitter 162 . Since the light is now P polarized relative the hypotenuse of polarizing beamsplitter 162 , the beam is transmitted and then continues onto mirror 153 . After being reflected by mirror 153 , light is sent to an adjustable telescope comprising a first optical element 164 and a second optical element (e.g., lens) 163 and a movable stage or platform 166 . The beam is also directed through a dynamic-range limiting aperture 165 for limiting a dynamic range of light provided to wavefront sensor 155 so as to preclude data ambiguity. When wavefront sensor 155 is a Shack-Hartmann sensor, the light is collected by the lenslet array in wavefront sensor 155 and an image of spots appears on the detector array (e.g., CCD) in wavefront sensor 155 . This image is then provided to a processor of controller 60 and analyzed to compute the refraction and aberrations of eye 101 . OCT subsystem 190 of assembly 100 may comprise an OCT assembly 191 , and a third optical path 192 which directs the OCT beam of the OCT light source to the first optical path 170 . The third optical path 192 may comprise a fiber optic line 196 , for conducting the OCT beam from the OCT light source of OCT assembly 191 , a Z-scan device 193 operable to alter the focus of the beam in the Z-direction (i.e., along the direction of propagation of the OCT beam) under control of the controller, and X-scan device 195 , and a Y-scan device 197 operable to translate the OCT beam in the X and Y directions (i.e., perpendicular to the direction of propagation of the of the OCT beam), respectively, under control of controller 60 . The OCT light source and reference arm may be incorporated into assembly 100 of optical measurement system 1 shown in FIG. 13A . Alternatively, OCT assembly 191 may be housed in a second unit or housing 200 and the OCT beam from the OCT source may be directed from second unit 200 to the main unit by optical pathway 192 . Beneficially, the OCT systems and methods employed in optical measurement system 1 and assembly 100 may employ swept source optical coherence tomography (SS-OCT) as described above. Beneficially, optical measurement system 1 , assembly 100 and OCT subsystem 190 may each comprise OCT interferometer 1000 , 3000 or 4000 . As explained above, in SS-OCT, a rapid-scanning laser source is employed. By rapidly sweeping the source wavelength over a broad wavelength range, and collecting all the scattering and reflection information at each wavelength and at each position, the collected spectral data may be inverse-Fourier-transformed to recover the spatial depth-dependent information for the object under test (e.g., eye 101 ). In operation, as shown in FIG. 13A , after exiting connector 212 , OCT probe beam 214 may be collimated, for example using a collimating optical fiber 196 . Following collimating fiber 196 OCT probe beam 214 is optionally directed to Z-scan device 193 operable to change the focal point of OCT probe beam 214 in the Z-direction, and X- and Y-scan devices 195 and 197 , which are operable to scan the OCT beam in X and Y-directions perpendicular to the Z-direction. Following the collimating optical fiber 196 , OCT probe beam 214 continues through a Z-scan device 193 . Z-scan device 193 may comprise a Z-telescope 194 which is operable to scan focus position of OCT probe beam 214 in the patient's eye 101 along the Z axis. For example, Z-telescope 194 may include a Galilean telescope with two lens groups (each lens group includes one or more lenses). One of the lens groups moves along the Z axis about the collimation position of Z-scan device 193 . In this way, the focus position in the patient's eye 101 moves along the Z axis. In general, there is a relationship between the motion of lens group and the motion of the focus point. The exact relationship between the motion of the lens and the motion of the focus in the Z axis of the eye coordinate system does not have to be a fixed linear relationship. The motion can be nonlinear and directed via a model or a calibration from measurement or a combination of both. Alternatively, the other lens group can be moved along the Z axis to adjust the position of the focus point along the Z axis. Z-telescope 194 functions as a Z-scan device for changing the focus point of OCT probe beam 214 in patient's eye 101 . Z-scan telescope 194 can be controlled automatically and dynamically by controller 60 and selected to be independent or to interplay with X and Y scan devices 195 and 197 . After passing through the z-scan device, the OCT probe beam 214 is incident upon an X-scan device 195 , which is operable to scan the OCT probe beam 214 in the X direction, which is dominantly transverse to the Z axis and transverse to the direction of propagation of OCT probe beam 214 . X-scan device 195 is controlled by controller 60 , and can include suitable components, such as a lens coupled to a MEMS device, a motor, galvanometer, or any other well-known optic moving device. The relationship of the motion of OCT probe beam 214 as a function of the motion of the actuator of X-scan device 195 does not have to be fixed or linear. Modeling or calibrated measurement of the relationship or a combination of both can be determined and used to direct the location of OCT probe beam 214 . After being directed by the X-scan device 195 , OCT probe beam 214 is incident upon a Y scan device 197 , which is operable to scan OCT probe beam 214 in the Y direction, which is dominantly transverse to the X and Z axes. Y-scan device 197 is controlled by the controller 60 , and can include suitable components, such as a lens coupled to a MEMS device, motor, galvanometer, or any other well-known optic moving device. The relationship of the motion of the beam as a function of the motion of the Y actuator of Y-scan device 197 does not have to be fixed or linear. Modeling or calibrated measurement of the relationship or a combination of both can be determined and used to direct the location of OCT probe beam 214 . Alternatively, the functionality of X-Scan device 195 and Y-Scan device 197 can be provided by an XY-scan device configured to scan OCT probe beam 214 in two dimensions transverse to the Z axis and the propagation direction of OCT probe beam 214 . The X-scan and Y scan devices 195 , 197 change the resulting direction of OCT probe beam 214 , causing lateral displacements of OCT probe beam 214 located in the patient's eye 101 . OCT probe beam 214 is then directed to beam splitter 1715 through lens 1720 , and thence through lens 1710 , quarter wave plate 171 and aperture 114 and to the patient eye 101 . Reflections and scattering off of structures within the eye provide return beams that retrace back through the patient interface quarter wave plate 171 , lens 1710 , beam splitter 1715 , lens 1720 , Y-scan device 197 , X-scan device 195 , Z-scan device 193 , optical fiber 196 and beam combiner 204 , and back into the OCT detection device. The returning back reflections of the sample arm are combined with the returning reference portion and directed into the detector portion of the OCT detection device, which generates OCT signals in response to the combined returning beams. The generated OCT signals that are in turn interpreted by controller 60 to determine the spatial disposition of the structures of interest in patient's eye 101 . The generated OCT signals can also be interpreted by the controller to determine the spatial disposition of the structures of interest in the patient's eye 101 . The generated OCT signals can also be interpreted by the control electronics to align the position and orientation of the patient eye 101 within patient interface 4 . Optical measurement systems disclosed herein may comprise an iris imaging subsystem 40 . Iris imaging subsystem 40 generally may comprise an infrared light source, for example an infrared light source 152 , and detector 141 . In operation light from light source 152 is directed along second optical path 160 to first optical path 170 and is subsequently directed to eye 101 as described above. Light reflected from the iris of eye 101 is reflected back along first optical path 170 to detector 141 . In normal use, an operator will adjust a position or alignment of system 100 in X, Y and Z directions to align the patient according to the image detector array 141 . In one embodiment of the iris imaging subsystem, eye 101 is illuminated with infrared light from light source 152 . In this way, the wavefront obtained by wavefront sensor 155 will be registered to the image from detector array 141 . The image that the operator sees is the iris of eye 101 . The cornea generally magnifies and slightly displaces the image from the physical location of the iris. So the alignment that is done is actually to the entrance pupil of the eye. This is generally the desired condition for wavefront sensing and iris registration. Iris images obtained by the iris imaging subsystem may be used for registering and/or fusing the multiple data sets obtained by the various subsystems of optical measurement system 1 by methods described, for instance, in “Method for registering multiple data sets,” U.S. patent application Ser. No. 12/418,841, which is incorporated herein by reference. As set forth in application Ser. No. 12/418,841, wavefront aberrometry may be fused with corneal topography, optical coherence tomography and wavefront, optical coherence tomography and topography, pachymetry and wavefront, etc. For instance, with image recognition techniques it is possible to find the position and extent of various features in an image. Regarding iris registration images, features that are available include the position, size and shape of the pupil, the position, size and shape of the outer iris boundary (OIB), salient iris features (landmarks) and other features as are determined to be needed. Using these techniques, patient movement between measurements (and/or during a measurement sequence) can be identified, as well as changes in the eye itself (including those induced by the measurement, such as changes in the size of the pupil, changes in pupil location, etc.). In many embodiments, optical measurement system 1 includes fixation target subsystem 50 ( FIG. 12 ), and accordingly assembly 100 shown in FIGS. 13A and 13B includes fixation target subsystem 180 which includes a fixation target 182 for the patient to view. Fixation target subsystem 180 is used to control the patient's accommodation and alignment, because it is often desired to measure the refraction and wavefront aberrations when eye 101 is focused at its far point (e.g., because LASIK treatments are primarily based on this). In fixation target subsystem 180 , a projection of a target, for instance a cross-hair pattern is projected onto eye 101 of the patient, the cross hair pattern being formed, e.g. by fixation target 182 comprising a backlit LED and a film. In operation, light originates from fixation target 182 and lenses 186 and 184 . Lens 185 collects the light and forms an aerial image T 2 . This aerial image T 2 is the one that the patient views. The patient focus is maintained on aerial image T 2 during measurement so as to maintain the eye in a fixed focal position. In some embodiments, fixation target 182 may comprise a video target which may have a variable center location under control of one or more processors 61 of controller 60 , for example a blinking dot which may cause aerial image T 2 to appear at a plurality of different angular locations (e.g., five different angular locations) relative to eye 101 . In this case, the patient may be instructed to gaze at the blinking dot as it moves from location to location to create a plurality of different gaze angles for eye 101 . Accordingly, optical coherence tomography subsystem 10 may collect OCT data sets for retina 409 for each of the plurality of gaze angles, e.g., five different gaze angles, causing five different regions of retina 409 to be sampled. In that way, in some embodiments the total combined scanned diameter retina 409 could be expanded from 3 mm to a larger region with a diameter of approximately 6 mm, providing a larger area of retina 409 from which retinal health may be evaluated. The operating sequence the optical measurement system and methods of the present is not particularly limited. A scan of the patient's eye may comprise one or more of a wavefront aberrometry measurement of a patient's eye utilizing the wavefront aberrometry subsystem, a corneal topography measurement of a patient's eye and an OCT scan of the patient's eye using the OCT subsystem, wherein the OCT scan includes a scan at each or one or more locations within the eye of the patient. These locations of the OCT scan may correspond to the location of the cornea, the location of the anterior portion of the lens, the location of the posterior portion of the lens and the location of the retina. In a preferred embodiment, the operating sequence includes each of a wavefront aberrometry measurement, a corneal topography measurement and an OCT scan, wherein the OCT scan measures at least the locations of the retina, the cornea and one of anterior portion of the patient's lens. An iris image may be taken simultaneously with or sequentially with each of the measurements taken with wavefront aberrometry subsystem, the corneal topography subsystem and the OCT subsystem, including an iris image take simultaneously with or sequentially with the location of each OCT scan. This results in improved accuracy in the 3-dimensional modeling of the patient's eye by permitting the various data sets to be fused and merged into a 3-dimensional model. Optical measurement system 1 and the optical measurements obtained therewith may be used pre-operatively, i.e. before a cataract surgery or other surgical procedure, for, e.g., eye biometry and other measurements, diagnostics and surgical planning. Surgical planning may include one or more predictive models. In the one or more predictive models, one or more characteristics of the postoperative condition of the patient's eye or vision is modeled based on one or more selected from the group consisting of pre-operative measurements obtained from the optical measurement system 1 , a contemplated surgical intervention, and on or more algorithms or models stored in the memory of the optical measurement system 1 and executed by the processor. The contemplated surgical intervention may include the selection of an IOL for placement, the alignment of a toric IOL in the eye, the selection of an IOL characteristic, the nature or type of incision to be used during surgery (e.g., relaxation incision), or one or more post-operative vision characteristics requested by the patient. Optical measurement system 1 and the optical measurements obtained therewith may be used intra-operatively, i.e., during a cataract surgery or other surgical procedure, for, e.g., intraoperative eye diagnostics, determining IOL placement and position, surgical planning, and control/or of a laser surgical system. For instance, in the case of laser cataract surgical procedure, any measurement data obtained preoperatively by the optical measurement instrument may be transferred to a memory associated with a cataract laser surgical system for use before, during or after either the placement of a capsulotomy, fragmentation or a patient's lens or IOL placement during the cataract surgery. In some embodiments, measurements using optical measurement system 1 may be taken during the surgical procedure to determine whether the IOL is properly placed in the patient's eye. In this regard, conditions measured during the surgical procedure may be compared to a predicted condition of the patient's eye based on pre-operative measurements, and a difference between the predicted condition and the actual measured condition may be used to undertake additional or corrective actions during the cataract surgery or other surgical procedure. Optical measurement system 1 and the optical measurements obtained therewith may be used postoperatively, i.e., after a cataract surgery or other surgical procedure, for, e.g., post-operative measurement, postoperative eye diagnostics, postoperative IOL placement and position determinations, and corrective treatment planning if necessary. The postoperative testing may occur sufficiently after the surgery that the patient's eye has had sufficient time to heal and the patient's vision has achieved a stable, postsurgical state. A postoperative condition may be compared to one or more predicted condition performed pre-operatively, and a difference between the preoperatively predicted condition and the postoperatively measured condition may be used to plan additional or corrective actions during the cataract surgery or other surgical procedure. Optical measurement system 1 , including the corneal topography subsystem, the OCT subsystem and the wavefront aberrometry subsystem, utilizing a suitable operating sequence as disclosed herein, is operable to measure one, more than one or all of the following: ocular biometry information, anterior corneal surface information, posterior corneal surface information, anterior lens surface information, posterior lens surface information, lens tilt information and lens position information. In some embodiments, the ocular biometry information may include a plurality of central corneal thicknesses (CCT), an anterior chamber depth (ACT), a pupil diameter (PD), a white to white distance (WTW), a lens thickness (LT), an axial length (AL) and a retinal layer thickness. This measurement data may be stored in memory 62 associated with controller 60 . The plurality of characteristics may be measured preoperatively, and where appropriate, intra-operatively, and postoperatively. In some embodiments, memory 62 associated with controller 60 may store intraocular lens (IOL) model data for a plurality of IOL models, each of the IOL models having associated with it a plurality of predetermined parameters selected from the group consisting of dioptic power, refractive index, asphericity, toricity, haptic angulation and lens filter. The IOL data may be used by one or more processors of optical measurement system 1 , in conjunction with measurement data of a subject's eye obtained by optical measurement system 1 , for cataract diagnostics or cataract treatment planning, which may include specifying and/or selecting a particular IOL for a subject's eye. For example, one or more processors of optical measurement system 1 may execute an algorithm which includes: accessing the plurality of IOL models stored in, and for each of the IOL models: (1) modeling the subject's eye with an intraocular lens corresponding to the IOL model and the measured characteristics of the subject's eye; (2) simulating the subject's eye based on the plurality of IOL predetermined parameters and the predicted IOL position; (3) performing one of a ray tracing and a power calculation based on said model of the subject's eye; and (4) selecting an IOL for the subject's eye from the plurality of IOL models corresponding to the optimized IOL based on a predetermined criteria. In some embodiments, one or more processors of optical measurement system 1 may execute an algorithm comprising: determining a desired postoperative condition of the subject's eye; empirically calculating a post-operative condition of the eye based at least partially on the measured eye characteristics; and predictively estimating, in accordance with an output of said empirically calculating and the eye characteristics, at least one parameter of an intraocular lens for implantation into the subject's eye to obtain the desired postoperative condition. In many embodiments, the eye imaging and diagnostic system further comprises a memory operable to store Intraocular Lens (“IOL”) Data, the IOL data including a plurality of dioptic power, anterior and posterior radius, IOL thickness, refractive index, asphericity, toricity, echelette features, haptic angulation and lens filter. In many embodiments, the eye imaging and diagnostic system further comprises a memory operable to store intraocular lens (“IOL”) model data for a plurality of IOL models, IOL model having associated with a plurality of predetermined parameters selected from the group consisting of dioptic power, anterior and posterior radius, IOL thickness, refractive index, asphericity, toricity, echelette features, haptic angulation and lens filter. An improved system for selecting an intraocular lens (IOL) for implantation, may comprise: a memory operable to store data acquired from each of the corneal topography subsystem, the wavefront sensor subsystem and the Optical Coherence Tomography subsystem, wherein the stored data includes a plurality of ocular biometry information, anterior corneal surface information, posterior corneal surface information, anterior lens surface information, and posterior lens surface information, lens tilt information and lens position information; the memory further operable to store intraocular lens (“IOL”) model data for a plurality of IOL models, IOL model having associated with it a plurality of predetermined parameters selected from the group consisting of dioptic power, anterior and posterior radius, IOL thickness, refractive index, asphericity, toricity, echelette features, haptic angulation and lens filter; and a processor coupled to the memory, the processor deriving the treatment of the eye of the patient applying, for each of the plurality of identified IOL Model, to: (1) predict a position of one of the identified IOL Models when implanted in the subject eye, based on the plurality of characteristics; (2) simulate the subject eye based on the plurality of IOL predetermined parameters and the predicted IOL position; (3) perform one or more of ray tracing and a IOL spherical equivalent (SE) and cylinder (C) power calculation, as well as optionally, to determine the optimum IOL orientation based on said eye model; and (4) propose one IOL power for one or more IOL models from the plurality of IOLs corresponding to the optimized IOL(s) based on predetermined criteria; and (5) show the simulated optical quality and/or visual performance provided by each of the proposed IOL models for distance and/or for any other vergence. A method of selecting an intraocular lens (IOL) to be implanted in a subject's eye, may comprise: measuring a plurality of eye characteristics comprising ocular biometry information, anterior corneal surface information, posterior corneal surface information, anterior lens surface information, and posterior lens surface information, lens tilt information and lens position information; and for each of Intraocular Lens (“IOL”) model having associated with it a plurality of predetermined parameters selected from the group consisting of dioptic power, refractive index, anterior and posterior radius, IOL thickness, asphericity, toricity, echelette design, haptic angulation and lens filter: (1) modeling the subject eye with the intraocular lens; (2) simulating the subject eye based on the plurality of IOL predetermined parameters and the predicted IOL position; (3) performing a ray tracing and a IOL spherical equivalent (SE) and cylinder (C) power calculation, as well as determine the optimum IOL orientation based on said eye model; and (4) proposing one IOL power for one or more IOL models from the plurality of IOLs corresponding to the optimized IOL(s) based on predetermined criteria; and optionally (5) show the simulated optical quality and/or visual performance provided by each of the proposed IOL models for distance and/or for any other vergence. A tangible computer-readable storage device may store computer instructions which, when read by a computer, cause the computer to perform a method comprising: receiving a plurality of eye characteristics comprising ocular biometry information, anterior corneal surface information, posterior corneal surface information, anterior lens surface information, and posterior lens surface information, lens tilt information and lens position information; for each of Intraocular Lens (“IOL”) model having associated with it a plurality of predetermined parameters selected from the group consisting of dioptic power, refractive index, anterior and posterior radius, IOL thickness, asphericity, toricity, echelette design, haptic angulation and lens filter: (1) simulating a geometry of the subject eye with each of the plurality of intraocular lenses (IOL) implanted, in accordance with the plurality of eye characteristics; (2) performing a ray tracing and a IOL spherical equivalent (SE) and cylinder (C) power calculation, as well as optionally determining the optimum IOL orientation based on said eye model; (3) proposing one IOL power for one or more IOL models from the plurality of IOLs corresponding to the optimized IOL(s) based on predetermined criteria; and optionally (4) showing the simulated optical quality and/or visual performance provided by each of the proposed IOL models for distance and/or for any other vergence. A method of predicting the intraocular lens position may comprise: determining a plurality of eye characteristics before cataract surgery, comprising ocular biometry information, anterior corneal surface information, posterior corneal surface information, anterior lens surface information, and posterior lens surface information, lens tilt information and lens position information; determining a plurality of eye characteristics after cataract surgery, comprising ocular biometry information, anterior corneal surface information, posterior corneal surface information, anterior lens surface information, and posterior lens surface information, lens tilt information and lens position information; calculating or measuring, based on a mathematical relationship, a distance from the apex to a plane of the intraocular lens after an ocular surgical procedure; calculating an optical power of the intraocular lens suitable for providing a predetermined refractive outcome; wherein a mathematical relationship is found between the preoperative and postoperative eye characteristics that accurately predict the measured distance from the apex to the plane where the intraocular lens is. An improved system for planning a refractive treatment of an eye of a patient, may comprise: a memory operable to store eye measurement data comprising ocular biometry information, anterior corneal surface information, posterior corneal surface information, anterior lens surface information, and posterior lens surface information, lens tilt information and lens position information; a processor coupled to the memory, the processor deriving the treatment of the eye of the patient applying an effective treatment transfer function, wherein the effective treatment transfer function is derived from, for each of a plurality of prior eye treatments, a correlation between a pre-treatment vector characterizing the eye measurement data before treatment, and a post-treatment vector characterizing post-treatment eye measurement data of the associated eye; an output coupled to the processor so as to transmit the treatment to facilitate improving refraction of the eye of the patient. The processor may comprise tangible media embodying machine readable instructions for implementing the derivation of the treatment. An improved method for planning a refractive treatment of an eye of a patient may comprise: measuring a plurality of ocular biometry information, anterior corneal surface information, posterior corneal surface information, anterior lens surface information, and posterior lens surface information, lens tilt information and lens position information. A method of customizing at least one parameter of an intraocular lens, may comprise: measuring a plurality of eye characteristics comprising ocular biometry information, anterior corneal surface information, posterior corneal surface information, anterior lens surface information, and posterior lens surface information, lens tilt information and lens position information; determining a desired postoperative condition of the eye; empirically calculating a post-operative condition of the eye based at least partially on the measured eye characteristics; and predictively estimating, in accordance with an output of said empirically calculating and the eye characteristics, with at least one parameter of the intraocular lens to obtain the desired postoperative condition. A method of adjusting the refractive power in an eye of a patient who has undergone cataract surgery may comprise: measuring a plurality of post-operative eye characteristics in an eye of a patient who has previously undergone cataract surgery, the eye characteristics comprising ocular biometry information, anterior corneal surface information, posterior corneal surface information, anterior lens surface information, and posterior lens surface information, lens tilt information and lens position information; identifying a plurality of corrective procedure based at least partially on one of (1) a comparison of at least one measured pre-operative eye characteristic and the corresponding measured post-operative eye characteristic; and (2) a comparison of at least one predicted post-operative eye characteristic and the corresponding measured post-operative eye characteristic; for each of a plurality of corrective procedures: modeling the subject eye with the corrective procedure; modeling the subject eye based on the corrective procedure; performing one of a ray tracing and a power calculation based on said eye model; and selecting a corrective procedure from the plurality of IOL models corresponding to the optimized IOL based on a predetermined criteria. In some embodiments, the system further comprises a processor configured to execute an algorithm. The algorithm comprises, for each of the IOL models: (1) modeling the subject's eye with an intraocular lens corresponding to the IOL model and the measured characteristics of the subject's eye; (2) simulating the subject's eye based on the plurality of IOL predetermined parameters and the predicted IOL position; (3) performing one of a ray tracing and a power calculation based on said model of the subject's eye; and (4) selecting an IOL from the plurality of IOL models corresponding to the optimized IOL based on a predetermined criteria. This summary and the following detailed description are merely exemplary, illustrative, and explanatory, and are not intended to limit, but to provide further explanation of the invention as claimed. Additional features and advantages of the invention will be set forth in the descriptions that follow, and in part will be apparent from the description, or may be learned by practice of the invention. The objectives and other advantages of the invention will be realized and attained by the structure particularly pointed out in the written description, claims and the appended drawings. All patents and patent applications cited here are hereby incorporated by reference hereby reference in their entirety. The use of the terms “a” and “an” and “the” and similar referents in the context of describing the invention (especially in the context of the following claims) are to be construed to cover both the singular and the plural, unless otherwise indicated here or clearly contradicted by context. The terms “comprising,” “having,” “including,” and “containing” are to be construed as open-ended terms (i.e., meaning “including, but not limited to,”) unless otherwise noted. The term “connected” is to be construed as partly or wholly contained within, attached to, or joined together, even if there is something intervening. Recitation of ranges of values here are merely intended to serve as a shorthand method of referring individually to each separate value falling within the range, unless otherwise indicated herein, and each separate value is incorporated into the specification as if it were individually recited herein. All methods described here can be performed in any suitable order unless otherwise indicated here or otherwise clearly contradicted by context. The use of any and all examples, or exemplary language (e.g., “such as”) provided herein, is intended merely to better illuminate embodiments of the invention, and does not pose a limitation on the scope of the invention unless otherwise claimed. No language in the specification should be construed as indicating any non-claimed element as essential to the practice of the invention. While certain illustrated embodiments of this disclosure have been shown and described in an exemplary form with a certain degree of particularity, those skilled in the art will understand that the embodiments are provided by way of example only, and that various variations can be made and remain within the concept without departing from the spirit or scope of the invention. Such variations would become clear to one of ordinary skill in the art after inspection of the specification, drawings and claims herein. Thus, it is intended that this disclosure cover all modifications, alternative constructions, changes, substitutions, variations, as well as the combinations and arrangements of parts, structures, and steps that come within the spirit and scope of the invention as generally expressed by the following claims and their equivalents.",en,PATENT_APPLICATION
123-297-104-475-175,US,20240384401,A1,2024-11-21,US_20240384401_A1_20241121,en,US,20240384401,A1,2024-11-21,US,18764338,2024-07-04,MULTILAYER ALD COATING FOR CRITICAL COMPONENTS IN PROCESS CHAMBER,en,TW,"Taiwan Semiconductor Manufacturing Company, Ltd.",Hsinchu,TW,Ren-Guan Duan,Hsinchu,TW,1,Chen-Hsiang Lu,Hsinchu,TW,2,Chiun-Da Shiue,Hsinchu,TW,3,Chih-Kai Hu,Taoyuan,C23C16/40,I,F,C23C16/455,I,L,C23C16/405,I,F,C23C16/45529,I,L,C23C16/45553,I,L,US,20240384401,A1,2024-11-21,123-297-104-475-175,1,US,20240384401,A1,2024-11-21,123-297-104-475-175,1,UNKNOWN,"A method includes: forming a first coating comprising amorphous rare earth metal-containing oxide directly on a surface of an article using a first atomic layer deposition (ALD) process that includes repeating a process of alumina deposition cycles followed by rare earth metal oxide deposition cycles N1 times, the first coating characterized by a first thickness; forming a second coating comprising crystalline rare earth metal oxide on the first coating using a second ALD process, the second coating characterized by a second thickness; forming a third coating comprising amorphous rare earth metal-containing oxide on the second coating using a third ALD process that includes repeating a process of alumina deposition cycles followed by rare earth metal oxide deposition cycles N2 times, the third coating characterized by a third thickness; and forming a fourth coating comprising crystalline rare earth metal oxide on the third coating using a fourth ALD process.",en,"1 . A method, comprising: forming a first coating comprising amorphous rare earth metal-containing oxide directly on a surface of an article using a first atomic layer deposition (ALD) process that includes repeating a process of alumina deposition cycles followed by rare earth metal oxide deposition cycles N1 times, where N1 is an integer, the first coating characterized by a first thickness; forming a second coating comprising crystalline rare earth metal oxide on the first coating using a second atomic layer deposition (ALD) process, the second coating characterized by a second thickness; forming a third coating comprising amorphous rare earth metal-containing oxide on the second coating using a third atomic layer deposition (ALD) process that includes repeating a process of alumina deposition cycles followed by rare earth metal oxide deposition cycles N2 times, where N2 is an integer, the third coating characterized by a third thickness; and forming a fourth coating comprising crystalline rare earth metal oxide on the third coating using a fourth atomic layer deposition (ALD) process, the fourth coating characterized by a fourth thickness.","2 . The method of claim 1 , further comprising repeating the third atomic layer deposition (ALD) process and fourth atomic layer deposition (ALD) process to repeatedly forming additional third coating and fourth coating until a sum of all coatings reaches a target thickness.","3 . The method of claim 1 , wherein a total number of coatings comprising crystalline rare earth metal oxide is between about 2 to about 100.","4 . The method of claim 1 , wherein the crystalline rare earth metal oxide comprises yttria.","5 . The method of claim 1 , wherein the first atomic layer deposition ALD process comprises N1 repetitions of three cycles of alumina deposition followed by two cycles of rare earth metal oxide deposition to reach the first thickness.","6 . The method of claim 1 , wherein, in the third atomic layer deposition (ALD) process, a ratio of N1 to N2 is between about 100 and about 150, and a ratio of the first thickness to the third thickness is between about 100 and about 150.","7 . The method of claim 1 , wherein the crystalline rare earth metal oxide rare earth metal oxide are selected from the group consisting of yttrium oxide (Y 2 O 3 ), lanthanum oxide (La 2 O 3 ), cerium oxide (Ce 2 O 3 ), praseodymium oxide (Pr 2 O 3 ), neodymium oxide (Nd 2 O 3 ), samarium oxide (Sm 2 O 3 ), europium oxide (Eu 2 O 3 ), gadolinium oxide (Gd 2 O 3 ), terbium oxide (Tb 4 O 7 ), dysprosium oxide (Dy 2 O 3 ), holmium oxide (Ho 2 O 3 ), erbium oxide (Er 2 O 3 ), ytterbium oxide (Yb 2 O 3 ), Lutetium (Lu 2 O 3 ), Scandium (Sc 2 O 3 ), thulium oxide (Tm 2 O 3 ), and mixtures thereof.","8 . A method for forming a protective coating on an article for semiconductor processing, the method comprising: loading the article for semiconductor processing in an atomic layer deposition (ALD) chamber; forming a first coating directly on a surface of an article using a first ALD process, the first coating comprising amorphous rare earth metal-containing oxide and characterized by a first thickness, the first ALD process comprising a first number of repetitions of three cycles of alumina deposition followed by two cycles of rare earth metal oxide deposition to reach the first thickness; forming a second coating on the first coating using a second ALD process, the second coating comprising crystalline rare earth metal oxide, the second coating characterized by a second thickness; forming a third coating on the second coating using a third ALD process, the third coating comprising amorphous rare earth metal-containing oxide and characterized by a third thickness, the third ALD process comprising a second number of repetitions of three cycles of alumina deposition followed by two cycles of rare earth metal oxide deposition to reach the third thickness, wherein a ratio of the first number of repetitions to the second number of repetitions is between about 100 and about 150, and a ratio of the first thickness to the third thickness is between about 100 and about 150; and forming a fourth coating on the third coating using a fourth ALD process, the fourth coating comprising crystalline rare earth metal oxide and characterized by a fourth thickness.","9 . The method of claim 8 , further comprising repeating the third ALD process and fourth ALD process to repeatedly forming the third coating and the fourth coating until a sum of all coatings reach a target thickness.","10 . The method of claim 8 , wherein a total number of coatings comprising crystalline rare earth metal oxide is between about 2 to about 100.","11 . The method of claim 8 , wherein a thickness of coatings comprising crystalline rare earth metal oxide is between about 50 nm to about 500 nm.","12 . The method of claim 8 , wherein a thickness of the first coating comprising amorphous rare earth metal-containing oxide is between about 10 nm to about 500 nm.","13 . The method of claim 8 , wherein in the first and third ALD processes, each cycle of alumina deposition comprises using trimethylaluminium (TMA) as a precursor for alumina and H2O as an oxidation agent.","14 . The method of claim 8 , wherein in the first and third ALD processes, each cycle of rare earth metal oxide deposition comprises using yttrium (III) tris(methylcyclopentadienyl) (Y(MeCp)3) as a precursor and H2O as an oxidation agent.","15 . The method of claim 8 , wherein in the second and fourth ALD processes, each cycle of rare earth metal oxide deposition comprises using Y(MeCp)3 as a precursor and H2O as an oxidation agent.","16 . The method of claim 8 , wherein the article for semiconductor processing is a process kit.","17 . A method for processing a process kit used in a plasma processing chamber, the method comprising: loading the process kit in an atomic layer deposition (ALD) chamber; forming a first coating directly on a surface of the process kit using a first ALD process, the first coating comprising amorphous rare earth metal-containing oxide and characterized by a first thickness, the first ALD process comprising a first number of repetitions of three cycles of alumina deposition followed by two cycles of rare earth metal oxide deposition to reach the first thickness; forming a second coating on the first coating using a second ALD process, the second coating comprising crystalline rare earth metal oxide, the second coating characterized by a second thickness; forming a third coating on the second coating using a third ALD process, the third coating comprising amorphous rare earth metal-containing oxide and characterized by a third thickness, the third ALD process comprising a second number of repetitions of three cycles of alumina deposition followed by two cycles of rare earth metal oxide deposition to reach the third thickness, wherein a ratio of the first number of repetitions to the second number of repetitions is between about 100 and about 150, and a ratio of the first thickness to the third thickness is between about 100 and about 150; forming a fourth coating on the third coating using a fourth ALD process, the fourth coating comprising crystalline rare earth metal oxide and characterized by a fourth thickness; and assembling the process kit in the plasma processing chamber.","18 . The method of claim 17 , further comprising repeating the third ALD process and fourth ALD process to repeatedly forming the third coating and the fourth coating until a sum of all coatings reach a target thickness.","19 . The method of claim 18 , wherein the target thickness is in a range of 50 nm to 30 μm.","20 . The method of claim 17 , wherein a thickness of the first coating comprising amorphous rare earth metal-containing oxide is between about 10 nm to about 500 nm.",en,"CROSS-REFERENCE TO RELATED APPLICATIONS The application is a divisional application of U.S. patent application Ser. No. 17/818,336, filed Aug. 8, 2022 and entitled “MULTILAYER ALD COATING FOR CRITICAL COMPONENTS IN PROCESS CHAMBER,” the entire disclosure of which is incorporated herein by reference. BACKGROUND The present invention relates, most generally, to semiconductor device manufacturing. More particularly, the present invention relates to semiconductor manufacturing tools and methods and systems. Plasma etching operations are used extensively in semiconductor device manufacturing. Various processing operations involve RIE (reactive ion etching) or other plasma etching operations to etch materials formed on a semiconductor device, typically to create a pattern in a material layer formed on a semiconductor substrate. Plasma cleaning operations are also commonly used in the semiconductor device manufacturing industry and in conjunction with various materials used in semiconductor device manufacturing. Etching and cleaning operations may, for example, include stripping operations used to remove a blanket film of material from a semiconductor device. Plasma processing operations often involve the plasma reaction to remove a material formed on a substrate. Such plasma processing operations often involve the use of fluorine-based gases, chlorine-based gases, hydrogen-based gases, etc. As device feature sizes continue to decrease, and fabrication process requirements become more stringent, plasma process equipment and operation are facing many challenges. BRIEF DESCRIPTION OF THE DRAWINGS Aspects of the present disclosure are best understood from the following detailed description when read with the accompanying figures. It is noted that, in accordance with the standard practice in the industry, various features are not drawn to scale. In fact, the dimensions of the various features may be arbitrarily increased or reduced for clarity of discussion. FIG. 1 is a cross-sectional view showing an exemplary plasma etching process chamber, in accordance with some embodiments. FIG. 2 is a perspective, top view of an exemplary process chamber, in accordance with some embodiments. FIG. 3 is a cross-sectional view showing an exemplary of a protective coating on an article for semiconductor processing, in accordance with some embodiments. FIG. 4 top view that illustrates a configuration of a semiconductor wafer manufacturing system that can be used in implementing the atomic layer deposition (ALD) process, in accordance with some embodiments. FIG. 5 is a flowchart illustrating a method for forming a coating on components in a semiconductor process chamber, in accordance with some embodiments. FIGS. 6A-6G are cross-sectional view diagrams illustrating a method for forming a coating on components in a semiconductor process chamber, in accordance with some embodiments. FIG. 7 is a TEM micrograph illustrating a cross-sectional view of a coating on a component in a semiconductor process chamber, in accordance with some embodiments. FIG. 8 is an X-ray diffraction (XRD) profile of a coating on a component in a semiconductor process chamber, in accordance with some embodiments. DETAILED DESCRIPTION The following disclosure provides many different embodiments, or examples, for implementing different features of the provided subject matter. Specific examples of components and arrangements are described below to simplify the present disclosure. These are, of course, merely examples and are not intended to be limiting. For example, the formation of a first feature over or on a second feature in the description that follows may include embodiments in which the first and second features are formed in direct contact, and may also include embodiments in which additional features may be formed between the first and second features, such that the first and second features may not be in direct contact. In addition, the present disclosure may repeat reference numerals and/or letters in the various examples. This repetition is for the purpose of simplicity and clarity and does not in itself dictate a relationship between the various embodiments and/or configurations discussed. Further, spatially relative terms, such as “beneath,” “below,” “lower,” “above,” “upper” and the like, may be used herein for ease of description to describe one element or feature's relationship to another element(s) or feature(s) as illustrated in the figures. The spatially relative terms are intended to encompass different orientations of the device in use or operation in addition to the orientation depicted in the figures. The apparatus may be otherwise oriented (rotated 90 degrees or at other orientations) and the spatially relative descriptors used herein may likewise be interpreted accordingly. Within a plasma processing chamber are various articles, including a process kit, i.e., the quartz or other ceramic or insulating hardware typically included within a plasma processing chamber and which influences the impedance of the chamber. The process kit typically includes a focus ring, one or more windows, and a large plate above the plasma or various other components, and therefore constitutes a significant amount of surface area within the process chamber. The materials typically used for process kits are prone to attack and degradation in the various chemistries. Such materials that are prone to attack include quartz, silicon, alumina, and anodized parts, and as a result, these parts have short lifetimes and require frequent and lengthy conditioning of the process chambers within which they are used. Quartz, for example, is a favored material for process kits but is subject to erosion and degradation in etching and cleaning chemistries. As the quartz process kit erodes, it causes particle contamination, it alters the impedance of the chamber and therefore the plasma performance, and it needs to be replaced. As such, some process kits are oxidized or coated with a material such as a ceramic to improve the integrity of the process kit in the plasma chemistry. By coating the exposed surfaces of the process kit with materials other than designed by the manufacturer, however, the impedance of the process chamber is changed and therefore the etch characteristics and cleaning characteristics of the tool are compromised, i.e., process shifting occurs. Moreover, existing coatings are subject to cracking, peeling, and delamination which produce particle contamination. Some embodiments of the disclosure are described. Additional operations can be provided before, during, and/or after the stages described in these embodiments. Some of the stages that are described can be replaced or eliminated for different embodiments. Some of the features described below can be replaced or eliminated and additional features can be added for different embodiments. Although some embodiments are discussed with operations performed in a particular order, these operations may be performed in another logical order. Semiconductor process chamber critical component is very important to wafer performance. Chamber component short lifetime problem also exists in current semiconductor equipment chambers due to limitations of component material and/or surface. Coating the surface of these components is one approach to modify the surface properties to improve chamber performance, especially for particle/defect issue, trace metal issue, plasma distribution, etc. Existing coating techniques are not satisfactory. For example, the plasma sprayed approach cannot make dense coatings. Aerosol deposition and PVD/CVD coating can be dense, but not conformal and cannot coat complicated surfaces. Anodization can only convert the surface to oxide layer, and the converted coating is porous and prone to cracking. Further, a coating of single crystalline layer cannot have strong adhesive to the substrate. On the other hand, an amorphous coating cannot have a good ion bombardment resistance. Some embodiments of this disclosure are directed to a rare earth metal-containing oxide coating structure formed by atomic layer deposition (ALD), that includes a coating of single amorphous layer on the substrate surface overlaid with a multilayer coating that includes alternating crystalline layers interleaved by thin amorphous layers. Formed by atomic layer deposition (ALD), the multilayer top coating structure with top multiple crystalline layers interrupted by thin amorphous layers provides a smooth top coating surface. The amorphous bottom coating provides strong adhesion to the substrate. The protective coating is conformal, even for complicated shape and high aspect ratio shapes. In some embodiments, the bottom amorphous layer has an yttrium aluminate composition, with a flexible mechanical property and provides a great adhesive strength with the substrate. In the top multilayered coating, thin amorphous yttrium aluminate layer between crystalline yttria layer plays a role to interrupt the yttria crystal growth and form a crystalline layer without grain preferred orientation growth with small grain size and a smooth surface. In some embodiments, the top crystalline layer of yttria, with cubic crystal structure and small grain size, exhibits a higher hardness and shows excellent ion bombardment resistance and radical erosion resistance. As used herein, yttria refers to yttrium oxide, Y 2 O 3 . Yttrium aluminate (Y 3 Al 5 O 12 ) is also known as yttrium aluminum garnet (YAG), which is a refractory, hard oxide ceramic that does not damage easily. Some embodiments of this disclosure are related to a protective coating of rare earth-containing oxides formed using atomic layer deposition (ALD) on surfaces of an article for a semiconductor process system, for example, a process kit inside a plasma processing chamber that may be used for plasma etching, plasma cleaning, or both. The plasma processing chamber may be a chamber in any of various plasma processing apparatuses made by various manufacturers that are commercially available and used in the semiconductor manufacturing industry. The plasma processing chamber may be a chamber that is primarily used for etching processes or cleaning processes and the processing chamber may be a chamber dedicated to a fluorine-based processing chemistry. As used herein, the article or process kit refers to the insulating components of the process chamber apparatus that are capable of reducing or eliminating electrical arcing from exposed metal in the chamber. The process kit may include various components such as an insulating plate, a focus ring, and one or more windows that enable a viewer to see inside the process chamber. The article or process kit may be formed of various suitable materials such as insulating materials including but not limited to quartz and various ceramics. FIG. 1 is a cross-sectional view showing an exemplary plasma etching process chamber, in accordance with some embodiments. FIG. 1 shows process chamber 1 disposed within chamber body 3 . Chamber body 3 may be formed of various suitable sturdy materials of high strength. Chamber liner 5 is also formed of various suitable materials available in the art of semiconductor processing apparatuses. The process chamber liner 5 may be formed of aluminum or other suitable materials and may include protective coatings on process chamber liner 5 that forms the internal walls of process chamber 1 , may be used. Process chamber 1 may be used for various applications such as various plasma processing operations including but not limited to etching, stripping, and cleaning. Plasma 7 including concentrated high-intensity plasma area 7 ′ and outer low-intensity plasma area 7 ″ may be produced using various suitable plasma generating operations. The distribution and configuration of high-intensity plasma area 7 ′ can be defined by tuning the pressure in process chamber 1 and therefore high-intensity plasma area 7 ′ and low-intensity plasma area 7 ″ may take on different relative configurations in other exemplary embodiments. According to one exemplary embodiment, the plasma processing operation may be a reactive ion etching (RIE) or plasma etching operation. Plasma 7 may be generated using conventional means. In the illustrated embodiment, the plasma generation may be effectuated by a potential difference between electrode 25 and chuck 9 /pedestal 11 . RF generating means 27 may be advantageously coupled to electrode 25 according to one exemplary embodiment. In other exemplary embodiments, a coil (not shown) may be used in place of electrode 25 . Chuck 9 may be any of various suitable chucks for retaining a substrate as used in the semiconductor manufacturing industry and in one exemplary embodiment, chuck 9 may be an electrostatic chuck. Pedestal 11 may be formed of various suitable materials. Various conventional means (not shown) may be used to deliver gases to process chamber 1 for use in the plasma to be generated in process chamber 1 . Various combinations of gases may be used. According to one exemplary embodiment, process chamber 1 may be utilized for etching or removing silicon, e.g., polysilicon etching, and included among the plasma processing gases may be fluorine-based, chlorine-based, and hydrogen-based etching/cleaning gases. The etching chemistries may include various other carrier and/or reactive gases along with the etching species in other exemplary embodiments. In the illustrated embodiment, the process kit includes quartz plate 19 , focus ring 13 and window 23 . Quartz plate 14 is seen to be generally parallel to the upper surface of chuck 9 but other configurations may be used in other exemplary embodiments. In the illustrated embodiment, the process kit may be a quartz process kit but process kits formed of other materials may also be used. Focus ring 13 rests on ceramic base ring 21 and the edge of chuck 9 . In each of the components of the process kit, thin film coating 15 is disposed on the surface of the process kit component that is exposed to the plasma. Coating 15 is formed on the respective surfaces of the process kit components as a thin film coating using another coating apparatus and advantageously prior to the installation of the process kit within process chamber 1 . The coating apparatus used to form coating 15 is an atomic layer deposition (ALD) apparatus sized to accommodate the various components of the process kit. Coating 15 is a thin film coating and may have a thickness about 20 μm according to one exemplary embodiment but other thicknesses may be used in other exemplary embodiments. In one exemplary embodiment, coating 15 may include thickness of 5 microns. In one exemplary embodiment, coating 15 may be a yttria, i.e., Y 2 O 3 coating. Coating 15 is resistant to attack from the etch or cleaning chemistries used in plasma 7 . In one exemplary embodiment, Y 2 O 3 coating 15 is resistant to attack from fluorine-based, chlorine-based, hydrogen-based, chemistries, etc. Coating 15 also provides the advantages of prolonging the lifetime of the process kit, e.g., window 23 , focus ring 13 and quartz plate 19 . The apparatus with process chamber 1 is used to carry out various etching and cleaning plasma operations. Because coating 15 is resistant to attack/degradation in the etching plasma, particle defect levels are reduced and the time needed for conditioning process chamber 1 after a chamber clean, for example, is reduced. Due to the material and the thin film nature of coating 15 , the overall impedance of process chamber 1 is maintained and little or no process shift is experienced, i.e., the plasma etching and plasma cleaning characteristics are uniform throughout a run and repeatable on a run-to-run basis and the processing chamber performs according to design and according to designs with uncoated process kit designed to be used within the chamber. FIG. 2 is a perspective, top view of an exemplary process chamber, in accordance with some embodiments. Liner 5 serves as a baffle as the bottom portion of liner 5 consists of ribs 31 and gaps 29 in between the ribs that may be used to maintain a uniformity in gas flow during pump down or processing. Focus ring 13 of the process kit laterally surrounds chuck 9 and is coated with coating 15 , as are other exposed surfaces of the process kit (not shown in FIG. 2 ). FIG. 3 is a cross-sectional view showing an exemplary protective coating on an article for semiconductor processing, in accordance with some embodiments. FIG. 3 illustrates a cross-sectional view of a portion of an article 300 including a protective coating 320 on a surface 310 - 1 of the article body 310 . The protective coating 320 includes a first coating 301 on the surface 310 - 1 of the article body 310 . In some embodiment, the first coating 301 includes amorphous rare earth metal-containing oxide formed by a first atomic layer deposition (ALD) process and characterized by a first thickness t1. The first coating 301 includes a first number (N1) of layers of a composite layer including an amorphous rare earth metal oxide layer 301 - 2 on an amorphous alumina layer 301 - 1 , N1 being an integer. The protective coating 320 also includes a second coating 302 on the first coating 301 . The second coating 302 includes crystalline rare earth metal oxide formed by a second atomic layer deposition (ALD) process and characterized by a second thickness t2. In FIG. 3 , the protective coating 320 also includes a third coating 303 on the second coating 302 . The third coating 303 includes amorphous rare earth metal-containing oxide formed by a third atomic layer deposition (ALD) process and characterized by a third thickness t3. The third coating 303 includes a second number (N2) of layers of a composite layer including an amorphous rare earth metal oxide layer on an amorphous alumina layer (not shown in FIG. 3 ), N2 being an integer. In some embodiments, the composite layer in the third coating is similar to composite layer 301 - 3 in the first coating and includes an amorphous rare earth metal oxide layer 301 - 2 on an amorphous alumina layer 301 - 1 . The protective coating 320 also includes a fourth coating 304 on the third coating 303 . The fourth coating 304 includes crystalline rare earth metal oxide formed by a fourth atomic layer deposition (ALD) process and characterized by a fourth thickness t4. In some embodiments, the protective coating 320 further includes additional coatings formed repeating the third atomic layer deposition (ALD) process and fourth atomic layer deposition (ALD) process to repeatedly forming additional third coating and additional fourth coating until a sum of all coatings reach a target thickness. For example, FIG. 3 shows additional third coating 303 - 2 and fourth coating 304 - 2 , and, further, third coating 303 - 3 and fourth coating 304 - 3 . In some embodiments, a total number of coatings comprising the crystalline rare earth metal oxide, such as coatings 302 and 304 , is between about 2 to about 100. In some embodiments, a ratio of the first number (N1) of layers to the second number (N2) of layers is between about 100 and about 150, and a ratio of the first thickness to the third thickness is between about 100 and about 150. In some embodiments, in the first atomic layer deposition (ALD) process comprises the first number of repetitions of three cycles of alumina deposition followed by two cycles of rare earth metal oxide deposition to reach the first thickness. In some embodiments, a total thickness of coatings including crystalline rare metal oxide is between about 50 nm to about 500 nm. In some embodiments, a thickness of the first coating including amorphous yttrium aluminate is between about 10 nm to about 500 nm. In some embodiments, a method for fabricating a semiconductor device includes plasma processing in one or more etching process chambers that include a coating as described in FIG. 3 . The method includes forming fin structures on a substrate. In some embodiments, such as in methods for forming fin field-effect transistor (FinFET) devices, the fin structures are fin-shaped structures etched in the semiconductor substrate. In some structures, such as in methods for forming gate-all-around (GAA) devices, the fin structures are etched in stacked semiconductor layers. In either case, the etching processes include plasma etch or reactive ion etch (RIE) in an etch system that includes components and/or process kits that have a coating, such as protective coating 320 described above in connection to FIG. 3 . Examples of process kits in a plasma etch chamber includes a focus ring, one or more windows, and a large plate above the plasma, a base plate below the wafer, or various other components. With reference to FIG. 3 , in some embodiments, the coating is a multi-layered coating. A first coating 301 is an amorphous rare earth metal-containing oxide formed by a first atomic layer deposition (ALD) process and comprising a first number of layers of a composite layer including an amorphous rare earth metal-containing oxide layer 301 - 2 on an amorphous alumina layer 301 - 1 . A second coating 302 includes a crystalline rare earth metal-containing oxide formed by a second atomic layer deposition (ALD) process. A third coating includes amorphous rare earth metal-containing oxide formed by a third atomic layer deposition (ALD) process and includes a second number of layers of a composite layer including an amorphous rare earth metal-containing oxide layer on an amorphous alumina layer. A fourth coating 304 includes crystalline rare earth metal-containing oxide formed by a fourth atomic layer deposition (ALD) process. In some embodiments, the method for fabricating the semiconductor device also includes forming isolation structures, for example, shallow trench isolation (STI). Forming the isolation structures can include etching trenches, filling the trenches with insulating materials, and removing excess insulating materials. The method can also include forming gate structures, forming contacts and vias structures, forming interconnect structures, which all involve one or more etching processes. In some embodiments, the some or all the etching processes are carried out in an etch system that includes components and/or process kits that have a coating as described above. Further details of the materials and properties of the coatings in FIG. 3 are described below. FIG. 4 top schematic view that illustrates a configuration of a semiconductor wafer processing system that can be used in implementing the atomic layer deposition (ALD) process, in accordance with some embodiments. Referring to FIG. 4 , a semiconductor wafer processing system 400 used to perform ALD processes for forming thin metal and metal oxide films for semiconductor wafers or other articles is illustrated. The semiconductor wafer processing system 400 has several growth chambers arranged in each of the two clusters 400 A and 400 B. Semiconductor wafer processing system 400 may also include other growth chambers for depositing dielectric layers, barrier layers, blocking layers, adhesion layers, anti-reflecting layers, and any other suitable layers. Each layer in the coating structures described above in FIG. 3 can be formed in semiconductor wafer processing system 400 without exposing the wafer to ambient contact between deposition processes. Two load lock chambers 413 A and 413 B are configured to receive a wafer transferred from a load port 402 . Load lock chambers 413 A and 413 B are vented to a pressure equivalent to the load port 402 while a wafer or an article is transferred between load port 402 and load lock chambers 413 A or 413 B. When moving the wafer or article from load lock chamber 413 A or 413 B into one of the chambers in semiconductor wafer processing system 400 , load lock chambers 413 A and 413 B are pumped down to a certain degree of vacuum that is closer to the vacuum level inside the clusters 400 A and 400 B. Clusters 400 A and 400 B each has at least one mechanical means such as a robot arm 404 or 408 which transfers the wafer parked in the pumped-down load lock chamber 413 A or 413 B to one of the growth chambers. Semiconductor wafer processing system 400 can also include degassing chambers 415 and 417 that are used to activate and remove gaseous and/or liquid substances, such as moisture and oxygen from substrates to prevent change in thin film characteristics and cause deposition failure. In some embodiments, semiconductor wafer processing system 400 includes multiple deposition chambers, such as 401 , 403 , 405 , 407 , and 409 , for depositing different thin films for wafer processing or article coating. In some embodiments, chamber 401 can be configured for pre-deposition ozone ( 03 ) treatment. In some embodiments, deposition chamber 403 can be maintained at a temperature between room temperature and about 300° C. In some embodiments, deposition chamber 403 can be maintained at a temperature between about 200° C. and about 400° C. In some embodiments, deposition chamber 403 can be an ALD deposition chamber for depositing the first coating layer 301 described above in connection to FIG. 3 . Deposition chamber 403 is attached to cluster 400 A and is connected to precursor supplies for amorphous yttrium aluminate formation. For example, each cycle of alumina deposition uses trimethylaluminium (TMA) as a precursor for alumina and H2O as an oxidation agent. Each cycle of yttria deposition uses yttrium (III) tris(methylcyclopentadienyl) (Y(MeCp)3) as a precursor for yttria and H2O as an oxidation agent. In some embodiments, deposition chamber 403 can be maintained at a temperature between room temperature and about 300° C. In some embodiments, deposition chamber 403 can be maintained at a temperature between about 200° C. and about 400° C. In some embodiments, deposition chamber 403 can be maintained at a chamber pressure between about 1 Torr and about 20 Torr. In some embodiments, deposition chamber 405 can be an ALD deposition chamber for depositing the second coating layer 302 described above in connection to FIG. 3 . Deposition chamber 405 is attached to cluster 400 B and is connected to precursor supplies for crystalline yttria formation. For example, each cycle of yttria deposition comprises using yttrium (III) tris(methylcyclopentadienyl) (Y(MeCp)3) as a precursor for yttria and H2O as an oxidation agent. In some embodiments, deposition chamber 405 can be maintained at a temperature between room temperature and about 300° C. In some embodiments, deposition chamber 403 can be maintained at a temperature between about 200° C. and about 400° C. In some embodiments, deposition chamber 403 can be maintained at a chamber pressure between about 1 Torr and about 20 Torr. In some embodiments, deposition chamber 407 can be an ALD deposition chamber for depositing the third coating layer 303 described above in connection to FIG. 3 . Deposition chamber 407 is attached to cluster 400 B and is connected to precursor supplies for amorphous yttrium aluminate formation. For example, each cycle of alumina deposition uses TMA as a precursor for alumina and H2O as an oxidation agent. Each cycle of yttria deposition uses yttrium (III) tris(methylcyclopentadienyl) (Y(MeCp)3) as a precursor for yttria and H2O as an oxidation agent. In some embodiments, deposition chamber 407 can be maintained at a temperature between room temperature and about 300° C. In some embodiments, deposition chamber 403 can be maintained at a temperature between about 200° C. and about 400° C. In some embodiments, deposition chamber 403 can be maintained at a chamber pressure between about 1 Torr and about 20 Torr. In some embodiments, deposition chamber 409 can be an ALD deposition chamber for depositing the second coating layer 302 described above in connection to FIG. 3 . Deposition chamber 409 is attached to cluster 400 B and is connected to precursor supplies for crystalline yttria formation. For example, each cycle of yttria deposition comprises using yttrium (III) tris(methylcyclopentadienyl) (Y(MeCp)3) as a precursor for yttria and H2O as an oxidation agent. In some embodiments, deposition chamber 405 can be maintained at a temperature between room temperature and about 300° C. In some embodiments, deposition chamber 403 can be maintained at a temperature between about 200° C. and about 400° C. In some embodiments, deposition chamber 403 can be maintained at a chamber pressure between about 1 Torr and about 20 Torr. The configuration of wafer processing system 400 described above is merely an example, and other arrangements can also be used. For example, in some embodiments, the first coating 301 and the third coating 303 can be formed in the same deposition chamber, for example, deposition 401 . Similarly, in some embodiments, the second coating 302 and the four coating 304 can be formed in the same deposition chamber, for example, deposition 403 . In some embodiments, cooling chambers 410 A and 410 B allow a wafer to cool down to a desired temperature at an appropriate cooling rate in between various thin film growths without ambient contact. In some embodiments, additional chambers can be included in semiconductor wafer processing system 400 for depositing any suitable material used to form the coating structures described above in connection to FIG. 3 . During the deposition of various layers in the deposition chambers of semiconductor wafer processing system 400 , the deposition chambers are kept under vacuum, such that no ambient contact or contamination is introduced. A user may enter a single recipe into a computer processor to control the deposition chamber for performing the multi-cycle deposition processes. For example, the recipe can include deposition parameters for the first and second precursors in the multi-cycle deposition process, such as pulsing time, purging time, gas flow rate, chamber temperature, chamber pressure, plasma power, substrate bias, and/or any suitable deposition parameters. Therefore, the entire deposition process for one or more layers can be controlled by a single recipe in the same chamber. Each wafer or article is assigned with a sequence of operations according to an operating recipe to achieve automatic wafer processing in semiconductor wafer processing system 400 . In some embodiments, a substrate or article is first transferred from load lock chamber 413 A and/or 413 B to cluster 400 A using robot arm 404 . The wafer or article can be sent into chamber 415 or 417 for degassing and then to various deposition chambers to form the coatings. The wafer can then be parked to load lock chambers 413 A and 413 B using robot arms 404 and 408 . The vacuum level inside the load lock chambers chamber 413 A and 413 B are raised to a level comparable to load port 402 , and the wafer is then transferred to load port 402 and taken out. FIG. 5 is a flowchart illustrating a method for forming a coating on an article for a semiconductor process chamber, in accordance with some embodiments. As shown in FIG. 5 , a method 500 for forming a coating on an article can be summarized as follows and described further with reference to FIGS. 3, 4, and 6A-6G . At 510 —loading an article into an ALD deposition chamber;At 520 —forming a first coating 301 comprising amorphous rare earth metal-containing oxide on a surface of an article using a first atomic layer deposition (ALD) process that includes repeating a process of alumina deposition cycles followed by rare earth metal oxide deposition cycles N1 times, where N1 is an integer;At 530 —forming a second coating comprising crystalline rare earth metal oxide on the first coating using a second atomic layer deposition (ALD) process;At 540 —forming a third coating comprising amorphous rare earth metal-containing oxide on the second coating using a third atomic layer deposition (ALD) process that includes repeating a process of alumina deposition cycles followed by rare earth metal oxide deposition cycles N2 times, where N2 is an integer;At 550 —forming a fourth coating comprising crystalline rare earth metal oxide on the third coating using a fourth atomic layer deposition (ALD) process;At 560 —Determining whether a target thickness of coating is reached;At 570 —If no, repeating the third atomic layer deposition (ALD) process and fourth atomic layer deposition (ALD) process to repeatedly forming additional third coating and additional fourth coating until a sum of all coatings reaches a target thickness;At 580 —If yes. Finish. FIGS. 6A-6G are cross-sectional view diagrams illustrating a method for forming a coating on components in a semiconductor process chamber, in accordance with some embodiments. Method 500 outlined in the flowchart in FIG. 5 for forming a protective coating on an article for a semiconductor process chamber is described below with reference to FIGS. 6A-6G . At 510 , method 500 includes loading an article into an ALD deposition chamber. An example of a semiconductor processing system 400 that includes ALD deposition chambers is described above in connection with FIG. 4 . FIG. 6A shows a top portion of an article 310 , which has a top surface 310 - 1 . The article 310 can be a component in or used in a semiconductor processing chamber that is subject to harsh processing conditions including plasma, high energy bombardment, reactive ions, and high temperature, etc. In some embodiments, the article can be a component to be used in assembling a plasma process chamber, such as discussed above in connection to FIGS. 1-2 . In some embodiments, method 500 can include a pre-deposition ozone ( 03 ) treatment at, for example, a temperature between room temperature and about 300° C. At 520 , method 500 includes forming a first coating 301 comprising amorphous rare earth metal-containing oxide on a surface 310 - 1 of the article 310 using a first atomic layer deposition (ALD) process that includes repeating a process of alumina deposition cycles followed by rare earth metal oxide deposition cycles N1 times, where N1 is an integer, as shown in FIG. 6B . In some embodiments, the rare earth metal oxide are selected from the group consisting of yttrium oxide (Y 2 O 3 ), lanthanum oxide (La 2 O 3 ), cerium oxide (Ce 2 O 3 ), praseodymium oxide (Pr 2 O 3 ), neodymium oxide (Nd 2 O 3 ), samarium oxide (Sm 2 O 3 ), europium oxide (Eu 2 O 3 ), gadolinium oxide (Gd 2 O 3 ), terbium oxide (Tb 4 O 7 ), dysprosium oxide (Dy 2 O 3 ), holmium oxide (Ho 2 O 3 ), erbium oxide (Er 2 O 3 ), ytterbium oxide (Yb 2 O 3 ), Lutetium (Lu 2 O 3 ), Scandium (Sc 2 O 3 ), thulium oxide (Tm 2 O 3 ), and mixtures thereof. In some embodiments, the amorphous rare earth metal-containing oxide is amorphous yttrium aluminate. In these embodiments, the first coating 301 includes amorphous yttrium aluminate and characterized by a first thickness t1. The first ALD process includes a first number N1 of repetitions of three cycles of alumina deposition 301 - 1 followed by two cycles of yttria deposition 301 - 2 to reach the first thickness t1. For amorphous yttrium aluminate formation, each cycle of alumina deposition uses TMA as a precursor for alumina and H2O as an oxidation agent. Each cycle of yttria deposition uses yttrium (III) tris(methylcyclopentadienyl) (Y(MeCp)3) as a precursor for yttria and H2O as an oxidation agent. In some embodiments, the ALD deposition chamber can be maintained at a temperature between room temperature and about 300° C. In some embodiments, the deposition chamber can be maintained at a temperature between about 200° C. and about 400° C. In some embodiments, deposition chamber can be maintained at a chamber pressure between about 1 Torr and about 20 Torr. Prior to introduction of a new precursor or reactant, the ALD chamber may be purged with an inert gas (such as nitrogen or argon) to remove any unreacted precursor and/or surface-precursor reaction byproducts. The precursors may be different for each layer. In some embodiments, the surface reactions may be done through co-deposition, i.e., where at least two precursors are used, in some embodiments at least three precursors are used and in yet further embodiments at least four precursors are used. The ALD processes may be conducted at various temperatures depending on the type of process. In various embodiments, the ALD temperature windows may range from about 100° C. to about 400° C. In some embodiments, the ALD temperature window is between about 120-300° C. In some embodiments, the first number of repetitions N1 of amorphous yttrium aluminate deposition is in a range of 400 to 800 cycles. The thickness of the first coating 301 is in a range of 10 nm to 500 nm. Depending on the application, the number of ALD cycles and the thickness of the coating film can vary. At 530 , as shown in FIG. 6C , the method 500 includes forming a second coating 302 comprising crystalline rare earth metal oxide on the first coating using a second atomic layer deposition (ALD) process. In some embodiments, the second coating 302 includes crystalline yttria and is characterized by a second thickness t2. For crystalline yttria formation, each cycle of yttria deposition comprises using yttrium (III) tris(methylcyclopentadienyl) (Y(MeCp)3) as a precursor for yttria and H 2 O as an oxidation agent. In some embodiments, the deposition chamber 405 can be maintained at a temperature between room temperature and about 300° C. In some embodiments, deposition chamber can be maintained at a temperature between about 200° C. and about 400° C. In some embodiments, the deposition chamber can be maintained at a chamber pressure between about 1 Torr and about 20 Torr. In some embodiments, the second atomic layer deposition (ALD) process includes about 300 to 600 cycles of yttria deposition. In some embodiments, the second coating 302 is characterized by a second thickness of 50 nm to 500 nm. Depending on the application, the number of ALD cycles and the thickness of the coating film can vary. At 540 , as shown in FIG. 6D , method 500 includes forming a third coating 303 comprising amorphous rare earth metal-containing oxide on the second coating using a third atomic layer deposition (ALD) process that includes repeating a process of alumina deposition cycles followed by rare earth metal oxide deposition cycles N2 times, where N2 is an integer. In some embodiments, the third coating 303 includes amorphous yttrium aluminate and characterized by a third thickness t3. The third ALD process comprising a second number of repetitions N2 of three cycles of alumina deposition followed by two cycles of yttria deposition to reach the third thickness t3. For amorphous yttrium aluminate formation, each cycle of alumina deposition uses TMA as a precursor for alumina and H2O as an oxidation agent. Each cycle of yttria deposition uses yttrium (III) tris(methylcyclopentadienyl) (Y(MeCp)3) as a precursor for yttria and H2O as an oxidation agent. In some embodiments, the ALD deposition chamber can be maintained at a temperature between room temperature and about 300° C. In some embodiments, the deposition chamber can be maintained at a temperature between about 200° C. and about 400° C. In some embodiments, deposition chamber can be maintained at a chamber pressure between about 1 Torr and about 20 Torr. In some embodiments, a ratio of the first number of repetitions N1 to the second number of repetitions N2 is between about 100 and about 150. In some embodiments, a ratio of the first thickness t1 to the third thickness t3 is between about 100 and about 150. In some embodiments, the second number of repetitions N2 of amorphous yttrium aluminate deposition is in a range of 2 to 8 cycles. The thickness of the third coating 303 is in a range of 1 nm to 5 nm. Depending on the application, the number of ALD cycles and the thickness of the coating film can vary. As shown in FIG. 6E , method 500 includes, at 550 , forming a fourth coating 304 comprising crystalline rare earth metal oxide on the third coating 303 using a fourth atomic layer deposition (ALD) process. In some embodiments, the fourth coating 304 includes crystalline yttria and is characterized by a fourth thickness t4. For crystalline yttria formation, each cycle of yttria deposition comprises using yttrium (III) tris(methylcyclopentadienyl) (Y(MeCp)3) as a precursor for yttria and H2O as an oxidation agent. In some embodiments, the deposition chamber 405 can be maintained at a temperature between room temperature and about 300° C. In some embodiments, deposition chamber can be maintained at a temperature between about 200° C. and about 400° C. In some embodiments, the deposition chamber can be maintained at a chamber pressure between about 1 Torr and about 20 Torr. In some embodiments, the fourth atomic layer deposition (ALD) process is similar to the second ALD process. For example, in some embodiments, the fourth atomic layer deposition (ALD) process includes about 300 to 600 cycles of yttria deposition. In some embodiments, the fourth coating 304 is characterized by a second thickness of 50 nm to 500 nm. Depending on the application, the number of ALD cycles and the thickness of the coating film can vary. For example, the fourth atomic layer deposition (ALD) process can be different from the second ALD process. At 560 , the method 500 includes determining whether a target thickness of coating is reached. If it is determined that a target thickness of all the coatings has been reached, the method finishes at 580 . Otherwise, the method proceeds to 570 to form additional coatings. At 570 , as shown in FIG. 6F , method 500 includes, repeating the third atomic layer deposition (ALD) process and fourth atomic layer deposition (ALD) process to repeatedly forming additional third coating 303 - 2 and additional fourth coating 304 - 2 until a sum of all coatings reaches a target thickness. In FIG. 6F , an additional third coating is shown as 303 - 2 and an additional fourth coating is shown as 304 - 2 . In some embodiments, the additional third coating 303 - 2 is similar to the third coating 303 and includes amorphous rare earth metal-containing oxide characterized by a third thickness t3. Similarly, in some embodiments, the additional fourth coating 304 - 2 is similar to the fourth coating 304 and includes crystalline rare earth metal oxide and characterized by a fourth thickness t4. The additional third coating 303 - 2 and the additional fourth coating 304 - 2 can be formed using similar processes described above in connection to the third coating 303 and the fourth coating 304 . In some embodiments, a ratio of the third thickness t3 to the fourth thickness t4 is between about 100 and about 150. In some embodiments, the additional third coating 303 - 2 includes amorphous yttrium aluminate and the additional fourth coating 304 - 2 includes crystalline yttria. The additional third coating 303 - 2 and the additional fourth coating 304 - 2 can be formed using similar processes described above in connection to the third coating 303 and the fourth coating 304 . For example, For amorphous yttrium aluminate formation, each cycle of alumina deposition uses TMA as a precursor for alumina and H2O as an oxidation agent. Each cycle of yttria deposition uses yttrium (III) tris(methylcyclopentadienyl) (Y(MeCp)3) as a precursor for yttria and H2O as an oxidation agent. Each cycle of yttria deposition uses yttrium (III) tris(methylcyclopentadienyl) (Y(MeCp)3) as a precursor for yttria and H2O as an oxidation agent. FIG. 6G shows an optional additional third coating 303 - 4 and an additional fourth coating 304 - 3 , etc., until a total thickness of all the coatings, 301 , 302 , 303 , 304 , 303 - 1 , 304 - 1 , . . . , etc. reaches the target thickness. In some embodiments, the additional third coatings 303 - 4 can be similar to the first layer of the third coating 303 , and the additional fourth coatings 304 - 4 can be similar to the first layer of the four coating 304 . Depending on the application, the number of ALD cycles and the thickness of the coating film can vary. In some embodiments, the target thickness is in a range of 50 nm to 30 μm. FIG. 7 is a line drawing representation of a Transmission Electron Microscopy (TEM) micrograph illustrating a cross-sectional view of a coating on an article for a semiconductor process chamber, in accordance with some embodiments. FIG. 7 illustrates a cross-sectional view of an article 700 including a protective coating 720 on a surface 710 - 1 of the article body 710 . The protective coating 720 includes a first coating 701 on the surface 710 - 1 of the article body 710 . The first coating 701 includes amorphous rare earth metal-containing oxide formed by a first atomic layer deposition (ALD) process. In this example, the first coating 701 is an amorphous YAG layer. The protective coating 720 also includes a second coating 702 on the first coating 701 . The second coating 702 includes crystalline rare earth metal oxide formed by a second atomic layer deposition (ALD) process. In this example, the second coating 702 is a crystalline Y 2 O 3 layer. The protective coating 720 also includes a third coating 703 on the second coating 702 . The third coating 703 includes amorphous rare earth metal-containing oxide formed by a third atomic layer deposition (ALD) process. In this example, the third first coating 703 is an amorphous YAG layer, similar to composition of the first coating 701 , but substantially thinner. The protective coating 720 also includes a fourth coating 704 on the third coating 703 . The fourth coating 704 includes crystalline rare earth metal oxide formed by a fourth atomic layer deposition (ALD) process. In this example, the fourth coating 704 is a crystalline Y 2 O 3 layer, similar to the second coating 702 and has a similar thickness. FIG. 8 is an X-ray Diffraction (XRD) profile of a coating on a component for a semiconductor process chamber, in accordance with some embodiments. In FIG. 8 , the horizontal axis shows the degree in 2 theta, and the vertical axis shows counts per second (CPS). The XRD profile depicted in FIG. 8 shows that the multilayer Y 2 O 3 /YAG coating is a cubic structure without preferred orientation growth. Further experiments were carried out to test the effectiveness of the protective coating described above. For example, in a test in a chlorine (Cl) plasma for 8100 seconds, no change in the coating was observed in TEM analysis. In contrast, a sprayed coating of Y 2 O 3 exhibited peeling in a 2700 seconds Cl 2 plasma test. In another test in 2700 second SF 6 plasma, the protective test formed by the method described above exhibited no change in the coating. In contrast, a spray coating of Y 2 O 3 exhibited peeling. Similar results were also obtained in H 2 plasma tests. Some embodiments of this disclosure are directed to a rare earth metal-containing oxide coating structure formed by atomic layer deposition (ALD), that includes a coating of single amorphous layer on the substrate surface overlaid with a multilayer coating that includes alternating crystalline layers interleaved by thin amorphous layers. Formed by atomic layer deposition (ALD), the multilayer top coating structure with top multiple crystalline layers interrupted by thin amorphous layers provides a smooth top coating surface. The smooth top surface reduces the attachment of by-products The amorphous bottom coating provides strong adhesion to the substrate. In some embodiments, the bottom amorphous layer has an yttrium aluminate composition, with a flexible mechanical property and provides a great adhesive strength with the substrate. In the top multilayered coating, thin amorphous yttrium aluminate layer between crystalline yttria layer plays a role to interrupt the yttria crystal growth and form a crystalline layer without preferred (400) orientation grain growth with small grain size and a smooth surface. In some embodiments, the top crystalline layer of yttria, with cubic crystal structure and small grain size, exhibits a higher hardness and shows excellent ion bombardment resistance and plasma radical erosion resistance. In some embodiments, other materials can be used instead of the amorphous yttrium aluminate layer. For example, the amorphous yttrium aluminate layer can be substituted by amorphous alumina layer, amorphous magnesium aluminate layer, and amorphous gadolinium aluminate layer, or other suitable material with compatible lattice structures and chemical properties. In some embodiments, a method includes forming a first coating on a surface of an article using a first atomic layer deposition (ALD) process, the first coating comprising amorphous yttrium aluminate and characterized by a first thickness, the first ALD process comprising a first number of repetitions of three cycles of alumina deposition followed by two cycles of yttria deposition to reach the first thickness. The method also includes forming a second coating on the first coating using a second atomic layer deposition (ALD) process, the second coating comprising crystalline yttria, the second coating characterized by a second thickness. The method also includes forming a third coating on the second coating using a third atomic layer deposition (ALD) process, the third coating comprising amorphous yttrium aluminate and characterized by a third thickness, the third ALD process comprising a second number of repetitions of three cycles of alumina deposition followed by two cycles of yttria deposition to reach the third thickness, wherein a ratio of the first number of repetitions to the second number of repetitions is between about 100 and about 150, and a ratio of the first thickness to the third thickness is between about 100 and about 150. The method further includes forming a fourth coating on the third coating using a fourth atomic layer deposition (ALD) process, the fourth coating comprising crystalline yttria and characterized by a fourth thickness. In some embodiment, a method includes forming a first coating comprising amorphous rare earth metal-containing oxide on a surface of an article using a first atomic layer deposition (ALD) process that includes repeating a process of alumina deposition cycles followed by rare earth metal oxide deposition cycles N1 times, where N1 is an integer, the first coating characterized by a first thickness. The method also includes forming a second coating comprising crystalline rare earth metal oxide on the first coating using a second atomic layer deposition (ALD) process, the second coating characterized by a second thickness. The method further includes forming a third coating comprising amorphous rare earth metal-containing oxide on the second coating using a third atomic layer deposition (ALD) process that includes repeating a process of alumina deposition cycles followed by rare earth metal oxide deposition cycles N2 times, where N2 is an integer, the third coating characterized by a third thickness. The method also includes forming a fourth coating comprising crystalline rare earth metal oxide on the third coating using a fourth atomic layer deposition (ALD) process, the fourth coating characterized by a fourth thickness. In some embodiments, an article includes a protective coating on a surface of an article body. The protective coating includes a first coating on the surface of the article body. The first coating includes amorphous rare earth metal-containing oxide formed by a first atomic layer deposition (ALD) process and characterized by a first thickness. The first coating includes a first number of layers of a composite layer including an amorphous rare earth metal-containing oxide layer on an amorphous alumina layer. The protective coating also includes a second coating on the first coating. The second coating includes crystalline rare earth metal-containing oxide formed by a second atomic layer deposition (ALD) process, and the second coating is characterized by a second thickness. The protective coating includes a third coating on the second coating. The third coating includes amorphous rare earth metal-containing oxide formed by a third atomic layer deposition (ALD) process and characterized by a third thickness. The third coating includes a second number of layers of a composite layer including an amorphous rare earth metal-containing oxide layer on an amorphous alumina layer. The protective coating further includes a fourth coating on the third coating. The fourth coating includes crystalline rare earth metal-containing oxide formed by a fourth atomic layer deposition (ALD) process and characterized by a fourth thickness. The foregoing outlines features of several embodiments so that those skilled in the art may better understand the aspects of the present disclosure. Those skilled in the art should appreciate that they may readily use the present disclosure as a basis for designing or modifying other processes and structures for carrying out the same purposes and/or achieving the same advantages of the embodiments introduced herein. Those skilled in the art should also realize that such equivalent constructions do not depart from the spirit and scope of the present disclosure, and that they may make various changes, substitutions, and alterations herein without departing from the spirit and scope of the present disclosure.",en,PATENT_APPLICATION
127-068-390-546-099,US,20240385403,A1,2024-11-21,US_20240385403_A1_20241121,en,US,20240385403,A1,2024-11-21,US,18789976,2024-07-31,ADAPTER PANEL WITH LATERAL SLIDING ADAPTER ARRAYS,en,US,CommScope Technologies LLC,"Hickory, NC",US,Mark Smrha,"West Chicago, IL",US,1,Hutch Coburn,"Eden Prairie, MN",US,2,Chad Sjodin,"Savage, MN",G02B6/44,I,F,G02B6/38,I,L,H04Q1/02,I,L,H04Q1/06,I,L,H04Q1/14,I,L,H04Q11/00,I,L,G02B6/4446,I,F,G02B6/3825,I,L,G02B6/3897,I,L,G02B6/4452,I,L,G02B6/4455,I,L,H04Q1/023,I,L,H04Q1/06,I,L,H04Q1/142,I,L,H04Q11/0003,I,L,US,20240385403,A1,2024-11-21,127-068-390-546-099,1,US,20240385403,A1,2024-11-21,127-068-390-546-099,1,UNKNOWN,An adapter panel arrangement including a chassis and a panel of adapters. The adapters defining open rearward cable connections and open forward cable connections of the panel arrangement. The adapters being arranged in arrays that slide independently of other adapter arrays to provide access to the open rearward and open forward cable connections.,en,"1 . A fiber optic apparatus, comprising: a rack-mountable chassis having first and second mounting flanges at opposite sides of the chassis, wherein the chassis is capable of supporting fiber optic connection equipment comprising first and second rows of fiber optic connection locations, each connection location being a front port, each row comprising forty-eight (48) front ports; a first frame member having a first plurality of front ports from the first row; and a second frame member having a second plurality of front ports from the first row, the second frame member being adjacent to the first frame member and aligned in the same horizontal plane with the first frame member.","2 . The fiber optic apparatus of claim 1 , further comprising a slidable member supporting the first and second frame members, wherein the slidable member defines a first bottom access opening sized and positioned to facilitate finger access to the first plurality of front ports.","3 . The fiber optic apparatus of claim 2 , wherein the slidable member defines a second bottom access opening sized and positioned to facilitate access to the second plurality of front ports.","4 . The fiber optic apparatus of claim 1 , wherein the first and second frame members are configured such that a first plurality of ports from the second row can be arranged on the first frame member, and a second plurality of ports from the second row can be arranged on the second frame member.","5 . The fiber optic apparatus of claim 1 , wherein the rack-mountable chassis is capable of supporting fiber optic connection equipment in the form of adapters that define the fiber optic connection locations in each of the first and second rows.","6 . The fiber optic apparatus of claim 1 , wherein each of the first and second frame members is slidable relative to the chassis.","7 . The fiber optic apparatus of claim 6 , wherein the first and second frame members slide relative to each other.","8 . The fiber optic apparatus of claim 1 , further including a sliding member that carries the first and second frame members, wherein the sliding member is slidable in a horizontal plane between a forward position and a rearward position relative to the chassis.","9 . The fiber optic apparatus of claim 1 , wherein the front ports within each of the first and second rows are not uniformly spaced.","10 . The fiber optic apparatus of claim 1 , wherein the first and second frame members are configured such that a gap separates the first plurality of front ports on the first frame member and the second plurality of front ports on the second frame member, wherein the gap is larger than the distance between adjacent ports on the first frame member and the distance between adjacent ports on the second frame member.","11 . The fiber optic apparatus of claim 10 , further comprising a forwardly extending member that is to be horizontally aligned with the gap, the forwardly extending member terminating in a vertical portion positioned in a vertical plane that is forward of a plane to be occupied by the first row of fiber optic connection locations, wherein the forwardly extending member can be used for cable routing.","12 . A fiber optic apparatus, comprising: a rack-mountable chassis having first and second mounting flanges at opposite sides of the chassis; a guide member disposed within the chassis, the guide member including at least one engaging member along the guide member, the engaging member being one of a projection or a detent; and a first support member that is slidable relative to the chassis; wherein the chassis is configured to receive fiber optic connection equipment comprising a row of at least forty-eight (48) fiber optic connection locations, each of the at least forty-eight (48) connection locations comprising a port configured to receive an LC connector; and wherein the first support member is configured to receive at least a first plurality of ports from the row of connection locations, wherein the first support member engages the guide member during sliding movement of the support member, and wherein the first support member includes a first complementary member that is engageable with the at least one engaging member of the guide member to retain the first support member in a first position.","13 . The fiber optic apparatus of claim 12 , wherein the first support member defines a bottom access opening sized and positioned to facilitate finger access to at least some of the first plurality of ports.","14 . The fiber optic apparatus of claim 12 , wherein the chassis is configured to receive fiber optic connection equipment that comprises a plurality of adapters that define the fiber optic connection locations in the row.","15 . The fiber optic apparatus of claim 12 , further including a second support member adjacent the first support member and aligned in the same horizontal plane with the first support member, wherein the second support member is configured to receive a second plurality of ports from the row, and wherein the second support member is slidable relative to the chassis and slidable relative to the first support member.","16 . The fiber optic apparatus of claim 12 , wherein the at least one engaging member is a projection and the first complementary member is a detent.","17 . The fiber optic apparatus of claim 12 , wherein the first support member includes a second complementary member configured to engage the at least one engaging member of the guide member to retain the first support member in a second position.","18 . The fiber optic apparatus of claim 17 , wherein each of the first and second complementary members is a detent, and wherein the at least one engaging member is a projection.","19 . The fiber optic apparatus of claim 12 , wherein the first support member includes a rail that is received within the guide member, and wherein the first complementary member is formed on the rail.","20 . The fiber optic apparatus of claim 12 , further comprising a forwardly extending member configured to be horizontally aligned with a gap between a first group and a second group of connection locations, the forwardly extending member terminating in a vertical portion to be positioned in a vertical plane that is forward the row of fiber optic connection locations, wherein the forwardly extending member can be used for cable routing.","21 . The fiber optic apparatus of claim 12 , wherein the chassis is configured to receive a first row of forty-eight (48) fiber optic connection locations and a second row of forty-eight (48) fiber optic connection locations between the opposite sides of the chassis, wherein the second row is to be substantially parallel to and positioned below the first row.","22 . A fiber optic apparatus, comprising: a chassis having a first side and a second side, a plurality of adapters disposed within the chassis, the adapters defining front ports, and the adapters enabling connections of at least one hundred forty-four (144) optical fibers in one rack unit height between the first and second sides of the chassis.","23 . The fiber optic apparatus of claim 22 , wherein the plurality of adapters include multi-fiber adapters.","24 . The fiber optic apparatus of claim 23 , wherein the plurality of adapters defining the front ports are MPO type adapters.",en,"CROSS-REFERENCE TO RELATED APPLICATIONS The present application is a continuation of application Ser. No. 17/730,680, filed Apr. 27, 2022; which is a continuation of application Ser. No. 16/849,054, filed Apr. 15, 2020, now U.S. Pat. No. 11,333,840; which is a continuation of application Ser. No. 16/427,963, filed May 31, 2019, now U.S. Pat. No. 10,739,544; which is a continuation of application Ser. No. 15/499,608, filed Apr. 27, 2017, now U.S. Pat. No. 10,310,204; which is a continuation of application Ser. No. 14/813,806, filed Jul. 30, 2015, now U.S. Pat. No. 9,638,879; which is a continuation of application Ser. No. 14/617,249, filed Feb. 9, 2015, now U.S. Pat. No. 9,448,378; which is a continuation of application Ser. No. 13/722,438, filed Dec. 20, 2012, now U.S. Pat. No. 8,953,921; which is a continuation of application Ser. No. 12/930,783, filed Jan. 14, 2011, now U.S. Pat. No. 8,340,490; which is a continuation of application Ser. No. 12/460, 161, filed Jul. 13, 2009, now U.S. Pat. No. 7,873,252; which is a continuation of application Ser. No. 11/655,760, filed Jan. 19, 2007, now U.S. Pat. No. 7,570,860; which applications are incorporated herein by reference in their entirety. FIELD This disclosure relates to devices for use in the telecommunications industry, and associated methods. More specifically, this disclosure relates to a termination panel for use in the telecommunications industry, and methods associated with termination panels. BACKGROUND Many local area networks and telecommunication systems utilize termination panels to provide cross-connections between telecommunications equipment. Demand for greater telecommunication services has prompted the increase in circuit densities of termination panels. Notwithstanding the advances made in the art, there is a continuous need for further advances to improve upon high-density termination panels and associated methods. Improvements are needed, for example, to enhance termination access and cable management associated with installation, maintenance, repair, upgrade, and cross-connection procedures related to termination panels. SUMMARY The present disclosure relates to an adapter panel arrangement including a chassis and a panel of adapters. The adapters define open rearward cable connections and open forward cable connections of the panel arrangement. The adapters are arranged in arrays that slide independently of other arrays to provide access to the open rearward and open forward cable connections. A variety of examples of desirable product features or methods are set forth in part in the description that follows, and in part will be apparent from the description, or may be learned by practicing various aspects of the disclosure. The aspects of the disclosure may relate to individual features as well as combinations of features. It is to be understood that both the foregoing general description and the following detailed description are explanatory only, and are not restrictive of the claimed invention. BRIEF DESCRIPTION OF THE DRAWINGS FIG. 1 is a front perspective view of one embodiment of an adapter panel arrangement, in accordance with the principles disclosed, shown with a drawer of the adapter panel arrangement in an open position. FIG. 2 is a front perspective view of the adapter panel arrangement of FIG. 1 , shown with the drawer in a closed position. FIG. 3 is a front perspective view of the adapter panel arrangement of FIG. 2 , shown with a cover of the arrangement closed. FIG. 4 is a rear perspective view of the adapter panel arrangement of FIG. 1 . FIG. 5 is a side elevation view of the adapter panel arrangement of FIG. 4 . FIG. 6 is a top plan view of the adapter panel arrangement of FIG. 5 . FIG. 7 is a top perspective view of one embodiment of a sliding frame piece and an adapter array of the adapter panel arrangement of FIG. 1 , shown in isolation. FIG. 8 is a side elevation view of the sliding frame piece and adapter array of FIG. 7 . FIG. 9 is a top plan view of the sliding frame piece and adapter array of FIG. 7 . FIG. 10 is a side elevation view of one embodiment of a guide of the adapter panel arrangement of FIG. 1 , shown in isolation. FIG. 11 is a bottom perspective view of the guide of FIG. 10 . FIG. 12 is a top plan view of the guide of FIG. 10 , and a portion of the sliding frame piece of FIG. 9 . FIG. 13 is a front perspective view of the adapter panel arrangement of FIG. 2 , shown with an adapter array positioned in a forward position. FIG. 14 is a side elevation view of the adapter panel arrangement of FIG. 13 . FIG. 15 is a top plan view of the adapter panel arrangement of FIG. 14 . DETAILED DESCRIPTION Reference will now be made in detail to exemplary aspects of the present disclosure that are illustrated in the accompanying drawings. Wherever possible, the same reference numbers will be used throughout the drawings to refer to the same or like parts. FIG. 1 illustrates a distribution frame or adapter panel arrangement 10 in accordance with the principles disclosed. The adapter panel arrangement 10 is designed to provide a high density of cable terminations, yet facilitate access to the cable terminations from the rear during installation procedures, and from the front during post-installation procedures. The adapter panel arrangement 10 of the present disclosure generally includes a chassis 12 having an interior 14 . The interior 14 is defined by a top wall 16 , a bottom wall 18 , a rear wall 20 , and side walls 22 , 24 . The adapter panel arrangement 10 also includes a sliding drawer 34 that slides between an open position ( FIG. 1 ) and a closed position ( FIG. 2 ). A front cover 26 is attached to the sliding drawer 34 . When the drawer 34 is in the closed position, the front cover 26 encloses the interior 14 of the chassis 12 when closed ( FIG. 3 ) and provides access to the interior 14 when open ( FIG. 2 ). Referring now to FIGS. 1 and 2 , the adapter panel arrangement 10 includes a framework structure 30 ( FIG. 1 ) that is attached or mounted to the drawer 34 . A panel of adapters 32 is mounted to the framework structure 30 . As will be described in greater detail hereinafter, the drawer 34 is designed to slide outward from the chassis 12 primarily for installation purposes. That is, the drawer 34 can be slid to the open position during installation or assembly of the adapter panel arrangement, but is position in the closed position ( FIG. 2 ) during operative use of the arrangement 10 . During operative use, the framework structure 30 and the panel of adapters 32 are located within the interior 14 of the chassis 12 and the drawer 34 is in the closed position ( FIG. 2 ). A user accesses the panel of adapters 32 from a front opening 28 of the chassis 12 without sliding the drawer 34 forward. Referring again to FIG. 1 , the panel of adapters 32 includes a face panel 42 that defines a number of openings 44 (only one shown). Adapters 46 are mounted within the openings 44 . In the illustrated embodiment, the adapters are LC type adapters; however, other types of adapters, such as SC, ST, FC and MPO type adapters can also be used in accordance with the principles disclosed. Further, in the illustrated embodiment, the adapters 46 are blocked or grouped; each adapter block 58 including eight adapters 46 (four adapter pairs). Other number of adapters can be provided in an adapter block, such as four adapters (two adapter pairs), for example; the openings in the face panel 42 being correspondingly sized to receive the four-adapter blocks. Alternative, single adapters can be used and mounted with openings sized to receive the single adapters. The openings 44 of the face panel 42 are arranged in rows; each row of mounted adapter blocks 58 defines an adapter array 48 . What is meant by a row is that the openings 44 are arranged in a generally horizontal alignment, as opposed to being arranged in a column or in a vertical alignment; accordingly, the adapter arrays 48 are generally horizontal adapter arrays. Referring now to FIGS. 1 and 4 , the adapters 46 of the adapter blocks 58 each includes a front connection end 50 ( FIG. 1 ) and a rear connection end 52 ( FIG. 4 ). When mounted within the openings 44 , the front connection ends 50 of the adapters 46 are located toward the front opening 28 of the chassis 12 , and the rear connection ends 52 of the adapters 46 are located toward the rear wall 20 of the chassis 12 . The front connection ends 50 of the adapters 46 define open frontward cable connection locations 54 ( FIG. 2 ) of the face panel 42 . The rear connection ends 52 of the adapters 46 define open rearward cable connection locations 56 ( FIG. 4 ) of the face panel 42 . What is meant by “open cable connection locations” are locations that are provided in an open region in the chassis 12 , as opposed to a connection location that is enclosed within a housing or module, the housing or modules in turn being mounted within the chassis. That is, the panel of adapters 32 is a panel of unenclosed adapters 46 that are not enclosed relative to the other adapters 46 on the face panel 42 . While the panel of adapters itself is enclosed within the chassis 12 , the plurality of adapters 46 , and each of the adapter arrays 48 are not enclosed separately from the other adapters 46 or the other adapter arrays 48 . Referring now to FIGS. 1, 5 and 6 , the adapter arrays 48 of the face panel 42 are designed to slide in a lateral direction independent of other adapter arrays. In particular, the face panel 42 is defined by a number of separate panel sections 60 . In the illustrated embodiment, each separate panel section defines one row of openings in which the blocks 58 of unenclosed adapters 46 are mounted, i.e., each panel section 60 contains one adapter array 48 . In other embodiments, the panel sections can include, for example, two rows of openings that receive four-adapter blocks, for example; this panel section embodiment containing two adapter arrays. The face panel 42 of the adapter panel arrangement 10 illustrated includes six panel sections 60 —two panel sections 60 positioned side-by-side, and stacked three panel sections high (see FIG. 1 ). Each panel section 60 contains six blocks 58 having eight adapters 46 for a total of 288 frontward connection locations and rearward connection locations. Each separate panel section 60 is designed to selectively slide in a forward, lateral direction (A) independent of the other panel sections. The forward, lateral direction (A) is a direction extending between the front opening 28 and the rear wall 20 , as opposed to a direction which is transverse to the bottom wall 18 of the chassis 12 , for example. Referring to FIGS. 7-9 , each separate panel section 60 of the panel of adapters 32 is attached to a sliding frame piece 62 . The sliding frame piece 62 includes a pair of elongated rail members 64 . In the illustrated embodiment, the elongated rail members 64 include a forward rail portion 84 that extends forwardly from the panel section 60 , and a rearward rail portion 86 that extends rearwardly from the panel section 60 . The sliding frame piece 62 can include a cross-support 88 to maintain the structural relationship of the rail members 64 . The pairs of elongated rail members 64 are arranged to engage and slide within pairs of guides 66 (one shown in FIGS. 10-12 ) that are mounted to the framework structure 30 ( FIG. 1 ) of the arrangement 10 . The rail members 64 and the guides 66 include a stop arrangement 68 that limits the sliding motion of the panel sections 60 between a rearward position (see the top panel section 60 in FIG. 5 ) and a forward position (see the bottom panel section 60 in FIG. 5 ). Referring to FIGS. 9-12 , the stop arrangement 68 ( FIG. 12 ) is defined by at least one projection 70 ( FIGS. 10 and 11 ) located on each guide 66 of the pair of guides, and first and second pockets or detents 72 , 74 ( FIG. 9 ) formed in the rail members 64 . In the illustrated embodiment, two projections 70 (upper and lower projections) are provided on each of the guides 66 . Correspondingly, upper and lower detents 72 , 74 (see FIG. 8 ) are formed in the rearward rail portions 86 of the rail members 64 . While the illustrated embodiment depicts the detents 72 , 74 formed in the rail members 64 and the projections 70 provided on the guides 66 , it is contemplated that the detents can be formed in the guides 66 and the projection correspondingly provided on the rail members 64 . Referring still to FIGS. 9-12 , when the panel section 60 is positioned in the rearward position, the projections 70 of the guides 66 seat within the first detents 72 of the rail members 64 to retain the panel section 60 in the rearward position. The guides 66 are flexibly constructed so that when the panel section 60 is pulled forward, the projections 70 un-seat and slide along top and bottom surfaces 76 , 77 ( FIG. 8 ) of the rail members 64 . Referring to FIG. 12 , when the panel section 60 reaches the forward position, the projections 70 seat within the second detents 74 of the rail members 64 . This stop arrangement 68 indicates to a user when the panel section 60 has reached the predetermined forward position, and similarly, the rearward position. Referring back to FIG. 5 , in general, the stop arrangement 68 provides an indication of when the panel section 60 has moved a lateral distance D forward from the rearward position to the forward position. In one embodiment, the lateral distance D is no more than about 4.0 inches forward from the rearward position. In the illustrated embodiment, the lateral distance D is about 1.7 inches. Providing such an indication to the user prevents the user from moving the panel section 60 a distance beyond that which cables interconnected to the panel section 60 will allow. In particular, as previously described, the present panel arrangement 10 is designed such that the drawer 34 is intended to slide only during installation procedures, as opposed to post-installation or during operative use. Referring to FIG. 4 , during installation, cables 36 , such as fiber optic cables, are routed into the chassis 12 through rear openings 38 and terminated to the open rearward connection locations 56 of the face panel 42 (i.e., the rear connector ends 52 of the adapters 46 ). The fiber optic cables 36 have a predetermined length that can be routed about cable storage spools or structures (see e.g., 78 , 80 in FIG. 1 ). The predetermined lengths of the cables, however, do not have enough slack to accommodate drawer 34 movement during operative use, and the arrangement 10 does not have devices such as sliding radius limiters that take up or manage excessive movement of such cable slack. In present panel arrangement 10 , the predetermined lengths of the cables generally accommodate only the limited sliding movement of the panel sections 60 . That is, while the drawer 34 may be slid out for purposes of installation, or for repairs requiring access to the region behind the panel of adapters 32 , the drawer 34 is not intended to slide for purposes of accessing the panel of adapters 32 during operative use of the adapter panel arrangement 10 . Operative use and access to the panel of adapters 32 is instead provided by the sliding movement of the panel sections 60 relative to the sliding movement of the drawer 34 . In general, the lateral sliding movement of the panel sections 60 provides access to the open cable connections (e.g., 54 , 56 ) defined by the adapter arrays 48 . Access to the open connection locations (e.g., 54 , 56 ) of the face panel 42 is important in two primary instances: the first instance being during installation (e.g., during initial install or assembly, or during repair, replacement, or upgrade of the cable terminations at the rearward connection locations 56 of the panel 32 ); the second instance being after installation during operative use of the arrangement 10 . Referring back to FIGS. 1 and 4 , during installation, the drawer 34 is pulled out to the open position. As previously described, a technician routes the fiber optic cables 36 through the rear openings 38 of the chassis 12 and terminates the cables to the open rearward connection locations 56 of the panel of adapters 32 . To provide better access to the rear connection ends 52 of the adapters 46 defining the rearward connection locations 56 , one of the adapter arrays 48 is positioned in the rearward position (e.g., the top array), while the remaining adapter arrays (e.g., the arrays located beneath the top array (see also FIGS. 5 and 6 )) are positioned in the forward position. In this configuration, the technician has better access to the open rearward connection locations 56 of the one panel section 60 positioned in the rearward position. Once cable terminations to that particular adapter array 48 are complete, that adapter array can be slid forward and the next array to which cables are to be terminated slid rearward. Referring to FIG. 4 , to provide even further access to the open rearward connection locations 56 , the top wall 16 of the chassis 12 includes removable access panels 92 . Referring to FIG. 2 , each of the panels 92 slides outward in a direction B from the top wall 16 of the chassis 12 . In FIG. 2 , the panels 92 are shown engaged with the top wall 16 . In particular, each panel 92 is locked in place by a flexible tab 94 that engages a hem or roll 98 formed in a top wall portion 100 of the top wall 16 . The flexible tab 94 is defined by slots 96 formed in the panel 92 . The hem or roll 98 is formed by bending or rolling a section of the top wall 16 over on itself; although structure can be attached to the top wall as an alternative to providing a hem. To slide one of the panels 92 out, the flexible tab 94 is flexed downward beyond the hem or roll 98 formed in the top wall portion 100 . The panel is then slid out in the direction shown in FIG. 2 and removed to define a top wall opening 104 (see e.g., FIG. 15 ) located adjacent to the front opening 28 of the chassis 12 . The top wall opening 104 provides further access to the open rear connection locations 56 . To re-attach the panel 92 , the panel 92 is place in relation to the top wall opening 104 , the flexible tab 94 is flexed downward, and the panel 92 is then slid back into place. As shown in FIG. 15 , retaining flanges 102 are formed in the top wall 16 at the top wall openings 104 . The retaining flanges 102 support the panels 92 when attached to the top wall 16 of the chassis 12 . The open rearward connection locations 56 are typically access only during installation procedures, with the exception of repairs or upgrades, for example. The open frontward connection locations 54 , however, are accessed on a more regular basis to provide cross-connections between telecommunications equipment. Such use is referred to as operative use, or use that is post-installation and primarily involves maintaining or establishing cable terminations at the front connection ends 50 of the adapters 46 . Referring now to FIGS. 13-15 , the adapter panel arrangement 10 is shown in operative use. During operative use, the panel of adapters 32 is accessed through the front opening 28 of the chassis 12 , with the drawer 34 positioned in the closed position. As previously described, the cables 36 that enter the interior 14 of the chassis 12 through rear openings 38 are terminated to the open rear connection locations 56 of the panel of adapters 32 . Referring to FIG. 13 , jumper cables or patching cables 40 are also terminated to the panel of adapters 32 ; and in particular, to the open frontward connection locations 54 of the panel 32 . The patching cables 40 provide the cross-connections between the adapter panel arrangement 10 and other telecommunications equipment (not shown). The patching cables 40 are routed from the front opening 28 and through side openings 90 ( FIG. 3 ) of the chassis 12 to cable routing structure (e.g., channels, not shown) of the telecommunications system. Because of the high-density arrangement of the adapters 46 , each panel section 60 of the panel of adapters 32 slides forward to separate the associated adapter array 48 from the other arrays. By separately positioning the panel section 60 and the associated adapter array 48 forward, a technician can more easily grasp a particular connector of a patching cable 40 , and/or more easily terminate a patching cable to a particular adapter 46 of the forwardly-positioned array. In addition, and as previously described, the access panels 92 ( FIG. 13 ) of the top wall 16 can be removed (as shown in FIG. 15 ) to provide even further access to the open frontward connection locations 54 of the panel sections. Referring again to FIG. 13 , the forward rail portion 84 of the rail member 64 can be used as a handle to pull the panel section 60 forward. Alternatively, the user can slide the panel section 60 forward by grasping a retaining ring 82 attached to the rail member 64 of the sliding frame piece 62 . In the illustrated embodiment, the retaining rings 82 are attached to the ends of outer rail members 64 of the sliding frame piece 62 to protect the patching cables 40 from exceeding a minimum bend radius. While the present disclosure is described with respect to use in a fiber optic application, the disclosed panel arrangement can be adapted for use in other applications. For example, in some applications, copper cables may be used exclusively from fiber optic cables; and accordingly various types of wire terminations or wire connectors can be provided on the face panel of the arrangement. Still, in other applications having hybrid cabling, or applications having both types of fiber optic and copper cabling, the face panel of the arrangement can be provided with a combination of fiber optic and copper connectors and/or adapters. In general, the present adapter panel arrangement 10 provides a high-density adapter panel arrangement while facilitating access to otherwise crowded front and rear connection locations. Because of the access design of the present arrangement, the amount of space utilized on racks and cabinets is minimized; or, in the alternative, allows for expansion and upgrade of systems having spatial constraints, as more densely packed connection locations are provided without sacrificing effective access to the connection locations. The above specification provides a complete description of the present invention. Since many embodiments of the invention can be made without departing from the spirit and scope of the invention, certain aspects of the invention reside in the claims hereinafter appended.",en,PATENT_APPLICATION
161-678-922-859-530,US,20240388410,A1,2024-11-21,US_20240388410_A1_20241121,en,US,20240388410,A1,2024-11-21,US,18198252,2023-05-16,Automated Frequency Coordination (AFC) System and Method For Controlling Frequency Sharing and/or Interference in a Communications System,en,US,"Charter Communications Operating, LLC","St. Louis, MO",US,Abdulrauf Hafeez,"Cary, NC",US,1,Saul A. Torrico,"Bethesda, MD",H04L5/00,I,F,H04W72/0453,I,L,H04W72/541,I,L,H04L5/0073,I,F,H04W72/0453,I,L,H04W72/541,I,L,US,20240388410,A1,2024-11-21,161-678-922-859-530,1,US,20240388410,A1,2024-11-21,161-678-922-859-530,1,UNKNOWN,"Methods and/or apparatus for implementing an automated or automatic frequency coordination (AFC) system are described. The method involves determining if allowing a transmitter to use spectrum, e.g., a frequency or band of frequencies, will result in unacceptable interference to another device or if the interference produced by the transmitter will be within an acceptable level. The method involves generating and storing Threshold to Interference (T/I) ratio tables, e.g., based on actual transmitter and/or receiver characteristic information, prior to a request to use spectrum being received. The T/I tables are stored in a T/I threshold database which is accessed and used to determine if a transmitter will create unacceptable interference to a device within the transmit area of the transmitter if frequency sharing is permitted. The T/I threshold tables can be computed, stored and the values stored therein can be used for multiple frequency use decisions by one or more AFC systems.",en,"1 . A method of controlling frequency sharing, the method comprising: storing in a T/I threshold table database a plurality of T/I threshold tables for different transmitter device and receiver device pairs, each T/I threshold table including a plurality of T/I threshold values for an individual transmitter and receiver device pair, each T/I threshold value in a T/I threshold table corresponding to a different amount of frequency separation; receiving, at a frequency coordination system, a frequency use request seeking permission for a first transmitter device to use a first frequency; identifying, at the frequency coordination system, a first potential victim device which may be subject to interference from the first transmitter device using the first frequency; retrieving from a first T/I threshold table stored in said T/I threshold table database, a first T/I threshold value based on a frequency separation between the first frequency used by the first transmitter and a frequency used by the first potential victim device, said first T/I threshold table being a T/I threshold table corresponding to the first transmitter device and the first potential victim device; and making a decision, at the frequency coordination system, to: i) authorize use of the first frequency by the first transmitter based on the expected transmit power or Effective Isotropic Radiated Power (EIRP) of the first transmitter and the first T/I threshold value or ii) deny use of the first frequency by the first transmitter based on the expected transmit power or Effective Isotropic Radiated Power (EIRP) of the first transmitter and the first T/I threshold value.","2 . The method of claim 1 , further comprising: communicating the decision to a device from which said frequency use request was received.","3 . The method of claim 1 , wherein the first frequency is a carrier frequency used by the first transmitter device and where the frequency separation between the first frequency and the frequency used by the first potential victim device is the frequency difference between first frequency and the center frequency of a digital receiver of the first potential victim device; and wherein the method further includes determining, by an interference computational engine, the frequency separation between the first frequency and the frequency used by the first potential victim device.","4 . The method of claim 3 , further comprising: determining, at the interference computational engine, if the first potential victim device will be subject to unacceptable interference based on a first interference margin determined based on transmit or EIRP of the first transmitter device and path loss between the first transmitter device and first potential victim device.","5 . The method of claim 4 , wherein determining if the first potential victim device will be subject to unacceptable interference is further based on a first threshold level (Tc) of the first potential victim device.","6 . The method of claim 5 , further comprising: determining, by the interference computational engine, said first interference margin by subtracting, from the first T/I threshold value, a threshold level (Tc) of the first victim receiver divided by an expected interference level (Ic) (at the first potential victim device.","7 . The method of claim 4 , wherein making a decision includes i) authorizing use of the first frequency by the first transmitter device when the first potential victim device will not be subjected to unacceptable interference and ii) denying use of the first frequency by the first transmitter device when the first potential victim device will be subjected to unacceptable interference.","8 . The method of claim 7 , further comprising: prior to receiving, the first frequency use request, computing T/I threshold tables for multiple different possible transmitter device and receiver device pairs, each of the different possible transmitter device and receive device pairs corresponding to a different transmitter device and receiver device combination, said first T/I threshold table being one of computed T/I threshold tables.","9 . The method of claim 8 , wherein threshold values in said first T/I threshold table are computed based on a first actual Power Density Function (PDF) of an interfering signal generated by the first transmitter and an approximated frequency response mask of a digital receiver filter included in the first receiver.","10 . The method of claim 8 , wherein threshold values in said first T/I threshold table are computed based on an approximated mask of the Power Density Function (PDF) of an interfering signal which is generated by the first transmitter and an actual frequency response of a digital receiver filter included in the first receiver.","11 . The method of claim 8 , wherein threshold values in said first T/I threshold table are computed based on actual known transmitter characteristics of the first transmitter and actual known characteristics of said first receiver prior to said first frequency request being received.","12 . The method of claim 11 , wherein the actual known transmitter characteristics of the first transmitter include a first actual Power Density Function (PDF) of an interfering signal which is generated by the first transmitter; and wherein the actual known characteristics of said first receiver include a first actual frequency response of a digital receiver filter included in the first receiver.","13 . The method of claim 12 , wherein computing T/I threshold tables includes computing said first T/I table, and wherein computing the first T/I table includes convolving the first actual PDF of the interfering signal with the actual frequency response of the digital receiver filter included in the first receiver.","14 . A system for controlling frequency sharing, the system comprising: a T/I threshold table database storing a plurality of T/I threshold tables for different transmitter device and receiver device pairs, each T/I threshold table including a plurality of T/I threshold values for an individual transmitter and receiver device pair, each T/I threshold value in a T/I threshold table corresponding to a different amount of frequency separation: a frequency coordination system including: an interface receiver; and a first processor, and wherein said first processor is configured to: operate the frequency coordination system to receive a frequency use request seeking permission for a first transmitter device to use a first frequency; identify, at the frequency coordination system, a first potential victim device which may be subject to interference from the first transmitter device using the first frequency; operate the frequency coordination system to retrieve from a first T/I threshold table stored in said T/I threshold table database, a first T/I threshold value based on a frequency separation between the first frequency used by the first transmitter and a frequency used by the first potential victim device, said first T/I threshold table being a T/I threshold table corresponding to the first transmitter device and the first potential victim device; and make a decision, at the frequency coordination system, to: i) authorize use of the first frequency by the first transmitter based on the expected transmit power or Effective Isotropic Radiated Power (EIRP) of the first transmitter and the first T/I threshold value or ii) deny use of the first frequency by the first transmitter based on the expected transmit power or Effective Isotropic Radiated Power (EIRP) of the first transmitter and the first T/I threshold value.","15 . The system of claim 14 , wherein said frequency coordination system further includes an interface transmitter; and wherein said first processor is further configured to operate the frequency coordination system to: communicate the decision to a device from which said frequency use request was received.","16 . The system of claim 14 , further comprising: an interference computational engine including a second processor; wherein the first frequency is a carrier frequency used by the first transmitter device and where the frequency separation between the first frequency and the frequency used by the first potential victim device is the frequency difference between first frequency and the center frequency of a digital receiver of the first potential victim device; and wherein the second processor is configured to determine the frequency separation between the first frequency and the frequency used by the first potential victim device.","17 . The system of claim 16 , wherein said second processor is configured to determine, at the interference computational engine, if the first potential victim device will be subject to unacceptable interference based on a first interference margin determined based on transmit or EIRP of the first transmitter device and path loss between the first transmitter device and first potential victim device.","18 . The system of claim 17 , wherein said determining if the first potential victim device will be subject to unacceptable interference is further based on a first threshold level (Tc) of the first potential victim device.","19 . The system of claim 18 , wherein said second processor is further configured to: determine, by the interference computational engine, said first interference margin by subtracting, from the first T/I threshold value, a threshold level (Tc) of the first victim receiver divided by an expected interference level (Ic) at the first potential victim device; and wherein said making a decision includes i) authorizing use of the first frequency by the first transmitter device when the first potential victim device will not be subjected to unacceptable interference and ii) denying use of the first frequency by the first transmitter device when the first potential victim device will be subjected to unacceptable interference.","20 . A non-transitory computer readable medium including machine readable instructions, which when executed by a processor of a frequency coordination system, controls the frequency coordination system to perform the steps of: receiving a frequency use request seeking permission for a first transmitter device to use a first frequency; identifying, at the frequency coordination system, a first potential victim device which may be subject to interference from the first transmitter device using the first frequency; retrieving from a first T/I threshold table stored in a T/I threshold table database, a first T/I threshold value based on a frequency separation between the first frequency used by the first transmitter and a frequency used by the first potential victim device, said first T/I threshold table being a T/I threshold table corresponding to the first transmitter device and the first potential victim device; and making a decision, at the frequency coordination system, to: i) authorize use of the first frequency by the first transmitter based on the expected transmit power or Effective Isotropic Radiated Power (EIRP) of the first transmitter and the first T/I threshold value or ii) deny use of the first frequency by the first transmitter based on the expected transmit power or Effective Isotropic Radiated Power (EIRP) of the first transmitter and the first T/I threshold value.",en,"FIELD The present application relates to frequency sharing and more particularly to methods and/or apparatus for implementing an automated frequency coordination system that uses stored information tables, e.g., tables including precomputed values used in estimating interference, generated from actual transmitter and/or receiver characteristic information. BACKGROUND 6 GHz band spectrum is shared between unlicensed users and incumbent fixed service (FS) receivers. To limit interference an Automated Frequency Coordination (AFC) system may be used. Use of an AFC system has been proposed in Wireless Innovation Forum (sometimes referred to as WINNFORUM). An AFC often includes a frequency controller (FC) that controls access to various frequencies in the band based on an interference estimate. One important component of how to share the spectrum is the knowledge of the level of interference a digital receiver can tolerate. The determination of the allowable interference into a digital system is based on the consideration of the ratio of the desired to the undesired signal power that degrades the digital receiver's noise sensitivity by a certain dB level (e.g., 1 dB). At present the allowable interference is determined by using the approximated mask of the Power Density Function (PDF) of the interfering signal and the approximated frequency response mask of the digital receiver filter. This approach tends to over-protect the digital receiver, resulting in a conservative frequency sharing between the two systems and thus a waste of frequency resources. The same frequency control mechanism is used in some cases for frequency sharing between two unlicensed or lightly licensed systems in shared spectrum such as CBRS Band, resulting in conservative frequency sharing. The commonly used current approach to interference management, which is based on approximations of both transmitter and receiver characteristics, is a conservative approach which tends to over-protect the higher priority device(s) leading to wasted resources. Based on the above discussion, there is a need for new methods and apparatus for determining more precise levels of expected interference and/or determining more precise levels of allowable interference, in a shared spectrum environment, than is possible using approximations of both receiver and transmitter characteristics or functions. Unfortunately, convolutions used in making interference estimates or frequency sharing decisions, such as those based on the approach highlighted in Telecommunications Industry Association (TIA) bulletin TSB-10F (the full cite to which is: TIA/EIA Telecommunication Systems Bulletin—Interference Criteria for Microwave Systems TSB-1OF dated June 1994 which is hereby expressly incorporated by reference in its entirety), are computationally complex to implement. Accordingly, even if a device requesting the right to use spectrum provided information about is transmitter power density function at the time of making a spectrum sharing request, there might not be a reasonable amount of time available to perform detailed interference computations involving potential devices subject to interference in the time in which a frequency sharing decision is expected. In view of the above, it should be appreciated that there is a need for methods and/or apparatus which will allow for improved spectrum sharing decisions which can take into consideration actual transmitter and/or receiver characteristics but not require convolutions relating to transmitter and receiver functions or masks to be performed at the time of decision making which can be computationally intensive and/or time consuming to implement. SUMMARY Methods and/or apparatus for implementing an automated frequency coordination system are described. The method involves determining if allowing a transmitter to use spectrum, e.g., a frequency or band of frequencies, will result in unacceptable interference to another device or if the interference produced by the transmitter will be within an acceptable level in which case the spectrum can be shared. The method involves generating and storing Threshold to Interference (T/I) ratio tables, based on actual transmitter and/or receiver characteristic information, prior to a request to use spectrum being received and processed. In at least some embodiments the T/I tables are generated for transmitter device and receiver device pairs using actual transmitter and/or receiver characteristic information rather than simply estimates or approximations relating to a transmitter or receiver device. The T/I tables are stored in a T/I threshold database which is accessed and used to determine if a transmitter will create unacceptable interference to a device within the transmit area of the transmitter if frequency sharing is permitted. The T/I table corresponding to an individual transmitter device and receiver device pair stores a threshold value corresponding to a frequency separation between the carrier frequency of the interfering signal and the receiver center frequency based, in some but not necessarily all implementations, on the approach discussed in in TIA Telecommunication Systems Bulletin—Interference Criteria for Microwave Systems TSB-10F, June 1994. The T/I threshold tables used by the frequency controller in some embodiments and is obtained by calculating the tables threshold values a priori based on the approach highlighted in the TIA-10F bulletin (TIA Telecommunication Systems Bulletin—Interference Criteria for Microwave Systems TSB-10F, June 1994) In various embodiments, a T/I threshold table database is used to store a plurality of T/I threshold tables for different transmitter device and receiver device pairs, each T/I threshold table including a plurality of T/I threshold values for an individual transmitter and receiver device pair, where each T/I threshold value in a T/I threshold table corresponds to a different amount of frequency separation. In one particular exemplary embodiment, a frequency coordination system, e.g., automatic frequency coordination (AFC) system, receives a frequency use request seeking permission for a first transmitter device to use a first frequency. The first frequency may be the center frequency of spectrum or a band which the first transmitter device seeks to use. The request may be sent by the first transmitter device or by another device seeking to determine if the first transmitter device is permitted to use the first frequency and/or spectrum corresponding to the first frequency. The AFC system identifies, e.g., based on location information and/or other stored or received information, a first potential victim device, e.g., a first receiver device, which may be subject to interference from the first transmitter device using the first frequency. The AFC system then retrieves from a first T/I threshold table stored in said T/I threshold table database, a first T/I threshold value based on a frequency separation between the first frequency used by the first transmitter and a frequency used by the first potential victim device. The first T/I threshold table is a T/I threshold table corresponding to the first transmitter device and the first potential victim device (e.g., a first victim receiver) which includes precomputed stored threshold values, e.g., one per possible frequency separation. The AFC system makes a decision to: i) authorize use of the first frequency by the first transmitter based on the expected transmit power or Effective Isotropic Radiated Power (EIRP) of the first transmitter and the first T/I threshold value or ii) deny use of the first frequency by the first transmitter based on the expected transmit power or Effective Isotropic Radiated Power (EIRP) of the first transmitter and the first T/I threshold value. In cases where AFC system determines that the transmitter use of the requested frequency will cause unacceptable interference the request to use the spectrum is denied. However, when an AFC system determines that use of the frequency/spectrum by the transmitter to which the frequency use request corresponds will not result in unacceptable interference the request is granted. While the computation of the values in the T/I threshold tables can be computationally complex and involve one or more convolution operations, because the tables are computed for possible transmitter device and receiver device pairs prior to a request to use frequency being received, the decision process implemented by the AFC system can be relatively quick since it can be based, at least in part, in the pre-computed values stored in the T/I threshold tables. Furthermore, it is possible to precompute the T/I threshold tables and load them into multiple AFC systems corresponding to different geographic regions. The computational resources used in generating the T/I threshold tables can be used in multiple different AFC systems without the need for individual AFC systems to perform the computations in real time in response to a frequency use request or other request to use spectrum. From a cost perspective this allows the hardware costs associated with generating the T/I threshold tables to be shared between AFC systems providing service to multiple different geographic regions. There are four cases that are considered to create the T/I table versus frequency separation between the carrier frequency of the interfering signal and the receiver center frequency. The T/I ratio table can be generated by using any one of the following quantities: i) the actual Power Density Function (PDF) of the interfering signal and the actual frequency response of the digital receiver filter; ii) the approximated mask of the PDF of the interfering signal and the actual frequency response mask of the digital receiver filter; iii) the actual PDF of the interfering signal and the approximated frequency response mask of the digital receiver filter; and iv) the approximated mask of the PDF of the interfering signal and the approximated frequency response mask of the digital receiver filter. In devices where information about the actual power density function of the interfering signal generated by a transmitter device and/or the actual mask of the digital receiver filter used in a receiver device are used in generating the T/I threshold table for a transmitter device and receiver device pair, the more reliable the prediction of the effect of interference will be and thus more efficient spectrum allocation can be achieved than when approximations are used for both the transmitter and receiver device. The methods of the invention have the advantages of generating, in some cases, a permanent or long-term database of actual T/I threshold values for different possible transmitter device and receiver device pairs, based on the actual PDF of the interfering signals and/or the actual frequency response of the digital receiver filter in the receiver device. Since the T/I tables can be generated and prestored they need not be computed on the fly and the computations performed in generating the T/I tables need not be repeated regardless of the number of AFC systems which use the database. Thus, a significant effort can be made in generating the database because it is an information resource that can be reused repeatedly as interference decisions are made by one or more AFC systems at different times. The methods of the present invention have the potential to provide more realistic T/I threshold values for a particular frequency separation between the carrier frequency of the interfering signal and the receiver center frequency than current methods because they can be based on accurate information and precomputed without concerns for the T/I values to be generated in real time when making a decision. Higher differences between the mask and the actual frequency response used in the present invention as compared to in previous systems can result in better sharing of spectrum between devices since the decisions can be made based, at least in some cases, on more accurate T/I threshold values than were available in previous systems which based such determinations completely on approximations rather than actual information corresponding to a transmitter device and receiver device pair. By using the threshold T/I tables and stored values included in the tables, frequency use authorization decisions can be make more quickly than in systems where T/I threshold values are generated in real time in response to a frequency use request and potentially with more accurate information being used since actual transmitter and/or receiver device characteristic information is used in at least some embodiments for computing the values stored in the T/I table database used in some embodiments. Numerous variations on the above described methods and apparatus are described in the detailed description which follows. BRIEF DESCRIPTION OF THE FIGURES FIG. 1 is drawing of an exemplary system for estimating interference and facilitating efficient spectrum allocation in an environment in which spectrum may be shared in accordance with an exemplary embodiment. FIG. 2A is a first part of a flowchart of an exemplary method of controlling frequency sharing in accordance with an exemplary embodiment. FIG. 2B is a second part of a flowchart of an exemplary method of controlling frequency sharing in accordance with an exemplary embodiment. FIG. 2C is a third part of a flowchart of an exemplary method of controlling frequency sharing in accordance with an exemplary embodiment. FIG. 2D is a fourth part of a flowchart of an exemplary method of controlling frequency sharing in accordance with an exemplary embodiment. FIG. 2 comprises the combination of FIG. 2A , FIG. 2B , FIG. 2C and FIG. 2D . FIG. 3 is a drawing illustrating a typical QAM transmitted spectrum in a 10 MHz RF channel assignment, which illustrates a Power Density Function (PDF) and an FCC spectrum mask for an exemplary transmitter device. FIG. 4 is a drawing illustrating typical QPR transmitted spectrum in a 3.75 MHz RF channel assignment, which illustrates a Power Density Function (PDF) and an FCC spectrum mask for another exemplary transmitter device. FIG. 5 is an exemplary T/I table for an exemplary transmitter device and receiver device pair. FIG. 6 is a drawing of an exemplary frequency coordination system, e.g., an automatic frequency coordination (AFC) system, in accordance with an exemplary embodiment. FIG. 7 is a drawing of an exemplary interference computational engine device in accordance with an exemplary embodiment. FIG. 8 is a drawing of an exemplary database device including a database of T/I threshold tables in accordance with an exemplary embodiment. FIG. 9 is drawing of an exemplary T/I threshold table database generation device in accordance with an exemplary embodiment. DETAILED DESCRIPTION FIG. 1 is drawing of an exemplary system 100 for estimating interference and facilitating efficient spectrum allocation in an environment in which spectrum may be shared in accordance with an exemplary embodiment. Exemplary communications system 100 includes an automatic frequency coordination (AFC) system 102 , an interference computational engine 104 , and a database 106 of T/I threshold tables. AFC system is coupled to interference computational system 104 via communications link 150 . Interference computational engine 104 is coupled to database 106 via communications link 152 . Database 106 includes a plurality of T/I threshold tables (T/I threshold table 1 108 , . . . , T/I threshold table N 110 . Each of the plurality of T/I threshold tables corresponds to different combinational pair of a transmitter device (interfering device) and a receiver device (potential victim receiver device). T/I threshold table 1 108 is T/I value vs frequency separation table, for a 1st TX device/RX device pair, where the frequency separation is between the carrier frequency of the interfering signal and the receiver center frequency. T/I threshold table 1 110 is T/I value vs frequency separation table, for a Nth TX device/RX device pair, where the frequency separation is between the carrier frequency of the interfering signal and the receiver filter frequency. Exemplary system 100 further includes a T/I threshold table database generation device 112 and an equipment database 114 , coupled together via communications link 154 . The equipment database 114 includes information identifying a plurality of different types of transmitter devices (transmitter (TX) type 1 device ID information 116 , . . . , transmitter (TX) type N device ID information 118 ), and corresponding information (actual PDF of interfering signal or approximated mask of PDF of interfering signal for TX type 1 device 120 , . . . , actual PDF of interfering signal or approximated mask of PDF of interfering signal for TX type N device 122 ), respectively. The equipment database 114 further includes information identifying a plurality of different types of receiver devices (receiver (RX) type 1 device ID information 124 , receiver (RX) type M device ID information 126 ), and corresponding information (actual frequency response of digital filter or approximated frequency response mask of digital filter for RX type 1 device 128 , . . . , actual frequency response of digital filter or approximated frequency response mask of digital filter for RX type M device 130 ), respectively. The T/I threshold table database generation device, uses the equipment database information to identify pairs of a TX device and a receiver device, and to generate a T/I threshold table for each transmitter device/receiver device pair. The generated T/I threshold tables are sent for the T/I threshold table database generation device, via signals 172 over communications link 156 to database 106 , where the T/I tables as stored, to be available to be used (accessed) in the future, e.g., by the interference computational engine. The generation of the tables is time intensive since convolution is involved. System 100 further includes a geo-spatial database 138 mapping location of transmitter devices (interferers) and receiver devices (potential receiver victims) coupled to AFC system via communications link 168 and coupled to interference computational engine 104 via communications link 170 . System 100 further includes device information database 140 including characteristic information for each transmitter and receiver device coupled to AFC system via communications link 164 and coupled to interference computational engine 104 via communications link 166 . System 100 further includes a topology information and propagation model information database 142 coupled to AFC system via communications link 164 and coupled to interference computational engine 104 via communications link 166 . Exemplary communications system 100 further includes a first base station 132 including an interfering device (transmitter device) 134 and a potential victim receiver device 136 . The interfering device 134 and the potential victim receiver device 136 are located within an area of interest 137 , e.g., within the vicinity of one another at a distance that is considered by the AFC system 102 to have the potential for transmitter device 134 to potentially generate an unacceptable level of interference at the location of potential victim receiver 136 . First base station 132 is coupled to AFC system 102 via communications link 158 . First base station 132 generates and sends a frequency user request 174 , e.g., for bandwidth with a particular carrier frequency, to AFC system 102 . In response to the request 174 , the AFC decides to request to run an interference analysis between the interfering device 134 and the potential victim receiver 136 , which are in proximity of each other. The processor 103 , e.g., frequency controller of the AFC system 102 generates and sends interference computational request 176 , via communications link 150 to interference computational engine 104 . The interference computational request 176 includes information identifying the equipment type, e.g., by type ID information, of the transmitter device 134 , information identifying the characteristics, e.g., interfering signals transmit power or EIRP, and interfering signal carrier frequency, of the transmitter device 134 , information identifying the equipment type, e.g., by type ID information, of the potential victim receiver device 136 , and characteristics, e.g. a center frequency of the digital receiver filter, of the digital receiver of potential victim receiver device 136 . The interference computational engine 104 receives the interference computational request 176 and recovers the communicated information. The interference computational engine performs an interference analysis for the transmitter device 134 /receiver device 136 pair, based on the interfering signal transmit power or EIRP as well as other information such as the transmission path losses, e.g., using information obtained from databases 140 , 142 , and 138 . The interference analysis includes computing a threshold level Tc of the victim receiver 136 , computing interference level Ic at the receiver victim 136 from interferer transmitter device 134 , and computing the ration Tc/Ic. Interference computational engine 104 determines frequency separation (different in frequency (Δf)) between the carrier frequency of the interfering signal and the center frequency of the digital filter receiver. The interference computational engine 104 retrieves a T/I threshold value, corresponding to the TX device 134 /RX device 136 pair, based on device type information, and the determined frequency separation. For example, the interference computational engine generates and sends a request 178 for a T/I value over communications link 152 to T/I threshold tables database 106 , said request 178 including TX device type ID information, TX device type ID information and the determined frequency separation value. The database 106 receives the request 106 , processes the received information, identifies the T/I threshold table corresponding to the identified device pair (based on the received device type information), and extracts the T/I threshold value from the identified table corresponding to the received frequency separation value. The database 106 generates and sends response signal 180 via communications link 152 to interference computational engine 104 , said response signal 180 including the extracted T/I threshold value mapping to the frequency separation from the T/I threshold table corresponding to the TX/RX pair. Interference computational engine 104 receives response signal 180 and recovers the communicated T/I threshold value. The interference computation engine determines an interference margin, where interference margin=T/I−Tc/Ic. The interference computation engine 104 compares the determined interference margin to 0. If the margin is greater than 0, then the interference computation engine 104 determines that the first potential victim receiver 136 will be subjected to unacceptable interference. However, if the margin is not greater than 0, then the interference computation engine 104 determines that the first potential victim receiver 136 will not be subjected to unacceptable interference. The interference computational engine 104 generates and sends, via communications link 150 , interference determination result message 182 to AFC system 102 , in response to interference computational request 176 , said interference result message 182 includes the determination as to whether or not the transmitter device 134 operation will result in an unacceptable level of interference to potential victim receiver 136 . The AFC system 102 receives the interference determination result message 182 and recovers the communicated interference determination result. The AFC system 102 makes a decision whether or not to authorize use of the requested frequency by the transmitter 134 of first base station 132 based on the received interference determination result. The AFC system 102 authorizes the use of the requested frequency by the transmitter device 134 when the potential victim receiver device 136 will not be subjected to unacceptable interference. Alternatively, the AFC system 102 denies the use of the requested frequency by the transmitter device 134 when the potential victim receiver device 136 will be subjected to unacceptable interference. The AFC system 102 sends, via communications link 158 , response message 184 to first base station 132 , in response to request 174 , said response communicating the AFC's decision with regard to transmitter 134 being allowed to use the requested frequency. The AFC's decision may, and sometimes does, includes a grant to use the requested frequency. FIG. 2 , comprising the combination of FIG. 2A , FIG. 2B , FIG. 2C and FIG. 2D , is a flowchart of an exemplary method of controlling frequency sharing in accordance with an exemplary embodiment. The exemplary methos of flowchart 200 is performed, e.g., by system 100 of FIG. 1 . Operation starts in step 202 and proceeds to step 203 . In step 203 T/I threshold table generation device 112 computes T/I threshold tables for multiple different possible transmitter device and receiver device pairs, each of the different possible transmitter device and receiver device pairs corresponding to a different transmitter device and receiver device combination, a first T/I threshold table being one of the computed T/I threshold tables. Step 203 includes step 2031 in the T/I threshold table generation device 112 computes a first T/I threshold table for a first transmitter device type/receiver device type pair combination and step 2037 in which the T/I threshold table generation device 112 computes an Nth T/I threshold table for an Nth transmitter device type/receiver device type pair combination. Step 2031 includes one of alternative steps 2032 , 2034 , 2035 , or 2036 , e.g., depending on the availability of actual or mask information, e.g., with an actual profile taking precedence over a mask. In alternative step 2032 the T/I threshold table generation device 112 computes the first T/I threshold table based on actual known transmitter characteristics of a first transmitter (e.g., a first actual power density function (PDF) of an interfering signal which is generated by a first transmitter) and actual know characteristics of a first receiver (e.g., a first actual frequency response of a digital filter included in the first receiver). Step 2032 includes step 2033 in which the T/I threshold table generation device 112 convolves the first actual PDF of the interfering signal with the actual frequency response of the digital filter included in the first receiver. In alternative step 2034 the T/I threshold table generation device 112 computes the first T/I threshold table based on a first actual power density function (PDF) of an interfering signal which is generated by a first transmitter and an approximated frequency response mask of a digital filter included in a first receiver device. In step 2034 the T/I threshold table generation device 112 convolves the first actual PDF of the interfering signal with the approximated frequency response mask of the digital filter included in the first receiver. In alternative step 2035 the T/I threshold table generation device 112 computes the first T/I threshold table based on an approximated mask of the power density function (PDF) of an interfering signal which is generated by a first transmitter and an actual frequency response of a digital filter included in a first receiver device. In step 2034 the T/I threshold table generation device 112 convolves the approximated mask of the PDF of the interfering signal with the actual frequency response of the digital filter included in the first receiver. In alternative step 2036 the T/I threshold table generation device 112 computes the first T/I threshold table based on an approximated mask of the power density function (PDF) of an interfering signal which is generated by a first transmitter and an approximated frequency response mask of a digital filter included in a first receiver device. In step 2036 the T/I threshold table generation device 112 convolves the approximated mask of the PDF of the interfering signal with the approximated frequency response mask of the digital filter included in the first receiver. Operation proceeds from step 203 , via connecting node 2038 , to step 2039 . In step 2039 , the T/I threshold table generation device 112 sends the generated T/I threshold tables to database 106 of TI threshold tables for storage. Operation proceeds from step 2039 to step 204 . In step 204 , the database 106 receives the T/I threshold tables and stores a plurality of T/I threshold tables for different transmitter and receiver device pairs, each T/I threshold table including a plurality of T/I threshold values for an individual transmitter device and receiver device pair, each T/I threshold value in a T/I threshold table corresponding to a different amount of frequency separation. Operation proceeds from step 204 to step 206 . In step 206 a frequency coordination system (e.g., an automatic frequency coordination system (e.g., AFC system 102 ) receives a frequency use request seeking permission for a first transmitter device, e.g., transmitter device 134 , to use a first frequency. Operation proceeds from step 206 to step 208 . In step 208 the frequency coordination system identifies (e.g., based on location information and/or other stored or received information such as, e.g., transmitter power or EIRP, and propagation information), a first potential victim device (e.g., a first receiver device, e.g., potential victim receiver device 136 ), which may be subject to interference from the first transmitter device using the first frequency. Operation proceeds from step 208 to step 210 . In step 210 the frequency coordination system requests an interference analysis between the first transmitter device and the first potential victim device (e.g., the first receiver device) which are in proximity of each other. Step 210 includes step 212 and step 218 . In step 212 the frequency coordination system generates an interference computation request corresponding to the first transmitter device and the first potential victim device (e.g., the first receiver device). Step 212 includes step 214 and step 216 . In step 214 the frequency coordination system includes in the interference request first transmitter device information, e.g., ID information, equipment type information, characteristics of the transmitter device, location of the transmitter device, transmit power level or EIRP, first frequency (interfering device signal carrier frequency). In step 216 the frequency coordination system includes in the interference request firs potential victim device (e.g., first receiver device) information, e.g., ID information, equipment type information, characteristics of the digital receiver, location of the first potential victim device, center frequency of the digital receiver filter. Operation proceeds from step 212 to step 218 . In step 218 the frequency coordination system sends the generated interference computation request corresponding to the first transmitter device and the first potential victim device (e.g., the first receiver device) to an interference computational engine, e.g., interference computational engine 104 . Operation proceeds from step 210 to step 220 . In step 220 the interference computation engine receives the interference computation request corresponding to the first transmitter device and the first potential victim device (e.g., the first receiver device). Operation proceeds from step 220 via connecting node B 222 to step 224 . In step 224 the interference computational engine performs an interference analysis (based on interfering signal transmit power or EIRP as well as other information such as transmission path losses. Step 224 includes steps 226 , 228 and 230 . In step 226 the interference computational engine computes the threshold level Tc of the first potential victim receiver. Operation proceeds from step 226 to step 228 , in which the interference computational engine computes the interference level Ic at the receiver victim from the interferer transmitter, e.g., based on transmit power or EIRP, locations of the victim receiver and interfering transmitter, and topology/propagation model information. Operation proceeds from step 228 to step 230 , in which the computational engine computes the ratio Tc/Ic. Operation proceeds from step 224 to step 232 . In step 232 the interference computational engine determines the frequency separation between the first frequency and frequency used by the first potential victim device. Step 232 includes step 234 , in which the interference computational engine determines the frequency separation (difference in frequency (Δf)) between the interfering signal carrier frequency and the center frequency of the digital receiver filter. Operation proceeds from step 232 to step 236 . In step 236 the interference computational engine retrieves from a first T/I threshold table stored in said T/I threshold database, a first T/I threshold value based on a frequency separation between the first frequency used by the first transmitter and a frequency used by the first potential victim device, said first T/I threshold table being a T/I threshold table corresponding to the first transmitter device and the first potential victim device (e.g., first victim receiver). Step 236 includes steps 238 , 240 , 242 , 244 , 246 and 248 . In step 238 the interference computational engine sends a request for a T/I threshold value to the database of T/I threshold tables, said request including information identifying the first transmitter (e.g., by TX device type), information identifying the first potential victim device (e.g., first victim receiver) (e.g., by RX device type), and the determined frequency separation. In step 240 the T/I threshold database receives the request for a T/I value and recovers the communicated information. In step 242 the T/I threshold database identifies the T/I threshold table (e.g., first T/I threshold table) corresponding to the pair of the first transmitter device and first potential victim device (e.g., first victim receiver). Operation proceeds from step 242 to step 244 . In step 244 the first T/I threshold database retrieves the T/I threshold value (e.g., first T/I threshold value) corresponding to the determined frequency separation from the identified table (e.g., first T/I threshold table) corresponding to the pair of first transmitter device and first potential victim device (e.g., first victim receiver). Operation proceeds from step 244 to step 246 . In step 246 the T/I threshold database is operated to send the retrieved T/I threshold value to the interference computational engine. Operation proceeds from step 246 to step 248 . In step 248 the interference computational engine recovers the first T/I threshold value. Operation proceeds from step 236 , via connecting node C 250 to step 252 . In step 252 the interference computational engine determines a first interference margin by subtracting, from the first T/I threshold value, a threshold level (Tc) of the first victim receiver divided by an expected interference level (Ic) at the first potential victim device. Step 252 includes step 254 in which the interference computational engine determines the first interference margin, where first interference margin=T/I−Tc/Ic. Operation proceeds from step 252 to step 256 . In step 256 the interference computational engine determines if the first potential victim device will be subject to unacceptable interference based on a first interference margin determined based on transmit or EIRP of first transmitter and path loss between the first transmitter and the first potential victim device. In various embodiments, said step of determining ( 256 ) if the first potential victim device will be subject to unacceptable interference is further based on a first threshold level (Tc) of the first potential victim device (first victim receiver). Step 256 includes steps 258 , 260 and 262 . In step 258 the interference computational engine compares the determined first interference margin to 0) and determines if the first interference margin is greater than 0. If the determination is that the first interference margin is greater than 0, then operation proceeds from step 258 to step 260 in which the interference computational engine determines that the first potential victim (e.g., potential victim receiver device 136 ) will be subjected to unacceptable interference. However, if the determination is that the first interference margin is not greater than 0, then operation proceeds from step 258 to step 262 in which the interference computational engine determines that the first potential victim (e.g., potential victim receiver device 136 ) will not be subjected to unacceptable interference. Operation proceeds from step 256 to step 264 . In step 264 the interference computational engine sends an interference determination result to the frequency coordination system indicating whether or not the first potential victim will be subjected to unacceptable interference from the first transmitter. Operation proceeds from step 264 to step 266 . In step 266 the frequency coordination system receives the interference determination result. Operation proceeds from step 266 to step 268 . In step 268 the frequency coordination system makes a decision to: i) authorize use of the first frequency by the first transmitter based on the expected transmit power or effective EIRP of the first transmitter and the first threshold value or ii) deny the used of the first frequency by the first transmitter based on the expected transmit power or effective EIRP of the first transmitter and the first threshold value. Step 268 includes steps 270 and 270 , one of which is implemented for an iteration of step 268 . In step 270 the frequency coordination system authorizes the use of the first frequency by the first transmitter device when the first potential victim device will not be subjected to unacceptable interference. In step 272 the frequency coordination system denies the use of the first frequency by the first transmitter device when the first potential victim device will be subjected to unacceptable interference. Operation proceeds from step 268 to step 274 . In step 274 the frequency coordination system communicates the decision to a device from which the frequency use request was received, e.g., first base station 132 , including the first transmitter device 134 . FIG. 3 is a drawing 300 illustrating a typical QAM transmitted spectrum in a 10 MHz RF channel assignment, which illustrates an actual Power Density Function (PDF) 302 and an FCC spectrum mask 304 for an exemplary transmitter device. FIG. 4 is a drawing 400 illustrating typical QPR transmitted spectrum in a 3.75 MHz RF channel assignment, which illustrates an actual Power Density Function (PDF) 402 and an FCC spectrum mask 404 for another exemplary transmitter device. FIG. 5 is an exemplary T/I table 500 for an exemplary transmitter device and receiver device pair. First column 502 identifies Δf and second column T/I value in (dB). First row 506 indicates that a Δf=0 maps to a T/I threshold value of 35 dB. Second row 508 indicates that a Δf=0 maps to a T/I threshold value of 35 dB. Third row 510 indicates that a Δf=4 maps to a T/I threshold value of 34 dB. Fourth row 512 indicates that a Δf=5 maps to a T/I threshold value of 34 dB. Fifth row 514 indicates that a Δf=6 maps to a T/I threshold value of 26 dB. Sixth row 516 indicates that a Δf=8 maps to a T/I threshold value of 12 dB. Seventh row 518 indicates that a Δf=10 maps to a T/I threshold value of 35 dB. Eighth row 520 indicates that a Δf=12 maps to a T/I threshold value of −12 dB. Ninth row 522 indicates that a Δf=14 maps to a T/I threshold value of −26 dB. Tenth row 524 indicates that a Δf=16 maps to a T/I threshold value of −36 dB. Eleventh row 526 indicates that a Δf=18 maps to a T/I threshold value of −46 dB. Twelfth row 528 indicates that a Δf=20 maps to a T/I threshold value of −53 dB. FIG. 6 is a drawing of an exemplary frequency coordination system 600 , e.g., an automatic frequency coordination (AFC) system, in accordance with an exemplary embodiment. Exemplary frequency coordination system 600 is, e.g., AFC system 102 of system 100 of FIG. 1 and/or a frequency coordination system implementing steps of the exemplary method of flowchart 200 of FIG. 2 . Exemplary frequency coordination system 600 includes a processor 602 , a network interface 604 , e.g., a wired or optical interface, an assembly of hardware components 606 , e.g., an assembly of circuits, and memory 608 coupled together via a bus 610 over which the various elements may interchange data and information. Network interface 604 includes a receiver 612 and a transmitter 614 coupled to connector 616 , which couples the frequency coordination system 600 to: an interference computational engine, base stations, a database of T/I threshold tables, a geo-spatial database, a device information database, a topology information and propagation model information database, and/or other network nodes and/or the Internet. Memory 608 includes a control routine 618 , an assembly of components 620 and data/information 622 . Control routine 618 includes instructions which when executed by processor 602 control the frequency coordination system 600 to implement basic operational functions, e.g., read memory, write to memory, control an interface, load a program, subroutine, or app, etc. Assembly of components 620 , e.g., an assembly of software components, e.g., routines, subroutines, applications, etc., includes, e.g., code which when executed by processor 602 , controls the frequency coordination system 600 to implement steps of a method, e.g., steps of the method of flowchart 200 of FIG. 2 which are performed by a frequency coordination system. Data/information 622 includes a received frequency user request 624 (e.g., a request to be allowed to use a band of spectrum with a carrier frequency) e.g., from a base station including a transmitter (which is potential interferer to a potential victim receiver, which is to be protected), an identified transmitter device 626 (interfering device), an identified potential victim receiver device 628 (e.g., in the proximity of the transmitter device such that there may be an interference issue), transmitter device characteristic information 630 , e.g., a carrier frequency, a transmit power level or a EIRP, and receiver device characteristic information 632 , e.g., a digital filter center frequency of the receiver device. Data/information 622 further includes a generated interference computation request 634 to be sent to a interference computational engine, a received interference determination result 636 (e.g., communicating an indication that the expected level of interference is unacceptable or is acceptable), and a generated response 638 to the frequency user request to be sent to the base station including the potential interfering transmitter (e.g., a grant allowing the requesting device to use the spectrum with or a denial to use the requested spectrum). FIG. 7 is a drawing of an exemplary interference computational engine device 700 in accordance with an exemplary embodiment. Exemplary interference computational engine device 700 is, e.g., interference computational engine 104 of system 100 of FIG. 1 and/or an interference computational engine implementing steps of the exemplary method of flowchart 200 of FIG. 2 . Exemplary interference computational engine device 700 includes a processor 702 , a network interface 704 , e.g., a wired or optical interface, an assembly of hardware components 706 , e.g., an assembly of circuits, and memory 708 coupled together via a bus 710 over which the various elements may interchange data and information. Network interface 704 includes a receiver 712 and a transmitter 714 coupled to connector 716 , which couples the interference computational engine 700 to: a frequency coordination system, e.g., an AFC system including a controller, a database of T/I threshold tables, a geo-spatial database, a device information database, a topology information and propagation model information database, and/or other network nodes and/or the Internet. Memory 708 includes a control routine 718 , an assembly of components 720 and data/information 722 . Control routine 718 includes instructions which when executed by processor 702 control the interference computational engine device 700 to implement basic operational functions, e.g., read memory, write to memory, control an interface, load a program, subroutine, or app, etc. Assembly of components 720 , e.g., an assembly of software components, e.g., routines, subroutines, applications, etc., includes, e.g., code which when executed by processor 702 , controls the interference computational engine device 700 to implement steps of a method, e.g., steps of the method of flowchart 200 of FIG. 2 which are performed by an interference computational engine. Data/information 722 includes a received interference computational request 724 , a determined Tc/Ic ratio 726 (for an interfering transmitter device and potential victim receiver device), a determined frequency separation 728 (between the carrier frequency of the interfering signal and the center frequency of the digital receiver), a generated request 732 for a T/I threshold value (corresponding to TX/RX pair and the determined frequency separation), a received T/I threshold value 732 from a database of T/I threshold values which includes pre-configured T/I threshold values (for different frequency separation values) for combinations of possible TX/RX pairs, a determined interference margin 734 (based on the computed Tc/Ic ratio and the received T/I threshold value from the table. Data/information 736 further includes an interference determination result, e.g., a determination that the transmitter produces an unacceptable level of interference to the victim receiver device or a determination that the transmitter does not produce an unacceptable level to interference to the victim receiver device, and a generated message 738 communicating the interference determination result to the AFC system. FIG. 8 is a drawing of an exemplary database device 800 including a database of T/I threshold tables in accordance with an exemplary embodiment. Exemplary database device 800 is, e.g., database of T/I threshold tables 106 of system 100 of FIG. 1 and/or a database device including a database of T/I threshold value implementing steps of the exemplary method of flowchart 200 of FIG. 2 . Exemplary database device 800 includes a processor 802 , a network interface 804 , e.g., a wired or optical interface, an assembly of hardware components 806 , e.g., an assembly of circuits, and memory 808 coupled together via a bus 810 over which the various elements may interchange data and information. Network interface 804 includes a receiver 812 and a transmitter 814 coupled to connector 816 , which couples the database device 800 to: an interference computational engine, a T/I threshold table database generation device, a frequency coordination system, e.g., an AFC system including a frequency controller, and/or other network nodes and/or the Internet. Memory 808 includes a control routine 818 , an assembly of components 820 and data/information 822 . Control routine 818 includes instructions which when executed by processor 802 control the database device 800 to implement basic operational functions, e.g., read memory, write to memory, control an interface, load a program, subroutine, or app, etc. Assembly of components 820 , e.g., an assembly of software components, e.g., routines, subroutines, applications, etc., includes, e.g., code which when executed by processor 802 , controls the database device 800 to implement steps of a method, e.g., steps of the method of flowchart 200 of FIG. 2 which are performed by a database device including a database of T/I threshold values. Data/information 822 includes database 824 of T/I threshold tables (T/I threshold table 1 826 (e.g., a T/I value vs frequency separation (between the carrier frequency on interfering signal and the receiver filter center frequency) table) for a 1st TX device/RX device pair, T/I threshold table 2 828 (e.g., a T/I value vs frequency separation (between the carrier frequency on interfering signal and the receiver filter center frequency) table) for a 2nd TX device/RX device pair, . . . , T/I threshold table N 830 (e.g., a T/I value vs frequency separation (between the carrier frequency on interfering signal and the receiver filter center frequency) table) for a Nth TX device/RX device pair), a received request 832 (from an interference computation engine) for a T/I value corresponding to a TX/RX pair (being evaluated) and a particular determined frequency separation value, and a generated response 834 includes an extracted T/I value, said generated response to be sent to the interference computational engine. FIG. 9 is a drawing of an exemplary T/I threshold table database generation device 900 in accordance with an exemplary embodiment. T/I threshold table database generation device 900 is, e.g., T/I threshold table database generation device 112 of system 100 of FIG. 1 and/or a T/I threshold table database generation device implementing steps of flowchart 200 of FIG. 2 . Exemplary T/I threshold table generation device 900 includes a processor 902 , a network interface 904 , e.g., a wired or optical interface, an assembly of hardware components 906 , e.g., an assembly of circuits, and memory 908 coupled together via a bus 910 over which the various elements may interchange data and information. Network interface 904 includes a receiver 912 and a transmitter 914 coupled to connector 916 , which couples the generation device 900 to: an equipment database, a database device including a database of T/I threshold values, and/or other network nodes and/or the Internet. Memory 908 includes a control routine 918 , an assembly of components 920 and data/information 922 . Control routine 918 includes instructions which when executed by processor 902 control the generation device 900 to implement basic operational functions, e.g., read memory, write to memory, control an interface, load a program, subroutine, or app, etc. Assembly of components 920 , e.g., an assembly of software components, e.g., routines, subroutines, applications, etc., includes, e.g., code which when executed by processor 902 , controls the generation device 900 to implement steps of a method, e.g., steps of the method of flowchart 200 of FIG. 2 which are performed by a T/I threshold table database generation device. Assembly of components 920 includes a TX/RX pairing routine 924 , which identified combinations of pairs of a TX device and a TX device, an actual/mask information extraction routine 926 which extracts an actual PDF of an interfering signal or an approximated mask of an interfering signal for a particular TX device (device type) from the equipment database, and an actual/mask information extraction routine 928 which extracts an actual frequency response of a digital filter or an approximated mask of an frequency response of a digital filter for a particular RX device (device type) from the equipment database. Assembly of components 920 further includes a frequency separation incrementing routine 930 which increments the frequency separation value in steps (e.g., a set of predefined steps) so that a T/I threshold value can be calculated (via convolution) for each frequency separation value, to populate the table for a TX/RX pair. Assembly of components 920 further includes a convolution routine 932 for performing a convolution between: i) an obtained actual PDF of an interfering signal or an obtained approximated mask of an interfering signal for a TX device of a TX/RX pair, and ii) an obtained actual frequency response of a digital filter or an obtained approximated mask of a digital filter for a RX of a TX/RX pair. Data/information 922 includes a generated request 934 for information from an equipment database, an identified TX/RX pair 936 for which a T/I table is to be generated, an obtained actual PDF of an interfering signal or an obtained approximated mask of an interfering signal for a TX device of a TX/RX device pair 938 , and obtained actual frequency response of a digital filter or an obtained approximated mask of a frequency response of a digital filter of a TX device to a TX/RX device pair 940 , a generated T/I threshold table 942 for a 1st Tx/RX pair, a generated T/I threshold table 944 for an Nth TX/RX pair, and generated messages 946 communicating and/or storing each generated T/I threshold table (corresponding to a TX/RX pair) to a database of T/I threshold tables. Various aspects and/or features of some embodiments of the present invention are further discussed below. Various exemplary embodiments are directed to methods and apparatus for the determination of the allowable interference into a digital system to be able to share the band among different users. An exemplary method, in some embodiments includes creating a Threshold to Interference (T/I) ratio table versus frequency separation between the carrier frequency of the interfering signal and the receiver center frequency based on the approach highlighted in TSB-10F bulletin (*TIA Telecommunication Systems Bulletin—Interference Criteria for Microwave Systems TSB-10F, June 1994). In some embodiments, the T/I table is incorporated in a frequency controller, e.g., in a frequency coordination system (e.g., an AFC system), and is obtained by calculating its threshold values a priori based on the approach highlighted in the TIA-1OF bulletin (*TIA Telecommunication Systems Bulletin—Interference Criteria for Microwave Systems TSB-10F, June 1994) There are four cases that are considered to create the T/I table versus frequency separation between the carrier frequency of the interfering signal and the receiver center frequency. The T/I ratio table can be generated by using any one of the following quantities: i) the actual Power Density Function (PDF) of the interfering signal and the actual frequency response of the digital receiver filter;ii) the approximated mask of the PDF of the interfering signal and the actual frequency response mask of the digital receiver filter;iii) the actual PDF of the interfering signal and the approximated frequency response mask of the digital receiver filter; oriv) the approximated mask of the PDF of the interfering signal and the approximated frequency response mask of the digital receiver filter. Benefits of some embodiments of the present invention will now be described. A permanent database of the actual T/I threshold values based on the actual PDF of the interfering signals and of the actual frequency response of the digital receiver filters is obtained. Much more realistic T/I threshold values for a particular frequency separation between the carrier frequency of the interfering signal and the receiver center frequency are obtained using the current invention approach, than are typically obtained used current methods. Higher differences between the mask and the actual frequency response result in better sharing between the two systems. By using the threshold T/I table, this approach, in accordance with the present invention, results in a faster computational time to find the T/I threshold value for a particular frequency separation between the carrier frequency of the interfering signal and the receiver center frequency. Thus, the interference calculation, performed in accordance with present invention, would be precise and time efficient. In one embodiment of the present invention, the AFC system that holds the frequency controller (FC) requests to run an interference analysis between an interfering device and a victim receiver in proximity of each other. The frequency controller requests interference computation from the Interference Computational Engine as follows: The frequency controller passes to the Interference Computational Engine, the equipment type/characteristics of the transmitter interferer and the equipment type/characteristics of the digital receiver. The Interference Computational Engine reads the database table of T/I threshold values with knowledge of the equipment types of the interferer and the digital receiver, the difference in frequency between the interfering signal carrier frequency and the center frequency of the digital receiver filter; and obtains the T/I threshold value from the database tables. Once the threshold T/I value is obtained, it is returned to the Interference Computational Engine to run the interference analysis based on the interfering signal transmit power or EIRP as well as other information, such as the transmission path losses, and inform the frequency controller if there is any interference. To find if there is or there is not interference the following steps are follow: i) Compute the Threshold level of the victim receiver Tc;ii) Compute the Interference level at the receiver victim from the interferer transmitter Ic;iii) From the Table obtain the threshold T/I value and evaluate the margin iv) If the Margin is greater than zero then there is interference. The Interference Computational Engine, in some embodiments, performs the following operations: Runs Interference Analysis by computing Tc/IcObtains T/I value from the database tableComputes the MarginIf Margin is positive then there is interference, and the computation engine informs to the Frequency Controller, e.g., included in the AFC system A Database Table of T/I values will now be described. A T/I table, in a database of T/I tables, includes T/I values (e.g., in dB) versus frequency, where the frequency separation is the separation between the carrier frequency of the interfering signal and the receiver (e.g., digital receiver filter) center frequency. The creation of a T/I Table, in accordance with some embodiments, will now be described. Actual Power Density Function (PDF) of the interfering signal or approximated mask of the PDF of the interfering signal is obtained, e.g., from an equipment database including specification sheets corresponding to a plurality of different vendors of transmitter devices (e.g., potential interfering transmitter devices). Actual frequency response of the digital receiver filter or approximated frequency response mask of the digital receiver filter is obtained, e.g., from an equipment database including specification sheets corresponding to a plurality of different vendors of receiver devices (e.g., potential victim receiver devices). Convolve the PDF of the interfering signal with the receiver filter response using as a reference TSB-10F bulletin (*TIA Telecommunication Systems Bulletin—Interference Criteria for Microwave Systems TSB-10F, June 1994) In another embodiment of the invention, the creation of the T/I Table is done by convolving the PDF or approximated mask of the interfering signal with the actual or approximated frequency response mask of the digital receiver filter following the guideline of the TSB-10F bulletin (*TIA Telecommunication Systems Bulletin—Interference Criteria for Microwave Systems TSB-10F, June 1994). This is done by the frequency controller for every interferer transmitter equipment type and victim receiver equipment type. The approach is as follows: Obtain from the Equipment Database the actual PDF of the interfering signal or the approximated PDF mask.Obtain from the Equipment Database the approximated mask with the actual or approximated frequency response mask of the digital receiver filter.Using a defined convolution as in TSB-10F bulletin (*TIA Telecommunication Systems Bulletin—Interference Criteria for Microwave Systems TSB-10F, June 1994), convolve the PDF of the interfering signal or approximated mask Pt with the actual or approximated frequency response mask of the digital receiver filter Hr Repeat this process for every equipment in the EquipmentDatabase, creating the threshold T/I Table For every new equipment repeat the process by following the steps above. A Database Table of T/I values will now be described. A T/I table, in a database of T/I tables, includes T/I values (e.g., in dB) versus frequency, where the frequency separation is the separation between the carrier frequency of the interfering signal and the receiver (e.g., digital receiver filter) center frequency. In some embodiments, the Creation of the T/I Table includes the following operations: Obtain actual Power Density Function (PDF) of the interfering signal or approximated mask of the PDF of the interfering signal.Obtain actual frequency response of the digital receiver filter or approximated frequency response mask of the digital receiver filter.Convolve the PDF of the interfering signal with the receiver filter response using as a reference TSB-10F bulletin (see TIA Telecommunication Systems Bulletin—Interference Criteria for Microwave Systems TSB-10F, June 1994).Find the T/I Table The PDF and the Federal Communication Commission (FCC) spectrum mask for a couple of incumbent microwave systems* are shown in FIGS. 3 and 4 . The higher the difference between the two, the better the sharing between the two systems becomes. The T/I Table (e.g., T/I Table 500 of FIG. 5 ) is obtained, in some embodiments, as follows: Obtain the PDF of the interfering signal Pt as given in FIG. 3 .Obtain the approximated mask with the actual or approximated frequency response mask of the digital receiver filter Hr. If the frequency response mask of the digital receiver filter is not available, then it could be used the PDF of the desired transmitted signal. In the present example the PDF of the desired transmitted signal is the same as the PDF of the interferer transmitter as given in FIG. 3 .To obtain the T/I table—Convolve the PDF of the interfering signal or approximated mask with the actual or approximated frequency response mask of the digital receiver filter as give in TSB-10F bulletin (see TIA Telecommunication Systems Bulletin—Interference Criteria for Microwave Systems TSB-10F, June 1994) While various components are shown as separate elements in one or more examples, it should be appreciated that the elements can be combined, e.g., to reduce the number of separate devices or components and reducing signaling between the components. For example, one more of all of the interference computational engine 104 , database of T/I threshold tables 106 and/or T/I threshold table database generation device 112 may and sometimes are combined or incorporated into the automatic frequency coordination (AFC) system 102 . Numbered List of Exemplary Method Embodiments Method Embodiment 1. A Method of Controlling Frequency sharing, the method comprising: storing ( 204 ) in a T/I threshold table database a plurality of T/I threshold tables for different transmitter device and receiver device pairs, each T/I threshold table including a plurality of T/I threshold values for an individual transmitter and receiver device pair, each T/I threshold value in a T/I threshold table corresponding to a different amount of frequency separation; receiving ( 206 ), at a frequency coordination system (e.g., automatic frequency coordination (AFC)), a frequency use request seeking permission for a first transmitter device to use a first frequency; identifying ( 208 ), at the frequency coordination system, (e.g., based on location information and/or other stored or received information) a first potential victim device (e.g., a first receiver device) which may be subject to interference from the first transmitter device using the first frequency; retrieving ( 236 ) from a first T/I threshold table stored in said T/I threshold table database, a first T/I threshold value based on a frequency separation between the first frequency used by the first transmitter and a frequency used by the first potential victim device, said first T/I threshold table being a T/I threshold table corresponding to the first transmitter device and the first potential victim device (e.g., a first victim receiver); and making ( 268 ) a decision, at the frequency coordination system, to: i) authorize use of the first frequency by the first transmitter based on the expected transmit power or Effective Isotropic Radiated Power (EIRP) of the first transmitter and the first T/I threshold value or ii) deny use of the first frequency by the first transmitter based on the expected transmit power or Effective Isotropic Radiated Power (EIRP) of the first transmitter and the first T/I threshold value. Method Embodiment 2. The method of Method Embodiment 1, further comprising: communicating ( 274 ) the decision to a device from which said frequency use request was received (e.g., a first base station including the first transmitter device). Method Embodiment 3. The method of Method Embodiment 1, wherein the first frequency is a carrier frequency used by the first transmitter device and where the frequency separation between the first frequency and the frequency used by the first potential victim device (e.g., first receiver) is the frequency difference between first frequency and the center frequency of a digital receiver of the first potential victim device (e.g., first receiver device); and wherein the method further includes determining ( 232 ), by an interference computational engine, the frequency separation between the first frequency and the frequency used by the first potential victim device. Method Embodiment 4. The method of Method Embodiment 3, further comprising: determining ( 256 ), at the interference computational engine, if the first potential victim device will be subject to unacceptable interference based on a first interference margin determined based on transmit or EIRP of the first transmitter device and path loss between the first transmitter device and first potential victim device. Method Embodiment 5. The method of Method Embodiment 4, wherein determining ( 256 ) if the first potential victim device will be subject to unacceptable interference is further based on a first threshold level (Tc) of the first potential victim device (first victim receiver). Method Embodiment 6 . The method of Method Embodiment 5 , further comprising: determining ( 252 ), by the interference computational engine, said first interference margin by subtracting, from the first T/I threshold value, a threshold level (Tc) of the first victim receiver divided by an expected interference level (Ic) (from the interfering transmitter which is the first transmitter) at the first potential victim device (where in some embodiments the first interference margin is determined ( 254 ) as T/I−Tc/Ic). Method Embodiment 7. The method of Method Embodiment 4, wherein making ( 268 ) a decision includes i) authorizing ( 270 ) use of the first frequency by the first transmitter device when the first potential victim device will not be subjected to unacceptable interference and ii) denying ( 272 ) use of the first frequency by the first transmitter device when the first potential victim device will be subjected to unacceptable interference. Method Embodiment 8. The method of Method Embodiment 7, further comprising: prior to receiving, the first frequency use request, computing ( 203 ) T/I threshold tables for multiple different possible transmitter device and receiver device pairs, each of the different possible transmitter device and receive device pairs corresponding to a different transmitter device and receiver device combination, said first T/I threshold table being one of computed T/I threshold tables (e.g., the first T/I threshold table is precomputed to be available to be used for requests). Method Embodiment 8A. The method of Method Embodiment 8, wherein threshold values in said first T/I threshold table are computed ( 2034 ) based on a first actual Power Density Function (PDF) of an interfering signal generated by the first transmitter and an approximated frequency response mask of a digital receiver filter included in the first receiver. Method Embodiment 8B. The method of Method Embodiment 8, wherein threshold values in said first T/I threshold table are computed based ( 2035 ) on an approximated mask of the Power Density Function (PDF) of an interfering signal which is generated by the first transmitter and an actual frequency response of a digital receiver filter included in the first receiver. Method Embodiment 9. The method of Method Embodiment 8, wherein threshold values in said first T/I threshold table are computed ( 2032 ) based on actual known transmitter characteristics of the first transmitter and actual known characteristics of said first receiver prior to said first frequency request being received. Method Embodiment 10. The method of Method Embodiment 9, wherein the actual known transmitter characteristics of the first transmitter include a first actual Power Density Function (PDF) of an interfering signal which is generated by the first transmitter; and wherein the actual known characteristics of said first receiver include a first actual frequency response of a digital receiver filter included in the first receiver. Method Embodiment 11. The method of Method Embodiment 10, wherein computing ( 203 ) T/I threshold tables includes computing ( 2032 ) said first T/I table, and wherein computing ( 2032 ) the first T/I table includes convolving ( 2033 ) the first actual PDF of the interfering signal with the actual frequency response of the digital receiver filter included in the first receiver (e.g., as described in Telecommunications Industry Association (TIA) bulletin TSB-10F (Telecommunication Systems Bulletin—Interference Criteria for Microwave Systems TSB-10F, June 1994)). Numbered List of Exemplary System Embodiments System Embodiment 1. A system ( 100 ) for controlling frequency sharing, the system ( 100 ) comprising: a T/I threshold table database ( 106 or 800 or 824 ) storing a plurality of T/I threshold tables for different transmitter device and receiver device pairs, each T/I threshold table including a plurality of T/I threshold values for an individual transmitter and receiver device pair, each T/I threshold value in a T/I threshold table corresponding to a different amount of frequency separation; a frequency coordination system (e.g., automatic frequency coordination (AFC) system) ( 102 or 600 ) including: an interface receiver ( 612 ); and a first processor (e.g., a frequency controller) ( 103 or 602 ), and wherein said first processor is configured to: operate the frequency coordination system to receive ( 206 ) (via interface receiver 612 ) a frequency use request seeking permission for a first transmitter device ( 134 ) to use a first frequency; identify ( 208 ), at the frequency coordination system, (e.g., based on location information and/or other stored or received information) a first potential victim device (e.g., a first receiver device) ( 136 ) which may be subject to interference from the first transmitter device ( 134 ) using the first frequency; operate the frequency coordination system to retrieve ( 236 ) from a first T/I threshold table ( 108 or 826 ) stored in said T/I threshold table database ( 106 or 800 or 824 ), a first T/I threshold value based on a frequency separation between the first frequency used by the first transmitter ( 134 ) and a frequency used by the first potential victim device ( 136 ), said first T/I threshold table being a T/I threshold table corresponding to the first transmitter device ( 134 ) and the first potential victim device (e.g., a first victim receiver) ( 136 ); and make ( 268 ) a decision, at the frequency coordination system, to: i) authorize use of the first frequency by the first transmitter ( 134 ) based on the expected transmit power or Effective Isotropic Radiated Power (EIRP) of the first transmitter ( 134 ) and the first T/I threshold value or ii) deny use of the first frequency by the first transmitter ( 134 ) based on the expected transmit power or Effective Isotropic Radiated Power (EIRP) of the first transmitter ( 134 ) and the first T/I threshold value. System Embodiment 2. The system ( 100 ) of System Embodiment 1, wherein said frequency coordination system ( 102 or 600 ) further includes an interface transmitter ( 614 ); and wherein said first processor ( 103 or 602 ) is further configured to operate the frequency coordination system to: communicate ( 274 ) (via interface transmitter 614 ) the decision to a device (e.g., first base station 132 ) from which said frequency use request was received (e.g., a first base station ( 132 ) including the first transmitter device ( 134 )). System Embodiment 3. The system ( 100 ) of System Embodiment 1, further comprising: an interference computational engine ( 104 or 700 ) including a second processor ( 702 ); wherein the first frequency is a carrier frequency used by the first transmitter device ( 134 ) and where the frequency separation between the first frequency and the frequency used by the first potential victim device (e.g., first receiver) ( 136 ) is the frequency difference between first frequency and the center frequency of a digital receiver of the first potential victim device (e.g., first receiver device) ( 136 ); and wherein the second processor ( 702 ) is configured to determine ( 232 ) the frequency separation between the first frequency and the frequency used by the first potential victim device ( 136 ). System Embodiment 4. The system ( 100 ) of System Embodiment 3, wherein said second processor ( 702 ) is configured to determine ( 256 ), at the interference computational engine, if the first potential victim device will be subject to unacceptable interference based on a first interference margin determined based on transmit or EIRP of the first transmitter device ( 134 ) and path loss between the first transmitter device ( 134 ) and first potential victim device ( 136 ). System Embodiment 5. The system ( 100 ) of System Embodiment 4, wherein said determining ( 256 ) if the first potential victim device will be subject to unacceptable interference is further based on a first threshold level (Tc) of the first potential victim device (first victim receiver) ( 136 ). System Embodiment 6. The system ( 100 ) of System Embodiment 5, wherein said second processor ( 702 ) is further configured to: determine ( 252 ), by the interference computational engine, said first interference margin by subtracting, from the first T/I threshold value, a threshold level (Tc) of the first victim receiver divided by an expected interference level (Ic) (from the interfering transmitter which is the first transmitter) at the first potential victim device (where in some embodiments the first interference margin is determined ( 254 ) as T/I−Tc/Ic). System Embodiment 7. The system ( 100 ) of System Embodiment 4, wherein said making ( 268 ) a decision includes i) authorizing ( 270 ) use of the first frequency by the first transmitter device when the first potential victim device will not be subjected to unacceptable interference and ii) denying ( 272 ) use of the first frequency by the first transmitter device when the first potential victim device will be subjected to unacceptable interference. System Embodiment 8. The system ( 100 ) of System Embodiment 7, further comprising: a T/I threshold table generation device ( 112 or 900 ) including a third processor ( 902 ) configured to: compute ( 203 ) T/I threshold tables for multiple different possible transmitter device and receiver device pairs, each of the different possible transmitter device and receive device pairs corresponding to a different transmitter device and receiver device combination, said first T/I threshold table being one of computed T/I threshold tables (e.g., the first T/I threshold table is precomputed to be available to be used for requests), said computing of T/I threshold tables being performed prior to said frequency coordination system receiving the first frequency use request. System Embodiment 8A. The system ( 100 ) of System Embodiment 8, wherein threshold values in said first T/I threshold table are computed ( 2034 ) based on a first actual Power Density Function (PDF) of an interfering signal generated by the first transmitter and an approximated frequency response mask of a digital receiver filter included in the first receiver. System Embodiment 8B. The system ( 100 ) of System Embodiment 8, wherein threshold values in said first T/I threshold table are computed based ( 2035 ) on an approximated mask of the Power Density Function (PDF) of an interfering signal which is generated by the first transmitter and an actual frequency response of a digital receiver filter included in the first receiver. System Embodiment 9. The system ( 100 ) of System Embodiment 8, wherein threshold values in said first T/I threshold table are computed ( 2032 ) based on actual known transmitter characteristics of the first transmitter and actual known characteristics of said first receiver prior to said first frequency request being received. System Embodiment 10. The system ( 100 ) of System Embodiment 9, wherein the actual known transmitter characteristics of the first transmitter ( 134 ) include a first actual Power Density Function (PDF) of an interfering signal which is generated by the first transmitter; and wherein the actual known characteristics of said first receiver ( 136 ) include a first actual frequency response of a digital receiver filter included in the first receiver. System Embodiment 11. The system ( 100 ) of System Embodiment 10, wherein said third processor ( 902 ) is configured to: compute ( 2032 ) said first T/I table, as part of being configured to compute ( 203 ) T/I threshold tables, and wherein said third processor ( 902 ) is configured to: convolve ( 2033 ) the first actual PDF of the interfering signal with the actual frequency response of the digital receiver filter included in the first receiver (e.g., as described in Telecommunications Industry Association (TIA) bulletin TSB-10F (Telecommunication Systems Bulletin—Interference Criteria for Microwave Systems TSB-10F, June 1994)), as part of being configured to compute ( 2032 ) the first T/I table. Numbered List of Exemplary Non-Transitory Computer Readable Medium Embodiments Non-Transitory Computer Readable Medium Embodiment 1. A non-transitory computer readable medium ( 608 ) including machine readable instructions, which when executed by a processor ( 103 or 602 ) of a frequency coordination system ( 102 or 600 ) (e.g., an AFC system) control the frequency coordination system ( 102 or 600 ) to perform the steps of: receiving ( 206 ), at a frequency coordination system (e.g., automatic frequency coordination (AFC)), a frequency use request seeking permission for a first transmitter device to use a first frequency; identifying ( 208 ), at the frequency coordination system, (e.g., based on location information and/or other stored or received information) a first potential victim device (e.g., a first receiver device) which may be subject to interference from the first transmitter device using the first frequency; retrieving ( 236 ) from a first T/I threshold table stored in a T/I threshold table database, a first T/I threshold value based on a frequency separation between the first frequency used by the first transmitter and a frequency used by the first potential victim device, said first T/I threshold table being a T/I threshold table corresponding to the first transmitter device and the first potential victim device (e.g., a first victim receiver); and making ( 268 ) a decision, at the frequency coordination system, to: i) authorize use of the first frequency by the first transmitter based on the expected transmit power or Effective Isotropic Radiated Power (EIRP) of the first transmitter and the first T/I threshold value or ii) deny use of the first frequency by the first transmitter based on the expected transmit power or Effective Isotropic Radiated Power (EIRP) of the first transmitter and the first T/I threshold value. Non-Transitory Computer Readable Medium Embodiment 2. A non-transitory computer readable medium ( 908 ) including machine readable instructions, which when executed by a processor ( 902 ) of a T/I threshold table generation device ( 112 or 900 ) (e.g., control the T/I threshold table generation device ( 112 or 900 ) to perform the step of: storing ( 204 ) in a T/I threshold table database ( 106 or 800 or 824 ) a plurality of T/I threshold tables for different transmitter device and receiver device pairs, each T/I threshold table including a plurality of T/I threshold values for an individual transmitter and receiver device pair, each T/I threshold value in a T/I threshold table corresponding to a different amount of frequency separation. Non-Transitory Computer Readable Medium Embodiment 3. The non-transitory computer readable medium ( 908 ) of non-transitory computer readable medium embodiment 2, further comprising: machine readable instructions, which when executed by a processor ( 902 ) of the T/I threshold table generation device ( 112 or 900 ) (e.g., control the T/I threshold table generation device ( 112 or 900 ) to perform the additional step of: computing ( 203 ) T/I threshold tables for multiple different possible transmitter device and receiver device pairs, each of the different possible transmitter device and receive device pairs corresponding to a different transmitter device and receiver device combination, said first T/I threshold table being one of computed T/I threshold tables (e.g., the first T/I threshold table is precomputed to be available to be used for requests) (said computing of T/I threshold tables being performed prior to a frequency coordination system receiving the a first frequency use request). Non-Transitory Computer Readable Medium Embodiment 4. A non-transitory computer readable medium ( 808 ) including machine readable instructions, which when executed by a processor ( 802 ) of a T/I threshold tables database device ( 106 or 800 ) control the T/I threshold tables database device ( 106 or 800 ) to perform the step of: storing ( 204 ) in a T/I threshold table database ( 824 ) a plurality of T/I threshold tables for different transmitter device and receiver device pairs, each T/I threshold table including a plurality of T/I threshold values for an individual transmitter and receiver device pair, each T/I threshold value in a T/I threshold table corresponding to a different amount of frequency separation. The techniques of various embodiments may be implemented using software, hardware and/or a combination of software and hardware. Various embodiments are directed to apparatus, e.g., an AFC system or components of an AFC system, user equipment devices, wireless devices, mobile devices, smartphones, subscriber devices, desktop computers, printers, IPTV, laptops, tablets, network edge devices, Access Points, wireless routers, switches, WLAN controllers, orchestration servers, orchestrators, Gateways, AAA servers, servers, nodes and/or elements. Various embodiments are also directed to methods, e.g., method of controlling and/or operating user equipment devices, wireless devices, mobile devices, smartphones, subscriber devices, desktop computers, printers, IPTV, laptops, tablets, network edge devices, Access Points, wireless routers, switches, WLAN controllers, orchestration servers, orchestrators, Gateways, AAA servers, servers, nodes and/or elements. Various embodiments are also directed to machine, e.g., computer, readable medium, e.g., ROM, RAM, CDs, hard discs, etc., which include machine readable instructions for controlling a machine to implement one or more steps of a method. The computer readable medium is, e.g., non-transitory computer readable medium. It is understood that the specific order or hierarchy of steps in the processes and methods disclosed is an example of exemplary approaches. Based upon design preferences, it is understood that the specific order or hierarchy of steps in the processes and methods may be rearranged while remaining within the scope of the present disclosure. The accompanying method claims present elements of the various steps in a sample order, and are not meant to be limited to the specific order or hierarchy presented. In some embodiments, one or more processors are used to carry out one or more steps of each of the described methods. In various embodiments each of the steps or elements of a method are implemented using one or more processors. In some embodiments, each of elements or steps are implemented using hardware circuitry. In various embodiments devices, e.g., user equipment devices, wireless devices, mobile devices, smartphones, subscriber devices, desktop computers, printers, IPTV, laptops, tablets, network edge devices, Access Points, wireless routers, switches, WLAN controllers, orchestration servers, orchestrators, Gateways, AAA servers, servers, nodes and/or elements described herein are implemented using one or more components to perform the steps corresponding to one or more methods, for example, provisioning user equipment devices, provisioning AP devices, provisioning AAA servers, provisioning orchestration servers, generating messages, message reception, message transmission, signal processing, sending, comparing, determining and/or transmission steps. Thus, in some embodiments various features are implemented using components or in some embodiments logic such as for example logic circuits. Such components may be implemented using software, hardware or a combination of software and hardware. Many of the above described methods or method steps can be implemented using machine executable instructions, such as software, included in a machine readable medium such as a memory device, e.g., RAM, floppy disk, etc. to control a machine, e.g., general purpose computer with or without additional hardware, to implement all or portions of the above described methods, e.g., in one or more devices, servers, nodes and/or elements. Accordingly, among other things, various embodiments are directed to a machine-readable medium, e.g., a non-transitory computer readable medium, including machine executable instructions for causing a machine, e.g., processor and associated hardware, to perform one or more of the steps of the above-described method(s). Some embodiments are directed to a device, e.g., a controller, including a processor configured to implement one, multiple or all of the steps of one or more methods of the invention. In some embodiments, the processor or processors, e.g., CPUs, of one or more devices, e.g., user equipment devices, wireless devices, mobile devices, smartphones, subscriber devices, desktop computers, printers, IPTV, laptops, tablets, network edge devices, Access Points, wireless routers, switches, WLAN controllers, orchestration servers, orchestrators, Gateways, AAA servers, servers, nodes and/or elements, are configured to perform the steps of the methods described as being performed by the user equipment devices, wireless devices, mobile devices, smartphones, subscriber devices, desktop computers, printers, IPTV, laptops, tablets, network edge devices, Access Points, wireless routers, switches, WLAN controllers, orchestration servers, orchestrators, Gateways, AAA servers, servers, nodes and/or elements. The configuration of the processor may be achieved by using one or more components, e.g., software components, to control processor configuration and/or by including hardware in the processor, e.g., hardware components, to perform the recited steps and/or control processor configuration. Accordingly, some but not all embodiments are directed to a device, e.g., user equipment devices, wireless devices, mobile devices, smartphones, subscriber devices, desktop computers, printers, IPTV, laptops, tablets, network edge devices, Access Points, wireless routers, switches, WLAN controllers, orchestration servers, orchestrators, Gateways, AAA servers, servers, nodes and/or elements, with a processor which includes a component corresponding to each of the steps of the various described methods performed by the device in which the processor is included. In some but not all embodiments a device, e.g., user equipment devices, wireless devices, mobile devices, smartphones, subscriber devices, desktop computers, printers, IPTV, laptops, tablets, network edge devices, Access Points, wireless routers, switches, WLAN controllers, orchestration servers, orchestrators, Gateways, AAA servers, servers, nodes and/or elements, includes a controller corresponding to each of the steps of the various described methods performed by the device in which the processor is included. The components may be implemented using software and/or hardware. Some embodiments are directed to a computer program product comprising a computer-readable medium, e.g., a non-transitory computer-readable medium, comprising code for causing a computer, or multiple computers, to implement various functions, steps, acts and/or operations, e.g., one or more steps described above. Depending on the embodiment, the computer program product can, and sometimes does, include different code for each step to be performed. Thus, the computer program product may, and sometimes does, include code for each individual step of a method, e.g., a method of controlling a device, e.g., user equipment devices, wireless devices, mobile devices, smartphones, subscriber devices, desktop computers, printers, IPTV, laptops, tablets, network edge devices, Access Points, wireless routers, switches, WLAN controllers, orchestration servers, orchestrators, Gateways, AAA servers, servers, nodes and/or elements. The code may be in the form of machine, e.g., computer, executable instructions stored on a computer-readable medium, e.g., a non-transitory computer-readable medium, such as a RAM (Random Access Memory), ROM (Read Only Memory) or other type of storage device. In addition to being directed to a computer program product, some embodiments are directed to a processor configured to implement one or more of the various functions, steps, acts and/or operations of one or more methods described above. Accordingly, some embodiments are directed to a processor, e.g., CPU, configured to implement some or all of the steps of the methods described herein. The processor may be for use in, e.g., a communications device such as a user equipment device, wireless device, mobile device, smartphone, subscriber device, desktop computer, printer, IPTV, laptop, tablets, network edge device, Access Point, wireless router, switch, WLAN controller, orchestration server, orchestrator, Gateway, AAA server, server, node and/or element or other device described in the present application. Numerous additional variations on the methods and apparatus of the various embodiments described above will be apparent to those skilled in the art in view of the above description. Such variations are to be considered within the scope. Numerous additional embodiments, within the scope of the present invention, will be apparent to those of ordinary skill in the art in view of the above description and the claims which follow. Such variations are to be considered within the scope of the invention.",en,PATENT_APPLICATION
180-928-613-370-780,US,20240388461,A1,2024-11-21,US_20240388461_A1_20241121,en,US,20240388461,A1,2024-11-21,US,18785749,2024-07-26,Merging A Call With A Video-Enabled Virtual Meeting,en,US,"Zoom Video Communications, Inc.","San Jose, CA",US,Vi Dinh Chau,"Seattle, WA",US,1,Haibing Xu,"San Jose, CA",H04L12/18,I,F,G06F21/31,I,L,H04L12/1822,I,F,G06F21/31,I,L,H04L12/1818,I,L,US,20240388461,A1,2024-11-21,180-928-613-370-780,1,US,20240388461,A1,2024-11-21,180-928-613-370-780,1,UNKNOWN,"A call is merged with a virtual meeting to allow an audio-only caller to join the virtual meeting while bypassing one or more security checks configured for the virtual meeting. After the virtual meeting is initiated, a call is established between a phone device of the audio-only caller and a customer endpoint. A request is received from the customer endpoint to join the phone device with a virtual meeting. A channel is opened between the phone device and a web service associated with the virtual meeting. The phone device is then joined to the virtual meeting over the channel. To facilitate a seamless transition from the call to the virtual meeting, the call may be maintained as an audio channel of the virtual meeting for the audio-only caller.",en,"1 . A method, comprising: opening a channel between a device of an audio-only caller and a server facilitating an in-progress video-enabled virtual meeting; merging a call between the device and a client device with the video-enabled virtual meeting over the channel to join the device to the video-enabled virtual meeting, without one or more security checks, wherein the client device is disconnected from the call in response to the device joining the video-enabled virtual meeting; and creating a dedicated meeting view for the audio-only caller in the video-enabled virtual meeting.","2 . The method of claim 1 , comprising: obtaining virtual meeting information associated with the video-enabled virtual meeting that corresponds to calendar information accessible by the client device via a calendar associated with a participant of the video-enabled virtual meeting.","3 . The method of claim 1 , wherein the one or more security checks that are bypassed include operations for authenticating participants of the video-enabled virtual meeting.","4 . The method of claim 1 , comprising: based on a client application being installed on the device, causing the channel between the device and the server to open using the client application.","5 . The method of claim 1 , comprising: determining that a client application is installed on the device; and transmitting, to the device, a push notification configured to cause the client application to launch.","6 . The method of claim 1 , wherein a user of the client device is an assistant to a participant of the video-enabled virtual meeting.","7 . The method of claim 1 , further comprising: receiving a request from the client device to transfer the call to the video-enabled virtual meeting.","8 . The method of claim 1 , wherein the call is maintained as an audio channel of the video-enabled virtual meeting for the audio-only caller based on a client application not being installed on the device.","9 . A system, comprising: a server that facilitates a call established between a device of an audio-only caller and a client device and that facilitates an in-progress video-enabled virtual meeting, wherein the server runs software to: open a channel between the device and the server; merge the call and the video-enabled virtual meeting over the channel to join the device to the video-enabled virtual meeting, without one or more security checks, wherein the client device is disconnected from the call in response to the device joining the video-enabled virtual meeting; and create a dedicated meeting view for the audio-only caller in the video-enabled virtual meeting","10 . The system of claim 9 , wherein the software is configured to: obtain virtual meeting information associated with the video-enabled virtual meeting that corresponds to calendar information accessible by the client device via a calendar associated with a participant of the video-enabled virtual meeting.","11 . The system of claim 9 , wherein the one or more security checks that are bypassed include operations to authenticate participants of the video-enabled virtual meeting.","12 . The system of claim 9 , wherein the software is configured to: based on a client application being installed on the device, cause the channel between the device and the server to open using the client application.","13 . The system of claim 9 , wherein the software is configured to: receive a request from the client device to transfer the call to the video-enabled virtual meeting.","14 . A server device, comprising: a memory; and a processor configured to execute instructions stored in the memory to: open a channel between a device of an audio-only caller and the server device that is facilitating an in-progress video-enabled virtual meeting; merge a call between the device and a client device with the video-enabled virtual meeting over the channel to join the device to the video-enabled virtual meeting, without one or more security checks, wherein the client device is disconnected from the call in response to the device joining the video-enabled virtual meeting; and create a dedicated meeting view for the device in the video-enabled virtual meeting.","15 . The server device of claim 14 , wherein the one or more security checks that are bypassed include operations for authenticating participants of the video-enabled virtual meeting.","16 . The server device of claim 14 , wherein a user of the client device is an assistant to a participant of the video-enabled virtual meeting, and wherein the virtual meeting information corresponds to calendar information accessible by the user of the client device via a calendar for the participant.","17 . The server device of claim 14 , wherein the processor is configured to execute the instructions to: receive a request from the client device to transfer the call to the video-enabled virtual meeting.","18 . The server device of claim 14 , wherein the call is maintained as an audio channel of the video-enabled virtual meeting for the device based on a client application not being installed on the device.","19 . The server device of claim 14 , wherein the processor is configured to execute the instructions to: based on a client application being installed on the device, cause the channel to open using the client application.","20 . The server device of claim 14 , wherein the processor is configured to execute the instructions to: based on a client application being installed on the device, transmitting, to the device, a push notification configured to cause the client application to launch.",en,"CROSS-REFERENCE TO RELATED APPLICATION(S) This application is a continuation of U.S. patent application Ser. No. 17/162,879, filed Jan. 29, 2021, the entire disclosure of which is hereby incorporated by reference. BACKGROUND Virtual meetings help people all around the world to connect with one another every day in a variety of business and personal settings. A virtual meeting may be video-enabled to allow participants to see each other in real-time and may also accommodate participants interacting with others through audio alone. Virtual meeting platforms use network connections with participant devices to facilitate audio and/or video communications between participants. The growing ubiquity of network-connected devices enables more and more people to communicate over virtual meetings every day. SUMMARY Disclosed herein are, inter alia, implementations of systems and techniques for merging a call with a virtual meeting. One aspect of this disclosure is a method, which includes establishing a call between a phone device of an audio-only caller and a customer endpoint. A request is received from the customer endpoint to join the phone device with a virtual meeting. A channel is opened between the phone device and a web service associated with the virtual meeting. The phone device is then joined to the virtual meeting over the channel. Another aspect of this disclosure is a system, which includes a first server and a second server. The first server facilitates a call between a phone device of an audio-only caller and a customer endpoint. The second server runs software to receive a request from the customer endpoint to join the phone device with a virtual meeting, open a channel between the phone device and a web service associated with the virtual meeting, and join the phone device to the virtual meeting over the channel. Yet another aspect of this disclosure is a server device, which includes a memory and a processor. The processor is configured to execute instructions stored in the memory. The instructions include instructions to initialize a virtual meeting between two or more participants, and, responsive to a request from a customer endpoint to join a phone device of an audio-only caller connected on a call with the virtual meeting, join the phone device to the virtual meeting over a channel opened between the phone device and a web service associated with the virtual meeting, in which the joining of the phone device bypasses one or more security checks configured for the virtual meeting. BRIEF DESCRIPTION OF THE DRAWINGS This disclosure is best understood from the following detailed description when read in conjunction with the accompanying drawings. It is emphasized that, according to common practice, the various features of the drawings are not to-scale. On the contrary, the dimensions of the various features are arbitrarily expanded or reduced for clarity. FIG. 1 is a block diagram of an example of an electronic computing and communications system. FIG. 2 is a block diagram of an example internal configuration of a computing device of an electronic computing and communications system. FIG. 3 is a block diagram of an example of a meeting system for delivering virtual meeting software services in an electronic computing and communications system. FIG. 4 is a block diagram of an example of a system for merging a call with a virtual meeting. FIG. 5 is a block diagram of example functionality of merging software. FIG. 6 is an illustration of swim lanes showing an example sequence of operations performed for merging a call with a virtual meeting. FIG. 7 is a flowchart of a first example of a technique for merging a call with a virtual meeting. FIG. 8 is a flowchart of a second example of a technique for merging a call with a virtual meeting. DETAILED DESCRIPTION A virtual meeting platform may provide many different ways for its users to connect to one another. For example, users can use a virtual meeting to connect over video, or they can connect over an audio-only call, for example, using telephony services. Sometimes, a caller may contact a user of the virtual meeting platform over a phone call and ask or be asked to join a virtual meeting with that user and/or another user of the virtual meeting platform. However, it is generally not easy for a caller connecting over a telephony service to join a virtual meeting even if the caller has a client application used for the virtual meeting service installed on their phone. This may, for example be because the caller in such a case still needs to switch from a telephone application to the client application and thereafter enter a meeting number to join the virtual meeting. One solution may involve the virtual meeting service switching the caller from telephony audio to voice over IP (VOIP) audio; however, that switching may take some amount of time to complete, during which the caller misses a portion of the conversation. That solution thus does not improve the user experience. Implementations of this disclosure address problems such as these by merging a call with a virtual meeting to allow an audio-only caller to join the virtual meeting while bypassing one or more security checks configured for the virtual meeting. After the virtual meeting is initiated, a call is established between a phone device of the audio-only caller and a customer endpoint. A request is received from the customer endpoint to join the phone device with a virtual meeting. A channel is opened between the phone device and a web service associated with the virtual meeting. The phone device is then joined to the virtual meeting over the channel. To facilitate a seamless transition from the call to the virtual meeting, the call may be maintained as an audio channel of the virtual meeting for the audio-only caller. In some cases, the virtual meeting is already in-progress when the request is received from the customer endpoint. In some cases, the virtual meeting is initiated responsive to that request. To describe some implementations in greater detail, reference is first made to examples of hardware and software structures used to merge a call with a virtual meeting. FIG. 1 is a block diagram of an example of an electronic computing and communications system 100 , which can be or include a distributed computing system (e.g., a client-server computing system), a cloud computing system, a clustered computing system, or the like. The system 100 connects various clients 102 and/or phones 104 to services implemented within or otherwise using a datacenter 106 . The system 100 can connect a number of clients 102 and/or phones 104 or can have a configuration of clients or phones different from that generally illustrated in FIG. 1 . For example, and without limitation, the system 100 can connect hundreds or thousands of clients and/or phones. A client 102 may be or otherwise refer to one or both of a client device or a client application. Where a client is or refers to a client device, the client can comprise a computing system, which can include one or more computing devices, such as a mobile phone, a tablet computer, a laptop computer, a notebook computer, a desktop computer, or another suitable computing device or combination of computing devices. Where a client instead is or refers to a client application, the client can be an instance of software running on a device. In some implementations, a client can be implemented as a single physical unit or as a combination of physical units. In some implementations, a single physical unit can include multiple clients. A phone 104 may be or otherwise refer to one or both of a phone device or a phone application such as a softphone. For example, a phone 104 may be a smart phone or other cell phone which may or may not be configured to run mobile applications, such as a client 102 . In another example, a phone 104 may be a desk phone, such as a desktop unit configured to at least send and receive calls and includes an input device for receiving a telephone number or extension to dial to and an output device for outputting audio and/or video for a call in progress. In yet another example, the phone 104 may be a softphone representing telephony functionality of a client 102 . A phone 104 may or may not be voice over IP (VOIP)-enabled. The datacenter 106 includes one or more servers. The datacenter 106 can represent a geographic location, which can include a facility, where the one or more servers are located. The system 100 can include a number of datacenters and servers or can include a configuration of datacenters and servers different from that generally illustrated in FIG. 1 . For example, and without limitation, the system 100 can include tens of datacenters, and at least some of the datacenters can include hundreds or another suitable number of servers. The datacenter 106 includes servers used for implementing software services. The datacenter 106 as generally illustrated includes an application server 108 , a database server 110 , and a telephony server 112 . The servers 108 through 112 can each be a computing system, which can include one or more computing devices, such as a desktop computer, a server computer, or another computer capable of operating as a server, or a combination thereof. A suitable number of each of the servers 108 through 112 can be implemented at the datacenter 106 . In some implementations, one or more of the servers 108 112 can be a non-hardware aspect implemented on a physical device, such as a hardware server. In some implementations, a combination of two or more of the application server 108 , the database server 110 , and the telephony server 112 can be implemented as a single hardware server or as a single non-hardware server implemented on a single hardware server. In some implementations, the datacenter 106 can include servers other than or in addition to the servers 108 through 112 , for example, a media server, a proxy server, or a web server. The application server 108 runs web-based software services deliverable to the clients 102 and at least partially to the phones 104 . The software services may be or include virtual meeting software which enables audio, video, and/or other forms of virtual meetings between multiple devices (e.g., between ones of the clients 102 , between ones of the phones 104 , or between ones of the clients 102 and ones of the phones 104 ), such as to facilitate a conference between the users of those devices. The virtual meeting software can include functionality for hosting, presenting scheduling, joining, or otherwise participating in a virtual meeting. The virtual meeting software may further include functionality for recording some or all of a virtual meeting and/or documenting a transcript for the virtual meeting. The application server 108 may, for example, be or include a unitary Java Virtual Machine (JVM). In some implementations, the application server 108 can include an application node, which can be a process executed on the application server 108 . For example, and without limitation, the application node can be executed in order to deliver software services to a client 102 as part of a software application. The application node can be implemented using processing threads, virtual machine instantiations, or other computing features of the application server 108 . In some such implementations, the application server 108 can include a suitable number of application nodes, depending upon a system load or other characteristics associated with the application server 108 . For example, and without limitation, the application server 108 can include two or more nodes forming a node cluster. In some such implementations, the application nodes implemented on a single application server 108 can run on different hardware servers. The database server 110 stores, manages, or otherwise provides data for delivering software services of the application server 108 to a client 102 . In particular, the database server 110 may implement one or more databases, tables, or other information sources suitable for use with a software application implemented using the application server 108 . The database server 110 may include a data storage unit accessible by software executed on the application server 108 . A database implemented by the database server 110 may be a relational database management system (RDBMS), an object database, an XML database, a configuration management database (CMDB), a management information base (MIB), one or more flat files, other suitable non-transient storage mechanisms, or a combination thereof. The system 100 can include one or more database servers, in which each database server can include one, two, three, or another suitable number of databases configured as or comprising a suitable database type or combination thereof. In some implementations, one or more databases, tables, other suitable information sources, or portions or combinations thereof may be stored, managed, or otherwise provided by one or more of the elements of the system 100 other than the database server 110 , for example, the client 104 or the application server 108 . The telephony server 112 enables network-based telephony and web communications from and to ones of the clients 102 and ones of the phones 104 which are VOIP-enabled devices configured to send and receive calls over a network, for example, a network 114 . In particular, the telephony server 112 includes a session initiation protocol (SIP) zone and a web zone. The SIP zone enables a client 102 or a VOIP-enabled phone 104 , to send and receive calls over the network 114 using SIP requests and responses. The web zone integrates telephony data with the application server 108 to enable telephony-based traffic access to software services run by the application server 108 . Given the combined functionality of the SIP zone and the web zone, the telephony server 112 may be or include a cloud-based private branch exchange (PBX) system. The SIP zone receives telephony traffic from a client 102 or VOIP-enabled phone 104 and directs same to a destination device. The SIP zone may include one or more call switches for routing the telephony traffic. For example, to route a VOIP call from a first VOIP-enabled client to a second VOIP-enabled client within the same domain or network, the telephony server 112 may initiate a SIP transaction between a first client and the second client using a PBX. However, in another example, to route a VOIP call from a VOIP-enabled client to a client or phone which is not VOIP-enabled, the telephony server 112 may initiate a SIP transaction via a VOIP gateway that transmits the SIP signal to a public switched telephone network (PSTN) system for outbound communication to the non-VOIP-enabled client or non-client phone. Hence, the telephony server 112 may include a PSTN system and may in some cases access an external PSTN system. The telephony server 112 includes one or more session border controllers (SBCs) for interfacing the SIP zone with one or more aspects external to the telephony server 112 . In particular, an SBC can act as an intermediary to transmit and receive SIP requests and responses between ones of the clients 102 and/or between ones of the phones 104 . When incoming telephony traffic for delivery to a client 102 or a phone 104 originating from outside the telephony server 112 is received, a SBC receives the traffic and forwards it to a call switch for routing to the client 102 or the phone 104 . The web zone receives telephony traffic from a client 102 or a phone 104 , via the SIP zone, and directs same to the application server 108 via one or more Domain Name System (DNS) resolutions. For example, a first DNS within the web zone may process a request received via the SIP zone and then deliver the processed request to a web service which connects to a second DNS at or otherwise associated with the application server 108 . Once the second DNS resolves the request, it is delivered to the destination service at the application server 108 . The web zone may also include a database for authenticating access to a software application for telephony traffic processed within the SIP zone, for example, a softphone. The clients 102 and the phones 104 communicate with aspects of the datacenter 106 via the network 114 . The network 114 can be or include, for example, the Internet, a local area network (LAN), a wide area network (WAN), a virtual private network (VPN), or another public or private means of electronic computer communication capable of transferring data between a client and one or more servers. In some implementations, a client can connect to the network 114 via a communal connection point, link, or path, or using a distinct connection point, link, or path. For example, a connection point, link, or path can be wired, wireless, use other communications technologies, or a combination thereof. In some implementations in which one or more of the phones 104 is not a VOIP-enabled device, those one or more phones 104 may communicate other than via the network 114 . The network 114 , the datacenter 106 , or another element, or combination of elements, of the system 100 can include network hardware such as routers, switches, other network devices, or combinations thereof. For example, the datacenter 106 can include a load balancer 116 for routing traffic from the network 114 to various servers associated with the datacenter 106 . The load balancer 116 can route, or direct, computing communications traffic, such as signals or messages, to respective elements of the datacenter 106 . For example, the load balancer 116 can operate as a proxy, or reverse proxy, for a service, such as a service provided to one or more remote clients, such as one or more of the clients 102 , by the application server 108 , and/or another server. Routing functions of the load balancer 116 can be configured directly or via a DNS. The load balancer 116 can coordinate requests from remote clients and can simplify client access by masking the internal configuration of the datacenter 106 from the remote clients. In some implementations, the load balancer 116 can operate as a firewall, allowing or preventing communications based on configuration settings. Although the load balancer 116 is depicted in FIG. 1 as being within the datacenter 106 , in some implementations, the load balancer 116 can instead be located outside of the datacenter 106 , for example, when providing global routing for multiple datacenters. In some implementations, load balancers can be included both within and outside of the datacenter 106 . In some implementations, the load balancer 116 can be omitted. FIG. 2 is a block diagram of an example internal configuration of a computing device 200 of an electronic computing and communications system, for example, a computing device which implements one or more of the client 104 , the application server 108 , the database server 110 , or the gateway 112 of the system 100 shown in FIG. 1 . The computing device 200 includes components or units, such as a processor 202 , a memory 204 , a bus 206 , a power source 208 , peripherals 210 , a user interface 212 , a network interface 214 , other suitable components, or a combination thereof. One or more of the memory 204 , the power source 208 , the peripherals 210 , the user interface 212 , or the network interface 214 can communicate with the processor 202 via the bus 206 . The processor 202 is a central processing unit, such as a microprocessor, and can include single or multiple processors having single or multiple processing cores. Alternatively, the processor 202 can include another type of device, or multiple devices, now existing or hereafter developed, configured for manipulating or processing information. For example, the processor 202 can include multiple processors interconnected in one or more manners, including hardwired or networked, including wirelessly networked. For example, the operations of the processor 202 can be distributed across multiple devices or units that can be coupled directly or across a local area or other suitable type of network. The processor 202 can include a cache, or cache memory, for local storage of operating data or instructions. The memory 204 includes one or more memory components, which may each be volatile memory or non-volatile memory. For example, the volatile memory of the memory 204 can be random access memory (RAM) (e.g., a DRAM module, such as DDR SDRAM) or another form of volatile memory. In another example, the non-volatile memory of the memory 204 can be a disk drive, a solid state drive, flash memory, phase-change memory, or another form of non-volatile memory configured for persistent electronic information storage. The memory 204 may also include other types of devices, now existing or hereafter developed, configured for storing data or instructions for processing by the processor 202 . In some implementations, the memory 204 can be distributed across multiple devices. For example, the memory 204 can include network-based memory or memory in multiple clients or servers performing the operations of those multiple devices. The memory 204 can include data for immediate access by the processor 202 . For example, the memory 204 can include executable instructions 216 , application data 218 , and an operating system 220 . The executable instructions 216 can include one or more application programs, which can be loaded or copied, in whole or in part, from non-volatile memory to volatile memory to be executed by the processor 202 . For example, the executable instructions 216 can include instructions for performing some or all of the techniques of this disclosure. The application data 218 can include user data, database data (e.g., database catalogs or dictionaries), or the like. In some implementations, the application data 218 can include functional programs, such as a web browser, a web server, a database server, another program, or a combination thereof. The operating system 220 can be, for example, Microsoft Windows®, Mac OS X®, or Linux®; an operating system for a mobile device, such as a smartphone or tablet device; or an operating system for a non-mobile device, such as a mainframe computer. The power source 208 includes a source for providing power to the computing device 200 . For example, the power source 208 can be an interface to an external power distribution system. In another example, the power source 208 can be a battery, such as where the computing device 200 is a mobile device or is otherwise configured to operate independently of an external power distribution system. In some implementations, the computing device 200 may include or otherwise use multiple power sources. In some such implementations, the power source 208 can be a backup battery. The peripherals 210 includes one or more sensors, detectors, or other devices configured for monitoring the computing device 200 or the environment around the computing device 200 . For example, the peripherals 210 can include a geolocation component, such as a global positioning system location unit. In another example, the peripherals can include a temperature sensor for measuring temperatures of components of the computing device 200 , such as the processor 202 . In some implementations, the computing device 200 can omit the peripherals 210 . The user interface 212 includes one or more input interfaces and/or output interfaces. An input interface may, for example, be a positional input device, such as a mouse, touchpad, touchscreen, or the like; a keyboard; or another suitable human or machine interface device. An output interface may, for example, be a display, such as a liquid crystal display, a cathode-ray tube, a light emitting diode display, or other suitable display. The network interface 214 provides a connection or link to a network (e.g., the network 114 shown in FIG. 1 ). The network interface 214 can be a wired network interface or a wireless network interface. The computing device 200 can communicate with other devices via the network interface 214 using one or more network protocols, such as using Ethernet, transmission control protocol (TCP), internet protocol (IP), power line communication, an IEEE 802.X protocol (e.g., Wi-Fi, Bluetooth, ZigBee, etc.), infrared, visible light, general packet radio service (GPRS), global system for mobile communications (GSM), code-division multiple access (CDMA), Z-Wave, another protocol, or a combination thereof. FIG. 3 is a block diagram of an example of a meeting system 300 for delivering virtual meeting software services in an electronic computing and communications system, for example, the system 100 shown in FIG. 1 . The meeting system 300 includes a thread encoding tool 302 , a switching/routing tool 304 , and virtual meeting software 306 . The meeting system 300 enables use of the virtual meeting software 306 by clients and phones, such as clients 308 and 310 and phone 312 . For example, one or both of the clients 308 or 310 may be a client 102 shown in FIG. 1 . In another example, the phone 312 may be a phone 104 shown in FIG. 1 . The meeting system 300 may in at least some cases be implemented using one or more servers of the system 100 . Although two clients and a phone are shown in FIG. 3 , other numbers of clients and/or other numbers of phones can connect to the meeting system 300 . A virtual meeting includes transmitting and receiving video, audio, and/or other data between clients and/or phones of virtual meeting participants. Each of the client 308 , the client 310 , and the phone 312 may connect through the meeting system 300 using separate input streams to enable users thereof to participate in a virtual meeting together using the virtual meeting software. The virtual meeting software 306 is software for implementing virtual meetings between users of two or more clients and/or phones. For example, the virtual meting software 306 can be the virtual meeting software described above with respect to the application server 108 of FIG. 1 or other virtual meeting software. The virtual meeting software 306 includes a dedicated meeting view for each input stream received and processed at the meeting system 300 . For example, a meeting view may be represented within a graphical user interface (GUI) of the virtual meeting software 306 by a dedicated box for a given participant. The content of the meeting view for a given participant may be dependent upon the source of the input stream for that participant. For example, where a participant accesses the virtual meeting software 306 from a client, such as the client 308 or 310 , the meeting view for the participant may include a video output stream transmitted from the meeting system for viewing by all participants based on a video input stream received from the client, although the participant may optionally disable video features to suspend the video output stream from being presented in the meeting view. In another example, where a participant access the virtual meeting software 306 from a phone, such as the phone 312 , the meeting view for the participant may be limited to a static image or other default background aspect since there is no video output stream produced for that participant. The thread encoding tool 302 receives video input streams separately from the clients 308 and 310 and encodes those video input streams using one or more transcoding tools, such as to produce variant streams at different resolutions. The video input streams may be received over a network, for example, the network 114 shown in FIG. 1 , or by a direct wired connection, such as using a universal serial bus (USB) connection or like coupling aspect. After the video input streams are encoded, the switching/routing tool 304 direct the encoded streams through applicable network infrastructure and/or other hardware to deliver the encoded streams to the virtual meeting software 306 . The virtual meeting software 306 delivers output video streams representative of the respective encoded streams to each connected client, such as the clients 308 and 310 , which receive and decode the output video streams to output them for display by video output components of the clients, such as within respective meeting views of the virtual meeting software 306 . A user of the phone 312 participates in the virtual meeting using an audio-only connection and may thus be referred to an audio-only caller. To participate in the virtual meeting from the phone 312 , an audio signal from the phone 312 is received and processed at a VOIP gateway 314 to prepare a digital telephony signal for processing at the meeting system 300 . The VOIP gateway 314 may be part of the system 100 , for example, implemented at or in connection with a server of the datacenter 106 . Alternatively, the VOIP gateway 314 may be located on the user-side, such as in a same location as the phone 312 . The digital telephony signal is a packet switched signal transmitted to the switching/routing tool 304 for delivery to the virtual meeting software 306 . The virtual meeting software 306 outputs an audio signal representing a combined audio capture for each participant of the virtual meeting for output by an audio output component of the phone 312 . In some implementations, the VOIP gateway 314 may be omitted, for example, where the phone 312 is a VOIP-enabled phone. A virtual meeting may be referred to as a video-enabled virtual meeting in which video streaming is enabled for one or more participants. The enabling of video streaming for a participant of a virtual meeting does not require that the participant activate or otherwise use video functionality for participating in the virtual meeting. For example, a virtual meeting may still be a video-enabled virtual meeting where none of the participants joining using clients turns on their video feed for any portion of the virtual meeting. In some cases, however, the virtual meeting may have video disabled, such as where each participant connects to the virtual meeting using a phone rather than a client, or where a host of the virtual meeting selectively configures the virtual meeting to exclude video functionality. In some implementations, other software services may be accessible in connection with a virtual meeting implemented using the meeting system 300 . For example, a virtual meeting may include or otherwise integrate functionality for instant messaging, unified messaging, and other types of messaging communications between participants of the virtual meeting, such as to facilitate a chat or like virtual conversation between users of those participants. Those other software services may be implemented at the meeting system 300 and/or a different aspect of the system 100 . FIG. 4 is a block diagram of an example of a system 400 for merging a call with a virtual meeting. The system includes a server environment 402 which includes telephony services 404 , merging software 406 , and virtual meeting software 408 . The server environment 402 may be implemented using one or more servers, for example, of a datacenter such as the datacenter 106 shown in FIG. 1 . In particular, the telephony services 404 , the switching software 406 , and the virtual meeting software 408 may be implemented by different servers or by the same server, which may include one or more of the servers 108 through 112 shown in FIG. 1 . The telephony services 404 represent hardware, software, infrastructure, and/or other aspects used to operate a telephony connection, such as a call between two or more clients or phones, for example, between the phone device 410 and the customer endpoint 412 . For example, the telephony services 404 may be implemented using the telephony server 112 shown in FIG. 1 . In another example, the telephony services may be implemented using the thread encoding tool 310 , the switching/routing tool 312 , and/or the VOIP gateway 314 shown in FIG. 3 , such as where those aspects are implemented other than as part of the telephony server 112 . The operator of a client or phone connected to the call may or may not be a user of a software platform associated with the system 400 (e.g., a unified communications as a service (UCaaS) platform). An operator of a client or phone may, for example, be a human user, a software intelligence unit, or another entity. The merging software 406 includes functionality for merging a call implemented using the telephony services 404 with a virtual meeting implemented using the virtual meeting software 408 , which may, for example, be the virtual meeting software 314 shown in FIG. 3 . For example, the operator of the phone device 410 may be attempting to reach an operator of a client 414 which is connected to the virtual meeting software 314 . A call may be established between the phone device 410 and the customer endpoint 412 , which may, for example, be a client or phone. For example, the operator of the client 414 may be participating a virtual meeting at the virtual meeting software 314 when the call is established. For example, the virtual meeting may be in-progress at the time the call is established between the phone device 410 and the customer endpoint 412 . The virtual meeting may be deemed in-progress where the virtual meeting has already been initialized such that at least one participant is already connected to the virtual meeting. Thus, a virtual meeting may be in-progress regardless of whether any virtual meeting functionality has been used by any such participant. An operator of the customer endpoint 412 may have access to meeting information associated with the operator of the client 414 . For example, the operator of the customer endpoint 412 and the operator of the client 414 may both be users of the software platform associated with the system 400 . The operator of the customer endpoint 412 may transmit a request for the phone device 410 to join the virtual meeting to the merging software 406 . Based on that request, the merging software 406 can open a channel between the phone device 410 and a web service associated with the virtual meeting software 408 . The phone device 410 may then join the virtual meeting at the virtual meeting software 408 over the opened channel to allow the operator of the phone device 410 (e.g., an audio-only caller) to participate in the virtual meeting with the operator of the client 414 . In one example, the operator of the customer endpoint 412 may be an assistant to the operator of the client 414 . The operator of the client 414 may be an employee of a customer of a software platform associated with the system 400 . The operator of the phone device 410 , an audio-only caller, may be another employee of that customer, a person with whom the operator of the client 414 has scheduled a meeting, or a person who is not scheduled to be in a meeting with the operator of the client 414 . The operator of the phone device 410 calls and speaks to the assistant operating the customer endpoint 412 . The operator of the phone device 410 tells the assistant that he or she would like to speak with the operator of the client 414 . The assistant, using his or her access to the meeting information associated with the operator of the client 414 , can determine that the client 414 is in a virtual meeting. The assistant can then cause the merging software 406 to join the operator of the phone device 410 with the virtual meeting. In another example, the operator of the customer endpoint 412 may be the same operator of the client 414 . For example, the operator of the client 414 may operate both the client 414 and the customer endpoint 412 in which the client 414 is connected to the virtual meeting and the customer endpoint 412 is not. The operator of the phone device 410 can call the customer endpoint 412 to speak with the operator thereof. The operator of the customer endpoint 412 may then decide to merge the call with the operator of the phone device 410 into the virtual meeting to which the client 414 is connected, or the operator of the phone device 410 may ask the operator of the customer endpoint 412 to merge the call. In either case, for example, the operator of the customer endpoint 412 may access virtual meeting information and interact therewith to cause the merging software 406 to merge the call, thereby enabling the operator of the phone device 410 to become a participate to the virtual meeting. In particular, the merging software 406 joins the operator of the phone device 410 with the virtual meeting without the operator of the phone device 410 having to exit the call or otherwise perform an action otherwise required to join a virtual meeting. For example, the virtual meeting software 408 may generally be configured to perform one or more security checks configured for a virtual meeting. A security check may include authenticating a prospective participant to the virtual meeting using one or more of a meeting identifier, a participant identifier, or other information usable to verify an identity of the prospective participant and/or to verify the subject virtual meeting. In another example, a prospective participant to a virtual meeting who is on a call may typically have to exit the call and either dial a new telephone number associated with the virtual meeting or open a client application to connect to the virtual meeting or otherwise navigate a web browser application to a web service for the virtual meeting. Thus, joining the operator of the phone device 410 with the virtual meeting includes bypassing one or more security checks configured for the virtual meeting. Joining the operator of the phone device 410 with the virtual meeting further includes automatically and seamlessly connecting the operator of the phone device 410 from the call with the customer endpoint 412 to the virtual meeting. As such, after the operator of the phone device 410 in the first above example asks the assistant to connect him or her with the operator of the client 414 or after the operator of the phone device 410 in the second above example asks the operator of the client 414 to join the virtual meeting or is asked by that operator to do so, the operator of the phone device 410 experiences a seamless connection from that call with the customer endpoint 412 to that virtual meeting. Where a client application associated with the virtual meeting software 408 is installed on the phone device 410 , the merging software 406 may cause the client application to launch at the phone device in which the operator of the phone device 410 joins the virtual meeting as a full participant rather than as an audio-only caller. For example, the merging software 406 can transmit a push notification for the client application to the phone device 410 that, when interacted with at the phone device, 410 causes the client application to launch. Alternatively, where a client application associated with the virtual meeting software 408 is not installed on the phone device 410 opening the channel between the phone device 410 and the virtual meeting software 408 can include connecting the telephony services 414 and the virtual meeting software 408 . For example, in such a case, the call can be maintained as an audio channel of the virtual meeting for the operator of the phone device 410 . The merging software 406 can determine whether the client application is installed on the phone device 410 by transmitting a request for response by the client application, such as a ping, to the phone device 410 , and awaiting that response. Alternatively, the merging software 406 can determine whether the client application is installed on the phone device by searching records of the virtual meeting software 408 or of the software platform associated with the system 400 to determine whether an account associated with the phone device 410 is registered therewith. For example, the merging software 406 may perform phone number matching against the phone number of the phone device 410 to recognize the phone device 410 and determine whether the client application is installed thereon. The request for the phone device 410 to join the virtual meeting is transmitted from the customer endpoint 412 based on an interaction with virtual meeting information at the customer endpoint 412 . The virtual meeting information is associated with the operator of the client 414 and may, for example, be, include, or otherwise refer to event data associated with a calendar of the operator of the client 414 or the customer, meeting data included in a list of virtual meetings for the operator of the client 414 or the customer, or the like. The interaction with the virtual meeting information at the customer endpoint 412 may refer to the operator of the customer endpoint 412 toggling a user interface element of software at the customer endpoint 412 to gain access to a uniform resource locator (URL) of the virtual meeting (e.g., for connecting to a web service used therefor), a meeting identifier of the virtual meeting, or other information associated with the virtual meeting. For example, the software at the customer endpoint may be run locally at the customer endpoint or may be an instance of software run on a server device or another computing aspect. In some implementations, the request for the phone device 410 may be based on an identification or selection of virtual meeting information using a software intelligence aspect at the customer endpoint 412 or otherwise in connection with software running at the customer endpoint 412 . For example, a learning model or other software intelligence aspect may be trained to recognize certain virtual meeting information based on one or more criteria, for example, a list of participants who have been invited to a given virtual meeting. For example, the learning model or other software intelligence, responsive to the call from the phone device 410 , can search through virtual meeting information based on information associated with the phone device 410 and/or the operator of the phone device 410 . Such information may, for example, be obtained from the telephony services 404 (e.g., signaled within the call data itself), stored in a phone data database accessible to the merging software 406 , or the like. In some implementations, the customer endpoint 412 may be a client or phone which is not an endpoint of a customer of a software platform. For example, another client or phone functionally operating directly or indirectly as an intermediary between the phone device 410 and the merging software 406 may replace the customer endpoint 412 . In some implementations, the merging software 406 may be used to facilitate a virtual meeting between three or more participants, including one or more audio-only callers. For example, after the customer endpoint transmits the request to join multiple audio-only callers, the server implementing the merging software 406 bridges the telephony connections for each of the audio-only callers individually with a server implementing the telephony services 404 . After the telephony connections are bridged, the join requests are transmitted to the phone devices of the audio-only callers. Bridging the telephony connections before transmitting the join requests enables the audio-only callers to connect to the virtual meeting without call disruption. FIG. 5 is a block diagram of example functionality of the merging software 406 shown in FIG. 4 . The merging software 406 includes tools for merging a call with a virtual meeting. As shown, the merging software 406 includes a request detection tool 500 , a request processing tool 502 , a channel processing tool 504 , and a meeting connection tool 506 . Although the tools 500 through 506 are shown as functionality of the merging software 400 as a single piece of software, in some implementations, some or all of the tools 500 through 506 may exist outside of the merging software 406 . The request detection tool 500 detects a request to join a phone device with a virtual meeting. The request may be from a participant of a call between the phone device and a customer endpoint. The request detection tool 500 may detect the request in one or more ways. For example, the detection may be based on a signal received from telephony services used to implement the call. In another example, the detection may be based on a signal received based on an interaction with software at the customer endpoint. The request processing tool 502 processes the detected request to initiate a process for merging the call with the virtual meeting. In particular, the request processing tool 502 processes the request to identify the operator of the phone device to be joined to the virtual meeting, a meeting identifier or other aspect usable to identify the virtual meeting, and other information necessary to join the phone device with the virtual meeting. The channel processing tool 504 opens a channel for connecting the phone device to the virtual meeting software service. The channel may be an audio-only channel or a video-enabled channel. The particular form of the channel may in at least some cases be based on whether a client application associated with the virtual meeting software service is installed on the phone device. If the client application is installed, the channel may be a channel for enabling audio and video streams to and from the virtual meeting software service. Otherwise, the channel may be an audio channel only. The meeting connection tool 506 uses the channel opened by the channel processing tool 504 to join the phone device with the virtual meeting. The meeting connection tool 506 uses the channel to create a dedicated meeting view for the phone device. Where the channel opened with the phone device enables video stream communication, the dedicated meeting view may optionally display a video stream from the phone device. Alternatively, where the channel is audio-only, a static image may be output within the dedicated meeting view of the virtual meeting. The contents of the dedicated meeting view are viewable by meeting participants. FIG. 6 is an illustration of swim lanes showing an example sequence of operations performed for merging a call with a virtual meeting between a phone device, a customer endpoint, merging software, and virtual meeting software, for example, respectively the phone device 410 , the customer endpoint 412 , the merging software 406 , and the virtual meeting software 408 shown in FIG. 4 . At 600 , a virtual meeting is initiated at the virtual meeting software 408 . At least one participant is connected to the virtual meeting. At 602 , at a time after the virtual meeting is initiated, a call is placed from the phone device 410 to the customer endpoint 412 . The call may be intended for the operator of the customer endpoint 412 or for an operator of another client or phone, for example, a participant of the virtual meeting. At 604 , the customer endpoint 412 receives the call from the phone device 410 , for example, connected over telephony services. At 606 , a request for the phone device 410 to join the virtual meeting is transmitted from the customer endpoint 412 to the merging software 406 . At 608 , the request is received by the merging software 406 . At 610 , the merging software 406 opens a channel between the phone device 410 and the virtual meeting software 408 based on the request. At 612 , the call with the phone device 410 is merged into the virtual meeting. At 614 , the phone device 410 joins the virtual meeting at the virtual meeting software 408 . In some implementations, the virtual meeting may be initiated at the virtual meeting software 408 in an order other than shown in FIG. 6 . For example, the virtual meeting may be initiated after the request is received at 608 . To further describe some implementations in greater detail, reference is next made to examples of techniques which may be performed to merge a call with a virtual meeting. FIG. 7 is a flowchart of a first example of a technique 700 for merging a call with a virtual meeting. FIG. 8 is a flowchart of a second example of a technique 800 for merging a call with a virtual meeting. The technique 700 and/or the technique 800 can be executed using computing devices, such as the systems, hardware, and software described with respect to FIGS. 1-6 . The technique 700 and/or the technique 800 can be performed, for example, by executing a machine-readable program or other computer-executable instructions, such as routines, instructions, programs, or other code. The steps, or operations, of the technique 700 and/or the technique 800 , or another technique, method, process, or algorithm described in connection with the implementations disclosed herein, can be implemented directly in hardware, firmware, software executed by hardware, circuitry, or a combination thereof. For simplicity of explanation, the technique 700 and the technique 800 are each depicted and described herein as a series of steps or operations. However, the steps or operations in accordance with this disclosure can occur in various orders and/or concurrently. Additionally, other steps or operations not presented and described herein may be used. Furthermore, not all illustrated steps or operations may be required to implement a technique in accordance with the disclosed subject matter. Referring first to FIG. 7 , the technique 700 for merging a call with a virtual meeting is shown. At 702 , a virtual meeting is initialized. At least one participant is connected to the virtual meeting, although there may be multiple participants connected to the virtual meeting when it is initialized or shortly thereafter. The virtual meeting is video-enabled such that a video streams of a participant connecting to the virtual meeting using a client can be presented in a dedicated meeting view within a GUI of the virtual meeting software implementing the virtual meeting. Participants connecting to the virtual meeting using a telephony connection (e.g., by calling into a service associated with the virtual meeting software) may have a dedicated meeting view with a static image. At 704 , a call is established between a phone device of an audio-only caller and a customer endpoint. The call is established over telephony services which route the call initiated at the phone device to the customer endpoint as a destination. Establishing the call includes creating or otherwise facilitating a telephony connection between the phone device and the customer endpoint, for example, over a VOIP telephone service or a standard PSTN. In some implementations, the call may have been intended by the audio-only caller to be placed between the phone device of the audio-only caller and a client or phone of an operator associated with the virtual meeting. For example, the telephone number dialed by the audio-only caller may direct the audio-only caller to an assistant or other operator for the intended recipient, rather than the intended recipient himself or herself. At 706 , a request for the phone device to join the virtual meeting, which may be in- progress, is received from the customer endpoint. The request from the customer endpoint to join the phone device with the virtual meeting may be based on an interaction with the virtual meeting information at the customer endpoint. For example, where the virtual meeting is associated with a customer operator and the customer endpoint is a device of an assistant to the customer operator, the customer endpoint may run software having access to virtual meeting information of the customer operator. In this way, the assistant may access the virtual meeting information at the customer endpoint to transmit the request. Alternatively, the request from the customer endpoint to join the phone device with the virtual meeting may be based on output of a software intelligence aspect. At 708 , a channel is opened between the phone device and a web service associated with the virtual meeting. The channel connects the phone device to the virtual meeting service. As will be described below, the channel may be an audio-only channel or a video-enabled channel. In either case, the opening of the channel signals for the virtual meeting service to create a dedicated meeting view for the phone device. At 710 , the phone device is joined to the virtual meeting over the channel. The joining of the phone device to the virtual meeting allows the audio-only caller to become a participant to the virtual meeting. Joining the phone device to the virtual meeting over the channel includes bypassing one or more security checks configured for the virtual meeting. In one example, the one or more security checks that are bypassed include operations for authenticating participants of the in-progress virtual meeting. Thus, the audio-only caller may in at least some cases be allowed to join the virtual meeting without entering authentication credentials, for example, a meeting identifier, a participant identifier, and/or a password. The audio channel used by the phone device for the virtual meeting may be provided over the channel opened between the phone device and the web service or may be provided by using the call between the phone device and the customer endpoint. For example, whether the channel opened between the phone device and the web service is used as the audio channel for the audio-only caller in the virtual meeting may depend upon whether a client application associated with the web service is installed on the phone device of the audio-only caller. At 712 , the customer endpoint media is bridged to the virtual meeting with a telephony server. For example, where the client application is installed on the phone device, the channel opened between the phone device and the web service is opened through the client application. In such a case, the phone device is joined to the virtual meeting using video and/or audio through that channel. However, where the client application is not installed on the phone device, the channel opened between the phone device and the web service may be used to receive audio of the call between the phone device and the customer endpoint in which the audio channel of the audio-only caller for the virtual meeting is the call itself. In some implementations, the customer endpoint may be disconnected from the call when the call is used as the audio channel of the audio-only caller for the virtual meeting. In some implementations, connecting the phone device to the virtual meeting may expose the audio-only caller to one or more of a video channel, a screen share, a chat message, or a file transfer. For example, where the client application is installed on the phone device and is used to connect to the virtual meeting service, the channel between the phone device and the virtual meeting service may, via the client application, cause output at the phone device associated with various functionality of the virtual meeting service. Referring next to FIG. 8 , the technique 800 for merging a call with a virtual meeting is shown. At 802 , a call is established between a phone device of an audio-only caller and a customer endpoint. The call is established over telephony services which route the call initiated at the phone device to the customer endpoint as a destination. Establishing the call includes creating or otherwise facilitating a telephony connection between the phone device and the customer endpoint, for example, over a VOIP telephone service or a standard PSTN. In some implementations, the call may have been intended by the audio-only caller to be placed between the phone device of the audio-only caller and a client or phone of an operator other than the operator of the customer endpoint. For example, the telephone number dialed by the audio-only caller may direct the audio-only caller to an assistant or other operator for the intended recipient, rather than the intended recipient himself or herself. At 804 , a request for the phone device to join a virtual meeting is received from the customer endpoint. The request from the customer endpoint to join the phone device with the virtual meeting may be based on an interaction with virtual meeting information at the customer endpoint. For example, where the virtual meeting is associated with a customer operator and the customer endpoint is a device of an assistant to the customer operator, the customer endpoint may run software having access to virtual meeting information of the customer operator. In this way, the assistant may access the virtual meeting information at the customer endpoint to transmit the request. Alternatively, the request from the customer endpoint to join the phone device with the virtual meeting may be based on output of a software intelligence aspect. At 806 , a virtual meeting is initialized. At least one participant may be able to connect to the virtual meeting when it is initialized or shortly thereafter. The virtual meeting is video-enabled such that a video streams of a participant connecting to the virtual meeting using a client can be presented in a dedicated meeting view within a GUI of the virtual meeting software implementing the virtual meeting. Participants connecting to the virtual meeting using a telephony connection (e.g., by calling into a service associated with the virtual meeting software) may have a dedicated meeting view with a static image. At 808 , a channel is opened between the phone device and a web service associated with the virtual meeting. The channel connects the phone device to the virtual meeting service. As will be described below, the channel may be an audio-only channel or a video-enabled channel. In either case, the opening of the channel signals for the virtual meeting service to create a dedicated meeting view for the phone device. At 810 , the phone device is joined to the virtual meeting over the channel. The joining of the phone device to the virtual meeting allows the audio-only caller to become a participant to the virtual meeting. Joining the phone device to the virtual meeting over the channel includes bypassing one or more security checks configured for the virtual meeting. In one example, the one or more security checks that are bypassed include operations for authenticating participants of the in-progress virtual meeting. Thus, the audio-only caller may in at least some cases be allowed to join the virtual meeting without entering authentication credentials, for example, a meeting identifier, a participant identifier, and/or a password. The audio channel used by the phone device for the virtual meeting may be provided over the channel opened between the phone device and the web service or may be provided by using the call between the phone device and the customer endpoint. For example, whether the channel opened between the phone device and the web service is used as the audio channel for the audio-only caller in the virtual meeting may depend upon whether a client application associated with the web service is installed on the phone device of the audio-only caller. At 812 , the customer endpoint media is bridged to the virtual meeting with a telephony server. For example, where the client application is installed on the phone device, the channel opened between the phone device and the web service is opened through the client application. In such a case, the phone device is joined to the virtual meeting using video and/or audio through that channel. However, where the client application is not installed on the phone device, the channel opened between the phone device and the web service may be used to receive audio of the call between the phone device and the customer endpoint in which the audio channel of the audio-only caller for the virtual meeting is the call itself. In some implementations, the customer endpoint may be disconnected from the call when the call is used as the audio channel of the audio-only caller for the virtual meeting. In some implementations, connecting the phone device to the virtual meeting may expose the audio-only caller to one or more of a video channel, a screen share, a chat message, or a file transfer. For example, where the client application is installed on the phone device and is used to connect to the virtual meeting service, the channel between the phone device and the virtual meeting service may, via the client application, cause output at the phone device associated with various functionality of the virtual meeting service. The implementations of this disclosure can be described in terms of functional block components and various processing operations. Such functional block components can be realized by a number of hardware or software components that perform the specified functions. For example, the disclosed implementations can employ various integrated circuit components (e.g., memory elements, processing elements, logic elements, look-up tables, and the like), which can carry out a variety of functions under the control of one or more microprocessors or other control devices. Similarly, where the elements of the disclosed implementations are implemented using software programming or software elements, the systems and techniques can be implemented with a programming or scripting language, such as C, C++, Java, JavaScript, assembler, or the like, with the various algorithms being implemented with a combination of data structures, objects, processes, routines, or other programming elements. Functional aspects can be implemented in algorithms that execute on one or more processors. Furthermore, the implementations of the systems and techniques disclosed herein could employ a number of conventional techniques for electronics configuration, signal processing or control, data processing, and the like. The words “mechanism” and “component” are used broadly and are not limited to mechanical or physical implementations, but can include software routines in conjunction with processors, etc. Likewise, the terms “system” or “tool” as used herein and in the figures, but in any event based on their context, may be understood as corresponding to a functional unit implemented using software, hardware (e.g., an integrated circuit, such as an ASIC), or a combination of software and hardware. In certain contexts, such systems or mechanisms may be understood to be a processor-implemented software system or processor-implemented software mechanism that is part of or callable by an executable program, which may itself be wholly or partly composed of such linked systems or mechanisms. Implementations or portions of implementations of the above disclosure can take the form of a computer program product accessible from, for example, a computer-usable or computer-readable medium. A computer-usable or computer-readable medium can be a device that can, for example, tangibly contain, store, communicate, or transport a program or data structure for use by or in connection with a processor. The medium can be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable mediums are also available. Such computer-usable or computer-readable media can be referred to as non-transitory memory or media, and can include volatile memory or non-volatile memory that can change over time. A memory of an apparatus described herein, unless otherwise specified, does not have to be physically contained by the apparatus, but is one that can be accessed remotely by the apparatus, and does not have to be contiguous with other memory that might be physically contained by the apparatus. While the disclosure has been described in connection with certain implementations, it is to be understood that the disclosure is not to be limited to the disclosed implementations but, on the contrary, is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims, which scope is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures as is permitted under the law.",en,PATENT_APPLICATION
